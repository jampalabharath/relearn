<!DOCTYPE html>
<html lang="en-us" dir="ltr" itemscope itemtype="http://schema.org/Article" data-r-output-format="print">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="height=device-height, width=device-width, initial-scale=1.0, minimum-scale=1.0">
    <meta name="generator" content="Hugo 0.148.2">
    <meta name="generator" content="Relearn 8.0.0+9803d5122ebb3276acea823f476e9eb44f607862">
    <meta name="description" content="Spark DF Basics DF Operations Functions Date Functions Handling Nulls Aggregate functions Joins When|Cast|Union Window Functions Explode Pivot Comparisons 1. Introduction Big Data Overview Introduction to Apache Spark Introduction to Azure Databricks Databricks Architecture Databricks Workspace Overview 2. Spark Core Concepts RDDs (Resilient Distributed Datasets) Transformations and Actions Lazy Evaluation Spark Execution Model Caching and Persistence Partitioning 3. Spark SQL DataFrames and Datasets Spark SQL Engine Schema Inference Catalog and Metadata Temporary and Global Views SQL Queries in Spark Performance Optimization (Catalyst Optimizer, Tungsten) 4. Spark Data Sources Reading/Writing CSV, JSON, Parquet, ORC Working with Delta Lake JDBC Data Sources Streaming Data Sources Handling Nested Data (Struct, Array, Map) 5. Delta Lake Introduction to Delta Lake ACID Transactions Schema Enforcement &amp; Evolution Time Travel Upserts &amp; Deletes (MERGE) Delta Lake Optimization (Z-order, Optimize, Vacuum) 6. Spark Streaming &amp; Structured Streaming Introduction to Streaming DStreams vs Structured Streaming Streaming Sources (Kafka, Event Hubs, Socket) Watermarking &amp; Late Data State Management Streaming Sinks (Delta, Console, Kafka, Event Hubs) 7. Spark MLlib (Machine Learning) MLlib Basics Feature Engineering Pipelines &amp; Transformers Classification Regression Clustering Model Persistence 8. Spark GraphX Introduction to GraphX Graph Processing Basics Graph Algorithms Use Cases 9. Performance Tuning Spark Configurations &amp; Parameters Memory Management Shuffle Optimizations Caching Strategies Adaptive Query Execution (AQE) Skew Handling 10. Azure Databricks Essentials Databricks Clusters (Types, Autoscaling, Pools) Databricks Notebooks Databricks Repos &amp; Git Integration Databricks Jobs &amp; Scheduling Databricks Secrets Databricks Utilities (DBUtils) Workspace Security &amp; Permissions 11. Integration with Azure Services Azure Data Lake Storage (ADLS) Integration Azure Blob Storage Integration Azure Synapse Analytics Azure Event Hubs Azure Data Factory (ADF) with Databricks Power BI with Databricks 12. Advanced Databricks Unity Catalog Databricks SQL Delta Live Tables (DLT) Databricks REST API Databricks CLI Monitoring &amp; Logging Cost Optimization in Databricks 13. Real-World Use Cases ETL Pipeline with Databricks Streaming Data Pipeline Batch Data Processing Machine Learning Pipeline End-to-End Data Lakehouse Implementation 14. Best Practices Cluster Sizing &amp; Configuration Code Versioning &amp; Collaboration Data Governance Security &amp; Compliance CI/CD with Databricks Productionizing Workloads 15. Capstone Project Build a Scalable Data Lakehouse with Databricks Real-Time Streaming Data Pipeline ML Model Training and Deployment">
    <meta name="author" content="">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Azure Data Bricks :: Data Engineering Notes">
    <meta name="twitter:description" content="Spark DF Basics DF Operations Functions Date Functions Handling Nulls Aggregate functions Joins When|Cast|Union Window Functions Explode Pivot Comparisons 1. Introduction Big Data Overview Introduction to Apache Spark Introduction to Azure Databricks Databricks Architecture Databricks Workspace Overview 2. Spark Core Concepts RDDs (Resilient Distributed Datasets) Transformations and Actions Lazy Evaluation Spark Execution Model Caching and Persistence Partitioning 3. Spark SQL DataFrames and Datasets Spark SQL Engine Schema Inference Catalog and Metadata Temporary and Global Views SQL Queries in Spark Performance Optimization (Catalyst Optimizer, Tungsten) 4. Spark Data Sources Reading/Writing CSV, JSON, Parquet, ORC Working with Delta Lake JDBC Data Sources Streaming Data Sources Handling Nested Data (Struct, Array, Map) 5. Delta Lake Introduction to Delta Lake ACID Transactions Schema Enforcement &amp; Evolution Time Travel Upserts &amp; Deletes (MERGE) Delta Lake Optimization (Z-order, Optimize, Vacuum) 6. Spark Streaming &amp; Structured Streaming Introduction to Streaming DStreams vs Structured Streaming Streaming Sources (Kafka, Event Hubs, Socket) Watermarking &amp; Late Data State Management Streaming Sinks (Delta, Console, Kafka, Event Hubs) 7. Spark MLlib (Machine Learning) MLlib Basics Feature Engineering Pipelines &amp; Transformers Classification Regression Clustering Model Persistence 8. Spark GraphX Introduction to GraphX Graph Processing Basics Graph Algorithms Use Cases 9. Performance Tuning Spark Configurations &amp; Parameters Memory Management Shuffle Optimizations Caching Strategies Adaptive Query Execution (AQE) Skew Handling 10. Azure Databricks Essentials Databricks Clusters (Types, Autoscaling, Pools) Databricks Notebooks Databricks Repos &amp; Git Integration Databricks Jobs &amp; Scheduling Databricks Secrets Databricks Utilities (DBUtils) Workspace Security &amp; Permissions 11. Integration with Azure Services Azure Data Lake Storage (ADLS) Integration Azure Blob Storage Integration Azure Synapse Analytics Azure Event Hubs Azure Data Factory (ADF) with Databricks Power BI with Databricks 12. Advanced Databricks Unity Catalog Databricks SQL Delta Live Tables (DLT) Databricks REST API Databricks CLI Monitoring &amp; Logging Cost Optimization in Databricks 13. Real-World Use Cases ETL Pipeline with Databricks Streaming Data Pipeline Batch Data Processing Machine Learning Pipeline End-to-End Data Lakehouse Implementation 14. Best Practices Cluster Sizing &amp; Configuration Code Versioning &amp; Collaboration Data Governance Security &amp; Compliance CI/CD with Databricks Productionizing Workloads 15. Capstone Project Build a Scalable Data Lakehouse with Databricks Real-Time Streaming Data Pipeline ML Model Training and Deployment">
    <meta property="og:url" content="https://example.org/azure_data_bricks/">
    <meta property="og:site_name" content="Data Engineering Notes">
    <meta property="og:title" content="Azure Data Bricks :: Data Engineering Notes">
    <meta property="og:description" content="Spark DF Basics DF Operations Functions Date Functions Handling Nulls Aggregate functions Joins When|Cast|Union Window Functions Explode Pivot Comparisons 1. Introduction Big Data Overview Introduction to Apache Spark Introduction to Azure Databricks Databricks Architecture Databricks Workspace Overview 2. Spark Core Concepts RDDs (Resilient Distributed Datasets) Transformations and Actions Lazy Evaluation Spark Execution Model Caching and Persistence Partitioning 3. Spark SQL DataFrames and Datasets Spark SQL Engine Schema Inference Catalog and Metadata Temporary and Global Views SQL Queries in Spark Performance Optimization (Catalyst Optimizer, Tungsten) 4. Spark Data Sources Reading/Writing CSV, JSON, Parquet, ORC Working with Delta Lake JDBC Data Sources Streaming Data Sources Handling Nested Data (Struct, Array, Map) 5. Delta Lake Introduction to Delta Lake ACID Transactions Schema Enforcement &amp; Evolution Time Travel Upserts &amp; Deletes (MERGE) Delta Lake Optimization (Z-order, Optimize, Vacuum) 6. Spark Streaming &amp; Structured Streaming Introduction to Streaming DStreams vs Structured Streaming Streaming Sources (Kafka, Event Hubs, Socket) Watermarking &amp; Late Data State Management Streaming Sinks (Delta, Console, Kafka, Event Hubs) 7. Spark MLlib (Machine Learning) MLlib Basics Feature Engineering Pipelines &amp; Transformers Classification Regression Clustering Model Persistence 8. Spark GraphX Introduction to GraphX Graph Processing Basics Graph Algorithms Use Cases 9. Performance Tuning Spark Configurations &amp; Parameters Memory Management Shuffle Optimizations Caching Strategies Adaptive Query Execution (AQE) Skew Handling 10. Azure Databricks Essentials Databricks Clusters (Types, Autoscaling, Pools) Databricks Notebooks Databricks Repos &amp; Git Integration Databricks Jobs &amp; Scheduling Databricks Secrets Databricks Utilities (DBUtils) Workspace Security &amp; Permissions 11. Integration with Azure Services Azure Data Lake Storage (ADLS) Integration Azure Blob Storage Integration Azure Synapse Analytics Azure Event Hubs Azure Data Factory (ADF) with Databricks Power BI with Databricks 12. Advanced Databricks Unity Catalog Databricks SQL Delta Live Tables (DLT) Databricks REST API Databricks CLI Monitoring &amp; Logging Cost Optimization in Databricks 13. Real-World Use Cases ETL Pipeline with Databricks Streaming Data Pipeline Batch Data Processing Machine Learning Pipeline End-to-End Data Lakehouse Implementation 14. Best Practices Cluster Sizing &amp; Configuration Code Versioning &amp; Collaboration Data Governance Security &amp; Compliance CI/CD with Databricks Productionizing Workloads 15. Capstone Project Build a Scalable Data Lakehouse with Databricks Real-Time Streaming Data Pipeline ML Model Training and Deployment">
    <meta property="og:locale" content="en_us">
    <meta property="og:type" content="website">
    <meta itemprop="name" content="Azure Data Bricks :: Data Engineering Notes">
    <meta itemprop="description" content="Spark DF Basics DF Operations Functions Date Functions Handling Nulls Aggregate functions Joins When|Cast|Union Window Functions Explode Pivot Comparisons 1. Introduction Big Data Overview Introduction to Apache Spark Introduction to Azure Databricks Databricks Architecture Databricks Workspace Overview 2. Spark Core Concepts RDDs (Resilient Distributed Datasets) Transformations and Actions Lazy Evaluation Spark Execution Model Caching and Persistence Partitioning 3. Spark SQL DataFrames and Datasets Spark SQL Engine Schema Inference Catalog and Metadata Temporary and Global Views SQL Queries in Spark Performance Optimization (Catalyst Optimizer, Tungsten) 4. Spark Data Sources Reading/Writing CSV, JSON, Parquet, ORC Working with Delta Lake JDBC Data Sources Streaming Data Sources Handling Nested Data (Struct, Array, Map) 5. Delta Lake Introduction to Delta Lake ACID Transactions Schema Enforcement &amp; Evolution Time Travel Upserts &amp; Deletes (MERGE) Delta Lake Optimization (Z-order, Optimize, Vacuum) 6. Spark Streaming &amp; Structured Streaming Introduction to Streaming DStreams vs Structured Streaming Streaming Sources (Kafka, Event Hubs, Socket) Watermarking &amp; Late Data State Management Streaming Sinks (Delta, Console, Kafka, Event Hubs) 7. Spark MLlib (Machine Learning) MLlib Basics Feature Engineering Pipelines &amp; Transformers Classification Regression Clustering Model Persistence 8. Spark GraphX Introduction to GraphX Graph Processing Basics Graph Algorithms Use Cases 9. Performance Tuning Spark Configurations &amp; Parameters Memory Management Shuffle Optimizations Caching Strategies Adaptive Query Execution (AQE) Skew Handling 10. Azure Databricks Essentials Databricks Clusters (Types, Autoscaling, Pools) Databricks Notebooks Databricks Repos &amp; Git Integration Databricks Jobs &amp; Scheduling Databricks Secrets Databricks Utilities (DBUtils) Workspace Security &amp; Permissions 11. Integration with Azure Services Azure Data Lake Storage (ADLS) Integration Azure Blob Storage Integration Azure Synapse Analytics Azure Event Hubs Azure Data Factory (ADF) with Databricks Power BI with Databricks 12. Advanced Databricks Unity Catalog Databricks SQL Delta Live Tables (DLT) Databricks REST API Databricks CLI Monitoring &amp; Logging Cost Optimization in Databricks 13. Real-World Use Cases ETL Pipeline with Databricks Streaming Data Pipeline Batch Data Processing Machine Learning Pipeline End-to-End Data Lakehouse Implementation 14. Best Practices Cluster Sizing &amp; Configuration Code Versioning &amp; Collaboration Data Governance Security &amp; Compliance CI/CD with Databricks Productionizing Workloads 15. Capstone Project Build a Scalable Data Lakehouse with Databricks Real-Time Streaming Data Pipeline ML Model Training and Deployment">
    <meta itemprop="wordCount" content="359">
    <title>Azure Data Bricks :: Data Engineering Notes</title>
    <link href="https://example.org/azure_data_bricks/" rel="canonical" type="text/html" title="Azure Data Bricks :: Data Engineering Notes">
    <link href="/azure_data_bricks/index.xml" rel="alternate" type="application/rss+xml" title="Azure Data Bricks :: Data Engineering Notes">
    <link href="/azure_data_bricks/index.md" rel="alternate" type="text/markdown" title="Azure Data Bricks :: Data Engineering Notes">
    <link href="/images/favicon.ico?1759918286" rel="icon" type="image/x-icon" sizes="any">
    <link href="/css/auto-complete/auto-complete.min.css?1759918286" rel="stylesheet">
    <script src="/js/auto-complete/auto-complete.min.js?1759918286" defer></script>
    <script src="/js/search-lunr.min.js?1759918286" defer></script>
    <script src="/js/search.min.js?1759918286" defer></script>
    <script>
      window.relearn = window.relearn || {};
      window.relearn.index_js_url="/searchindex.en.js?1759918286";
    </script>
    <script src="/js/lunr/lunr.min.js?1759918286" defer></script>
    <script src="/js/lunr/lunr.stemmer.support.min.js?1759918286" defer></script>
    <script src="/js/lunr/lunr.multi.min.js?1759918286" defer></script>
    <script src="/js/lunr/lunr.en.min.js?1759918286" defer></script>
    <script>
      window.relearn = window.relearn || {};
      window.relearn.contentLangs=['en'];
    </script>
    <link href="/fonts/fontawesome/css/fontawesome-all.min.css?1759918286" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="/fonts/fontawesome/css/fontawesome-all.min.css?1759918286" rel="stylesheet"></noscript>
    <link href="/css/perfect-scrollbar/perfect-scrollbar.min.css?1759918286" rel="stylesheet">
    <link href="/css/theme.min.css?1759918286" rel="stylesheet">
    <link href="/css/format-print.min.css?1759918286" rel="stylesheet" id="R-format-style">
    <script>
      window.relearn = window.relearn || {};
      // configuration
      window.relearn.min = `.min`;
      window.relearn.path='\/azure_data_bricks\/';
      window.relearn.relBasePath='..';
      window.relearn.relBaseUri='..';
      window.relearn.absBaseUri='https:\/\/example.org';
      window.relearn.disableAnchorCopy=false;
      window.relearn.disableAnchorScrolling=false;
      window.relearn.disableInlineCopyToClipboard=false;
      window.relearn.enableBlockCodeWrap=true;
      // legal
      window.relearn.getItem = (s,n) => {return s.getItem(n)};
      window.relearn.setItem = (s,n,v) => {return s.setItem(n,v)};
      window.relearn.removeItem = (s,n) => {return s.removeItem(n)};
      // translations
      window.T_Copy_to_clipboard = `Copy to clipboard`;
      window.T_Copied_to_clipboard = `Copied to clipboard!`;
      window.T_Copy_link_to_clipboard = `Copy link to clipboard`;
      window.T_Link_copied_to_clipboard = `Copied link to clipboard!`;
      window.T_Reset_view = `Reset view`;
      window.T_View_reset = `View reset!`;
      window.T_No_results_found = `No results found for "{0}"`;
      window.T_N_results_found = `{1} results found for "{0}"`;
      // variant stuff
      window.relearn.themevariants = [ 'auto', 'zen-light', 'zen-dark' ];
      window.relearn.customvariantname = "my-custom-variant";
      window.relearn.changeVariant = function(variant) {
        var oldVariant = document.documentElement.dataset.rThemeVariant;
        window.relearn.setItem(window.localStorage, window.relearn.absBaseUri + "/variant", variant);
        document.documentElement.dataset.rThemeVariant = variant;
        if (oldVariant != variant) {
          document.dispatchEvent( new CustomEvent('themeVariantLoaded', { detail: { variant, oldVariant } }) );
          window.relearn.markVariant();
        }
      }
      window.relearn.markVariant = function() {
        var variant = window.relearn.getItem(window.localStorage, window.relearn.absBaseUri + "/variant");
        document.querySelectorAll(".R-variantswitcher select").forEach((select) => {select.value = variant;});
      }
      window.relearn.initVariant = function() {
        var variant = window.relearn.getItem(window.localStorage, window.relearn.absBaseUri + "/variant") ?? "";
        if( variant == window.relearn.customvariantname ){
        }else if( !variant || !window.relearn.themevariants.includes(variant) ){
          variant = window.relearn.themevariants[0];
          window.relearn.setItem(window.localStorage, window.relearn.absBaseUri + "/variant", variant);
        }
        document.documentElement.dataset.rThemeVariant = variant;
      }
      window.relearn.initVariant();
      window.relearn.markVariant();
    </script><style>
:root {
    --MENU-WIDTH-S: 14.375rem;
    --MENU-WIDTH-M: 14.375rem;
    --MENU-WIDTH-L: 18.75rem;
    --MAIN-WIDTH-MAX: 1000rem;
}
</style>
  </head>
  <body class="mobile-support print" data-url="/azure_data_bricks/">
    <div id="R-body" class="default-animation">
      <div id="R-body-overlay"></div>
      <nav id="R-topbar">
        <div class="topbar-wrapper">
          <div class="topbar-sidebar-divider"></div>
          <div class="topbar-area topbar-area-start" data-area="start">
            <div class="topbar-button topbar-button-sidebar" data-content-empty="disable" data-width-s="show" data-width-m="hide" data-width-l="hide"><button class="topbar-control" onclick="toggleNav()" type="button" title="Menu (CTRL&#43;ALT&#43;n)"><i class="fa-fw fas fa-bars"></i></button>
            </div>
            <div class="topbar-button topbar-button-toc" data-content-empty="hide" data-width-s="show" data-width-m="show" data-width-l="show"><button class="topbar-control" onclick="toggleTopbarFlyout(this)" type="button" title="Table of Contents (CTRL&#43;ALT&#43;t)"><i class="fa-fw fas fa-list-alt"></i></button>
              <div class="topbar-content">
                <div class="topbar-content-wrapper">
<nav class="TableOfContents">
  <ul>
    <li><a href="#1-introduction">1. Introduction</a></li>
    <li><a href="#2-spark-core-concepts">2. Spark Core Concepts</a></li>
    <li><a href="#3-spark-sql">3. Spark SQL</a></li>
    <li><a href="#4-spark-data-sources">4. Spark Data Sources</a></li>
    <li><a href="#5-delta-lake">5. Delta Lake</a></li>
    <li><a href="#6-spark-streaming--structured-streaming">6. Spark Streaming &amp; Structured Streaming</a></li>
    <li><a href="#7-spark-mllib-machine-learning">7. Spark MLlib (Machine Learning)</a></li>
    <li><a href="#8-spark-graphx">8. Spark GraphX</a></li>
    <li><a href="#9-performance-tuning">9. Performance Tuning</a></li>
    <li><a href="#10-azure-databricks-essentials">10. Azure Databricks Essentials</a></li>
    <li><a href="#11-integration-with-azure-services">11. Integration with Azure Services</a></li>
    <li><a href="#12-advanced-databricks">12. Advanced Databricks</a></li>
    <li><a href="#13-real-world-use-cases">13. Real-World Use Cases</a></li>
    <li><a href="#14-best-practices">14. Best Practices</a></li>
    <li><a href="#15-capstone-project">15. Capstone Project</a></li>
  </ul>
</nav>
                </div>
              </div>
            </div>
          </div>
          <ol class="topbar-breadcrumbs breadcrumbs highlightable" itemscope itemtype="http://schema.org/BreadcrumbList"><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class="a11y-only"><a itemprop="item" href="/"><span itemprop="name">Data Engineering Notes</span></a><meta itemprop="position" content="1">&nbsp;/&nbsp;</li><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><span itemprop="name">ADB</span><meta itemprop="position" content="2"></li>
          </ol>
          <div class="topbar-area topbar-area-end" data-area="end">
            <div class="topbar-button topbar-button-edit" data-content-empty="disable" data-width-s="area-more" data-width-m="show" data-width-l="show"><a class="topbar-control" href="https://github.com/jampalabharath/relearn/tree/main/content/Azure_Data_Bricks/_index.md" rel="external" target="_blank" title="Edit (CTRL+ALT+w)"><i class="fa-fw fas fa-pen"></i></a>
            </div>
            <div class="topbar-button topbar-button-markdown" data-content-empty="disable" data-width-s="area-more" data-width-m="show" data-width-l="show"><a class="topbar-control" href="/azure_data_bricks/index.md" title="Show Markdown"><i class="fa-fw fab fa-markdown"></i></a>
            </div>
            <div class="topbar-button topbar-button-print" data-content-empty="disable" data-width-s="area-more" data-width-m="show" data-width-l="show"><a class="topbar-control" href="/azure_data_bricks/index.print.html" title="Print whole chapter (CTRL+ALT+p)"><i class="fa-fw fas fa-print"></i></a>
            </div>
            <div class="topbar-button topbar-button-prev" data-content-empty="disable" data-width-s="show" data-width-m="show" data-width-l="show"><a class="topbar-control" href="/azure_data_factory/" title="Azure Data Factory (ðŸ¡)"><i class="fa-fw fas fa-chevron-left"></i></a>
            </div>
            <div class="topbar-button topbar-button-next" data-content-empty="disable" data-width-s="show" data-width-m="show" data-width-l="show"><a class="topbar-control" href="/azure_data_bricks/spark/" title="Spark (ðŸ¡’)"><i class="fa-fw fas fa-chevron-right"></i></a>
            </div>
            <div class="topbar-button topbar-button-more" data-content-empty="hide" data-width-s="show" data-width-m="show" data-width-l="show"><button class="topbar-control" onclick="toggleTopbarFlyout(this)" type="button" title="More"><i class="fa-fw fas fa-ellipsis-v"></i></button>
              <div class="topbar-content">
                <div class="topbar-content-wrapper">
                  <div class="topbar-area topbar-area-more" data-area="more">
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </nav>
      <div id="R-main-overlay"></div>
      <main id="R-body-inner" class="highlightable chapter" tabindex="-1">
        <div class="flex-block-wrapper">
<article class="chapter narrow">
  <header class="headline">
  </header>
<div class="article-subheading">Chapter 3</div>

<h1 id="azure-data-bricks">Azure Data Bricks</h1>

<ul class="children children-li children-sort-">
  <li class="children-title"><a href="/azure_data_bricks/spark/">Spark</a><ul></ul></li>
  <li class="children-title"><a href="/azure_data_bricks/df-creation/">DF Basics</a><ul></ul></li>
  <li class="children-title"><a href="/azure_data_bricks/df-operations/">DF Operations</a><ul></ul></li>
  <li class="children-title"><a href="/azure_data_bricks/functions/">Functions</a><ul></ul></li>
  <li class="children-title"><a href="/azure_data_bricks/date/">Date Functions</a><ul></ul></li>
  <li class="children-title"><a href="/azure_data_bricks/nulls/">Handling Nulls</a><ul></ul></li>
  <li class="children-title"><a href="/azure_data_bricks/aggregations/">Aggregate functions</a><ul></ul></li>
  <li class="children-title"><a href="/azure_data_bricks/joins/">Joins</a><ul></ul></li>
  <li class="children-title"><a href="/azure_data_bricks/when/">When|Cast|Union</a><ul></ul></li>
  <li class="children-title"><a href="/azure_data_bricks/window-functions/">Window Functions</a><ul></ul></li>
  <li class="children-title"><a href="/azure_data_bricks/explode/">Explode</a><ul></ul></li>
  <li class="children-title"><a href="/azure_data_bricks/pivot/">Pivot</a><ul></ul></li>
  <li class="children-title"><a href="/azure_data_bricks/comparisons/">Comparisons</a><ul></ul></li>
</ul>
<h2 id="1-introduction">1. Introduction</h2>
<ul>
<li>Big Data Overview</li>
<li>Introduction to Apache Spark</li>
<li>Introduction to Azure Databricks</li>
<li>Databricks Architecture</li>
<li>Databricks Workspace Overview</li>
</ul>
<h2 id="2-spark-core-concepts">2. Spark Core Concepts</h2>
<ul>
<li>RDDs (Resilient Distributed Datasets)</li>
<li>Transformations and Actions</li>
<li>Lazy Evaluation</li>
<li>Spark Execution Model</li>
<li>Caching and Persistence</li>
<li>Partitioning</li>
</ul>
<h2 id="3-spark-sql">3. Spark SQL</h2>
<ul>
<li>DataFrames and Datasets</li>
<li>Spark SQL Engine</li>
<li>Schema Inference</li>
<li>Catalog and Metadata</li>
<li>Temporary and Global Views</li>
<li>SQL Queries in Spark</li>
<li>Performance Optimization (Catalyst Optimizer, Tungsten)</li>
</ul>
<h2 id="4-spark-data-sources">4. Spark Data Sources</h2>
<ul>
<li>Reading/Writing CSV, JSON, Parquet, ORC</li>
<li>Working with Delta Lake</li>
<li>JDBC Data Sources</li>
<li>Streaming Data Sources</li>
<li>Handling Nested Data (Struct, Array, Map)</li>
</ul>
<h2 id="5-delta-lake">5. Delta Lake</h2>
<ul>
<li>Introduction to Delta Lake</li>
<li>ACID Transactions</li>
<li>Schema Enforcement &amp; Evolution</li>
<li>Time Travel</li>
<li>Upserts &amp; Deletes (MERGE)</li>
<li>Delta Lake Optimization (Z-order, Optimize, Vacuum)</li>
</ul>
<h2 id="6-spark-streaming--structured-streaming">6. Spark Streaming &amp; Structured Streaming</h2>
<ul>
<li>Introduction to Streaming</li>
<li>DStreams vs Structured Streaming</li>
<li>Streaming Sources (Kafka, Event Hubs, Socket)</li>
<li>Watermarking &amp; Late Data</li>
<li>State Management</li>
<li>Streaming Sinks (Delta, Console, Kafka, Event Hubs)</li>
</ul>
<h2 id="7-spark-mllib-machine-learning">7. Spark MLlib (Machine Learning)</h2>
<ul>
<li>MLlib Basics</li>
<li>Feature Engineering</li>
<li>Pipelines &amp; Transformers</li>
<li>Classification</li>
<li>Regression</li>
<li>Clustering</li>
<li>Model Persistence</li>
</ul>
<h2 id="8-spark-graphx">8. Spark GraphX</h2>
<ul>
<li>Introduction to GraphX</li>
<li>Graph Processing Basics</li>
<li>Graph Algorithms</li>
<li>Use Cases</li>
</ul>
<h2 id="9-performance-tuning">9. Performance Tuning</h2>
<ul>
<li>Spark Configurations &amp; Parameters</li>
<li>Memory Management</li>
<li>Shuffle Optimizations</li>
<li>Caching Strategies</li>
<li>Adaptive Query Execution (AQE)</li>
<li>Skew Handling</li>
</ul>
<h2 id="10-azure-databricks-essentials">10. Azure Databricks Essentials</h2>
<ul>
<li>Databricks Clusters (Types, Autoscaling, Pools)</li>
<li>Databricks Notebooks</li>
<li>Databricks Repos &amp; Git Integration</li>
<li>Databricks Jobs &amp; Scheduling</li>
<li>Databricks Secrets</li>
<li>Databricks Utilities (DBUtils)</li>
<li>Workspace Security &amp; Permissions</li>
</ul>
<h2 id="11-integration-with-azure-services">11. Integration with Azure Services</h2>
<ul>
<li>Azure Data Lake Storage (ADLS) Integration</li>
<li>Azure Blob Storage Integration</li>
<li>Azure Synapse Analytics</li>
<li>Azure Event Hubs</li>
<li>Azure Data Factory (ADF) with Databricks</li>
<li>Power BI with Databricks</li>
</ul>
<h2 id="12-advanced-databricks">12. Advanced Databricks</h2>
<ul>
<li>Unity Catalog</li>
<li>Databricks SQL</li>
<li>Delta Live Tables (DLT)</li>
<li>Databricks REST API</li>
<li>Databricks CLI</li>
<li>Monitoring &amp; Logging</li>
<li>Cost Optimization in Databricks</li>
</ul>
<h2 id="13-real-world-use-cases">13. Real-World Use Cases</h2>
<ul>
<li>ETL Pipeline with Databricks</li>
<li>Streaming Data Pipeline</li>
<li>Batch Data Processing</li>
<li>Machine Learning Pipeline</li>
<li>End-to-End Data Lakehouse Implementation</li>
</ul>
<h2 id="14-best-practices">14. Best Practices</h2>
<ul>
<li>Cluster Sizing &amp; Configuration</li>
<li>Code Versioning &amp; Collaboration</li>
<li>Data Governance</li>
<li>Security &amp; Compliance</li>
<li>CI/CD with Databricks</li>
<li>Productionizing Workloads</li>
</ul>
<h2 id="15-capstone-project">15. Capstone Project</h2>
<ul>
<li>Build a Scalable Data Lakehouse with Databricks</li>
<li>Real-Time Streaming Data Pipeline</li>
<li>ML Model Training and Deployment</li>
</ul>

  <footer class="footline">
  </footer>
</article>
          <section>
            <h1 class="a11y-only">Subsections of ADB</h1>
<article class="default">
  <header class="headline">
  </header>

<h1 id="spark">Spark</h1>

<h2 id="what-is-spark">What is Spark?</h2>
<p><strong>Spark</strong> is an open source unified computing engine with a set of libraries for <strong>parallel data processing</strong> on a computer cluster.</p>
<p>It supports widely used programming languages such as:</p>
<ul>
<li><strong>Scala</strong></li>
<li><strong>Python</strong></li>
<li><strong>Java</strong></li>
<li><strong>R</strong></li>
</ul>
<p>It processes data in <strong>memory (RAM)</strong>, which makes it 100 times faster than traditional Hadoop MapReduce.</p>
<pre class="mermaid align-center ">flowchart TD
    subgraph DriverProgram[Driver Program]
        SS[SparkSession]
        SC[SparkContext]
        SS --&gt; SC
    end

    CM[Cluster Manager]

    subgraph Worker1[Worker Node]
        subgraph Executor1[Executor]
            T1[Task]
            T2[Task]
            C1[Cache]
        end
    end

    subgraph Worker2[Worker Node]
        subgraph Executor2[Executor]
            T3[Task]
            T4[Task]
            C2[Cache]
        end
    end

    SC --&gt; CM
    CM --&gt; Executor1
    CM --&gt; Executor2
    SC --&gt; Executor1
    SC --&gt; Executor2
    Executor1 --&gt; T1
    Executor1 --&gt; T2
    Executor1 --&gt; C1
    Executor2 --&gt; T3
    Executor2 --&gt; T4
    Executor2 --&gt; C2</pre>
<h3 id="spark-components">Spark Components</h3>
<p>Following represents Spark components at a high level:</p>
<ul>
<li>Low Level API â€“ RDD &amp; Distributed Variables</li>
<li>Structured API â€“ DataFrames, Datasets, and SQL</li>
<li>Libraries and Ecosystem â€“ Structured Streaming and Advanced Analytics</li>
</ul>
<table>
  <thead>
      <tr>
          <th style="text-align: center">LAYER STRUCTURE</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center">Libraries &amp; Ecosystem</td>
      </tr>
      <tr>
          <td style="text-align: center">Structured API</td>
      </tr>
      <tr>
          <td style="text-align: center">Low Level API</td>
      </tr>
  </tbody>
</table>
<h2 id="how-spark-works">How Spark Works</h2>
<h3 id="1-drivers-and-executors">1. Drivers and Executors</h3>
<ul>
<li><strong>Driver</strong>: The brain of a Spark application. It translates your code into a logical execution plan and coordinates work.</li>
<li><strong>Executors</strong>: The workers. They run on cluster nodes, do the actual computation, and store data in memory/disk.</li>
</ul>
<p>Think of the <strong>Driver as a manager</strong> and <strong>Executors as employees</strong> doing the tasks.</p>
<hr>
<h3 id="2-jobs-stages-and-tasks">2. Jobs, Stages, and Tasks</h3>
<ul>
<li><strong>Job</strong>: Triggered when you call an action (like <code>.collect()</code> or <code>.save()</code>). A job = big unit of work.</li>
<li><strong>Stage</strong>: Spark splits the job into smaller parts based on shuffle boundaries (data movement points).</li>
<li><strong>Task</strong>: The smallest unit. Each stage is broken into many tasks, one per partition of data.</li>
</ul>
<p>ðŸ‘‰ Flow: <strong>Job â†’ Stages â†’ Tasks â†’ Results</strong></p>
<pre class="mermaid align-center ">  flowchart LR
    JOB[JOB]
    STAGE1[STAGE]
    STAGE2[STAGE]
    TASK1[TASK]
    TASK2[TASK]
    TASK3[TASK]
    
    JOB --&gt; STAGE1
    JOB --&gt; STAGE2
    STAGE1 --&gt; TASK1
    STAGE1 --&gt; TASK2
    STAGE2 --&gt; TASK3</pre>
<h2 id="what-is-partition">What is Partition?</h2>
<p>To allow every executor to work in parallel, Spark breaks down the data into chunks called partitions.</p>
<h2 id="what-is-transformation">What is Transformation?</h2>
<p>The <strong>instruction</strong> or <strong>code</strong> to modify and transform data is known as Transformation.</p>
<p><strong>Examples</strong>: select,where,groupBy etc.</p>
<p>Transformation helps in building the <strong>logical plan</strong>.</p>
<p>Two Types:</p>
<ul>
<li><strong>Narrow Transformation</strong></li>
<li><strong>Wide Transformation</strong></li>
</ul>
<h2 id="what-are-actions">What are Actions?</h2>
<p>To <strong>trigger the execution</strong> we need to call an <strong>Action</strong>.</p>
<p>This basically executes the plan created by Transformation.</p>
<p>Actions are of three types:</p>
<ul>
<li><strong>View data</strong> in console</li>
<li><strong>Collect data</strong> to native language</li>
<li><strong>Write data</strong> to output data sources</li>
</ul>
<h3 id="spark-prefers-lazy-evaluation">Spark prefers Lazy Evaluation</h3>
<p><strong>Transformations are lazy</strong> â†’ Spark doesnâ€™t execute them immediately; it just builds a logical plan (DAG).
<strong>Execution happens only on actions</strong> â†’ When an action (collect, count, save) is called, Spark optimizes the DAG and runs it.</p>
<h3 id="shuffle-in-spark">Shuffle in Spark</h3>
<ul>
<li><strong>When</strong>: Happens during <strong>wide transformations</strong> (<code>groupByKey</code>, <code>reduceByKey</code>, <code>join</code>, etc.).</li>
<li><strong>What</strong>: Data is <strong>redistributed across the cluster</strong> so records with the same key end up in the same partition.</li>
<li><strong>Impact</strong>: A <strong>new stage</strong> is created in the DAG because shuffle requires data movement across executors.</li>
</ul>
<blockquote>
<p>Shuffling occurs because wide transformations require <strong>related data (e.g., same keys)</strong> to be co-located in the <strong>same partition</strong>, which necessitates <strong>repartitioning</strong> and <strong>redistributing</strong> data across nodes.</p></blockquote>

<details open class=" box cstyle notices note">
  <summary class="box-label" tabindex="-1">
    <i class="fa-fw fas fa-exclamation-circle"></i> 
    Note
  </summary>
  <div class="box-content">
<p>In Spark, <strong>actions (like count, collect, saveAsTextFile) create a job and trigger its execution, wide transformations (such as groupByKey, reduceByKey, join) introduce shuffle boundaries by redistributing data across partitions, and these boundaries split the job into stages, where tasks are scheduled and executed in parallel on cluster nodes.</strong></p>
  </div>
</details>
<h2 id="what-is-spark-session">What is Spark Session?</h2>
<ul>
<li>The Driver Process is known as Spark Session.</li>
<li>It is the entry point for a Spark execution.</li>
<li>The Spark Session instance executes the code in the cluster.</li>
<li>The relation is one-to-one, i.e., for one Spark Application, there will be one Spark Session instance.</li>
</ul>
<h3 id="structured-api---dataframes">Structured API - DataFrames</h3>
<ul>
<li>DataFrame is the most common Structured API, represented like a table.</li>
<li>The table is represented in form of Rows and Columns.</li>
<li>DataFrame has schema, which is the metadata for the columns.</li>
<li>Data in DataFrames are in partitions.</li>
<li>DataFrames are immutable.</li>
</ul>

<details open class=" box cstyle notices note">
  <summary class="box-label" tabindex="-1">
    <i class="fa-fw fas fa-exclamation-circle"></i> 
    Note
  </summary>
  <div class="box-content">
<p>DataFrames are <strong>immutable</strong>, meaning every transformation creates a <strong>new DataFrame</strong> without altering the original one.</p>
  </div>
</details>
<h3 id="structured-api-execution-plan">Structured API Execution Plan</h3>
<h3 id="1-logial-planning">1. <strong>Logial Planning</strong></h3>
<p>The Spark <strong>Driver</strong> first converts your code (Transformations/Actions) into a <strong>logical plan</strong>.
Represents <strong>what</strong> needs to be done without worrying about <strong>how</strong> it will be executed.</p>
<pre class="mermaid align-center ">flowchart LR
    A[Unresolved Logical Plan] --&gt; B[Resolved Logical Plan]
    B --&gt; C[Catalyst Optimizer]
    C --&gt; D[Optimized Logical Plan]</pre>
<h3 id="2-physical-planning">2. <strong>Physical Planning</strong></h3>
<p>Spark converts the <strong>logical plan</strong> into a <strong>physical plan</strong>, deciding <strong>how</strong> to execute it (which operations run where, partitioning, joins, etc.).
Optimizes for performance (e.g., choosing sort merge join vs broadcast join).</p>
<pre class="mermaid align-center ">flowchart LR
    A[Optimized Logical Plan] --&gt; B[Multiple Physical Plans]
    B --&gt; C[Cost Optimizer]
    C --&gt; D[Best Physical Plan]
    D --&gt; E[Sent to Cluster for Execution]</pre>
<h3 id="3-dag-directed-acyclic-graph">3. DAG (Directed Acyclic Graph)</h3>
<p>Spark breaks the physical plan into a <strong>DAG of stages</strong>.
Shows <strong>dependencies between stages</strong> and ensures tasks are executed in the correct order.</p>
<p><strong>Flow</strong>: DAG â†’ Stages â†’ Tasks â†’ Execution</p>
<hr>
<h3 id="summary-flow">Summary Flow</h3>
<pre class="mermaid align-center ">flowchart LR
    A[User Code] --&gt; B[Logical Plan] --&gt; C[Physical Plan] --&gt; D[DAG of Stages] --&gt; E[Tasks executed by Executors]</pre>
<h2 id="creating-a-spark-session-in-pyspark">Creating a Spark Session in PySpark</h2>
<p>A <strong>SparkSession</strong> is the entry point to using PySpark. It allows interaction with Sparkâ€™s functionalities (DataFrame, SQL, etc.).</p>
<h3 id="example">Example</h3>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">spark</span> <span class="o">=</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">SparkSession</span>
</span></span><span class="line"><span class="cl">    <span class="o">.</span><span class="n">builder</span>
</span></span><span class="line"><span class="cl">    <span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&#34;Spark Introduction&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="o">.</span><span class="n">master</span><span class="p">(</span><span class="s2">&#34;local[*]&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span></span></span></code></pre></div>
<ul>
<li><code>SparkSession.builder</code>: used to configure the session.</li>
<li><code>.appName(&quot;...&quot;)</code>: sets the application name.</li>
<li><code>.master(&quot;local[*]&quot;)</code>: runs Spark locally using all CPU cores.</li>
<li><code>.getOrCreate()</code>: returns an existing session or creates a new one.</li>
</ul>
<p><strong>Note</strong>:In Databricks, you donâ€™t need to create a SparkSession manually. A spark session is already available by default, so you can directly use spark.read, spark.sql, etc.</p>
<h2 id="creating-dataframes-in-pyspark">Creating DataFrames in PySpark</h2>
<h3 id="1-from-in-memory-data">1. From In-Memory Data</h3>
<p>You can create a DataFrame directly from Python objects (lists, tuples, dicts).</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">data</span> <span class="o">=</span> <span class="p">[(</span><span class="s2">&#34;Alice&#34;</span><span class="p">,</span> <span class="mi">25</span><span class="p">),</span> <span class="p">(</span><span class="s2">&#34;Bob&#34;</span><span class="p">,</span> <span class="mi">30</span><span class="p">),</span> <span class="p">(</span><span class="s2">&#34;Charlie&#34;</span><span class="p">,</span> <span class="mi">35</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl"><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;Name&#34;</span><span class="p">,</span> <span class="s2">&#34;Age&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></span></span></code></pre></div>
<h3 id="2-from-file">2. From File</h3>
<p>You can load data from CSV, JSON, Parquet, etc.</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">csv</span><span class="p">(</span><span class="s2">&#34;path/to/file.csv&#34;</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">inferSchema</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">json</span><span class="p">(</span><span class="s2">&#34;path/to/file.json&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></span></span></code></pre></div>

  <footer class="footline">
  </footer>
</article>
<article class="default">
  <header class="headline">
  </header>

<h1 id="df-basics">DF Basics</h1>

<h2 id="creating-dataframe">Creating DataFrame</h2>
<h3 id="creating-dataframe-from-liststuples">Creating DataFrame from Lists/Tuples</h3>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Sample Data</span>
</span></span><span class="line"><span class="cl"><span class="n">data</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="s2">&#34;Alice&#34;</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="s2">&#34;Bob&#34;</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="s2">&#34;Charlie&#34;</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="s2">&#34;David&#34;</span><span class="p">),</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="s2">&#34;Eve&#34;</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl"><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;ID&#34;</span><span class="p">,</span> <span class="s2">&#34;Name&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create DataFrame</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Show DataFrame</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></span></span></code></pre></div>
<h3 id="creating-dataframe-from-pandas">Creating DataFrame from Pandas</h3>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Sample Pandas DataFrame</span>
</span></span><span class="line"><span class="cl"><span class="n">pandas_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">columns</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Convert to PySpark DataFrame</span>
</span></span><span class="line"><span class="cl"><span class="n">df_from_pandas</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">pandas_df</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">df_from_pandas</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></span></span></code></pre></div>
<h3 id="create-dataframe-from-dictionary">Create DataFrame from Dictionary</h3>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">data_dict</span> <span class="o">=</span> <span class="p">[{</span><span class="s2">&#34;ID&#34;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&#34;Name&#34;</span><span class="p">:</span> <span class="s2">&#34;Alice&#34;</span><span class="p">},</span> <span class="p">{</span><span class="s2">&#34;ID&#34;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&#34;Name&#34;</span><span class="p">:</span> <span class="s2">&#34;Bob&#34;</span><span class="p">}]</span>
</span></span><span class="line"><span class="cl"><span class="n">df_from_dict</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">data_dict</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">df_from_dict</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></span></span></code></pre></div>
<h3 id="create-empty-dataframe">Create Empty DataFrame</h3>
<p>You can create an empty DataFrame with just schema definitions.</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">StructType</span><span class="p">,</span> <span class="n">StructField</span><span class="p">,</span> <span class="n">StringType</span><span class="p">,</span> <span class="n">IntegerType</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Define Schema</span>
</span></span><span class="line"><span class="cl"><span class="n">schema</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">([</span>
</span></span><span class="line"><span class="cl">    <span class="n">StructField</span><span class="p">(</span><span class="s2">&#34;ID&#34;</span><span class="p">,</span> <span class="n">IntegerType</span><span class="p">(),</span> <span class="kc">True</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">StructField</span><span class="p">(</span><span class="s2">&#34;Name&#34;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create Empty DataFrame</span>
</span></span><span class="line"><span class="cl"><span class="n">empty_df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([],</span> <span class="n">schema</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">empty_df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></span></span></code></pre></div>
<h3 id="creating-dataframe-from-structured-data-csv-json-parquet">Creating DataFrame from Structured Data (CSV, JSON, Parquet)</h3>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Reading CSV file into DataFrame</span>
</span></span><span class="line"><span class="cl"><span class="n">df_csv</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">csv</span><span class="p">(</span><span class="s2">&#34;/path/to/file.csv&#34;</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">inferSchema</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">df_csv</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Reading JSON file into DataFrame</span>
</span></span><span class="line"><span class="cl"><span class="n">df_json</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">json</span><span class="p">(</span><span class="s2">&#34;/path/to/file.json&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">df_json</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Reading Parquet file into DataFrame</span>
</span></span><span class="line"><span class="cl"><span class="n">df_parquet</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s2">&#34;/path/to/file.parquet&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">df_parquet</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></span></span></code></pre></div>
<h3 id="show-function-in-pyspark-dataframes">show() Function in PySpark DataFrames</h3>
<p>The <code>show()</code> function in PySpark displays the contents of a DataFrame in a tabular format. It has several useful parameters for customization:</p>
<h3 id="parameters">Parameters:</h3>
<ol>
<li><strong>n</strong>: Number of rows to display (default is 20)</li>
<li><strong>truncate</strong>: If set to True, it truncates column values longer than 20 characters (default is True)</li>
<li><strong>vertical</strong>: If set to True, prints rows in a vertical format</li>
</ol>
<h3 id="usage-examples">Usage Examples:</h3>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Show the first 3 rows, truncate columns to 25 characters, and display vertically:</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">truncate</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">vertical</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Show entire DataFrame (default settings):</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Show the first 10 rows:</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Show DataFrame without truncating any columns:</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="n">truncate</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></span></span></code></pre></div>
<hr>
<h2 id="loading-data-from-csv-file-into-a-dataframe">Loading Data from CSV File into a DataFrame</h2>
<p>Loading data into DataFrames is a fundamental step in any data processing workflow in PySpark. This document outlines how to load data from CSV files into a DataFrame, including using a custom schema and the implications of using the inferSchema option.</p>
<h3 id="1-import-required-libraries">1. Import Required Libraries</h3>
<p>Before loading the data, ensure you import the necessary modules:</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">StructType</span><span class="p">,</span> <span class="n">StructField</span><span class="p">,</span> <span class="n">IntegerType</span><span class="p">,</span> <span class="n">StringType</span><span class="p">,</span> <span class="n">DoubleType</span></span></span></code></pre></div>
<h3 id="2-define-the-schema">2. Define the Schema</h3>
<p>You can define a custom schema for your CSV file. This allows you to explicitly set the data types for each column.</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Define the schema for the CSV file</span>
</span></span><span class="line"><span class="cl"><span class="n">custom_schema</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">([</span>
</span></span><span class="line"><span class="cl">    <span class="n">StructField</span><span class="p">(</span><span class="s2">&#34;id&#34;</span><span class="p">,</span> <span class="n">IntegerType</span><span class="p">(),</span> <span class="kc">True</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">StructField</span><span class="p">(</span><span class="s2">&#34;name&#34;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="kc">True</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">StructField</span><span class="p">(</span><span class="s2">&#34;age&#34;</span><span class="p">,</span> <span class="n">IntegerType</span><span class="p">(),</span> <span class="kc">True</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">StructField</span><span class="p">(</span><span class="s2">&#34;salary&#34;</span><span class="p">,</span> <span class="n">DoubleType</span><span class="p">(),</span> <span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">])</span></span></span></code></pre></div>
<h3 id="3-read-the-csv-file">3. Read the CSV File</h3>
<p>Load the CSV file into a DataFrame using the <code>read.csv()</code> method. Here, <code>header=True</code> treats the first row as headers, and <code>inferSchema=True</code> allows Spark to automatically assign data types to columns.</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Read the CSV file with the custom schema</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">csv</span><span class="p">(</span><span class="s2">&#34;your_file.csv&#34;</span><span class="p">,</span> <span class="n">schema</span><span class="o">=</span><span class="n">custom_schema</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></span></span></code></pre></div>
<h3 id="4-load-multiple-csv-files">4. Load Multiple CSV Files</h3>
<p>To read multiple CSV files into a single DataFrame, you can pass a list of file paths. Ensure that the schema is consistent across all files.</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># List of file paths</span>
</span></span><span class="line"><span class="cl"><span class="n">file_paths</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;file1.csv&#34;</span><span class="p">,</span> <span class="s2">&#34;file2.csv&#34;</span><span class="p">,</span> <span class="s2">&#34;file3.csv&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Read multiple CSV files into a single DataFrame</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">csv</span><span class="p">(</span><span class="n">file_paths</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">inferSchema</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></span></span></code></pre></div>
<h3 id="5-load-a-csv-from-filestore">5. Load a CSV from FileStore</h3>
<p>Here is an example of loading a CSV file from Databricks FileStore:</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">csv</span><span class="p">(</span><span class="s2">&#34;/FileStore/tables/Order.csv&#34;</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">inferSchema</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s1">&#39;,&#39;</span><span class="p">)</span></span></span></code></pre></div>
<h3 id="6-display-the-dataframe">6. Display the DataFrame</h3>
<p>Use the following commands to check the schema and display the DataFrame:</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Print the schema of the DataFrame</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span><span class="o">.</span><span class="n">printSchema</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Show the first 20 rows of the DataFrame</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>  <span class="c1"># Displays only the first 20 rows</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Display the DataFrame in a tabular format</span>
</span></span><span class="line"><span class="cl"><span class="n">display</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>  <span class="c1"># For Databricks notebooks</span></span></span></code></pre></div>
<h3 id="interview-question-how-does-inferschema-work">Interview Question: How Does inferSchema Work?</h3>
<p><strong>Behind the Scenes:</strong> When you use <code>inferSchema</code>, Spark runs a job that scans the CSV file from top to bottom to identify the best-suited data type for each column based on the values it encounters.</p>
<h3 id="does-it-make-sense-to-use-inferschema">Does It Make Sense to Use inferSchema?</h3>
<p><strong>Pros:</strong></p>
<ul>
<li>Useful when the schema of the file keeps changing, as it allows Spark to automatically detect the data types.</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li><strong>Performance Impact:</strong> Spark must scan the entire file, which can take extra time, especially for large files.</li>
<li><strong>Loss of Control:</strong> You lose the ability to explicitly define the schema, which may lead to incorrect data types if the data is inconsistent.</li>
</ul>
<h3 id="conclusion">Conclusion</h3>
<p>Loading data from CSV files into a DataFrame is straightforward in PySpark. Understanding how to define a schema and the implications of using <code>inferSchema</code> is crucial for optimizing your data processing workflows.</p>
<p>This document provides a comprehensive overview of how to load CSV data into DataFrames in PySpark, along with considerations for using schema inference. Let me know if you need any more details or adjustments!</p>
<h2 id="pyspark-dataframe-schema-definition">PySpark DataFrame Schema Definition</h2>
<h3 id="1-defining-schema-programmatically-with-structtype">1. Defining Schema Programmatically with StructType</h3>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="o">*</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Define the schema using StructType</span>
</span></span><span class="line"><span class="cl"><span class="n">employeeSchema</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">([</span>
</span></span><span class="line"><span class="cl">   <span class="n">StructField</span><span class="p">(</span><span class="s2">&#34;ID&#34;</span><span class="p">,</span> <span class="n">IntegerType</span><span class="p">(),</span> <span class="kc">True</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">   <span class="n">StructField</span><span class="p">(</span><span class="s2">&#34;Name&#34;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="kc">True</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">   <span class="n">StructField</span><span class="p">(</span><span class="s2">&#34;Age&#34;</span><span class="p">,</span> <span class="n">IntegerType</span><span class="p">(),</span> <span class="kc">True</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">   <span class="n">StructField</span><span class="p">(</span><span class="s2">&#34;Salary&#34;</span><span class="p">,</span> <span class="n">DoubleType</span><span class="p">(),</span> <span class="kc">True</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">   <span class="n">StructField</span><span class="p">(</span><span class="s2">&#34;Joining_Date&#34;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="kc">True</span><span class="p">),</span>  <span class="c1"># Keeping as String for date issues</span>
</span></span><span class="line"><span class="cl">   <span class="n">StructField</span><span class="p">(</span><span class="s2">&#34;Department&#34;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="kc">True</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">   <span class="n">StructField</span><span class="p">(</span><span class="s2">&#34;Performance_Rating&#34;</span><span class="p">,</span> <span class="n">IntegerType</span><span class="p">(),</span> <span class="kc">True</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">   <span class="n">StructField</span><span class="p">(</span><span class="s2">&#34;Email&#34;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="kc">True</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">   <span class="n">StructField</span><span class="p">(</span><span class="s2">&#34;Address&#34;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="kc">True</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">   <span class="n">StructField</span><span class="p">(</span><span class="s2">&#34;Phone&#34;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Load the DataFrame with the defined schema</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&#34;/FileStore/tables/employees.csv&#34;</span><span class="p">,</span><span class="nb">format</span><span class="o">=</span><span class="s2">&#34;csv&#34;</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">schema</span><span class="o">=</span><span class="n">employeeSchema</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Print the schema of the DataFrame</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span><span class="o">.</span><span class="n">printSchema</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Optionally display the DataFrame</span>
</span></span><span class="line"><span class="cl"><span class="c1"># display(df)</span></span></span></code></pre></div>
<h3 id="2-defining-schema-as-a-string">2. Defining Schema as a String</h3>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Define the schema as a string</span>
</span></span><span class="line"><span class="cl"><span class="n">employeeSchemaString</span> <span class="o">=</span> <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">ID Integer,
</span></span></span><span class="line"><span class="cl"><span class="s1">Name String,
</span></span></span><span class="line"><span class="cl"><span class="s1">Age Integer,
</span></span></span><span class="line"><span class="cl"><span class="s1">Salary Double,
</span></span></span><span class="line"><span class="cl"><span class="s1">Joining_Date String,
</span></span></span><span class="line"><span class="cl"><span class="s1">Department String,
</span></span></span><span class="line"><span class="cl"><span class="s1">Performance_Rating Integer,
</span></span></span><span class="line"><span class="cl"><span class="s1">Email String,
</span></span></span><span class="line"><span class="cl"><span class="s1">Address String,
</span></span></span><span class="line"><span class="cl"><span class="s1">Phone String
</span></span></span><span class="line"><span class="cl"><span class="s1">&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Load the DataFrame with the defined schema</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&#34;dbfs:/FileStore/shared_uploads/imsvk11@gmail.com/employee_data.csv&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                     <span class="nb">format</span><span class="o">=</span><span class="s2">&#34;csv&#34;</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">schema</span><span class="o">=</span><span class="n">employeeSchemaString</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Print the schema of the DataFrame</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span><span class="o">.</span><span class="n">printSchema</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Optionally display the DataFrame</span>
</span></span><span class="line"><span class="cl"><span class="c1"># display(df)</span></span></span></code></pre></div>
<h3 id="explanation">Explanation</h3>
<ul>
<li><strong>Schema Definition</strong>: Both methods define a schema for the DataFrame, accommodating the dataset&rsquo;s requirements, including handling null values where applicable.</li>
<li><strong>Data Types</strong>: The Joining_Date column is defined as StringType to accommodate potential date format issues or missing values.</li>
<li><strong>Loading the DataFrame</strong>: The <code>spark.read.load</code> method is used to load the CSV file into a DataFrame using the specified schema.</li>
<li><strong>Printing the Schema</strong>: The <code>df.printSchema()</code> function allows you to verify that the DataFrame is structured as intended.</li>
</ul>

  <footer class="footline">
  </footer>
</article>
<article class="default">
  <header class="headline">
  </header>

<h1 id="df-operations">DF Operations</h1>

<h2 id="column-selection--manipulation">Column Selection &amp; Manipulation</h2>
<h3 id="1-different-methods-to-select-columns">1. Different Methods to Select Columns</h3>
<p>In PySpark, you can select specific columns in multiple ways:</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Using col() function</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s2">&#34;Name&#34;</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Using column() function</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">column</span><span class="p">(</span><span class="s2">&#34;Age&#34;</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Directly using string name</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&#34;Salary&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></span></span></code></pre></div>
<h3 id="2-selecting-multiple-columns-together">2. Selecting Multiple Columns Together</h3>
<p>You can combine different methods to select multiple columns:</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># multiple column</span>
</span></span><span class="line"><span class="cl"><span class="n">df2</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&#34;ID&#34;</span><span class="p">,</span> <span class="s2">&#34;Name&#34;</span><span class="p">,</span> <span class="n">col</span><span class="p">(</span><span class="s2">&#34;Salary&#34;</span><span class="p">),</span> <span class="n">column</span><span class="p">(</span><span class="s2">&#34;Department&#34;</span><span class="p">),</span> <span class="n">df</span><span class="o">.</span><span class="n">Phone</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">df2</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></span></span></code></pre></div>
<h3 id="3-listing-all-columns-in-a-dataframe">3. Listing All Columns in a DataFrame</h3>
<p>To get a list of all the column names:</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># get all column name</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span><span class="o">.</span><span class="n">columns</span></span></span></code></pre></div>
<h3 id="4-renaming-columns-with-alias">4. Renaming Columns with alias()</h3>
<p>You can rename columns using the <code>alias()</code> method:</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">  <span class="n">col</span><span class="p">(</span><span class="s2">&#34;Name&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;EmployeeName&#39;</span><span class="p">),</span>  <span class="c1"># Rename &#34;Name&#34; to &#34;EmployeeName&#34;</span>
</span></span><span class="line"><span class="cl">  <span class="n">col</span><span class="p">(</span><span class="s2">&#34;Salary&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;EmployeeSalary&#39;</span><span class="p">),</span>  <span class="c1"># Rename &#34;Salary&#34; to &#34;EmployeeSalary&#34;</span>
</span></span><span class="line"><span class="cl">  <span class="n">column</span><span class="p">(</span><span class="s2">&#34;Department&#34;</span><span class="p">),</span>  <span class="c1"># Select &#34;Department&#34;</span>
</span></span><span class="line"><span class="cl">  <span class="n">df</span><span class="o">.</span><span class="n">Joining_Date</span>  <span class="c1"># Select &#34;Joining_Date&#34;</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></span></span></code></pre></div>
<h3 id="5-using-selectexpr-for-concise-column-selection">5. Using selectExpr() for Concise Column Selection</h3>
<p><code>selectExpr()</code> allows you to use SQL expressions directly and rename columns concisely:</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">df</span><span class="o">.</span><span class="n">selectExpr</span><span class="p">(</span><span class="s2">&#34;Name as EmployeeName&#34;</span><span class="p">,</span> <span class="s2">&#34;Salary as EmployeeSalary&#34;</span><span class="p">,</span> <span class="s2">&#34;Department&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></span></span></code></pre></div>
<h3 id="summary">Summary</h3>
<ul>
<li>Use <code>col()</code>, <code>column()</code>, or string names to select columns.</li>
<li>Use <code>expr()</code> and <code>selectExpr()</code> for SQL-like expressions and renaming.</li>
<li>Use <code>alias()</code> to rename columns.</li>
<li>Get the list of columns using <code>df.columns</code>.</li>
</ul>
<hr>
<h2 id="adding-renaming-and-dropping-columns">Adding, Renaming, and Dropping Columns</h2>
<h3 id="1-adding-new-columns-with-withcolumn">1. Adding New Columns with withColumn()</h3>
<p>In PySpark, the <code>withColumn()</code> function is widely used to add new columns to a DataFrame. You can either assign a constant value using <code>lit()</code> or perform transformations using existing columns.</p>
<h4 id="add-a-constant-value-column">Add a constant value column:</h4>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">newdf</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&#34;NewColumn&#34;</span><span class="p">,</span> <span class="n">lit</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span></span></span></code></pre></div>
<h4 id="add-a-column-based-on-an-expression">Add a column based on an expression:</h4>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">newdf</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&#34;withinCountry&#34;</span><span class="p">,</span> <span class="n">expr</span><span class="p">(</span><span class="s2">&#34;Country == &#39;India&#39;&#34;</span><span class="p">))</span></span></span></code></pre></div>
<p>This function allows adding multiple columns, including calculated ones:</p>
<p><strong>Example:</strong></p>
<ul>
<li>Assign a constant value with <code>lit()</code>.</li>
<li>Perform calculations using existing columns like multiplying values.</li>
</ul>
<h3 id="2-renaming-columns-with-withcolumnrenamed">2. Renaming Columns with withColumnRenamed()</h3>
<p>PySpark provides the <code>withColumnRenamed()</code> method to rename columns. This is especially useful when you want to change the names for clarity or to follow naming conventions:</p>
<h3 id="renaming-a-column">Renaming a column:</h3>
<p>python</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">new_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumnRenamed</span><span class="p">(</span><span class="s2">&#34;oldColumnName&#34;</span><span class="p">,</span> <span class="s2">&#34;newColumnName&#34;</span><span class="p">)</span></span></span></code></pre></div>
<h4 id="handling-column-names-with-special-characters-or-spaces">Handling column names with special characters or spaces:</h4>
<p>If a column has special characters or spaces, you need to use backticks (`) to escape it:</p>
<p>python</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">newdf</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&#34;`New Column Name`&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></span></span></code></pre></div>
<h3 id="3-dropping-columns-with-drop">3. Dropping Columns with drop()</h3>
<p>To remove unwanted columns, you can use the <code>drop()</code> method:</p>
<h4 id="drop-a-single-column">Drop a single column:</h4>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">df2</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s2">&#34;Country&#34;</span><span class="p">)</span></span></span></code></pre></div>
<h4 id="drop-multiple-columns">Drop multiple columns:</h4>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">df2</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s2">&#34;Country&#34;</span><span class="p">,</span> <span class="s2">&#34;Region&#34;</span><span class="p">)</span></span></span></code></pre></div>
<p>Dropping columns creates a new DataFrame, and the original DataFrame remains unchanged.</p>
<h3 id="4-immutability-of-dataframes">4. Immutability of DataFrames</h3>
<p>In Spark, DataFrames are immutable by nature. This means that after creating a DataFrame, its contents cannot be changed. All transformations like adding, renaming, or dropping columns result in a new DataFrame, keeping the original one intact.</p>
<h4 id="for-instance-dropping-columns-creates-a-new-dataframe-without-altering-the-original">For instance, dropping columns creates a new DataFrame without altering the original:</h4>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">newdf</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s2">&#34;ItemType&#34;</span><span class="p">,</span> <span class="s2">&#34;SalesChannel&#34;</span><span class="p">)</span></span></span></code></pre></div>
<p>This immutability ensures data consistency and supports Spark&rsquo;s parallel processing, as transformations do not affect the source data.</p>
<h3 id="key-points">Key Points</h3>
<ul>
<li>Use <code>withColumn()</code> for adding columns, with <code>lit()</code> for constant values and expressions for computed values.</li>
<li>Use <code>withColumnRenamed()</code> to rename columns and backticks for special characters or spaces.</li>
<li>Use <code>drop()</code> to remove one or more columns.</li>
<li>DataFrames are immutable in Sparkâ€”transformations result in new DataFrames, leaving the original unchanged.</li>
</ul>
<hr>
<h2 id="data-types-filtering-and-unique-values">Data Types, Filtering, and Unique Values</h2>
<p>Here&rsquo;s a structured set of notes with code to cover changing data types, filtering data, and handling unique/distinct values in PySpark using the employee data:</p>
<h3 id="1-changing-data-types-schema-transformation">1. Changing Data Types (Schema Transformation)</h3>
<p>In PySpark, you can change the data type of a column using the <code>cast()</code> method. This is helpful when you need to convert data types for columns like Salary or Phone.</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">col</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Change the &#39;Salary&#39; column from integer to double</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&#34;Salary&#34;</span><span class="p">,</span> <span class="n">col</span><span class="p">(</span><span class="s2">&#34;Salary&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="s2">&#34;double&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Convert &#39;Phone&#39; column to string</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&#34;Phone&#34;</span><span class="p">,</span> <span class="n">col</span><span class="p">(</span><span class="s2">&#34;Phone&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="s2">&#34;string&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">df</span><span class="o">.</span><span class="n">printSchema</span><span class="p">()</span></span></span></code></pre></div>
<h3 id="2-filtering-data">2. Filtering Data</h3>
<p>You can filter rows based on specific conditions. For instance, to filter employees with a salary greater than 50,000:</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Filter rows where Salary is greater than 50,000</span>
</span></span><span class="line"><span class="cl"><span class="n">filtered_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s2">&#34;Salary&#34;</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">50000</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">filtered_df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Filtering rows where Age is not null</span>
</span></span><span class="line"><span class="cl"><span class="n">filtered_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&#34;Age&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">isNotNull</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="n">filtered_df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></span></span></code></pre></div>
<h3 id="3-multiple-filters-chaining-conditions">3. Multiple Filters (Chaining Conditions)</h3>
<p>You can also apply multiple conditions using <code>&amp;</code> or <code>|</code> (AND/OR) to filter data. For example, finding employees over 30 years old and in the IT department:</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Filter rows where Age &gt; 30 and Department is &#39;IT&#39;</span>
</span></span><span class="line"><span class="cl"><span class="n">filtered_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">filter</span><span class="p">((</span><span class="n">df</span><span class="p">[</span><span class="s2">&#34;Age&#34;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">30</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&#34;Department&#34;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&#34;IT&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">filtered_df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></span></span></code></pre></div>
<h3 id="4-filtering-on-null-or-non-null-values">4. Filtering on Null or Non-Null Values</h3>
<p>Filtering based on whether a column has NULL values or not is crucial for data cleaning:</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Filter rows where &#39;Address&#39; is NULL</span>
</span></span><span class="line"><span class="cl"><span class="n">filtered_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&#34;Address&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">isNull</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="n">filtered_df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Filter rows where &#39;Email&#39; is NOT NULL</span>
</span></span><span class="line"><span class="cl"><span class="n">filtered_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&#34;Email&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">isNotNull</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="n">filtered_df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></span></span></code></pre></div>
<h3 id="5-handling-unique-or-distinct-data">5. Handling Unique or Distinct Data</h3>
<p>To get distinct rows or unique values from your dataset:</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Get distinct rows from the entire DataFrame</span>
</span></span><span class="line"><span class="cl"><span class="n">unique_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">distinct</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">unique_df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Get distinct values from the &#39;Department&#39; column</span>
</span></span><span class="line"><span class="cl"><span class="n">unique_departments_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&#34;Department&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">distinct</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">unique_departments_df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></span></span></code></pre></div>
<p>To remove duplicates based on specific columns, such as Email or Phone, use <code>dropDuplicates()</code>:</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Remove duplicates based on &#39;Email&#39; column</span>
</span></span><span class="line"><span class="cl"><span class="n">unique_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">dropDuplicates</span><span class="p">([</span><span class="s2">&#34;Email&#34;</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">unique_df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Remove duplicates based on both &#39;Phone&#39; and &#39;Email&#39;</span>
</span></span><span class="line"><span class="cl"><span class="n">unique_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">dropDuplicates</span><span class="p">([</span><span class="s2">&#34;Phone&#34;</span><span class="p">,</span> <span class="s2">&#34;Email&#34;</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">unique_df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></span></span></code></pre></div>
<h3 id="6-counting-distinct-values">6. Counting Distinct Values</h3>
<p>You can count distinct values in a particular column, or combinations of columns:</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Count distinct values in the &#39;Department&#39; column</span>
</span></span><span class="line"><span class="cl"><span class="n">distinct_count_department</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&#34;Department&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">distinct</span><span class="p">()</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Distinct Department Count:&#34;</span><span class="p">,</span> <span class="n">distinct_count_department</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Count distinct combinations of &#39;Department&#39; and &#39;Performance_Rating&#39;</span>
</span></span><span class="line"><span class="cl"><span class="n">distinct_combinations_count</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&#34;Department&#34;</span><span class="p">,</span> <span class="s2">&#34;Performance_Rating&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">distinct</span><span class="p">()</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Distinct Department and Performance Rating Combinations:&#34;</span><span class="p">,</span> <span class="n">distinct_combinations_count</span><span class="p">)</span></span></span></code></pre></div>
<p>This set of operations will help you efficiently manage and transform your data in PySpark, ensuring data integrity and accuracy for your analysis!</p>
<h3 id="mastering-pyspark-dataframe-operations">Mastering PySpark DataFrame Operations</h3>
<ol>
<li><strong>Changing Data Types</strong>: Easily modify column types using <code>.cast()</code>. E.g., change &lsquo;Salary&rsquo; to double or &lsquo;Phone&rsquo; to string for better data handling.</li>
<li><strong>Filtering Data</strong>: Use <code>.filter()</code> or <code>.where()</code> to extract specific rows. For example, filter employees with a salary over 50,000 or non-null Age.</li>
<li><strong>Multiple Conditions</strong>: Chain filters with <code>&amp;</code> and <code>|</code> to apply complex conditions, such as finding employees over 30 in the IT department.</li>
<li><strong>Handling NULLs</strong>: Use <code>.isNull()</code> and <code>.isNotNull()</code> to filter rows with missing or available values, such as missing addresses or valid emails.</li>
<li><strong>Unique/Distinct Values</strong>: Use <code>.distinct()</code> to get unique rows or distinct values in a column. Remove duplicates based on specific fields like Email or Phone using <code>.dropDuplicates()</code>.</li>
<li><strong>Count Distinct Values</strong>: Count distinct values in one or multiple columns to analyze data diversity, such as counting unique departments or combinations of Department and Performance_Rating.</li>
</ol>

  <footer class="footline">
  </footer>
</article>
<article class="default">
  <header class="headline">
  </header>

<h1 id="functions">Functions</h1>

<h2 id="sorting-and-string-functions">Sorting and String Functions</h2>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">col</span><span class="p">,</span> <span class="n">desc</span><span class="p">,</span> <span class="n">asc</span><span class="p">,</span> <span class="n">concat</span><span class="p">,</span> <span class="n">concat_ws</span><span class="p">,</span> <span class="n">initcap</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="p">,</span> <span class="n">instr</span><span class="p">,</span> <span class="n">length</span><span class="p">,</span> <span class="n">lit</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create a Spark session</span>
</span></span><span class="line"><span class="cl"><span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&#34;SortingAndStringFunctions&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Sample data</span>
</span></span><span class="line"><span class="cl"><span class="n">data</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">   <span class="p">(</span><span class="s2">&#34;USA&#34;</span><span class="p">,</span> <span class="s2">&#34;North America&#34;</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mf">50.5</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">   <span class="p">(</span><span class="s2">&#34;India&#34;</span><span class="p">,</span> <span class="s2">&#34;Asia&#34;</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mf">20.0</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">   <span class="p">(</span><span class="s2">&#34;Germany&#34;</span><span class="p">,</span> <span class="s2">&#34;Europe&#34;</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mf">30.5</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">   <span class="p">(</span><span class="s2">&#34;Australia&#34;</span><span class="p">,</span> <span class="s2">&#34;Oceania&#34;</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mf">60.0</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">   <span class="p">(</span><span class="s2">&#34;Japan&#34;</span><span class="p">,</span> <span class="s2">&#34;Asia&#34;</span><span class="p">,</span> <span class="mi">120</span><span class="p">,</span> <span class="mf">45.0</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">   <span class="p">(</span><span class="s2">&#34;Brazil&#34;</span><span class="p">,</span> <span class="s2">&#34;South America&#34;</span><span class="p">,</span> <span class="mi">180</span><span class="p">,</span> <span class="mf">25.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Define the schema</span>
</span></span><span class="line"><span class="cl"><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;Country&#34;</span><span class="p">,</span> <span class="s2">&#34;Region&#34;</span><span class="p">,</span> <span class="s2">&#34;UnitsSold&#34;</span><span class="p">,</span> <span class="s2">&#34;UnitPrice&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create DataFrame</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Display the original DataFrame</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></span></span></code></pre></div>
<h3 id="sorting-the-dataframe">Sorting the DataFrame</h3>
<h3 id="1-sort-by-a-single-column-ascending-order">1. Sort by a single column (ascending order)</h3>
<p><code>df.orderBy(&quot;Country&quot;).show(5)</code></p>
<blockquote>
<p>By default, sorting is ascending. This shows the first 5 countries alphabetically.</p></blockquote>
<h3 id="2-sort-by-multiple-columns">2. Sort by multiple columns</h3>
<p><code>df.orderBy(&quot;Country&quot;, &quot;UnitsSold&quot;).show(5)</code></p>
<blockquote>
<p>First sorts by <code>Country</code>, then within each country sorts by <code>UnitsSold</code>.</p></blockquote>
<h3 id="3-sort-by-column-in-descending-order-and-limit">3. Sort by column in descending order and limit</h3>
<p><code>df.orderBy(desc(&quot;Country&quot;)).limit(3).show(5)</code></p>
<blockquote>
<p>Sorts by <code>Country</code> in descending order and returns the top 3 rows.</p></blockquote>
<h3 id="4-sorting-with-null-values-last">4. Sorting with null values last</h3>
<p><code>df.orderBy(col(&quot;Country&quot;).desc(), nulls_last=True).show(5)</code></p>
<blockquote>
<p>Ensures null values appear at the end when sorting.</p></blockquote>
<p><strong>Key Functions:</strong></p>
<ul>
<li>
<p>Use <code>.orderBy()</code> or <code>.sort()</code> to sort DataFrames.</p>
</li>
<li>
<p>Control order with <code>asc()</code> or <code>desc()</code>.</p>
</li>
</ul>
<hr>
<h2 id="string-functions">String Functions</h2>
<h3 id="1-capitalize-first-letter-of-each-word">1. Capitalize first letter of each word</h3>
<p><code>df.select(initcap(col(&quot;Country&quot;))).show()</code></p>
<blockquote>
<p>Converts <code>&quot;united states&quot;</code> â†’ <code>&quot;United States&quot;</code>.</p></blockquote>
<h3 id="2-convert-all-text-to-lowercase">2. Convert all text to lowercase</h3>
<p><code>df.select(lower(col(&quot;Country&quot;))).show()</code></p>
<h3 id="3-convert-all-text-to-uppercase">3. Convert all text to uppercase</h3>
<p><code>df.select(upper(col(&quot;Country&quot;))).show()</code></p>
<p><strong>Key Functions:</strong></p>
<ul>
<li>
<p><code>initcap()</code> â†’ Capitalize first letter of each word.</p>
</li>
<li>
<p><code>lower()</code> â†’ Convert to lowercase.</p>
</li>
<li>
<p><code>upper()</code> â†’ Convert to uppercase.</p>
</li>
</ul>
<hr>
<h2 id="concatenation-functions">Concatenation Functions</h2>
<h3 id="1-concatenate-two-columns">1. Concatenate two columns</h3>
<p><code>df.select(concat(col(&quot;Region&quot;), col(&quot;Country&quot;))).show()</code></p>
<blockquote>
<p>Joins <code>Region</code> and <code>Country</code> without separator.</p></blockquote>
<h3 id="2-concatenate-with-a-separator">2. Concatenate with a separator</h3>
<p><code>df.select(concat_ws(&quot; | &quot;, col(&quot;Region&quot;), col(&quot;Country&quot;))).show()</code></p>
<blockquote>
<p>Joins with <code>&quot; | &quot;</code> as separator.</p></blockquote>
<h3 id="3-create-a-new-concatenated-column">3. Create a new concatenated column</h3>
<p><code>df.withColumn(&quot;concatenated&quot;, concat(df[&quot;Region&quot;], lit(&quot; &quot;), df[&quot;Country&quot;])).show()</code></p>
<blockquote>
<p>Adds a new column combining <code>Region</code> and <code>Country</code>.</p></blockquote>
<p><strong>Key Functions:</strong></p>
<ul>
<li>
<p><code>concat()</code> â†’ Join columns directly.</p>
</li>
<li>
<p><code>concat_ws()</code> â†’ Join columns with a separator.</p>
</li>
</ul>
<hr>
<h3 id="-summary">ðŸ“Œ Summary</h3>
<ul>
<li><strong>Sorting:</strong> Use <code>.orderBy()</code> or <code>.sort()</code> with <code>asc()</code> / <code>desc()</code>.</li>
<li><strong>String Manipulation:</strong> Use <code>initcap()</code>, <code>lower()</code>, <code>upper()</code>.</li>
<li><strong>Concatenation:</strong> Use <code>concat()</code> or <code>concat_ws()</code> for flexible joins.</li>
</ul>
<hr>
<h2 id="split-function-in-dataframe">Split Function in DataFrame</h2>
<p>Letâ€™s create a PySpark DataFrame for employee data with columns such as <strong>EmployeeID, Name, Department, and Skills</strong>. Weâ€™ll explore <code>split</code>, <code>explode</code>, and other useful array functions.</p>
<h3 id="sample-data-creation-for-employee-data">Sample Data Creation for Employee Data</h3>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">split</span><span class="p">,</span> <span class="n">explode</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">array_contains</span><span class="p">,</span> <span class="n">col</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Sample employee data</span>
</span></span><span class="line"><span class="cl"><span class="n">data</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">   <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s2">&#34;Alice&#34;</span><span class="p">,</span> <span class="s2">&#34;HR&#34;</span><span class="p">,</span> <span class="s2">&#34;Communication Management&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">   <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="s2">&#34;Bob&#34;</span><span class="p">,</span> <span class="s2">&#34;IT&#34;</span><span class="p">,</span> <span class="s2">&#34;Programming Networking&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">   <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="s2">&#34;Charlie&#34;</span><span class="p">,</span> <span class="s2">&#34;Finance&#34;</span><span class="p">,</span> <span class="s2">&#34;Accounting Analysis&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">   <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="s2">&#34;David&#34;</span><span class="p">,</span> <span class="s2">&#34;HR&#34;</span><span class="p">,</span> <span class="s2">&#34;Recruiting Communication&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">   <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="s2">&#34;Eve&#34;</span><span class="p">,</span> <span class="s2">&#34;IT&#34;</span><span class="p">,</span> <span class="s2">&#34;Cloud DevOps&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Define the schema</span>
</span></span><span class="line"><span class="cl"><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;EmployeeID&#34;</span><span class="p">,</span> <span class="s2">&#34;Name&#34;</span><span class="p">,</span> <span class="s2">&#34;Department&#34;</span><span class="p">,</span> <span class="s2">&#34;Skills&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create DataFrame</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Display the original DataFrame</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="n">truncate</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></span></span></code></pre></div>
<h3 id="examples">Examples</h3>
<h3 id="1-split-the-skills-column">1. Split the <code>Skills</code> column</h3>
<p><code>df.select(col(&quot;EmployeeID&quot;), col(&quot;Name&quot;), split(col(&quot;Skills&quot;), &quot; &quot;).alias(&quot;Skills_Array&quot;)).show(truncate=False)</code></p>
<blockquote>
<p>Splits the <code>Skills</code> column into an array of skills using space as a delimiter.</p></blockquote>
<hr>
<h3 id="2-select-the-first-skill-from-skills_array">2. Select the first skill from <code>Skills_Array</code></h3>
<p><code>df2.select(col(&quot;EmployeeID&quot;), col(&quot;Name&quot;), col(&quot;Skills_Array&quot;)[0].alias(&quot;First_Skill&quot;)).show(truncate=False)</code></p>
<blockquote>
<p>Uses index notation (<code>Skills_Array[0]</code>) to pick the first skill. Indexing starts from <strong>0</strong>.</p></blockquote>
<hr>
<h3 id="3-count-the-number-of-skills-per-employee">3. Count the number of skills per employee</h3>
<p><code>df2.select(col(&quot;EmployeeID&quot;), col(&quot;Name&quot;), size(col(&quot;Skills_Array&quot;)).alias(&quot;Number_of_Skills&quot;)).show(truncate=False)</code></p>
<blockquote>
<p>The <code>size()</code> function returns the number of elements in the array.</p></blockquote>
<hr>
<h3 id="4-check-if-the-employee-has-cloud-skill">4. Check if the employee has &ldquo;Cloud&rdquo; skill</h3>
<p><code>df.select(col(&quot;EmployeeID&quot;), col(&quot;Name&quot;), array_contains(split(col(&quot;Skills&quot;), &quot; &quot;), &quot;Cloud&quot;).alias(&quot;Has_Cloud_Skill&quot;)).show(truncate=False)</code>
`</p>
<blockquote>
<p><code>array_contains()</code> returns <strong>True</strong> if <code>&quot;Cloud&quot;</code> exists in the skill set.</p></blockquote>
<hr>
<h3 id="5-explode-the-skills_array-into-multiple-rows">5. Explode the <code>Skills_Array</code> into multiple rows</h3>
<p><code>df3 = df2.withColumn(&quot;Skill&quot;, explode(col(&quot;Skills_Array&quot;)))</code></p>
<p><code>df3.select(&quot;EmployeeID&quot;, &quot;Name&quot;, &quot;Skill&quot;).show(truncate=False)</code></p>
<blockquote>
<p><code>explode()</code> flattens the array into rows, where each skill becomes a separate row for that employee.</p></blockquote>
<hr>
<h3 id="-summary-of-key-functions">ðŸ“Œ Summary of Key Functions</h3>
<ul>
<li><strong>split()</strong> â†’ Splits a string into an array.</li>
<li><strong>explode()</strong> â†’ Converts an array into multiple rows.</li>
<li><strong>size()</strong> â†’ Counts elements in an array.</li>
<li><strong>array_contains()</strong> â†’ Checks if an array contains a value.</li>
<li><strong>selectExpr()</strong> â†’ Lets you query arrays using SQL expressions like <code>Skills_Array[0]</code>.</li>
</ul>
<hr>
<hr>
<h2 id="trim-function-in-dataframe">Trim Function in DataFrame</h2>
<p>Letâ€™s create a sample dataset for employees and demonstrate string trimming and padding functions in PySpark:</p>
<ul>
<li><code>ltrim()</code></li>
<li><code>rtrim()</code></li>
<li><code>trim()</code></li>
<li><code>lpad()</code></li>
<li><code>rpad()</code></li>
</ul>
<hr>
<h3 id="sample-data-creation-for-employees">Sample Data Creation for Employees</h3>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">lit</span><span class="p">,</span> <span class="n">ltrim</span><span class="p">,</span> <span class="n">rtrim</span><span class="p">,</span> <span class="n">rpad</span><span class="p">,</span> <span class="n">lpad</span><span class="p">,</span> <span class="n">trim</span><span class="p">,</span> <span class="n">col</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Sample employee data with leading and trailing spaces in the &#39;Name&#39; column</span>
</span></span><span class="line"><span class="cl"><span class="n">data</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">   <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s2">&#34; Alice   &#34;</span><span class="p">,</span> <span class="s2">&#34;HR&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">   <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="s2">&#34;  Bob&#34;</span><span class="p">,</span> <span class="s2">&#34;IT&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">   <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="s2">&#34;Charlie  &#34;</span><span class="p">,</span> <span class="s2">&#34;Finance&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">   <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="s2">&#34;  David &#34;</span><span class="p">,</span> <span class="s2">&#34;HR&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">   <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="s2">&#34;Eve  &#34;</span><span class="p">,</span> <span class="s2">&#34;IT&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Define the schema for the DataFrame</span>
</span></span><span class="line"><span class="cl"><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;EmployeeID&#34;</span><span class="p">,</span> <span class="s2">&#34;Name&#34;</span><span class="p">,</span> <span class="s2">&#34;Department&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create DataFrame</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Show the original DataFrame</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="n">truncate</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></span></span></code></pre></div>
<h3 id="applying-trimming-and-padding-functions">Applying Trimming and Padding Functions</h3>
<h3 id="1-trimming-functions">1. Trimming Functions</h3>
<ul>
<li>
<p><strong><code>ltrim()</code></strong> â†’ Removes leading spaces.</p>
</li>
<li>
<p><strong><code>rtrim()</code></strong> â†’ Removes trailing spaces.</p>
</li>
<li>
<p><strong><code>trim()</code></strong> â†’ Removes both leading and trailing spaces.</p>
</li>
</ul>
<h3 id="2-padding-functions">2. Padding Functions</h3>
<ul>
<li>
<p><strong><code>lpad()</code></strong> â†’ Pads the left side of a string with a character up to a given length.</p>
</li>
<li>
<p><strong><code>rpad()</code></strong> â†’ Pads the right side of a string with a character up to a given length.</p>
</li>
</ul>
<hr>
<h3 id="example">Example</h3>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Apply trimming and padding functions</span>
</span></span><span class="line"><span class="cl"><span class="n">result_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">col</span><span class="p">(</span><span class="s2">&#34;EmployeeID&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">col</span><span class="p">(</span><span class="s2">&#34;Department&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">ltrim</span><span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s2">&#34;Name&#34;</span><span class="p">))</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&#34;ltrim_Name&#34;</span><span class="p">),</span>   <span class="c1"># Remove leading spaces</span>
</span></span><span class="line"><span class="cl">    <span class="n">rtrim</span><span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s2">&#34;Name&#34;</span><span class="p">))</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&#34;rtrim_Name&#34;</span><span class="p">),</span>   <span class="c1"># Remove trailing spaces</span>
</span></span><span class="line"><span class="cl">    <span class="n">trim</span><span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s2">&#34;Name&#34;</span><span class="p">))</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&#34;trim_Name&#34;</span><span class="p">),</span>     <span class="c1"># Remove both leading &amp; trailing spaces</span>
</span></span><span class="line"><span class="cl">    <span class="n">lpad</span><span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s2">&#34;Name&#34;</span><span class="p">),</span> <span class="mi">10</span><span class="p">,</span> <span class="s2">&#34;X&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&#34;lpad_Name&#34;</span><span class="p">),</span>  <span class="c1"># Left pad with &#34;X&#34; to length 10</span>
</span></span><span class="line"><span class="cl">    <span class="n">rpad</span><span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s2">&#34;Name&#34;</span><span class="p">),</span> <span class="mi">10</span><span class="p">,</span> <span class="s2">&#34;Y&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&#34;rpad_Name&#34;</span><span class="p">)</span>   <span class="c1"># Right pad with &#34;Y&#34; to length 10</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Show the resulting DataFrame</span>
</span></span><span class="line"><span class="cl"><span class="n">result_df</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="n">truncate</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></span></span></code></pre></div>
<hr>
<h3 id="output-explanation">Output Explanation</h3>
<ul>
<li>
<p><strong><code>ltrim_Name</code></strong> â†’ Leading spaces removed.</p>
</li>
<li>
<p><strong><code>rtrim_Name</code></strong> â†’ Trailing spaces removed.</p>
</li>
<li>
<p><strong><code>trim_Name</code></strong> â†’ Both leading &amp; trailing spaces removed.</p>
</li>
<li>
<p><strong><code>lpad_Name</code></strong> â†’ Padded left with <code>&quot;X&quot;</code> until length = 10.</p>
</li>
<li>
<p><strong><code>rpad_Name</code></strong> â†’ Padded right with <code>&quot;Y&quot;</code> until length = 10.</p>
</li>
</ul>
<hr>
<h3 id="-summary-1">ðŸ“Œ Summary</h3>
<ul>
<li>Use <strong>trim functions</strong> (<code>ltrim</code>, <code>rtrim</code>, <code>trim</code>) to clean up unwanted spaces.</li>
<li>Use <strong>padding functions</strong> (<code>lpad</code>, <code>rpad</code>) to format strings with fixed lengths.</li>
</ul>

  <footer class="footline">
  </footer>
</article>
<article class="default">
  <header class="headline">
  </header>

<h1 id="date-functions">Date Functions</h1>

<p>In PySpark, you can use various date functions to manipulate and analyze date and timestamp columns.<br>
Weâ€™ll explore:</p>
<ul>
<li><code>current_date</code></li>
<li><code>current_timestamp</code></li>
<li><code>date_add</code></li>
<li><code>date_sub</code></li>
<li><code>datediff</code></li>
<li><code>months_between</code></li>
</ul>
<hr>
<h3 id="sample-code">Sample Code</h3>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">current_date</span><span class="p">,</span> <span class="n">current_timestamp</span><span class="p">,</span> <span class="n">date_add</span><span class="p">,</span> <span class="n">date_sub</span><span class="p">,</span> <span class="n">col</span><span class="p">,</span> <span class="n">datediff</span><span class="p">,</span> <span class="n">months_between</span><span class="p">,</span> <span class="n">to_date</span><span class="p">,</span> <span class="n">lit</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Generate a DataFrame with 10 rows, adding &#34;today&#34; and &#34;now&#34; columns</span>
</span></span><span class="line"><span class="cl"><span class="n">dateDF</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&#34;today&#34;</span><span class="p">,</span> <span class="n">current_date</span><span class="p">())</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&#34;now&#34;</span><span class="p">,</span> <span class="n">current_timestamp</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Show the DataFrame with today and now columns</span>
</span></span><span class="line"><span class="cl"><span class="n">dateDF</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="n">truncate</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></span></span></code></pre></div>
<h3 id="key-functions">Key Functions</h3>
<ul>
<li><strong><code>current_date()</code></strong> â†’ Returns current date.</li>
<li><strong><code>current_timestamp()</code></strong> â†’ Returns current timestamp (date + time).</li>
<li><strong><code>date_sub(col(&quot;today&quot;), 5)</code></strong> â†’ Subtracts 5 days.</li>
<li><strong><code>date_add(col(&quot;today&quot;), 5)</code></strong> â†’ Adds 5 days.</li>
<li><strong><code>datediff(date1, date2)</code></strong> â†’ Returns difference in days.</li>
<li><strong><code>months_between(date1, date2)</code></strong> â†’ Returns difference in months.</li>
</ul>
<hr>
<p>Working with dates and timestamps often requires converting formats and extracting components.<br>
Weâ€™ll explore:</p>
<ul>
<li><code>to_date</code></li>
<li><code>to_timestamp</code></li>
<li><code>year</code>, <code>month</code>, <code>dayofmonth</code></li>
<li><code>hour</code>, <code>minute</code>, <code>second</code></li>
</ul>
<hr>
<h3 id="1-to_date">1. to_date</h3>
<ul>
<li>Converts a string to a date (default format: <code>yyyy-MM-dd</code>).</li>
<li>If format doesnâ€™t match, returns <strong>null</strong>.</li>
<li>Example:
<code>to_date(lit(&quot;2017-12-11&quot;), &quot;yyyy-dd-MM&quot;)</code></li>
</ul>
<h3 id="2-to_timestamp">2. to_timestamp</h3>
<ul>
<li>Converts a string with date &amp; time into a timestamp.</li>
<li>Allows extraction of time components.</li>
</ul>
<h3 id="3-extracting-components">3. Extracting Components</h3>
<ul>
<li><strong>year()</strong> â†’ Extracts year.</li>
<li><strong>month()</strong> â†’ Extracts month.</li>
<li><strong>dayofmonth()</strong> â†’ Extracts day.</li>
<li><strong>hour()</strong> â†’ Extracts hour.</li>
<li><strong>minute()</strong> â†’ Extracts minute.</li>
<li><strong>second()</strong> â†’ Extracts second.</li>
</ul>
<h3 id="example-output">Example Output</h3>
<p>For input <code>&quot;2017-12-11&quot;</code> (format <code>yyyy-dd-MM</code>):</p>
<ul>
<li>Year: <strong>2017</strong></li>
<li>Month: <strong>12</strong></li>
<li>Day: <strong>11</strong></li>
<li>Hour: <strong>0</strong></li>
<li>Minute: <strong>0</strong></li>
<li>Second: <strong>0</strong></li>
</ul>
<p>For invalid input (e.g., <code>&quot;2017-20-12&quot;</code>):</p>
<ul>
<li>Result: <strong>null</strong></li>
</ul>

  <footer class="footline">
  </footer>
</article>
<article class="default">
  <header class="headline">
  </header>

<h1 id="handling-nulls">Handling Nulls</h1>

<h2 id="sample-sales-data-with-null-values">Sample Sales Data with Null Values</h2>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Sample data: sales data with nulls</span>
</span></span><span class="line"><span class="cl"><span class="n">data</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;John&#34;</span><span class="p">,</span> <span class="s2">&#34;North&#34;</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Doe&#34;</span><span class="p">,</span> <span class="s2">&#34;East&#34;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="s2">&#34;West&#34;</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">30</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Alice&#34;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">40</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Bob&#34;</span><span class="p">,</span> <span class="s2">&#34;South&#34;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;Name&#34;</span><span class="p">,</span> <span class="s2">&#34;Region&#34;</span><span class="p">,</span> <span class="s2">&#34;UnitsSold&#34;</span><span class="p">,</span> <span class="s2">&#34;Revenue&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create DataFrame</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></span></span></code></pre></div>
<h3 id="1-detecting-null-values">1. Detecting Null Values</h3>
<ul>
<li>Use <strong><code>isNull()</code></strong> to identify rows where a column contains null values.</li>
<li>The output is a boolean flag indicating whether the value is null.</li>
</ul>
<h3 id="2-dropping-rows-with-null-values">2. Dropping Rows with Null Values</h3>
<ul>
<li><strong><code>dropna()</code></strong> removes rows with nulls in any column (default mode).</li>
<li>Use <strong><code>how=&quot;all&quot;</code></strong> to remove rows only if <em>all</em> columns are null.</li>
<li>Use <strong><code>subset=[&quot;col1&quot;, &quot;col2&quot;]</code></strong> to target specific columns.</li>
</ul>
<h3 id="3-filling-null-values">3. Filling Null Values</h3>
<ul>
<li><strong><code>fillna()</code></strong> replaces nulls with specified default values.</li>
<li>Replace across all columns or selectively.</li>
<li>Example:
<ul>
<li>Replace <code>Region</code> nulls with <code>&quot;Unknown&quot;</code>.</li>
<li>Replace <code>UnitsSold</code> and <code>Revenue</code> nulls with <code>0</code>.</li>
</ul>
</li>
</ul>
<h3 id="4-coalesce-function">4. Coalesce Function</h3>
<ul>
<li><strong><code>coalesce()</code></strong> returns the first non-null value among multiple columns.</li>
<li>Useful when providing fallback values if some columns contain nulls.</li>
</ul>
<h3 id="5-handling-nulls-in-aggregations">5. Handling Nulls in Aggregations</h3>
<ul>
<li>Nulls can distort aggregations like <code>mean()</code>.</li>
<li>Use <strong><code>coalesce()</code></strong> to substitute nulls with defaults (e.g., <code>0.0</code>).</li>
<li>This prevents inaccurate results.</li>
</ul>
<h3 id="-summary">ðŸ“Œ Summary</h3>
<ol>
<li><strong>Detecting Nulls</strong>: Use <code>isNull()</code> to find null values.</li>
<li><strong>Dropping Nulls</strong>: Use <code>dropna()</code> to remove rows with nulls (all or specific columns).</li>
<li><strong>Filling Nulls</strong>: Use <code>fillna()</code> to replace nulls with defaults.</li>
<li><strong>Coalesce Function</strong>: Use <code>coalesce()</code> to return the first non-null value.</li>
<li><strong>Aggregations</strong>: Use <code>coalesce()</code> in aggregations to handle nulls safely.</li>
</ol>

  <footer class="footline">
  </footer>
</article>
<article class="default">
  <header class="headline">
  </header>

<h1 id="aggregate-functions">Aggregate functions</h1>

<h2 id="basic-aggregate-functions">Basic Aggregate Functions</h2>
<h3 id="sample-data">Sample Data</h3>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">Row</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create sample data</span>
</span></span><span class="line"><span class="cl"><span class="n">data</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="n">Row</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">Row</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">20</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">Row</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">30</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">Row</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">Row</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">40</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">Row</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create DataFrame</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Show the DataFrame</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></span></span></code></pre></div>
<h3 id="aggregate-functions-in-pyspark">Aggregate Functions in PySpark</h3>
<ol>
<li><strong>Summation (<code>sum</code>)</strong> â€“ Adds up the values in a column.</li>
<li><strong>Average (<code>avg</code>)</strong> â€“ Computes the average of values in a column.</li>
<li><strong>Count (<code>count</code>)</strong> â€“ Counts the number of non-null values in a column.</li>
<li><strong>Maximum (<code>max</code>) / Minimum (<code>min</code>)</strong> â€“ Finds the highest and lowest values.</li>
<li><strong>Distinct Count (<code>countDistinct</code>)</strong> â€“ Counts unique values in a column.</li>
</ol>
<h3 id="notes">Notes</h3>
<ul>
<li>
<p><strong>Handling Nulls</strong>:</p>
<ul>
<li><code>count()</code> counts only <strong>non-null</strong> values.</li>
<li><code>sum()</code>, <code>avg()</code>, <code>max()</code>, and <code>min()</code> ignore null values.</li>
</ul>
</li>
<li>
<p><strong>Performance</strong>:<br>
Aggregate functions can be expensive on large datasets; partitioning improves performance.</p>
</li>
<li>
<p><strong>Use Cases</strong>:</p>
<ul>
<li><strong>Summation</strong>: Total sales, total revenue.</li>
<li><strong>Average</strong>: Average sales per day.</li>
<li><strong>Count</strong>: Number of transactions.</li>
<li><strong>Max/Min</strong>: Highest and lowest values (e.g., max sales in a day).</li>
<li><strong>Distinct Count</strong>: Unique customers, unique products.</li>
</ul>
</li>
</ul>
<hr>
<h2 id="advanced-aggregation-functions">Advanced Aggregation Functions</h2>
<h3 id="sample-data-1">Sample Data</h3>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">functions</span> <span class="k">as</span> <span class="n">F</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">StructType</span><span class="p">,</span> <span class="n">StructField</span><span class="p">,</span> <span class="n">StringType</span><span class="p">,</span> <span class="n">IntegerType</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create Spark session</span>
</span></span><span class="line"><span class="cl"><span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&#34;AggregationExamples&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Sample data</span>
</span></span><span class="line"><span class="cl"><span class="n">data</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;HR&#34;</span><span class="p">,</span> <span class="mi">10000</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="s2">&#34;John&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Finance&#34;</span><span class="p">,</span> <span class="mi">20000</span><span class="p">,</span> <span class="mi">1500</span><span class="p">,</span> <span class="s2">&#34;Doe&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;HR&#34;</span><span class="p">,</span> <span class="mi">15000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="s2">&#34;Alice&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Finance&#34;</span><span class="p">,</span> <span class="mi">25000</span><span class="p">,</span> <span class="mi">2000</span><span class="p">,</span> <span class="s2">&#34;Eve&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;HR&#34;</span><span class="p">,</span> <span class="mi">20000</span><span class="p">,</span> <span class="mi">1500</span><span class="p">,</span> <span class="s2">&#34;Mark&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Define schema</span>
</span></span><span class="line"><span class="cl"><span class="n">schema</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">([</span>
</span></span><span class="line"><span class="cl">    <span class="n">StructField</span><span class="p">(</span><span class="s2">&#34;department&#34;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="kc">True</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">StructField</span><span class="p">(</span><span class="s2">&#34;salary&#34;</span><span class="p">,</span> <span class="n">IntegerType</span><span class="p">(),</span> <span class="kc">True</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">StructField</span><span class="p">(</span><span class="s2">&#34;bonus&#34;</span><span class="p">,</span> <span class="n">IntegerType</span><span class="p">(),</span> <span class="kc">True</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">StructField</span><span class="p">(</span><span class="s2">&#34;employee_name&#34;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create DataFrame</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">schema</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></span></span></code></pre></div>
<h3 id="1-grouped-aggregation">1. Grouped Aggregation</h3>
<p>Perform aggregation within groups based on a column.</p>
<ul>
<li><strong><code>sum()</code></strong> â†’ Adds values within the group.</li>
<li><strong><code>avg()</code></strong> â†’ Computes group average.</li>
<li><strong><code>max()</code></strong> â†’ Finds maximum value.</li>
<li><strong><code>min()</code></strong> â†’ Finds minimum value.</li>
</ul>
<h3 id="2-multiple-aggregations">2. Multiple Aggregations</h3>
<p>Perform several aggregations in one step.</p>
<ul>
<li><strong><code>count()</code></strong> â†’ Number of rows in each group.</li>
<li><strong><code>avg()</code></strong> â†’ Average of values.</li>
<li><strong><code>max()</code></strong> â†’ Maximum value in group.</li>
</ul>
<h3 id="3-concatenate-strings">3. Concatenate Strings</h3>
<ul>
<li><strong><code>concat_ws()</code></strong> â†’ Concatenates string values within a column, separated by a delimiter (<code>,</code>).</li>
</ul>
<h3 id="4-first-and-last">4. First and Last</h3>
<ul>
<li><strong><code>first()</code></strong> â†’ Retrieves the first value of a column in a group.</li>
<li><strong><code>last()</code></strong> â†’ Retrieves the last value of a column in a group.</li>
</ul>
<h3 id="5-standard-deviation-and-variance">5. Standard Deviation and Variance</h3>
<ul>
<li><strong><code>stddev()</code></strong> â†’ Standard deviation of values.</li>
<li><strong><code>variance()</code></strong> â†’ Variance of values.</li>
</ul>
<h3 id="6-aggregation-with-alias">6. Aggregation with Alias</h3>
<ul>
<li><strong><code>.alias()</code></strong> â†’ Rename the result columns after aggregation.</li>
</ul>
<h3 id="7-sum-of-distinct-values">7. Sum of Distinct Values</h3>
<ul>
<li><strong><code>sumDistinct()</code></strong> â†’ Sums only <strong>unique values</strong> in a column (avoids double-counting duplicates).</li>
</ul>
<h3 id="-summary">ðŸ“Œ Summary</h3>
<ul>
<li>
<p>Use basic aggregations (<code>sum</code>, <code>avg</code>, <code>count</code>, <code>max</code>, <code>min</code>, <code>countDistinct</code>) for general metrics.</p>
</li>
<li>
<p>Apply advanced aggregations (<code>grouped</code>, <code>concat_ws</code>, <code>first</code>, <code>last</code>, <code>stddev</code>, <code>variance</code>, <code>sumDistinct</code>) for deeper analysis.</p>
</li>
<li>
<p>Always consider <strong>null handling</strong> and <strong>performance optimizations</strong> when using aggregate functions in PySpark.</p>
</li>
</ul>

  <footer class="footline">
  </footer>
</article>
<article class="default">
  <header class="headline">
  </header>

<h1 id="joins">Joins</h1>

<h2 id="joins-in-pyspark">Joins in PySpark</h2>
<p>In PySpark Joins are used to combine two DataFrames based on a common column or condition.</p>
<hr>
<h3 id="types-of-joins-in-pyspark">Types of Joins in PySpark</h3>
<ul>
<li><strong>Inner Join</strong>: Matches rows from both DataFrames.</li>
</ul>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">df1</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df2</span><span class="p">,</span> <span class="n">df1</span><span class="o">.</span><span class="n">common_column</span> <span class="o">==</span> <span class="n">df2</span><span class="o">.</span><span class="n">common_column</span><span class="p">,</span> <span class="s2">&#34;inner&#34;</span><span class="p">)</span></span></span></code></pre></div>
<ul>
<li><strong>Left/Right Join</strong>: Keeps all rows from the left or right DataFrame and matches where possible.</li>
</ul>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"> <span class="n">df1</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df2</span><span class="p">,</span> <span class="n">df1</span><span class="o">.</span><span class="n">common_column</span> <span class="o">==</span> <span class="n">df2</span><span class="o">.</span><span class="n">common_column</span><span class="p">,</span> <span class="s2">&#34;left&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"> <span class="n">df1</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df2</span><span class="p">,</span> <span class="n">df1</span><span class="o">.</span><span class="n">common_column</span> <span class="o">==</span> <span class="n">df2</span><span class="o">.</span><span class="n">common_column</span><span class="p">,</span> <span class="s2">&#34;right&#34;</span><span class="p">)</span></span></span></code></pre></div>
<ul>
<li><strong>Full Join</strong>: Keeps all rows from both DataFrames.</li>
</ul>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">df1</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df2</span><span class="p">,</span> <span class="n">df1</span><span class="o">.</span><span class="n">common_column</span> <span class="o">==</span> <span class="n">df2</span><span class="o">.</span><span class="n">common_column</span><span class="p">,</span> <span class="s2">&#34;outer&#34;</span><span class="p">)</span></span></span></code></pre></div>
<ul>
<li><strong>Left Semi</strong>: Filters <code>df1</code> to rows that match <code>df2</code> without including columns from <code>df2</code>.</li>
</ul>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">df1</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df2</span><span class="p">,</span> <span class="n">df1</span><span class="o">.</span><span class="n">common_column</span> <span class="o">==</span> <span class="n">df2</span><span class="o">.</span><span class="n">common_column</span><span class="p">,</span> <span class="s2">&#34;left_semi&#34;</span><span class="p">)</span> </span></span></code></pre></div>
<ul>
<li><strong>Left Anti</strong>: Filters <code>df1</code> to rows that do not match <code>df2</code>.</li>
</ul>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">df1</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df2</span><span class="p">,</span> <span class="n">df1</span><span class="o">.</span><span class="n">common_column</span> <span class="o">==</span> <span class="n">df2</span><span class="o">.</span><span class="n">common_column</span><span class="p">,</span> <span class="s2">&#34;left_anti&#34;</span><span class="p">)</span></span></span></code></pre></div>
<ul>
<li><strong>Cross Join</strong>: Returns the Cartesian product, combining all rows of both DataFrames.</li>
</ul>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">df1</span><span class="o">.</span><span class="n">crossJoin</span><span class="p">(</span><span class="n">df2</span><span class="p">)</span></span></span></code></pre></div>
<ul>
<li><strong>Explicit Condition Join</strong>: Allows complex join conditions, including columns with different names.</li>
</ul>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">df1</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df2</span><span class="p">,</span> <span class="n">df1</span><span class="o">.</span><span class="n">columnA</span> <span class="o">==</span> <span class="n">df2</span><span class="o">.</span><span class="n">columnB</span><span class="p">,</span> <span class="s2">&#34;inner&#34;</span><span class="p">)</span></span></span></code></pre></div>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">df1</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df2</span><span class="p">,</span> <span class="n">df1</span><span class="o">.</span><span class="n">common_column</span> <span class="o">==</span> <span class="n">df2</span><span class="o">.</span><span class="n">common_column</span><span class="p">,</span> <span class="s2">&#34;inner&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">df1</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df2</span><span class="p">,</span> <span class="n">df1</span><span class="o">.</span><span class="n">common_column</span> <span class="o">==</span> <span class="n">df2</span><span class="o">.</span><span class="n">common_column</span><span class="p">,</span> <span class="s2">&#34;left&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">df1</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df2</span><span class="p">,</span> <span class="n">df1</span><span class="o">.</span><span class="n">common_column</span> <span class="o">==</span> <span class="n">df2</span><span class="o">.</span><span class="n">common_column</span><span class="p">,</span> <span class="s2">&#34;right&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">df1</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df2</span><span class="p">,</span> <span class="n">df1</span><span class="o">.</span><span class="n">common_column</span> <span class="o">==</span> <span class="n">df2</span><span class="o">.</span><span class="n">common_column</span><span class="p">,</span> <span class="s2">&#34;outer&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">df1</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df2</span><span class="p">,</span> <span class="n">df1</span><span class="o">.</span><span class="n">common_column</span> <span class="o">==</span> <span class="n">df2</span><span class="o">.</span><span class="n">common_column</span><span class="p">,</span> <span class="s2">&#34;left_semi&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">df1</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df2</span><span class="p">,</span> <span class="n">df1</span><span class="o">.</span><span class="n">common_column</span> <span class="o">==</span> <span class="n">df2</span><span class="o">.</span><span class="n">common_column</span><span class="p">,</span> <span class="s2">&#34;left_anti&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">df1</span><span class="o">.</span><span class="n">crossJoin</span><span class="p">(</span><span class="n">df2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">df1</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df2</span><span class="p">,</span> <span class="n">df1</span><span class="o">.</span><span class="n">columnA</span> <span class="o">==</span> <span class="n">df2</span><span class="o">.</span><span class="n">columnB</span><span class="p">,</span> <span class="s2">&#34;inner&#34;</span><span class="p">)</span></span></span></code></pre></div>
<hr>
<h3 id="practice-1">Practice-1</h3>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">Row</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">broadcast</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Initialize Spark session</span>
</span></span><span class="line"><span class="cl"><span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&#34;JoinsExample&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Sample DataFrames</span>
</span></span><span class="line"><span class="cl"><span class="n">data1</span> <span class="o">=</span> <span class="p">[</span><span class="n">Row</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">Row</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">Row</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">Row</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span> <span class="n">Row</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="kc">None</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl"><span class="n">data2</span> <span class="o">=</span> <span class="p">[</span><span class="n">Row</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">Row</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">Row</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="kc">None</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">df1</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">data1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">df2</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">data2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Inner Join</span>
</span></span><span class="line"><span class="cl"><span class="n">inner_join</span> <span class="o">=</span> <span class="n">df1</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df2</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="s2">&#34;id&#34;</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="s2">&#34;inner&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Inner Join:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">inner_join</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Right Join</span>
</span></span><span class="line"><span class="cl"><span class="n">right_join</span> <span class="o">=</span> <span class="n">df1</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df2</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="s2">&#34;id&#34;</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="s2">&#34;right&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Right Join:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">right_join</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Full (Outer) Join</span>
</span></span><span class="line"><span class="cl"><span class="n">full_join</span> <span class="o">=</span> <span class="n">df1</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df2</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="s2">&#34;id&#34;</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="s2">&#34;outer&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Full (Outer) Join:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">full_join</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Left Anti Join</span>
</span></span><span class="line"><span class="cl"><span class="n">left_anti_join</span> <span class="o">=</span> <span class="n">df1</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df2</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="s2">&#34;id&#34;</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="s2">&#34;left_anti&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Left Anti Join:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">left_anti_join</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Right Anti Join (Equivalent to swapping DataFrames and performing Left Anti Join)</span>
</span></span><span class="line"><span class="cl"><span class="n">right_anti_join</span> <span class="o">=</span> <span class="n">df2</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df1</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="s2">&#34;id&#34;</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="s2">&#34;left_anti&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Right Anti Join:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">right_anti_join</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Broadcast Join (Optimizing a join with a smaller DataFrame)</span>
</span></span><span class="line"><span class="cl"><span class="n">broadcast_join</span> <span class="o">=</span> <span class="n">df1</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">broadcast</span><span class="p">(</span><span class="n">df2</span><span class="p">),</span> <span class="n">on</span><span class="o">=</span><span class="s2">&#34;id&#34;</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="s2">&#34;inner&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Broadcast Join:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">broadcast_join</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></span></span></code></pre></div>
<hr>
<h3 id="practice-2">Practice 2</h3>
<h4 id="pyspark-coding-questions">PySpark Coding Questions</h4>
<ol>
<li>
<p><strong>Find employees whose location matches the location of their department</strong></p>
<ul>
<li>Display: <code>emp_id</code>, <code>emp_name</code>, <code>emp_location</code>, <code>dept_name</code>, <code>dept_location</code>.</li>
</ul>
</li>
<li>
<p><strong>Find departments that have no employees assigned to them</strong></p>
<ul>
<li>Display: <code>dept_id</code>, <code>dept_name</code>, <code>dept_head</code>.</li>
</ul>
</li>
<li>
<p><strong>Get the average salary of employees in each department</strong></p>
<ul>
<li>Display: <code>dept_name</code>, <code>average_salary</code>.</li>
</ul>
</li>
<li>
<p><strong>List the employees who earn more than the average salary of their department</strong></p>
<ul>
<li>Display: <code>emp_id</code>, <code>emp_name</code>, <code>emp_salary</code>, <code>dept_name</code>, <code>dept_location</code>.</li>
</ul>
</li>
</ol>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Sample DataFrames</span>
</span></span><span class="line"><span class="cl"><span class="n">emp_data</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="n">Row</span><span class="p">(</span><span class="n">emp_id</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">emp_name</span><span class="o">=</span><span class="s2">&#34;Alice&#34;</span><span class="p">,</span> <span class="n">emp_salary</span><span class="o">=</span><span class="mi">50000</span><span class="p">,</span> <span class="n">emp_dept_id</span><span class="o">=</span><span class="mi">101</span><span class="p">,</span> <span class="n">emp_location</span><span class="o">=</span><span class="s2">&#34;New York&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">Row</span><span class="p">(</span><span class="n">emp_id</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">emp_name</span><span class="o">=</span><span class="s2">&#34;Bob&#34;</span><span class="p">,</span> <span class="n">emp_salary</span><span class="o">=</span><span class="mi">60000</span><span class="p">,</span> <span class="n">emp_dept_id</span><span class="o">=</span><span class="mi">102</span><span class="p">,</span> <span class="n">emp_location</span><span class="o">=</span><span class="s2">&#34;Los Angeles&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">Row</span><span class="p">(</span><span class="n">emp_id</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">emp_name</span><span class="o">=</span><span class="s2">&#34;Charlie&#34;</span><span class="p">,</span> <span class="n">emp_salary</span><span class="o">=</span><span class="mi">55000</span><span class="p">,</span> <span class="n">emp_dept_id</span><span class="o">=</span><span class="mi">101</span><span class="p">,</span> <span class="n">emp_location</span><span class="o">=</span><span class="s2">&#34;Chicago&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">Row</span><span class="p">(</span><span class="n">emp_id</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">emp_name</span><span class="o">=</span><span class="s2">&#34;David&#34;</span><span class="p">,</span> <span class="n">emp_salary</span><span class="o">=</span><span class="mi">70000</span><span class="p">,</span> <span class="n">emp_dept_id</span><span class="o">=</span><span class="mi">103</span><span class="p">,</span> <span class="n">emp_location</span><span class="o">=</span><span class="s2">&#34;San Francisco&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">Row</span><span class="p">(</span><span class="n">emp_id</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">emp_name</span><span class="o">=</span><span class="s2">&#34;Eve&#34;</span><span class="p">,</span> <span class="n">emp_salary</span><span class="o">=</span><span class="mi">48000</span><span class="p">,</span> <span class="n">emp_dept_id</span><span class="o">=</span><span class="mi">102</span><span class="p">,</span> <span class="n">emp_location</span><span class="o">=</span><span class="s2">&#34;Houston&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">dept_data</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="n">Row</span><span class="p">(</span><span class="n">dept_id</span><span class="o">=</span><span class="mi">101</span><span class="p">,</span> <span class="n">dept_name</span><span class="o">=</span><span class="s2">&#34;Engineering&#34;</span><span class="p">,</span> <span class="n">dept_head</span><span class="o">=</span><span class="s2">&#34;John&#34;</span><span class="p">,</span> <span class="n">dept_location</span><span class="o">=</span><span class="s2">&#34;New York&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">Row</span><span class="p">(</span><span class="n">dept_id</span><span class="o">=</span><span class="mi">102</span><span class="p">,</span> <span class="n">dept_name</span><span class="o">=</span><span class="s2">&#34;Marketing&#34;</span><span class="p">,</span> <span class="n">dept_head</span><span class="o">=</span><span class="s2">&#34;Mary&#34;</span><span class="p">,</span> <span class="n">dept_location</span><span class="o">=</span><span class="s2">&#34;Los Angeles&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">Row</span><span class="p">(</span><span class="n">dept_id</span><span class="o">=</span><span class="mi">103</span><span class="p">,</span> <span class="n">dept_name</span><span class="o">=</span><span class="s2">&#34;Finance&#34;</span><span class="p">,</span> <span class="n">dept_head</span><span class="o">=</span><span class="s2">&#34;Frank&#34;</span><span class="p">,</span> <span class="n">dept_location</span><span class="o">=</span><span class="s2">&#34;Chicago&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">emp_columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;emp_id&#34;</span><span class="p">,</span> <span class="s2">&#34;emp_name&#34;</span><span class="p">,</span> <span class="s2">&#34;emp_salary&#34;</span><span class="p">,</span> <span class="s2">&#34;emp_dept_id&#34;</span><span class="p">,</span> <span class="s2">&#34;emp_location&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">dept_columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;dept_id&#34;</span><span class="p">,</span> <span class="s2">&#34;dept_name&#34;</span><span class="p">,</span> <span class="s2">&#34;dept_head&#34;</span><span class="p">,</span> <span class="s2">&#34;dept_location&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">emp_df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">emp_data</span><span class="p">,</span> <span class="n">emp_columns</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">dept_df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">dept_data</span><span class="p">,</span> <span class="n">dept_columns</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Display emp data</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;emp_data:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">emp_df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Display dept data</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;dept_data:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">dept_df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></span></span></code></pre></div>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Inner Join on emp_dept_id and dept_id</span>
</span></span><span class="line"><span class="cl"><span class="n">inner_join</span> <span class="o">=</span> <span class="n">emp_df</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dept_df</span><span class="p">,</span> <span class="n">emp_df</span><span class="p">[</span><span class="s2">&#34;emp_dept_id&#34;</span><span class="p">]</span> <span class="o">==</span> <span class="n">dept_df</span><span class="p">[</span><span class="s2">&#34;dept_id&#34;</span><span class="p">],</span> <span class="s2">&#34;inner&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Display the result</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Inner Join Result:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">inner_join</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Inner Join with Filtering Columns and WHERE Condition</span>
</span></span><span class="line"><span class="cl"><span class="n">inner_join</span> <span class="o">=</span> <span class="n">emp_df</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dept_df</span><span class="p">,</span> <span class="n">emp_df</span><span class="p">[</span><span class="s2">&#34;emp_dept_id&#34;</span><span class="p">]</span> <span class="o">==</span> <span class="n">dept_df</span><span class="p">[</span><span class="s2">&#34;dept_id&#34;</span><span class="p">],</span> <span class="s2">&#34;inner&#34;</span><span class="p">)</span> \
</span></span><span class="line"><span class="cl">    <span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&#34;emp_id&#34;</span><span class="p">,</span> <span class="s2">&#34;emp_name&#34;</span><span class="p">,</span> <span class="s2">&#34;emp_salary&#34;</span><span class="p">,</span> <span class="s2">&#34;dept_name&#34;</span><span class="p">,</span> <span class="s2">&#34;dept_location&#34;</span><span class="p">)</span> \
</span></span><span class="line"><span class="cl">    <span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="s2">&#34;emp_salary &gt; 55000&#34;</span><span class="p">)</span>  <span class="c1"># Add a WHERE condition</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Display the result</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Inner Join with Filter and WHERE Condition:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">inner_join</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Left Join with Filtering Columns and WHERE Condition</span>
</span></span><span class="line"><span class="cl"><span class="n">left_join_filtered</span> <span class="o">=</span> <span class="n">emp_df</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dept_df</span><span class="p">,</span> <span class="n">emp_df</span><span class="p">[</span><span class="s2">&#34;emp_dept_id&#34;</span><span class="p">]</span> <span class="o">==</span> <span class="n">dept_df</span><span class="p">[</span><span class="s2">&#34;dept_id&#34;</span><span class="p">],</span> <span class="s2">&#34;left&#34;</span><span class="p">)</span> \
</span></span><span class="line"><span class="cl">    <span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&#34;emp_id&#34;</span><span class="p">,</span> <span class="s2">&#34;emp_name&#34;</span><span class="p">,</span> <span class="s2">&#34;dept_name&#34;</span><span class="p">,</span> <span class="s2">&#34;dept_location&#34;</span><span class="p">)</span> \
</span></span><span class="line"><span class="cl">    <span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="s2">&#34;emp_salary &gt; 55000&#34;</span><span class="p">)</span>  <span class="c1"># Add a WHERE condition</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Display the result</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Left Join with Filter and WHERE Condition:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">left_join_filtered</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Left Anti Join</span>
</span></span><span class="line"><span class="cl"><span class="n">left_anti_join</span> <span class="o">=</span> <span class="n">emp_df</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">dept_df</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">emp_df</span><span class="p">[</span><span class="s2">&#34;emp_dept_id&#34;</span><span class="p">]</span> <span class="o">==</span> <span class="n">dept_df</span><span class="p">[</span><span class="s2">&#34;dept_id&#34;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;left_anti&#34;</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Display the result</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Left Anti Join Result:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">left_anti_join</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></span></span></code></pre></div>
<hr>
<h3 id="practice-3">Practice 3</h3>
<h4 id="pyspark-coding-questions-1">PySpark Coding Questions</h4>
<ol>
<li>
<p><strong>List each employee along with their manager&rsquo;s name</strong></p>
<ul>
<li>Display: <code>employee</code>, <code>manager</code>.</li>
</ul>
</li>
<li>
<p><strong>Find employees who do not have a manager (CEO-level employees)</strong></p>
<ul>
<li>Display: <code>employee</code>, <code>manager</code>.</li>
</ul>
</li>
<li>
<p><strong>Find all employees who directly report to &ldquo;Manager A&rdquo;</strong></p>
<ul>
<li>Display: <code>empid</code>, <code>ename</code>, <code>mrgid</code>.</li>
</ul>
</li>
<li>
<p><strong>Determine the hierarchy level of each employee</strong></p>
<ul>
<li>CEO â†’ Level 1, direct reports to CEO â†’ Level 2, and so on.</li>
<li>Display: <code>empid</code>, <code>ename</code>, <code>mrgid</code>, <code>level</code>.</li>
</ul>
</li>
</ol>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">col</span><span class="p">,</span> <span class="n">expr</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create a Spark session</span>
</span></span><span class="line"><span class="cl"><span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&#34;EmployeeHierarchy&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Sample data</span>
</span></span><span class="line"><span class="cl"><span class="n">data</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&#34;CEO&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&#34;Manager A&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&#34;Manager B&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&#34;Employee X&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="s2">&#34;Employee Y&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;empid&#34;</span><span class="p">,</span> <span class="s2">&#34;mrgid&#34;</span><span class="p">,</span> <span class="s2">&#34;ename&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">employee_df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Display the result</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;emp_data:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">employee_df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Self-join to find the manager and CEO</span>
</span></span><span class="line"><span class="cl"><span class="n">manager_df</span> <span class="o">=</span> <span class="n">employee_df</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&#34;e&#34;</span><span class="p">)</span> \
</span></span><span class="line"><span class="cl">    <span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">employee_df</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&#34;m&#34;</span><span class="p">),</span> <span class="n">col</span><span class="p">(</span><span class="s2">&#34;e.mrgid&#34;</span><span class="p">)</span> <span class="o">==</span> <span class="n">col</span><span class="p">(</span><span class="s2">&#34;m.empid&#34;</span><span class="p">),</span> <span class="s2">&#34;left&#34;</span><span class="p">)</span> \
</span></span><span class="line"><span class="cl">    <span class="o">.</span><span class="n">select</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">col</span><span class="p">(</span><span class="s2">&#34;e.ename&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&#34;employee&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="n">col</span><span class="p">(</span><span class="s2">&#34;m.ename&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&#34;manager&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Display the result</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;mgr:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">manager_df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Filter for employees without a manager (CEO)</span>
</span></span><span class="line"><span class="cl"><span class="n">manager_df2</span> <span class="o">=</span> <span class="n">employee_df</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&#34;e1&#34;</span><span class="p">)</span> \
</span></span><span class="line"><span class="cl">    <span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">employee_df</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&#34;m1&#34;</span><span class="p">),</span> <span class="n">col</span><span class="p">(</span><span class="s2">&#34;e1.mrgid&#34;</span><span class="p">)</span> <span class="o">==</span> <span class="n">col</span><span class="p">(</span><span class="s2">&#34;m1.empid&#34;</span><span class="p">),</span> <span class="s2">&#34;left&#34;</span><span class="p">)</span> \
</span></span><span class="line"><span class="cl">    <span class="o">.</span><span class="n">select</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">col</span><span class="p">(</span><span class="s2">&#34;e1.ename&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&#34;employee&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="n">col</span><span class="p">(</span><span class="s2">&#34;m1.ename&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&#34;manager&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span> \
</span></span><span class="line"><span class="cl">    <span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s2">&#34;manager&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">isNull</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Display the result</span>
</span></span><span class="line"><span class="cl"><span class="n">manager_df2</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></span></span></code></pre></div>

  <footer class="footline">
  </footer>
</article>
<article class="default">
  <header class="headline">
  </header>

<h1 id="whencastunion">When|Cast|Union</h1>

<h2 id="when-and-otherwise"><code>when</code> and <code>otherwise</code></h2>
<p>The <code>when</code> and <code>otherwise</code> functions in <strong>PySpark</strong> provide a way to create conditional expressions within a DataFrame, allowing you to specify different values for new or existing columns based on specific conditions.</p>
<ul>
<li>
<p><strong><code>when</code></strong>:<br>
The <code>when</code> function in PySpark is used to define a condition.<br>
If the condition is met, it returns the specified value.<br>
You can chain multiple <code>when</code> conditions to handle various cases.</p>
</li>
<li>
<p><strong><code>otherwise</code></strong>:<br>
The <code>otherwise</code> function specifies a default value to return if none of the conditions in the <code>when</code> statements are met.</p>
</li>
</ul>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">when</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">StructType</span><span class="p">,</span> <span class="n">StructField</span><span class="p">,</span> <span class="n">IntegerType</span><span class="p">,</span> <span class="n">StringType</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Initialize Spark session</span>
</span></span><span class="line"><span class="cl"><span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&#34;WhenOtherwiseExample&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Define the schema for the dataset</span>
</span></span><span class="line"><span class="cl"><span class="n">schema</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">([</span>
</span></span><span class="line"><span class="cl">    <span class="n">StructField</span><span class="p">(</span><span class="s2">&#34;name&#34;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="kc">True</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">StructField</span><span class="p">(</span><span class="s2">&#34;age&#34;</span><span class="p">,</span> <span class="n">IntegerType</span><span class="p">(),</span> <span class="kc">True</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">StructField</span><span class="p">(</span><span class="s2">&#34;salary&#34;</span><span class="p">,</span> <span class="n">IntegerType</span><span class="p">(),</span> <span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create a sample dataset</span>
</span></span><span class="line"><span class="cl"><span class="n">data</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Alice&#34;</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">3000</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Bob&#34;</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">4000</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Charlie&#34;</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">5000</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;David&#34;</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">4500</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Eve&#34;</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">3500</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create DataFrame</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">schema</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Apply &#39;when&#39; and &#39;otherwise&#39; to add new columns based on conditions</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&#34;status&#34;</span><span class="p">,</span> <span class="n">when</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span> <span class="o">&lt;</span> <span class="mi">30</span><span class="p">,</span> <span class="s2">&#34;Young&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">otherwise</span><span class="p">(</span><span class="s2">&#34;Adult&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">      <span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&#34;income_bracket&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                  <span class="n">when</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">salary</span> <span class="o">&lt;</span> <span class="mi">4000</span><span class="p">,</span> <span class="s2">&#34;Low&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                  <span class="o">.</span><span class="n">when</span><span class="p">((</span><span class="n">df</span><span class="o">.</span><span class="n">salary</span> <span class="o">&gt;=</span> <span class="mi">4000</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">salary</span> <span class="o">&lt;=</span> <span class="mi">4500</span><span class="p">),</span> <span class="s2">&#34;Medium&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                  <span class="o">.</span><span class="n">otherwise</span><span class="p">(</span><span class="s2">&#34;High&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Show the result</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></span></span></code></pre></div>
<hr>
<h4 id="explanation">Explanation</h4>
<ol>
<li>
<p><strong><code>status</code> column</strong></p>
<ul>
<li>Assigns <code>&quot;Young&quot;</code> if <code>age &lt; 30</code>.</li>
<li>Otherwise assigns <code>&quot;Adult&quot;</code>.</li>
</ul>
</li>
<li>
<p><strong><code>income_bracket</code> column</strong></p>
<ul>
<li>Assigns <code>&quot;Low&quot;</code> if <code>salary &lt; 4000</code>.</li>
<li>Assigns <code>&quot;Medium&quot;</code> if <code>4000 &lt;= salary &lt;= 4500</code>.</li>
<li>Assigns <code>&quot;High&quot;</code> for any other salary values.</li>
</ul>
</li>
</ol>
<p>This approach allows for flexible handling of multiple conditions in PySpark DataFrames using <code>when</code> and <code>otherwise</code>.</p>
<h2 id="cast-and-printschema"><code>cast()</code> and <code>printSchema()</code></h2>
<p>In PySpark, the <code>cast()</code> function is used to change the data type of a column within a DataFrame.<br>
This is helpful when you need to standardize column data types for data processing, schema consistency, or compatibility with other operations.</p>
<ul>
<li>
<p><strong>Purpose</strong>:<br>
The <code>cast()</code> function allows you to change the data type of a column, useful in situations like standardizing formats (e.g., converting strings to dates or integers).</p>
</li>
<li>
<p><strong>Syntax</strong>:<br>
The <code>cast()</code> function is applied on individual columns and requires specifying the target data type in quotes.</p>
</li>
<li>
<p><strong>Multiple Columns</strong>:<br>
You can cast multiple columns at once by using a list of cast expressions and passing them to <code>select()</code>.</p>
</li>
<li>
<p><strong>Supported Data Types</strong>:<br>
PySpark supports various data types for casting, including:</p>
<ul>
<li><code>StringType</code></li>
<li><code>IntegerType</code> (or <code>&quot;int&quot;</code>)</li>
<li><code>DoubleType</code> (or <code>&quot;double&quot;</code>)</li>
<li><code>DateType</code></li>
<li><code>TimestampType</code></li>
<li><code>BooleanType</code></li>
<li>Others, based on the data types available in PySpark.</li>
</ul>
</li>
</ul>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">col</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Single column cast</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&#34;column_name&#34;</span><span class="p">,</span> <span class="n">col</span><span class="p">(</span><span class="s2">&#34;column_name&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="s2">&#34;target_data_type&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Multiple columns cast with select</span>
</span></span><span class="line"><span class="cl"><span class="n">cast_expr</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="n">col</span><span class="p">(</span><span class="s2">&#34;column1_name&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="s2">&#34;target_data_type1&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">col</span><span class="p">(</span><span class="s2">&#34;column2_name&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="s2">&#34;target_data_type2&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># More columns and data types as needed</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="o">*</span><span class="n">cast_expr</span><span class="p">)</span></span></span></code></pre></div>
<h4 id="example">Example</h4>
<p>Let&rsquo;s create a dataset and apply <code>cast()</code> to change the data types of multiple columns:</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">col</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">StructType</span><span class="p">,</span> <span class="n">StructField</span><span class="p">,</span> <span class="n">StringType</span><span class="p">,</span> <span class="n">IntegerType</span><span class="p">,</span> <span class="n">FloatType</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Initialize Spark session</span>
</span></span><span class="line"><span class="cl"><span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&#34;CastExample&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Define the schema for the dataset</span>
</span></span><span class="line"><span class="cl"><span class="n">schema</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">([</span>
</span></span><span class="line"><span class="cl">    <span class="n">StructField</span><span class="p">(</span><span class="s2">&#34;name&#34;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="kc">True</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">StructField</span><span class="p">(</span><span class="s2">&#34;age&#34;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="kc">True</span><span class="p">),</span>    <span class="c1"># Stored as StringType initially</span>
</span></span><span class="line"><span class="cl">    <span class="n">StructField</span><span class="p">(</span><span class="s2">&#34;height&#34;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="kc">True</span><span class="p">)</span>  <span class="c1"># Stored as StringType initially</span>
</span></span><span class="line"><span class="cl"><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create a sample dataset</span>
</span></span><span class="line"><span class="cl"><span class="n">data</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Alice&#34;</span><span class="p">,</span> <span class="s2">&#34;25&#34;</span><span class="p">,</span> <span class="s2">&#34;5.5&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Bob&#34;</span><span class="p">,</span> <span class="s2">&#34;35&#34;</span><span class="p">,</span> <span class="s2">&#34;6.1&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Charlie&#34;</span><span class="p">,</span> <span class="s2">&#34;40&#34;</span><span class="p">,</span> <span class="s2">&#34;5.8&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create DataFrame</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">schema</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Show schema and data before casting</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span><span class="o">.</span><span class="n">printSchema</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></span></span></code></pre></div>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Define cast expressions for multiple columns</span>
</span></span><span class="line"><span class="cl"><span class="n">cast_expr</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="n">col</span><span class="p">(</span><span class="s2">&#34;name&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="s2">&#34;string&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">col</span><span class="p">(</span><span class="s2">&#34;age&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="s2">&#34;int&#34;</span><span class="p">),</span>       <span class="c1"># Casting age to IntegerType</span>
</span></span><span class="line"><span class="cl">    <span class="n">col</span><span class="p">(</span><span class="s2">&#34;height&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="s2">&#34;double&#34;</span><span class="p">)</span>  <span class="c1"># Casting height to DoubleType</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Apply the cast expressions to the DataFrame</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="o">*</span><span class="n">cast_expr</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Show the result</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span><span class="o">.</span><span class="n">printSchema</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></span></span></code></pre></div>
<h4 id="explanation-1">Explanation</h4>
<ul>
<li><strong><code>age</code> column</strong>: Initially stored as <code>StringType</code>, itâ€™s cast to <code>IntegerType</code> (or <code>&quot;int&quot;</code>).</li>
<li><strong><code>height</code> column</strong>: Initially stored as <code>StringType</code>, itâ€™s cast to <code>DoubleType</code> (or <code>&quot;double&quot;</code>).</li>
</ul>
<hr>
<h4 id="advantages-of-using-cast">Advantages of Using <code>cast()</code></h4>
<ul>
<li><strong>Schema Alignment</strong>: Ensures data types in different tables or DataFrames are compatible for joining or union operations.</li>
<li><strong>Data Consistency</strong>: Ensures all columns conform to expected data types for downstream data processing.</li>
<li><strong>Error Reduction</strong>: Minimizes issues arising from mismatched data types in computations or transformations.</li>
</ul>
<p>This approach using <code>cast()</code> provides a flexible and powerful way to manage data types in PySpark.</p>
<hr>
<h4 id="printschema-method-in-pyspark"><code>printSchema()</code> Method in PySpark</h4>
<ul>
<li>
<p><strong>Purpose</strong>:</p>
<ul>
<li>To display the schema of a DataFrame, which includes the column names, data types, and nullability of each column.</li>
</ul>
</li>
<li>
<p><strong>Output Structure</strong>:<br>
The schema is presented in a tree-like structure showing:</p>
<ul>
<li><strong>Column Name</strong>: The name of the column.</li>
<li><strong>Data Type</strong>: The data type of the column (e.g., <code>string</code>, <code>integer</code>, <code>double</code>, <code>boolean</code>, etc.).</li>
<li><strong>Nullability</strong>: Indicates whether the column can contain null values (e.g., <code>nullable = true</code>).</li>
</ul>
</li>
<li>
<p><strong>Usage</strong>:</p>
<ul>
<li>Call <code>df.printSchema()</code> on a DataFrame <code>df</code> to see its structure.</li>
<li>Useful for verifying the structure of the DataFrame after operations like <code>select()</code>, <code>withColumn()</code>, or <code>cast()</code>.</li>
</ul>
</li>
</ul>
<h2 id="union-and-unionall"><code>union</code> and <code>unionAll</code></h2>
<h4 id="overview">Overview</h4>
<ul>
<li><strong>Purpose</strong>: Both <code>union</code> and <code>unionAll</code> are used to combine two DataFrames into a single DataFrame.</li>
<li><strong>DataFrame Compatibility</strong>: The two DataFrames must have the same schema (i.e., the same column names and data types) to perform the union operation.</li>
</ul>
<hr>
<h4 id="union"><code>union()</code></h4>
<ul>
<li>
<p><strong>Functionality</strong>:</p>
<ul>
<li>Combines two DataFrames and retains all rows, including duplicates.</li>
</ul>
</li>
<li>
<p><strong>Behavior</strong>:</p>
<ul>
<li>The <code>union()</code> method does not remove duplicate rows, resulting in a DataFrame that may contain duplicates.</li>
</ul>
</li>
</ul>
<hr>
<h4 id="unionall"><code>unionAll()</code></h4>
<ul>
<li>
<p><strong>Functionality</strong>:</p>
<ul>
<li>Combines two DataFrames and retains all rows, including duplicates.</li>
</ul>
</li>
<li>
<p><strong>Behavior</strong>:</p>
<ul>
<li>The <code>unionAll()</code> method performs the union operation but does not eliminate duplicate rows (similar to <code>union</code>).</li>
</ul>
</li>
</ul>
<hr>
<h4 id="syntax">Syntax</h4>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Using union to retain all rows including duplicates</span>
</span></span><span class="line"><span class="cl"><span class="n">unioned_df</span> <span class="o">=</span> <span class="n">df1</span><span class="o">.</span><span class="n">union</span><span class="p">(</span><span class="n">df2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Using unionAll to retain all rows including duplicates</span>
</span></span><span class="line"><span class="cl"><span class="n">unionAll_df</span> <span class="o">=</span> <span class="n">df1</span><span class="o">.</span><span class="n">unionAll</span><span class="p">(</span><span class="n">df2</span><span class="p">)</span></span></span></code></pre></div>
<h4 id="example-using-union-and-unionall-in-pyspark">Example: Using <code>union</code> and <code>unionAll</code> in PySpark</h4>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Initialize Spark session</span>
</span></span><span class="line"><span class="cl"><span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&#34;UnionExample&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Sample DataFrames</span>
</span></span><span class="line"><span class="cl"><span class="n">data1</span> <span class="o">=</span> <span class="p">[(</span><span class="s2">&#34;Alice&#34;</span><span class="p">,</span> <span class="mi">25</span><span class="p">),</span> <span class="p">(</span><span class="s2">&#34;Bob&#34;</span><span class="p">,</span> <span class="mi">30</span><span class="p">),</span> <span class="p">(</span><span class="s2">&#34;Charlie&#34;</span><span class="p">,</span> <span class="mi">35</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl"><span class="n">data2</span> <span class="o">=</span> <span class="p">[(</span><span class="s2">&#34;David&#34;</span><span class="p">,</span> <span class="mi">40</span><span class="p">),</span> <span class="p">(</span><span class="s2">&#34;Eve&#34;</span><span class="p">,</span> <span class="mi">45</span><span class="p">),</span> <span class="p">(</span><span class="s2">&#34;Alice&#34;</span><span class="p">,</span> <span class="mi">25</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl"><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;name&#34;</span><span class="p">,</span> <span class="s2">&#34;age&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">df1</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">data1</span><span class="p">,</span> <span class="n">columns</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">df2</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">data2</span><span class="p">,</span> <span class="n">columns</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Using union to retain all rows including duplicates</span>
</span></span><span class="line"><span class="cl"><span class="n">unioned_df</span> <span class="o">=</span> <span class="n">df1</span><span class="o">.</span><span class="n">union</span><span class="p">(</span><span class="n">df2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Using unionAll to retain all rows</span>
</span></span><span class="line"><span class="cl"><span class="n">unionAll_df</span> <span class="o">=</span> <span class="n">df1</span><span class="o">.</span><span class="n">unionAll</span><span class="p">(</span><span class="n">df2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Show the results</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;unioned_df (No duplicates removed):&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">unioned_df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;unionAll_df (duplicates retained):&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">unionAll_df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">## Removing Duplicate Rows in PySpark</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Remove duplicate rows and create a new DataFrame</span>
</span></span><span class="line"><span class="cl"><span class="n">unique_df</span> <span class="o">=</span> <span class="n">unioned_df</span><span class="o">.</span><span class="n">dropDuplicates</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="c1"># or</span>
</span></span><span class="line"><span class="cl"><span class="n">unique_df</span> <span class="o">=</span> <span class="n">unioned_df</span><span class="o">.</span><span class="n">distinct</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;unique_df (after removing duplicates):&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">unique_df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></span></span></code></pre></div>
<h4 id="union-and-unionbyname">Union and UnionByName</h4>
<p>In PySpark, both <code>union</code> and <code>unionByName</code> are operations that allow you to combine two or more DataFrames. However, they do this in slightly different ways, particularly regarding how they handle column names.</p>
<hr>
<h5 id="1-union">1. <code>union</code></h5>
<p><strong>Definition</strong>:<br>
The <code>union()</code> function is used to combine two DataFrames with the same schema (i.e., the same number of columns with the same data types).<br>
It appends the rows of one DataFrame to the other.</p>
<p><strong>Key Characteristics</strong>:</p>
<ul>
<li>The DataFrames must have the same number of columns.</li>
<li>The columns must have compatible data types.</li>
<li>It does <strong>not</strong> automatically handle column names that differ between DataFrames.</li>
</ul>
<h4 id="syntax-1">Syntax</h4>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">DataFrame</span><span class="o">.</span><span class="n">union</span><span class="p">(</span><span class="n">otherDataFrame</span><span class="p">)</span></span></span></code></pre></div>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create a Spark session</span>
</span></span><span class="line"><span class="cl"><span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&#34;Union Example&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create two DataFrames with the same schema</span>
</span></span><span class="line"><span class="cl"><span class="n">data1</span> <span class="o">=</span> <span class="p">[(</span><span class="s2">&#34;Alice&#34;</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="s2">&#34;Bob&#34;</span><span class="p">,</span> <span class="mi">2</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl"><span class="n">data2</span> <span class="o">=</span> <span class="p">[(</span><span class="s2">&#34;Cathy&#34;</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="s2">&#34;David&#34;</span><span class="p">,</span> <span class="mi">4</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl"><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;Name&#34;</span><span class="p">,</span> <span class="s2">&#34;Id&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">df1</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">data1</span><span class="p">,</span> <span class="n">columns</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">df2</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">data2</span><span class="p">,</span> <span class="n">columns</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Perform union</span>
</span></span><span class="line"><span class="cl"><span class="n">result_union</span> <span class="o">=</span> <span class="n">df1</span><span class="o">.</span><span class="n">union</span><span class="p">(</span><span class="n">df2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Show the result</span>
</span></span><span class="line"><span class="cl"><span class="n">result_union</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></span></span></code></pre></div>
<h4 id="2-unionbyname">2. <code>unionByName</code></h4>
<p><strong>Definition</strong>:<br>
The <code>unionByName()</code> function allows you to combine two DataFrames by matching column names.<br>
If the DataFrames do not have the same schema, it will fill in missing columns with <code>null</code>.</p>
<p><strong>Key Characteristics</strong>:</p>
<ul>
<li>Matches DataFrames by <strong>column names</strong> rather than position.</li>
<li>If the DataFrames have different columns, it will include all columns and fill in <code>null</code> for missing values in any DataFrame.</li>
<li>You can specify <code>allowMissingColumns=True</code> to ignore missing columns.</li>
</ul>
<h4 id="syntax-2">Syntax</h4>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">DataFrame</span><span class="o">.</span><span class="n">unionByName</span><span class="p">(</span><span class="n">otherDataFrame</span><span class="p">,</span> <span class="n">allowMissingColumns</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></span></span></code></pre></div>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Create two DataFrames with different schemas</span>
</span></span><span class="line"><span class="cl"><span class="n">data3</span> <span class="o">=</span> <span class="p">[(</span><span class="s2">&#34;Eve&#34;</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="p">(</span><span class="s2">&#34;Frank&#34;</span><span class="p">,</span> <span class="mi">6</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl"><span class="n">data4</span> <span class="o">=</span> <span class="p">[(</span><span class="s2">&#34;Grace&#34;</span><span class="p">,</span> <span class="s2">&#34;New York&#34;</span><span class="p">),</span> <span class="p">(</span><span class="s2">&#34;Hannah&#34;</span><span class="p">,</span> <span class="s2">&#34;Los Angeles&#34;</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">columns1</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;Name&#34;</span><span class="p">,</span> <span class="s2">&#34;Id&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">columns2</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;Name&#34;</span><span class="p">,</span> <span class="s2">&#34;City&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">df3</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">data3</span><span class="p">,</span> <span class="n">columns1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">df4</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">data4</span><span class="p">,</span> <span class="n">columns2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Perform unionByName (with allowMissingColumns=True to handle schema differences)</span>
</span></span><span class="line"><span class="cl"><span class="n">result_union_by_name</span> <span class="o">=</span> <span class="n">df3</span><span class="o">.</span><span class="n">unionByName</span><span class="p">(</span><span class="n">df4</span><span class="p">,</span> <span class="n">allowMissingColumns</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Show the result result_union_by_name.show()</span>
</span></span><span class="line"><span class="cl"><span class="n">result_union_by_name</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></span></span></code></pre></div>
<h4 id="summary-of-differences">Summary of Differences</h4>
<ul>
<li><strong><code>union()</code></strong>: Requires DataFrames to have the <strong>same schema</strong> (same number of columns and compatible data types). It combines rows without checking column names.</li>
<li><strong><code>unionByName()</code></strong>: Matches DataFrames by <strong>column names</strong>. It can handle different schemas and fill missing columns with <code>null</code> (when <code>allowMissingColumns=True</code>).</li>
</ul>
<hr>
<table>
  <thead>
      <tr>
          <th>Feature</th>
          <th>Union</th>
          <th>UnionByName</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Column Matching</td>
          <td>Positional</td>
          <td>By Name</td>
      </tr>
      <tr>
          <td>Missing Columns Handling</td>
          <td>Does not allow</td>
          <td>Allows with <code>null</code> for missing</td>
      </tr>
      <tr>
          <td>Schema Requirement</td>
          <td>Must be identical</td>
          <td>Can differ</td>
      </tr>
  </tbody>
</table>
<h4 id="conclusion">Conclusion</h4>
<p>In PySpark:</p>
<ul>
<li>Use <strong><code>union()</code></strong> when you have DataFrames with the <strong>same schema</strong> and need a straightforward concatenation.</li>
<li>Use <strong><code>unionByName()</code></strong> when your DataFrames have <strong>different schemas</strong> and you want to combine them by matching column names while handling missing columns.</li>
</ul>

  <footer class="footline">
  </footer>
</article>
<article class="default">
  <header class="headline">
  </header>

<h1 id="window-functions">Window Functions</h1>

<h2 id="windows-function-in-pyspark">Windows Function in PySpark</h2>
<h3 id="1-introduction-to-window-functions">1. Introduction to Window Functions</h3>
<p>Window functions allow you to perform calculations across a set of rows related to the current row within a specified partition.</p>
<p>Unlike groupBy functions, window functions do not reduce the number of rows in the result; instead, they calculate a value for each row based on the specified window.</p>
<h3 id="2-importing-required-libraries">2. Importing Required Libraries</h3>
<p>To use window functions, import the necessary modules from PySpark:</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">functions</span> <span class="k">as</span> <span class="n">F</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql.window</span> <span class="kn">import</span> <span class="n">Window</span></span></span></code></pre></div>
<h3 id="3-creating-a-window-specification">3. Creating a Window Specification</h3>
<p>A window specification defines how rows will be grouped (partitioned) and ordered within each group.</p>
<p><strong>Example â€“ Basic Window Specification:</strong></p>
<p><code>window_spec = Window.partitionBy(&quot;category&quot;).orderBy(&quot;timestamp&quot;)</code></p>
<p><strong>Example â€“ Advanced Window Specification:</strong></p>
<p><code>window_spec = Window.partitionBy(&quot;category&quot;, &quot;sub_category&quot;).orderBy(F.col(&quot;timestamp&quot;), F.col(&quot;score&quot;))</code></p>
<h3 id="4-common-window-functions">4. Common Window Functions</h3>
<h4 id="a-row-number">a. Row Number</h4>
<p><strong>Function:</strong> <code>row_number()</code></p>
<p><strong>Description:</strong> Assigns a unique integer to each row within the partition (starting from 1).</p>
<p><code>df = df.withColumn(&quot;row_number&quot;, F.row_number().over(window_spec))</code></p>
<h4 id="b-rank">b. Rank</h4>
<p><strong>Function:</strong> <code>rank()</code></p>
<p><strong>Description:</strong> Assigns the same rank to rows with the same values in the order criteria. The next rank has a gap.</p>
<p><code>df = df.withColumn(&quot;rank&quot;, F.rank().over(window_spec))</code></p>
<h4 id="c-dense-rank">c. Dense Rank</h4>
<p><strong>Function:</strong> <code>dense_rank()</code></p>
<p><strong>Description:</strong> Similar to rank, but does not leave gaps in ranking.</p>
<p><code>df = df.withColumn(&quot;dense_rank&quot;, F.dense_rank().over(window_spec))</code></p>
<h4 id="d-lead-and-lag">d. Lead and Lag</h4>
<p><strong>Functions:</strong> <code>lead()</code>, <code>lag()</code></p>
<p><strong>Description:</strong></p>
<p><code>lead()</code> â†’ value of the next row within the window.</p>
<p><code>lag()</code> â†’ value of the previous row within the window.</p>
<p><code>df = df.withColumn(&quot;next_value&quot;, F.lead(&quot;value&quot;).over(window_spec))</code>
<code>df = df.withColumn(&quot;previous_value&quot;, F.lag(&quot;value&quot;).over(window_spec))</code></p>
<h4 id="e-aggregation-functions">e. Aggregation Functions</h4>
<p>Window functions can also compute aggregated values across the specified window.</p>
<p><code>df = df.withColumn(&quot;avg_value&quot;, F.avg(&quot;value&quot;).over(window_spec))</code></p>
<p>Other common aggregations:</p>
<p><strong>Sum:</strong> <code>F.sum(&quot;column_name&quot;).over(window_spec)</code></p>
<p><strong>Min:</strong> <code>F.min(&quot;column_name&quot;).over(window_spec)</code></p>
<p><strong>Max:</strong> <code>F.max(&quot;column_name&quot;).over(window_spec)</code></p>
<h3 id="5-putting-it-all-together--example">5. Putting It All Together â€“ Example</h3>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">functions</span> <span class="k">as</span> <span class="n">F</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql.window</span> <span class="kn">import</span> <span class="n">Window</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Initialize Spark session</span>
</span></span><span class="line"><span class="cl"><span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&#34;WindowFunctionsExample&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Sample DataFrame</span>
</span></span><span class="line"><span class="cl"><span class="n">data</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;A&#34;</span><span class="p">,</span> <span class="s2">&#34;X&#34;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&#34;2023-01-01&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;A&#34;</span><span class="p">,</span> <span class="s2">&#34;X&#34;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&#34;2023-01-02&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;A&#34;</span><span class="p">,</span> <span class="s2">&#34;Y&#34;</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="s2">&#34;2023-01-01&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;A&#34;</span><span class="p">,</span> <span class="s2">&#34;Y&#34;</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="s2">&#34;2023-01-02&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;B&#34;</span><span class="p">,</span> <span class="s2">&#34;X&#34;</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="s2">&#34;2023-01-01&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;B&#34;</span><span class="p">,</span> <span class="s2">&#34;X&#34;</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="s2">&#34;2023-01-02&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;category&#34;</span><span class="p">,</span> <span class="s2">&#34;sub_category&#34;</span><span class="p">,</span> <span class="s2">&#34;value&#34;</span><span class="p">,</span> <span class="s2">&#34;timestamp&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Define window specification</span>
</span></span><span class="line"><span class="cl"><span class="n">window_spec</span> <span class="o">=</span> <span class="n">Window</span><span class="o">.</span><span class="n">partitionBy</span><span class="p">(</span><span class="s2">&#34;category&#34;</span><span class="p">,</span> <span class="s2">&#34;sub_category&#34;</span><span class="p">)</span> \
</span></span><span class="line"><span class="cl">                   <span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">&#34;timestamp&#34;</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">&#34;value&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Apply window functions</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&#34;row_number&#34;</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">row_number</span><span class="p">()</span><span class="o">.</span><span class="n">over</span><span class="p">(</span><span class="n">window_spec</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&#34;rank&#34;</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span><span class="o">.</span><span class="n">over</span><span class="p">(</span><span class="n">window_spec</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&#34;dense_rank&#34;</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">dense_rank</span><span class="p">()</span><span class="o">.</span><span class="n">over</span><span class="p">(</span><span class="n">window_spec</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&#34;next_value&#34;</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">lead</span><span class="p">(</span><span class="s2">&#34;value&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">over</span><span class="p">(</span><span class="n">window_spec</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&#34;previous_value&#34;</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">lag</span><span class="p">(</span><span class="s2">&#34;value&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">over</span><span class="p">(</span><span class="n">window_spec</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&#34;avg_value&#34;</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">avg</span><span class="p">(</span><span class="s2">&#34;value&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">over</span><span class="p">(</span><span class="n">window_spec</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></span></span></code></pre></div>
<h3 id="6-conclusion">6. Conclusion</h3>
<p>Window functions in PySpark are powerful tools for analyzing data within groups while retaining row-level details.</p>
<p>By defining window specifications and applying functions like rank, dense_rank, lead, lag, and aggregations, you can perform complex analytics efficiently.</p>
<h2 id="windows-function-in-pyspark--part-2">Windows Function in PySpark â€“ Part 2</h2>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql.window</span> <span class="kn">import</span> <span class="n">Window</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">pyspark.sql.functions</span> <span class="k">as</span> <span class="nn">F</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Sample data</span>
</span></span><span class="line"><span class="cl"><span class="n">data</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Alice&#34;</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Bob&#34;</span><span class="p">,</span> <span class="mi">200</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Charlie&#34;</span><span class="p">,</span> <span class="mi">200</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;David&#34;</span><span class="p">,</span> <span class="mi">300</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Eve&#34;</span><span class="p">,</span> <span class="mi">400</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Frank&#34;</span><span class="p">,</span> <span class="mi">500</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Grace&#34;</span><span class="p">,</span> <span class="mi">500</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Hank&#34;</span><span class="p">,</span> <span class="mi">600</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Ivy&#34;</span><span class="p">,</span> <span class="mi">700</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Jack&#34;</span><span class="p">,</span> <span class="mi">800</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;Name&#34;</span><span class="p">,</span> <span class="s2">&#34;Score&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Define window</span>
</span></span><span class="line"><span class="cl"><span class="n">window_spec</span> <span class="o">=</span> <span class="n">Window</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="s2">&#34;Score&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Ranking functions</span>
</span></span><span class="line"><span class="cl"><span class="n">df1</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&#34;Rank&#34;</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span><span class="o">.</span><span class="n">over</span><span class="p">(</span><span class="n">window_spec</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">df2</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&#34;DenseRank&#34;</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">dense_rank</span><span class="p">()</span><span class="o">.</span><span class="n">over</span><span class="p">(</span><span class="n">window_spec</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">df3</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&#34;RowNumber&#34;</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">row_number</span><span class="p">()</span><span class="o">.</span><span class="n">over</span><span class="p">(</span><span class="n">window_spec</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Lead &amp; Lag</span>
</span></span><span class="line"><span class="cl"><span class="n">df4</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&#34;ScoreDifferenceWithNext&#34;</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">lead</span><span class="p">(</span><span class="s2">&#34;Score&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">over</span><span class="p">(</span><span class="n">window_spec</span><span class="p">)</span> <span class="o">-</span> <span class="n">df</span><span class="p">[</span><span class="s2">&#34;Score&#34;</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">df5</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&#34;ScoreDifferenceWithPrevious&#34;</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="s2">&#34;Score&#34;</span><span class="p">]</span> <span class="o">-</span> <span class="n">F</span><span class="o">.</span><span class="n">lag</span><span class="p">(</span><span class="s2">&#34;Score&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">over</span><span class="p">(</span><span class="n">window_spec</span><span class="p">))</span></span></span></code></pre></div>
<h2 id="windows-function-in-pyspark--part-3-student-marks-analysis">Windows Function in PySpark â€“ Part 3 (Student Marks Analysis)</h2>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql.window</span> <span class="kn">import</span> <span class="n">Window</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">pyspark.sql.functions</span> <span class="k">as</span> <span class="nn">F</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Updated sample data</span>
</span></span><span class="line"><span class="cl"><span class="n">data</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Alice&#34;</span><span class="p">,</span> <span class="s2">&#34;Math&#34;</span><span class="p">,</span> <span class="mi">90</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Alice&#34;</span><span class="p">,</span> <span class="s2">&#34;Science&#34;</span><span class="p">,</span> <span class="mi">85</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Alice&#34;</span><span class="p">,</span> <span class="s2">&#34;History&#34;</span><span class="p">,</span> <span class="mi">78</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Bob&#34;</span><span class="p">,</span> <span class="s2">&#34;Math&#34;</span><span class="p">,</span> <span class="mi">80</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Bob&#34;</span><span class="p">,</span> <span class="s2">&#34;Science&#34;</span><span class="p">,</span> <span class="mi">81</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Bob&#34;</span><span class="p">,</span> <span class="s2">&#34;History&#34;</span><span class="p">,</span> <span class="mi">77</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Charlie&#34;</span><span class="p">,</span> <span class="s2">&#34;Math&#34;</span><span class="p">,</span> <span class="mi">75</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Charlie&#34;</span><span class="p">,</span> <span class="s2">&#34;Science&#34;</span><span class="p">,</span> <span class="mi">82</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Charlie&#34;</span><span class="p">,</span> <span class="s2">&#34;History&#34;</span><span class="p">,</span> <span class="mi">79</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Alice&#34;</span><span class="p">,</span> <span class="s2">&#34;Physics&#34;</span><span class="p">,</span> <span class="mi">86</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Alice&#34;</span><span class="p">,</span> <span class="s2">&#34;Chemistry&#34;</span><span class="p">,</span> <span class="mi">92</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Alice&#34;</span><span class="p">,</span> <span class="s2">&#34;Biology&#34;</span><span class="p">,</span> <span class="mi">80</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Bob&#34;</span><span class="p">,</span> <span class="s2">&#34;Physics&#34;</span><span class="p">,</span> <span class="mi">94</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Bob&#34;</span><span class="p">,</span> <span class="s2">&#34;Chemistry&#34;</span><span class="p">,</span> <span class="mi">91</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Bob&#34;</span><span class="p">,</span> <span class="s2">&#34;Biology&#34;</span><span class="p">,</span> <span class="mi">96</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Charlie&#34;</span><span class="p">,</span> <span class="s2">&#34;Physics&#34;</span><span class="p">,</span> <span class="mi">89</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Charlie&#34;</span><span class="p">,</span> <span class="s2">&#34;Chemistry&#34;</span><span class="p">,</span> <span class="mi">88</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Charlie&#34;</span><span class="p">,</span> <span class="s2">&#34;Biology&#34;</span><span class="p">,</span> <span class="mi">85</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Alice&#34;</span><span class="p">,</span> <span class="s2">&#34;Computer Science&#34;</span><span class="p">,</span> <span class="mi">95</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Alice&#34;</span><span class="p">,</span> <span class="s2">&#34;Electronics&#34;</span><span class="p">,</span> <span class="mi">91</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Alice&#34;</span><span class="p">,</span> <span class="s2">&#34;Geography&#34;</span><span class="p">,</span> <span class="mi">97</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Bob&#34;</span><span class="p">,</span> <span class="s2">&#34;Computer Science&#34;</span><span class="p">,</span> <span class="mi">88</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Bob&#34;</span><span class="p">,</span> <span class="s2">&#34;Electronics&#34;</span><span class="p">,</span> <span class="mi">66</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Bob&#34;</span><span class="p">,</span> <span class="s2">&#34;Geography&#34;</span><span class="p">,</span> <span class="mi">92</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Charlie&#34;</span><span class="p">,</span> <span class="s2">&#34;Computer Science&#34;</span><span class="p">,</span> <span class="mi">92</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Charlie&#34;</span><span class="p">,</span> <span class="s2">&#34;Electronics&#34;</span><span class="p">,</span> <span class="mi">97</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Charlie&#34;</span><span class="p">,</span> <span class="s2">&#34;Geography&#34;</span><span class="p">,</span> <span class="mi">99</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;First Name&#34;</span><span class="p">,</span> <span class="s2">&#34;Subject&#34;</span><span class="p">,</span> <span class="s2">&#34;Marks&#34;</span><span class="p">,</span> <span class="s2">&#34;Semester&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 1. Max marks in each semester</span>
</span></span><span class="line"><span class="cl"><span class="n">window_spec_max_marks</span> <span class="o">=</span> <span class="n">Window</span><span class="o">.</span><span class="n">partitionBy</span><span class="p">(</span><span class="s2">&#34;Semester&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">desc</span><span class="p">(</span><span class="s2">&#34;Marks&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">max_marks_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&#34;Rank&#34;</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span><span class="o">.</span><span class="n">over</span><span class="p">(</span><span class="n">window_spec_max_marks</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">top_scorer</span> <span class="o">=</span> <span class="n">max_marks_df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">max_marks_df</span><span class="p">[</span><span class="s2">&#34;Rank&#34;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 2. Percentage of each student</span>
</span></span><span class="line"><span class="cl"><span class="n">window_spec_total_marks</span> <span class="o">=</span> <span class="n">Window</span><span class="o">.</span><span class="n">partitionBy</span><span class="p">(</span><span class="s2">&#34;First Name&#34;</span><span class="p">,</span> <span class="s2">&#34;Semester&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&#34;TotalMarks&#34;</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="s2">&#34;Marks&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">over</span><span class="p">(</span><span class="n">window_spec_total_marks</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&#34;Percentage&#34;</span><span class="p">,</span> <span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">&#34;TotalMarks&#34;</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">3</span><span class="o">*</span><span class="mi">100</span><span class="p">))</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="s2">&#34;decimal(5,2)&#34;</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">df2</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s2">&#34;First Name&#34;</span><span class="p">,</span><span class="s2">&#34;Semester&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="s2">&#34;TotalMarks&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&#34;TotalMarks&#34;</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="s2">&#34;Percentage&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&#34;Percentage&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 3. Top rank holder in each semester</span>
</span></span><span class="line"><span class="cl"><span class="n">window_spec_rank</span> <span class="o">=</span> <span class="n">Window</span><span class="o">.</span><span class="n">partitionBy</span><span class="p">(</span><span class="s2">&#34;Semester&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">desc</span><span class="p">(</span><span class="s2">&#34;Percentage&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">rank_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&#34;Rank&#34;</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span><span class="o">.</span><span class="n">over</span><span class="p">(</span><span class="n">window_spec_rank</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">top_rank_holder</span> <span class="o">=</span> <span class="n">rank_df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">rank_df</span><span class="p">[</span><span class="s2">&#34;Rank&#34;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&#34;First Name&#34;</span><span class="p">,</span><span class="s2">&#34;Semester&#34;</span><span class="p">,</span><span class="s2">&#34;Rank&#34;</span><span class="p">,</span><span class="s2">&#34;Percentage&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">distinct</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 4. Max marks in each subject in each semester</span>
</span></span><span class="line"><span class="cl"><span class="n">window_spec_max_subject_marks</span> <span class="o">=</span> <span class="n">Window</span><span class="o">.</span><span class="n">partitionBy</span><span class="p">(</span><span class="s2">&#34;Semester&#34;</span><span class="p">,</span><span class="s2">&#34;Subject&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">desc</span><span class="p">(</span><span class="s2">&#34;Marks&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">max_subject_marks_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&#34;Rank&#34;</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span><span class="o">.</span><span class="n">over</span><span class="p">(</span><span class="n">window_spec_max_subject_marks</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">max_subject_scorer</span> <span class="o">=</span> <span class="n">max_subject_marks_df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">max_subject_marks_df</span><span class="p">[</span><span class="s2">&#34;Rank&#34;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span></span></span></code></pre></div>
<h2 id="windows-function-in-pyspark--part-4-highest-salary-per-department">Windows Function in PySpark â€“ Part 4 (Highest Salary per Department)</h2>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">functions</span> <span class="k">as</span> <span class="n">F</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql.window</span> <span class="kn">import</span> <span class="n">Window</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Employee Data</span>
</span></span><span class="line"><span class="cl"><span class="n">emp_data</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s2">&#34;Alice&#34;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">6300</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="s2">&#34;Bob&#34;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">6200</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="s2">&#34;Charlie&#34;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">7000</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="s2">&#34;David&#34;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">7200</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="s2">&#34;Eve&#34;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">6300</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="s2">&#34;Frank&#34;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">7100</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Department Data</span>
</span></span><span class="line"><span class="cl"><span class="n">dept_data</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s2">&#34;HR&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="s2">&#34;Finance&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create DataFrames</span>
</span></span><span class="line"><span class="cl"><span class="n">emp_df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">emp_data</span><span class="p">,</span> <span class="p">[</span><span class="s2">&#34;EmpId&#34;</span><span class="p">,</span><span class="s2">&#34;EmpName&#34;</span><span class="p">,</span><span class="s2">&#34;DeptId&#34;</span><span class="p">,</span><span class="s2">&#34;Salary&#34;</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">dept_df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">dept_data</span><span class="p">,</span> <span class="p">[</span><span class="s2">&#34;DeptId&#34;</span><span class="p">,</span><span class="s2">&#34;DeptName&#34;</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Window for salary ranking</span>
</span></span><span class="line"><span class="cl"><span class="n">window_spec</span> <span class="o">=</span> <span class="n">Window</span><span class="o">.</span><span class="n">partitionBy</span><span class="p">(</span><span class="s2">&#34;DeptId&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">desc</span><span class="p">(</span><span class="s2">&#34;Salary&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Add rank &amp; filter top salary</span>
</span></span><span class="line"><span class="cl"><span class="n">ranked_salary_df</span> <span class="o">=</span> <span class="n">emp_df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&#34;Rank&#34;</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span><span class="o">.</span><span class="n">over</span><span class="p">(</span><span class="n">window_spec</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">result_df</span> <span class="o">=</span> <span class="n">ranked_salary_df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">&#34;Rank&#34;</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Join department names</span>
</span></span><span class="line"><span class="cl"><span class="n">result_df</span> <span class="o">=</span> <span class="n">result_df</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dept_df</span><span class="p">,</span> <span class="p">[</span><span class="s2">&#34;DeptId&#34;</span><span class="p">],</span> <span class="s2">&#34;left&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Final Output</span>
</span></span><span class="line"><span class="cl"><span class="n">result_df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&#34;EmpName&#34;</span><span class="p">,</span><span class="s2">&#34;DeptName&#34;</span><span class="p">,</span><span class="s2">&#34;Salary&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></span></span></code></pre></div>

  <footer class="footline">
  </footer>
</article>
<article class="default">
  <header class="headline">
  </header>

<h1 id="explode">Explode</h1>

<h2 id="explode-vs-explode_outer">Explode vs Explode_outer</h2>
<p>In PySpark, <code>explode</code> and <code>explode_outer</code> are functions used to work with nested data structures, like <strong>arrays</strong> or <strong>maps</strong>, by <em>â€œexplodingâ€</em> (flattening) each element of an array or key-value pair in a map into separate rows.</p>
<p>The key difference between <strong>explode</strong> and <strong>explode_outer</strong> is in handling <strong>null</strong> or <strong>empty arrays</strong>, which makes them useful in different scenarios.</p>
<hr>
<h2 id="1-explode">1. explode()</h2>
<p>The <code>explode()</code> function takes a column with array or map data and creates a new row for each element in the array (or each key-value pair in the map).<br>
If the array is empty or null, <code>explode()</code> will <strong>drop the row entirely</strong>.</p>
<h4 id="key-characteristics">Key Characteristics</h4>
<ul>
<li>Converts each element in an array or each entry in a map into its own row.</li>
<li>Drops rows with <strong>null</strong> or <strong>empty arrays</strong>.</li>
</ul>
<h3 id="example-using-explode-in-pyspark">Example: Using <code>explode()</code> in PySpark</h3>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">explode</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Initialize Spark session</span>
</span></span><span class="line"><span class="cl"><span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&#34;ExplodeExample&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Sample DataFrame with arrays</span>
</span></span><span class="line"><span class="cl"><span class="n">data</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Alice&#34;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&#34;Math&#34;</span><span class="p">,</span> <span class="s2">&#34;Science&#34;</span><span class="p">]),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Bob&#34;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&#34;History&#34;</span><span class="p">]),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;Cathy&#34;</span><span class="p">,</span> <span class="p">[]),</span>   <span class="c1"># Empty array</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;David&#34;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>  <span class="c1"># Null array</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="p">[</span><span class="s2">&#34;Name&#34;</span><span class="p">,</span> <span class="s2">&#34;Subjects&#34;</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Use explode to flatten the array</span>
</span></span><span class="line"><span class="cl"><span class="n">exploded_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&#34;Name&#34;</span><span class="p">,</span> <span class="n">explode</span><span class="p">(</span><span class="s2">&#34;Subjects&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&#34;Subject&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Show the result</span>
</span></span><span class="line"><span class="cl"><span class="n">exploded_df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></span></span></code></pre></div>
<h3 id="explanation">Explanation:</h3>
<ul>
<li><code>explode()</code> expands the Subjects array into individual rows.</li>
<li>Rows with empty (<code>[]</code>) or null arrays (<code>None</code>) are removed, which is why Cathy and David do not appear in the output.</li>
</ul>
<h2 id="2-explode_outer">2. explode_outer()</h2>
<p>The <code>explode_outer()</code> function works similarly to <code>explode()</code>, but it <strong>keeps rows</strong> with null or empty arrays.<br>
When <code>explode_outer()</code> encounters a null or empty array, it still generates a row for that entry, with <strong>null</strong> as the value in the resulting column.</p>
<h4 id="key-characteristics-1">Key Characteristics</h4>
<ul>
<li>Converts each element in an array or each entry in a map into its own row.</li>
<li><strong>Retains</strong> rows with null or empty arrays, using <code>null</code> values in the exploded column.</li>
</ul>
<h3 id="example-using-explode_outer-in-pyspark">Example: Using <code>explode_outer()</code> in PySpark</h3>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Use explode_outer to flatten the array while keeping null or empty rows</span>
</span></span><span class="line"><span class="cl"><span class="n">exploded_outer_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;Name&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">F</span><span class="o">.</span><span class="n">explode_outer</span><span class="p">(</span><span class="s2">&#34;Subjects&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&#34;Subject&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Show the result</span>
</span></span><span class="line"><span class="cl"><span class="n">exploded_outer_df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></span></span></code></pre></div>
<h3 id="explanation-explode_outer">Explanation: explode_outer()</h3>
<ul>
<li><code>explode_outer()</code> expands the <strong>Subjects</strong> array into individual rows.</li>
<li>Unlike <code>explode()</code>, rows with empty (<code>[]</code>) or null arrays (<code>None</code>) are <strong>kept</strong> in the result, with <strong>null values</strong> in the <code>Subject</code> column for these cases.</li>
</ul>
<hr>
<h2 id="summary-table-of-differences">Summary Table of Differences</h2>
<table>
  <thead>
      <tr>
          <th>Function</th>
          <th>Description</th>
          <th>Null/Empty Arrays Behavior</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>explode()</strong></td>
          <td>Expands each element of an array or map into rows</td>
          <td><strong>Drops</strong> rows with null or empty arrays</td>
      </tr>
      <tr>
          <td><strong>explode_outer()</strong></td>
          <td>Similar to <code>explode()</code>, but retains null/empty</td>
          <td><strong>Keeps</strong> rows with null/empty arrays, fills with <code>null</code></td>
      </tr>
  </tbody>
</table>
<hr>
<p>These functions are very useful when working with <strong>complex, nested data structures</strong>, especially when dealing with <strong>JSON</strong> or other hierarchical data.</p>

  <footer class="footline">
  </footer>
</article>
<article class="default">
  <header class="headline">
  </header>

<h1 id="pivot">Pivot</h1>

<h2 id="pivot-in-pyspark">Pivot in PySpark</h2>
<p>The <strong>pivot</strong> operation in PySpark is used to <strong>transpose rows into columns</strong> based on a specified column&rsquo;s unique values.<br>
It&rsquo;s particularly useful for creating <strong>wide-format data</strong>, where values in one column become new column headers, and corresponding values from another column fill those headers.</p>
<hr>
<h3 id="key-concepts">Key Concepts</h3>
<ol>
<li>
<p><strong>groupBy and pivot</strong></p>
<ul>
<li>The <code>pivot</code> method is typically used in combination with <code>groupBy</code>.</li>
<li>You group by certain columns and pivot one column to create new columns.</li>
</ul>
</li>
<li>
<p><strong>Aggregation Function</strong></p>
<ul>
<li>You need to specify an aggregation function (like <code>sum</code>, <code>avg</code>, <code>count</code>, etc.) to fill the values in the pivoted columns.</li>
</ul>
</li>
<li>
<p><strong>Performance Consideration</strong></p>
<ul>
<li>Pivoting can be <strong>computationally expensive</strong>, especially with a high number of unique values in the pivot column.</li>
<li>For better performance, explicitly specify the values to pivot if possible.</li>
</ul>
</li>
<li>
<p><strong>Syntax</strong></p>
</li>
</ol>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">dataframe</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s2">&#34;group_column&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">pivot</span><span class="p">(</span><span class="s2">&#34;pivot_column&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">aggregation_function</span><span class="p">)</span></span></span></code></pre></div>
<h3 id="example-code-pivot-in-pyspark">Example Code: Pivot in PySpark</h3>
<h4 id="sample-data">Sample Data</h4>
<p>Imagine we have a DataFrame of sales data with the following schema:</p>
<table>
  <thead>
      <tr>
          <th>Product</th>
          <th>Region</th>
          <th>Sales</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>A</td>
          <td>North</td>
          <td>100</td>
      </tr>
      <tr>
          <td>B</td>
          <td>North</td>
          <td>150</td>
      </tr>
      <tr>
          <td>A</td>
          <td>South</td>
          <td>200</td>
      </tr>
      <tr>
          <td>B</td>
          <td>South</td>
          <td>300</td>
      </tr>
  </tbody>
</table>
<p>We want to pivot the data so that regions (North, South) become columns and the sales values are aggregated.</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="nb">sum</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create a Spark session</span>
</span></span><span class="line"><span class="cl"><span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&#34;PivotExample&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create a sample DataFrame</span>
</span></span><span class="line"><span class="cl"><span class="n">data</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;A&#34;</span><span class="p">,</span> <span class="s2">&#34;North&#34;</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;B&#34;</span><span class="p">,</span> <span class="s2">&#34;North&#34;</span><span class="p">,</span> <span class="mi">150</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;A&#34;</span><span class="p">,</span> <span class="s2">&#34;South&#34;</span><span class="p">,</span> <span class="mi">200</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;B&#34;</span><span class="p">,</span> <span class="s2">&#34;South&#34;</span><span class="p">,</span> <span class="mi">300</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;Product&#34;</span><span class="p">,</span> <span class="s2">&#34;Region&#34;</span><span class="p">,</span> <span class="s2">&#34;Sales&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Pivot the DataFrame</span>
</span></span><span class="line"><span class="cl"><span class="n">pivoted_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s2">&#34;Product&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">pivot</span><span class="p">(</span><span class="s2">&#34;Region&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="s2">&#34;Sales&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Show the results</span>
</span></span><span class="line"><span class="cl"><span class="n">pivoted_df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></span></span></code></pre></div>
<h4 id="output">Output</h4>
<table>
  <thead>
      <tr>
          <th>Product</th>
          <th>North</th>
          <th>South</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>A</td>
          <td>100</td>
          <td>200</td>
      </tr>
      <tr>
          <td>B</td>
          <td>150</td>
          <td>300</td>
      </tr>
  </tbody>
</table>
<h3 id="explanation-of-code">Explanation of Code</h3>
<ol>
<li>
<p><strong>groupBy(&ldquo;Product&rdquo;)</strong></p>
<ul>
<li>Groups the data by the <code>Product</code> column.</li>
</ul>
</li>
<li>
<p><strong>pivot(&ldquo;Region&rdquo;)</strong></p>
<ul>
<li>Transforms unique values in the <code>Region</code> column (<code>North</code>, <code>South</code>) into new columns.</li>
</ul>
</li>
<li>
<p><strong>agg(sum(&ldquo;Sales&rdquo;))</strong></p>
<ul>
<li>Computes the sum of <code>Sales</code> for each combination of <code>Product</code> and the new columns created by the pivot.</li>
</ul>
</li>
</ol>
<hr>
<h3 id="notes">Notes</h3>
<ul>
<li><strong>Explicit Pivot Values</strong>: To improve performance, you can specify the pivot values explicitly.</li>
</ul>
<p><code>df.groupBy(&quot;Product&quot;).pivot(&quot;Region&quot;, [&quot;North&quot;, &quot;South&quot;]).agg(sum(&quot;Sales&quot;))</code></p>
<ul>
<li><strong>Handling Null Values</strong>: If some combinations of <code>groupBy</code> and pivot values have no corresponding rows, the resulting cells will contain <code>null</code>.</li>
<li><strong>Alternative Aggregations</strong>: You can use other aggregation functions like <code>avg</code>, <code>max</code>, <code>min</code>, etc.</li>
</ul>
<hr>
<p>This approach is commonly used in creating <strong>summary reports</strong> or preparing data for <strong>machine learning models</strong> where wide-format data is required.</p>
<h2 id="unpivot-in-pyspark">Unpivot in PySpark</h2>
<p>The unpivot operation (also called melting) is used to transform a wide-format table into a long-format table. This means columns are turned into rows, effectively reversing the pivot operation. PySpark doesn&rsquo;t have a direct unpivot function like Pandas&rsquo; melt, but you can achieve it using the selectExpr method or a combination of stack and other DataFrame transformations.</p>
<h3 id="key-concepts-1">Key Concepts</h3>
<p><strong>1. Purpose of Unpivot:</strong></p>
<ul>
<li>
<p>Simplifies data analysis by converting column headers into a single column (e.g., categorical variables).</p>
</li>
<li>
<p>Ideal for scenarios where you need to aggregate data further or visualize it in a long format.</p>
</li>
</ul>
<p><strong>2. Syntax Overview:</strong></p>
<ul>
<li>
<p>Use the stack function inside a selectExpr to unpivot.</p>
</li>
<li>
<p>Stack reshapes the DataFrame by creating multiple rows for specified columns.</p>
</li>
</ul>
<p><strong>3. Performance:</strong></p>
<ul>
<li>Unpivoting can generate many rows, especially if the original DataFrame is wide with numerous columns. Ensure your environment can handle the resulting data volume.</li>
</ul>
<h3 id="example-unpivot-in-pyspark">Example: Unpivot in PySpark</h3>
<h4 id="sample-data-1">Sample Data</h4>
<p>Suppose we have the following DataFrame:</p>
<table>
  <thead>
      <tr>
          <th>Product</th>
          <th>North</th>
          <th>South</th>
          <th>East</th>
          <th>West</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>A</td>
          <td>100</td>
          <td>200</td>
          <td>150</td>
          <td>130</td>
      </tr>
      <tr>
          <td>B</td>
          <td>150</td>
          <td>300</td>
          <td>200</td>
          <td>180</td>
      </tr>
  </tbody>
</table>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create a Spark session</span>
</span></span><span class="line"><span class="cl"><span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&#34;UnpivotExample&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Sample data</span>
</span></span><span class="line"><span class="cl"><span class="n">data</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;A&#34;</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">130</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;B&#34;</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">180</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;Product&#34;</span><span class="p">,</span> <span class="s2">&#34;North&#34;</span><span class="p">,</span> <span class="s2">&#34;South&#34;</span><span class="p">,</span> <span class="s2">&#34;East&#34;</span><span class="p">,</span> <span class="s2">&#34;West&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create the DataFrame</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Unpivot the DataFrame using stack</span>
</span></span><span class="line"><span class="cl"><span class="n">unpivoted_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">selectExpr</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;Product&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;stack(4, &#39;North&#39;, North, &#39;South&#39;, South, &#39;East&#39;, East, &#39;West&#39;, West) as (Region, Sales)&#34;</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Show the results</span>
</span></span><span class="line"><span class="cl"><span class="n">unpivoted_df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></span></span></code></pre></div>
<h3 id="explanation-of-code-1">Explanation of Code</h3>
<ol>
<li>
<p><strong>Input DataFrame</strong>:</p>
<ul>
<li>Each column (North, South, East, West) represents a region&rsquo;s sales for each product.</li>
</ul>
</li>
<li>
<p><strong>selectExpr with stack</strong>:</p>
<ul>
<li>The stack function takes two arguments:
<ul>
<li>The number of columns being unpivoted (4 in this case).</li>
<li>A sequence of column-value pairs: &lsquo;ColumnName1&rsquo;, ColumnValue1, &lsquo;ColumnName2&rsquo;, ColumnValue2, &hellip;.</li>
</ul>
</li>
<li>The result is two new columns: the first contains the column names (now rows, Region), and the second contains the corresponding values (Sales).</li>
</ul>
</li>
<li>
<p><strong>Aliasing Columns</strong>:</p>
<ul>
<li>The stack result is aliased as (Region, Sales) to give meaningful names to the new columns.</li>
</ul>
</li>
</ol>
<hr>
<h3 id="alternative-methods">Alternative Methods</h3>
<p><strong>Using withColumn and union</strong>:<br>
If stack isn&rsquo;t flexible enough, you can manually combine rows for each column:</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">functions</span> <span class="k">as</span> <span class="n">F</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create a DataFrame with union operations for unpivoting</span>
</span></span><span class="line"><span class="cl"><span class="n">north</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&#34;Product&#34;</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">lit</span><span class="p">(</span><span class="s2">&#34;North&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&#34;Region&#34;</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">&#34;North&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&#34;Sales&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">south</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&#34;Product&#34;</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">lit</span><span class="p">(</span><span class="s2">&#34;South&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&#34;Region&#34;</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">&#34;South&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&#34;Sales&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">east</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&#34;Product&#34;</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">lit</span><span class="p">(</span><span class="s2">&#34;East&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&#34;Region&#34;</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">&#34;East&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&#34;Sales&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">west</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&#34;Product&#34;</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">lit</span><span class="p">(</span><span class="s2">&#34;West&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&#34;Region&#34;</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">&#34;West&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&#34;Sales&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Combine all rows using union</span>
</span></span><span class="line"><span class="cl"><span class="n">unpivoted_df</span> <span class="o">=</span> <span class="n">north</span><span class="o">.</span><span class="n">union</span><span class="p">(</span><span class="n">south</span><span class="p">)</span><span class="o">.</span><span class="n">union</span><span class="p">(</span><span class="n">east</span><span class="p">)</span><span class="o">.</span><span class="n">union</span><span class="p">(</span><span class="n">west</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Show results</span>
</span></span><span class="line"><span class="cl"><span class="n">unpivoted_df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></span></span></code></pre></div>
<h3 id="notes-1">Notes</h3>
<ol>
<li>
<p><strong>Performance Considerations</strong></p>
<ul>
<li><code>stack</code> is efficient for unpivoting a large number of columns.</li>
<li>The <code>union</code> method may become unwieldy for many columns, but it offers more control over the transformation process.</li>
</ul>
</li>
<li>
<p><strong>Dynamic Column Unpivoting</strong></p>
<ul>
<li>If the column names are not fixed (dynamic), you can:
<ul>
<li>Collect the column names dynamically using <code>df.columns</code>.</li>
<li>Construct the <code>selectExpr</code> or <code>union</code> queries programmatically.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Resulting Format</strong></p>
<ul>
<li>After unpivoting, the data will have more rows but fewer columns.</li>
<li>Ensure downstream processes are optimized to handle the increased row count.</li>
</ul>
</li>
</ol>
<hr>
<p>Unpivoting is a powerful operation for <strong>restructuring data</strong> and is frequently used in <strong>data preprocessing, reporting, and machine learning pipelines</strong>.</p>

  <footer class="footline">
  </footer>
</article>
<article class="default">
  <header class="headline">
  </header>

<h1 id="comparisons">Comparisons</h1>

<h2 id="hadoop-vs-spark-architecture">Hadoop vs. Spark Architecture</h2>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>Hadoop</th>
          <th>Spark</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Storage</strong></td>
          <td>Uses HDFS for storage</td>
          <td>Uses in-memory processing for speed</td>
      </tr>
      <tr>
          <td><strong>Processing</strong></td>
          <td>MapReduce is disk-based</td>
          <td>In-memory processing improves performance</td>
      </tr>
      <tr>
          <td><strong>Integration</strong></td>
          <td>Runs independently or with Hadoop ecosystem</td>
          <td>Can run on top of Hadoop; more flexible</td>
      </tr>
      <tr>
          <td><strong>Complexity</strong></td>
          <td>More complex setup and deployment</td>
          <td>Simpler to deploy and configure</td>
      </tr>
      <tr>
          <td><strong>Performance</strong></td>
          <td>Slower for iterative tasks due to disk I/O</td>
          <td>Better performance for iterative tasks</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="rdd-vs-dataframe-vs-dataset">RDD vs. DataFrame vs. Dataset</h2>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>RDD</th>
          <th>DataFrame</th>
          <th>Dataset</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>API Level</strong></td>
          <td>Low-level, more control</td>
          <td>High-level, optimized with Catalyst</td>
          <td>High-level, type-safe</td>
      </tr>
      <tr>
          <td><strong>Schema</strong></td>
          <td>No schema, unstructured</td>
          <td>Uses schema for structured data</td>
          <td>Strongly typed, compile-time type safety</td>
      </tr>
      <tr>
          <td><strong>Optimization</strong></td>
          <td>No built-in optimization</td>
          <td>Optimized using Catalyst</td>
          <td>Optimized using Catalyst, with type safety</td>
      </tr>
      <tr>
          <td><strong>Type Safety</strong></td>
          <td>No type safety</td>
          <td>No compile-time type safety</td>
          <td>Provides compile-time type safety</td>
      </tr>
      <tr>
          <td><strong>Performance</strong></td>
          <td>Less optimized for performance</td>
          <td>Better performance due to optimizations</td>
          <td>Combines type safety with optimization</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="action-vs-transformation-in-spark">Action vs. Transformation in Spark</h2>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>Action</th>
          <th>Transformation</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Execution</strong></td>
          <td>Triggers execution of the Spark job</td>
          <td>Builds up a logical plan of data operations</td>
      </tr>
      <tr>
          <td><strong>Return Type</strong></td>
          <td>Returns results or output</td>
          <td>Returns a new RDD/DataFrame</td>
      </tr>
      <tr>
          <td><strong>Evaluation</strong></td>
          <td>Eager evaluation; executes immediately</td>
          <td>Lazy evaluation; executed when an action is triggered</td>
      </tr>
      <tr>
          <td><strong>Computation</strong></td>
          <td>Involves actual computation (e.g., <code>collect()</code>)</td>
          <td>Defines data transformations (e.g., <code>map()</code>)</td>
      </tr>
      <tr>
          <td><strong>Performance</strong></td>
          <td>Can cause data processing; affects performance</td>
          <td>Does not affect performance until an action is called</td>
      </tr>
  </tbody>
</table>
<h3 id="map-vs-flatmap">Map vs. FlatMap</h3>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>Map</th>
          <th>FlatMap</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Output</td>
          <td>Returns one output element per input element</td>
          <td>Can return zero or more output elements per input</td>
      </tr>
      <tr>
          <td>Flattening</td>
          <td>Does not flatten output</td>
          <td>Flattens the output into a single level</td>
      </tr>
      <tr>
          <td>Use Case</td>
          <td>Suitable for one-to-one transformations</td>
          <td>Suitable for one-to-many transformations</td>
      </tr>
      <tr>
          <td>Complexity</td>
          <td>Simpler, straightforward</td>
          <td>More complex due to variable number of outputs</td>
      </tr>
      <tr>
          <td>Examples</td>
          <td><code>map(x =&gt; x * 2)</code></td>
          <td><code>flatMap(x =&gt; x.split(&quot; &quot;))</code></td>
      </tr>
  </tbody>
</table>
<h3 id="groupbykey-vs-reducebykey">GroupByKey vs. ReduceByKey</h3>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>GroupByKey</th>
          <th>ReduceByKey</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Operation</td>
          <td>Groups all values by key</td>
          <td>Aggregates values with the same key</td>
      </tr>
      <tr>
          <td>Efficiency</td>
          <td>Can lead to high shuffling</td>
          <td>More efficient due to partial aggregation</td>
      </tr>
      <tr>
          <td>Data Movement</td>
          <td>Requires shuffling of all values</td>
          <td>Minimizes data movement through local aggregation</td>
      </tr>
      <tr>
          <td>Use Case</td>
          <td>Useful for simple grouping</td>
          <td>Preferred for aggregations and reductions</td>
      </tr>
      <tr>
          <td>Performance</td>
          <td>Less efficient with large datasets</td>
          <td>Better performance for large datasets</td>
      </tr>
  </tbody>
</table>
<h3 id="repartition-vs-coalesce">Repartition vs. Coalesce</h3>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>Repartition</th>
          <th>Coalesce</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Partitioning</td>
          <td>Can increase or decrease the number of partitions</td>
          <td>Only decreases the number of partitions</td>
      </tr>
      <tr>
          <td>Shuffling</td>
          <td>Involves full shuffle</td>
          <td>Avoids full shuffle, more efficient</td>
      </tr>
      <tr>
          <td>Efficiency</td>
          <td>More expensive due to shuffling</td>
          <td>More efficient for reducing partitions</td>
      </tr>
      <tr>
          <td>Use Case</td>
          <td>Used for increasing partitions or balancing load</td>
          <td>Used for reducing partitions, typically after filtering</td>
      </tr>
      <tr>
          <td>Performance</td>
          <td>Can be costly for large datasets</td>
          <td>More cost-effective for reducing partitions</td>
      </tr>
  </tbody>
</table>
<h3 id="cache-vs-persist">Cache vs. Persist</h3>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>Cache</th>
          <th>Persist</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Storage Level</td>
          <td>Defaults to MEMORY_ONLY</td>
          <td>Can use various storage levels (e.g., MEMORY_AND_DISK)</td>
      </tr>
      <tr>
          <td>Flexibility</td>
          <td>Simplified, with default storage level</td>
          <td>Offers more options for storage levels</td>
      </tr>
      <tr>
          <td>Use Case</td>
          <td>Suitable for simple caching scenarios</td>
          <td>Suitable for complex caching scenarios requiring different storage levels</td>
      </tr>
      <tr>
          <td>Implementation</td>
          <td>Easier to use, shorthand for MEMORY_ONLY</td>
          <td>More flexible, allows custom storage options</td>
      </tr>
      <tr>
          <td>Performance</td>
          <td>Suitable when memory suffices</td>
          <td>More efficient when dealing with larger datasets and limited memory</td>
      </tr>
  </tbody>
</table>
<h3 id="narrow-vs-wide-transformation">Narrow vs. Wide Transformation</h3>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>Narrow Transformation</th>
          <th>Wide Transformation</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Partitioning</td>
          <td>Each parent partition is used by one child partition</td>
          <td>Requires data from multiple partitions</td>
      </tr>
      <tr>
          <td>Shuffling</td>
          <td>No shuffling required</td>
          <td>Involves shuffling of data</td>
      </tr>
      <tr>
          <td>Performance</td>
          <td>More efficient and less costly</td>
          <td>Less efficient due to data movement</td>
      </tr>
      <tr>
          <td>Examples</td>
          <td><code>map()</code>, <code>filter()</code></td>
          <td><code>groupByKey()</code>, <code>join()</code></td>
      </tr>
      <tr>
          <td>Complexity</td>
          <td>Simpler and faster</td>
          <td>More complex and slower due to data movement</td>
      </tr>
  </tbody>
</table>
<h3 id="collect-vs-take">Collect vs. Take</h3>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>Collect</th>
          <th>Take</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Output</td>
          <td>Retrieves all data from the RDD/DataFrame</td>
          <td>Retrieves a specified number of elements</td>
      </tr>
      <tr>
          <td>Memory Usage</td>
          <td>Can be expensive and use a lot of memory</td>
          <td>More memory-efficient</td>
      </tr>
      <tr>
          <td>Use Case</td>
          <td>Used when you need the entire dataset</td>
          <td>Useful for sampling or debugging</td>
      </tr>
      <tr>
          <td>Performance</td>
          <td>Can cause performance issues with large data</td>
          <td>Faster and more controlled</td>
      </tr>
      <tr>
          <td>Action Type</td>
          <td>Triggers full data retrieval</td>
          <td>Triggers partial data retrieval</td>
      </tr>
  </tbody>
</table>
<h3 id="broadcast-variable-vs-accumulator">Broadcast Variable vs. Accumulator</h3>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>Broadcast Variable</th>
          <th>Accumulator</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Purpose</td>
          <td>Efficiently shares read-only data across tasks</td>
          <td>Tracks metrics and aggregates values</td>
      </tr>
      <tr>
          <td>Data Type</td>
          <td>Data that is shared and read-only</td>
          <td>Counters and sums, often numerical</td>
      </tr>
      <tr>
          <td>Use Case</td>
          <td>Useful for large lookup tables or configurations</td>
          <td>Useful for aggregating metrics like counts</td>
      </tr>
      <tr>
          <td>Efficiency</td>
          <td>Reduces data transfer by broadcasting data once</td>
          <td>Efficient for aggregating values across tasks</td>
      </tr>
      <tr>
          <td>Mutability</td>
          <td>Immutable, read-only</td>
          <td>Mutable, can be updated during computation</td>
      </tr>
  </tbody>
</table>
<h3 id="spark-sql-vs-dataframe-api">Spark SQL vs. DataFrame API</h3>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>Spark SQL</th>
          <th>DataFrame API</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Interface</td>
          <td>Executes SQL queries</td>
          <td>Provides a programmatic interface</td>
      </tr>
      <tr>
          <td>Syntax</td>
          <td>Uses SQL-like syntax</td>
          <td>Uses function-based syntax</td>
      </tr>
      <tr>
          <td>Optimization</td>
          <td>Optimized with Catalyst</td>
          <td>Optimized with Catalyst</td>
      </tr>
  </tbody>
</table>
<h3 id="spark-streaming-vs-structured-streaming">Spark Streaming vs. Structured Streaming</h3>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>Spark Streaming</th>
          <th>Structured Streaming</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Processing</td>
          <td>Micro-batch processing</td>
          <td>Micro-batch and continuous processing</td>
      </tr>
      <tr>
          <td>API</td>
          <td>RDD-based API</td>
          <td>SQL-based API with DataFrame/Dataset support</td>
      </tr>
      <tr>
          <td>Complexity</td>
          <td>More complex and lower-level</td>
          <td>Simplified with high-level APIs</td>
      </tr>
      <tr>
          <td>Consistency</td>
          <td>Can be less consistent due to micro-batches</td>
          <td>Provides stronger consistency guarantees</td>
      </tr>
      <tr>
          <td>Performance</td>
          <td>Can be slower for complex queries</td>
          <td>Better performance with optimizations</td>
      </tr>
  </tbody>
</table>
<h3 id="shuffle-vs-mapreduce">Shuffle vs. MapReduce</h3>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>Shuffle</th>
          <th>MapReduce</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Operation</td>
          <td>Data reorganization across partitions</td>
          <td>Data processing model for distributed computing</td>
      </tr>
      <tr>
          <td>Efficiency</td>
          <td>Can be costly due to data movement</td>
          <td>Designed for batch processing with high I/O</td>
      </tr>
      <tr>
          <td>Performance</td>
          <td>Affects performance based on the amount of data movement</td>
          <td>Optimized for large-scale data processing but less efficient for iterative tasks</td>
      </tr>
      <tr>
          <td>Use Case</td>
          <td>Used in Spark for data redistribution</td>
          <td>Used in Hadoop for data processing tasks</td>
      </tr>
      <tr>
          <td>Implementation</td>
          <td>Integrated into Spark operations</td>
          <td>Core component of the Hadoop ecosystem</td>
      </tr>
  </tbody>
</table>
<h3 id="union-vs-join">Union vs. Join</h3>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>Union</th>
          <th>Join</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Operation</td>
          <td>Combines two DataFrames/RDDs into one</td>
          <td>Combines rows from two DataFrames/RDDs based on a key</td>
      </tr>
      <tr>
          <td>Data Requirements</td>
          <td>Requires same schema for both DataFrames/RDDs</td>
          <td>Requires a common key for joining</td>
      </tr>
      <tr>
          <td>Performance</td>
          <td>Generally faster as it does not require key matching</td>
          <td>Can be slower due to key matching and shuffling</td>
      </tr>
      <tr>
          <td>Output</td>
          <td>Stacks data vertically</td>
          <td>Merges data horizontally based on keys</td>
      </tr>
      <tr>
          <td>Use Case</td>
          <td>Appending data or combining datasets</td>
          <td>Merging related data based on keys</td>
      </tr>
  </tbody>
</table>
<h3 id="executor-vs-driver">Executor vs. Driver</h3>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>Executor</th>
          <th>Driver</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Role</td>
          <td>Executes tasks and processes data</td>
          <td>Coordinates and manages the Spark application</td>
      </tr>
      <tr>
          <td>Memory</td>
          <td>Memory allocated per executor for data processing</td>
          <td>Memory used for managing application execution</td>
      </tr>
      <tr>
          <td>Lifecycle</td>
          <td>Exists throughout the application execution</td>
          <td>Starts and stops the Spark application</td>
      </tr>
      <tr>
          <td>Tasks</td>
          <td>Runs the tasks assigned by the driver</td>
          <td>Schedules and coordinates tasks and jobs</td>
      </tr>
      <tr>
          <td>Parallelism</td>
          <td>Multiple executors run in parallel</td>
          <td>Single driver coordinates multiple executors</td>
      </tr>
  </tbody>
</table>
<h3 id="checkpointing-vs-caching">Checkpointing vs. Caching</h3>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>Checkpointing</th>
          <th>Caching</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Purpose</td>
          <td>Provides fault tolerance and reliability</td>
          <td>Improves performance by storing intermediate data</td>
      </tr>
      <tr>
          <td>Storage</td>
          <td>Writes data to stable storage (e.g., HDFS)</td>
          <td>Stores data in memory or on disk (depends on storage level)</td>
      </tr>
      <tr>
          <td>Use Case</td>
          <td>Used for recovery in case of failures</td>
          <td>Used for optimizing repeated operations</td>
      </tr>
      <tr>
          <td>Impact</td>
          <td>Can be more costly and slow</td>
          <td>Generally faster but not suitable for fault tolerance</td>
      </tr>
      <tr>
          <td>Data</td>
          <td>Data is written to external storage</td>
          <td>Data is kept in memory or disk storage for quick access</td>
      </tr>
  </tbody>
</table>
<h3 id="reducebykey-vs-aggregatebykey">ReduceByKey vs. AggregateByKey</h3>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>ReduceByKey</th>
          <th>AggregateByKey</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Operation</td>
          <td>Combines values with the same key using a function</td>
          <td>Performs custom aggregation and combinatory operations</td>
      </tr>
      <tr>
          <td>Efficiency</td>
          <td>More efficient for simple aggregations</td>
          <td>Flexible for complex aggregation scenarios</td>
      </tr>
      <tr>
          <td>Shuffling</td>
          <td>Involves shuffling but can be optimized</td>
          <td>Can be more complex due to custom aggregation</td>
      </tr>
      <tr>
          <td>Use Case</td>
          <td>Suitable for straightforward aggregations</td>
          <td>Ideal for advanced and custom aggregations</td>
      </tr>
      <tr>
          <td>Performance</td>
          <td>Generally faster for simple operations</td>
          <td>Performance varies with complexity</td>
      </tr>
  </tbody>
</table>
<h3 id="sqlcontext-vs-hivecontext-vs-sparksession">SQLContext vs. HiveContext vs. SparkSession</h3>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>SQLContext</th>
          <th>HiveContext</th>
          <th>SparkSession</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Purpose</td>
          <td>Provides SQL query capabilities</td>
          <td>Provides integration with Hive for SQL queries</td>
          <td>Unified entry point for Spark functionality</td>
      </tr>
      <tr>
          <td>Integration</td>
          <td>Basic SQL capabilities</td>
          <td>Integrates with Hive Metastore</td>
          <td>Combines SQL, DataFrame, and Streaming APIs</td>
      </tr>
      <tr>
          <td>Usage</td>
          <td>Legacy, less functionality</td>
          <td>Supports HiveQL and Hive UDFs</td>
          <td>Supports all Spark functionalities including Hive</td>
      </tr>
      <tr>
          <td>Configuration</td>
          <td>Less flexible and older</td>
          <td>Requires Hive setup and configuration</td>
          <td>Modern and flexible, manages configurations</td>
      </tr>
      <tr>
          <td>Capabilities</td>
          <td>Limited to SQL queries</td>
          <td>Extends SQL capabilities with Hive integration</td>
          <td>Comprehensive access to all Spark features</td>
      </tr>
  </tbody>
</table>
<h3 id="broadcast-join-vs-shuffle-join">Broadcast Join vs. Shuffle Join</h3>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>Broadcast Join</th>
          <th>Shuffle Join</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Operation</td>
          <td>Broadcasts a small dataset to all nodes</td>
          <td>Shuffles data across nodes for joining</td>
      </tr>
      <tr>
          <td>Data Size</td>
          <td>Suitable for small datasets</td>
          <td>Suitable for larger datasets</td>
      </tr>
      <tr>
          <td>Efficiency</td>
          <td>More efficient for small tables</td>
          <td>More suited for large datasets</td>
      </tr>
      <tr>
          <td>Performance</td>
          <td>Faster due to reduced shuffling</td>
          <td>Can be slower due to extensive shuffling</td>
      </tr>
      <tr>
          <td>Use Case</td>
          <td>Use when one dataset is small relative to others</td>
          <td>Use when both datasets are large</td>
      </tr>
  </tbody>
</table>
<h3 id="sparkcontext-vs-sparksession">SparkContext vs. SparkSession</h3>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>SparkContext</th>
          <th>SparkSession</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Purpose</td>
          <td>Entry point for Spark functionality</td>
          <td>Unified entry point for Spark functionalities</td>
      </tr>
      <tr>
          <td>Lifecycle</td>
          <td>Created before Spark jobs start</td>
          <td>Manages the Spark application lifecycle</td>
      </tr>
      <tr>
          <td>Functionality</td>
          <td>Provides access to RDD and basic Spark functionality</td>
          <td>Provides access to RDD, DataFrame, SQL, and Streaming APIs</td>
      </tr>
      <tr>
          <td>Configuration</td>
          <td>Configuration is less flexible</td>
          <td>More flexible and easier to configure</td>
      </tr>
      <tr>
          <td>Usage</td>
          <td>Older, used for legacy applications</td>
          <td>Modern and recommended for new applications</td>
      </tr>
  </tbody>
</table>
<h3 id="structured-streaming-vs-spark-streaming">Structured Streaming vs. Spark Streaming</h3>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>Structured Streaming</th>
          <th>Spark Streaming</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Processing</td>
          <td>Micro-batch and continuous processing</td>
          <td>Micro-batch processing</td>
      </tr>
      <tr>
          <td>API</td>
          <td>SQL-based API with DataFrame/Dataset support</td>
          <td>RDD-based API</td>
      </tr>
      <tr>
          <td>Complexity</td>
          <td>Simplified and high-level</td>
          <td>More complex and low-level</td>
      </tr>
      <tr>
          <td>Consistency</td>
          <td>Provides stronger consistency guarantees</td>
          <td>Can be less consistent due to micro-batches</td>
      </tr>
      <tr>
          <td>Performance</td>
          <td>Better performance with built-in optimizations</td>
          <td>Can be slower for complex queries</td>
      </tr>
  </tbody>
</table>
<h3 id="partitioning-vs-bucketing">Partitioning vs. Bucketing</h3>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>Partitioning</th>
          <th>Bucketing</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Purpose</td>
          <td>Divides data into multiple partitions based on a key</td>
          <td>Divides data into buckets based on a hash function</td>
      </tr>
      <tr>
          <td>Usage</td>
          <td>Used to optimize queries by reducing data scanned</td>
          <td>Used to improve join performance and maintain sorted data</td>
      </tr>
      <tr>
          <td>Shuffling</td>
          <td>Reduces shuffling by placing related data together</td>
          <td>Reduces shuffle during joins and aggregations</td>
      </tr>
      <tr>
          <td>Data Layout</td>
          <td>Data is physically separated based on partition key</td>
          <td>Data is organized into fixed-size buckets</td>
      </tr>
      <tr>
          <td>Performance</td>
          <td>Improves performance for queries involving partition keys</td>
          <td>Enhances performance for join operations</td>
      </tr>
  </tbody>
</table>

  <footer class="footline">
  </footer>
</article>
          </section>
        </div>
      </main>
    </div>
    <script src="/js/js-yaml/js-yaml.min.js?1759918286" defer></script>
    <script src="/js/d3/d3-color.min.js?1759918286" defer></script>
    <script src="/js/d3/d3-dispatch.min.js?1759918286" defer></script>
    <script src="/js/d3/d3-drag.min.js?1759918286" defer></script>
    <script src="/js/d3/d3-ease.min.js?1759918286" defer></script>
    <script src="/js/d3/d3-interpolate.min.js?1759918286" defer></script>
    <script src="/js/d3/d3-selection.min.js?1759918286" defer></script>
    <script src="/js/d3/d3-timer.min.js?1759918286" defer></script>
    <script src="/js/d3/d3-transition.min.js?1759918286" defer></script>
    <script src="/js/d3/d3-zoom.min.js?1759918286" defer></script>
    <script src="/js/mermaid/mermaid.min.js?1759918286" defer></script>
    <script>
      window.relearn.themeUseMermaid = JSON.parse("{}");
    </script>
    <script src="/js/clipboard/clipboard.min.js?1759918286" defer></script>
    <script src="/js/perfect-scrollbar/perfect-scrollbar.min.js?1759918286" defer></script>
    <script src="/js/theme.min.js?1759918286" defer></script>
  </body>
</html>
