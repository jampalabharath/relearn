var relearn_searchindex = [
  {
    "breadcrumb": "ADB",
    "content": "Creating DataFrame Creating DataFrame from Lists/Tuples # Sample Data data = [(1, \"Alice\"), (2, \"Bob\"), (3, \"Charlie\"), (4, \"David\"), (5, \"Eve\")] columns = [\"ID\", \"Name\"] # Create DataFrame df = spark.createDataFrame(data, columns) # Show DataFrame df.show() Creating DataFrame from Pandas import pandas as pd # Sample Pandas DataFrame pandas_df = pd.DataFrame(data, columns=columns) # Convert to PySpark DataFrame df_from_pandas = spark.createDataFrame(pandas_df) df_from_pandas.show() Create DataFrame from Dictionary data_dict = [{\"ID\": 1, \"Name\": \"Alice\"}, {\"ID\": 2, \"Name\": \"Bob\"}] df_from_dict = spark.createDataFrame(data_dict) df_from_dict.show() Create Empty DataFrame You can create an empty DataFrame with just schema definitions.\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType # Define Schema schema = StructType([ StructField(\"ID\", IntegerType(), True), StructField(\"Name\", StringType(), True) ]) # Create Empty DataFrame empty_df = spark.createDataFrame([], schema) empty_df.show() Creating DataFrame from Structured Data (CSV, JSON, Parquet) # Reading CSV file into DataFrame df_csv = spark.read.csv(\"/path/to/file.csv\", header=True, inferSchema=True) df_csv.show() # Reading JSON file into DataFrame df_json = spark.read.json(\"/path/to/file.json\") df_json.show() # Reading Parquet file into DataFrame df_parquet = spark.read.parquet(\"/path/to/file.parquet\") df_parquet.show() show() Function in PySpark DataFrames The show() function in PySpark displays the contents of a DataFrame in a tabular format. It has several useful parameters for customization:\nParameters: n: Number of rows to display (default is 20) truncate: If set to True, it truncates column values longer than 20 characters (default is True) vertical: If set to True, prints rows in a vertical format Usage Examples: # Show the first 3 rows, truncate columns to 25 characters, and display vertically: df.show(n=3, truncate=25, vertical=True) # Show entire DataFrame (default settings): df.show() # Show the first 10 rows: df.show(10) # Show DataFrame without truncating any columns: df.show(truncate=False) Loading Data from CSV File into a DataFrame Loading data into DataFrames is a fundamental step in any data processing workflow in PySpark. This document outlines how to load data from CSV files into a DataFrame, including using a custom schema and the implications of using the inferSchema option.\n1. Import Required Libraries Before loading the data, ensure you import the necessary modules:\nfrom pyspark.sql import SparkSession from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType 2. Define the Schema You can define a custom schema for your CSV file. This allows you to explicitly set the data types for each column.\n# Define the schema for the CSV file custom_schema = StructType([ StructField(\"id\", IntegerType(), True), StructField(\"name\", StringType(), True), StructField(\"age\", IntegerType(), True), StructField(\"salary\", DoubleType(), True) ]) 3. Read the CSV File Load the CSV file into a DataFrame using the read.csv() method. Here, header=True treats the first row as headers, and inferSchema=True allows Spark to automatically assign data types to columns.\n# Read the CSV file with the custom schema df = spark.read.csv(\"your_file.csv\", schema=custom_schema, header=True) 4. Load Multiple CSV Files To read multiple CSV files into a single DataFrame, you can pass a list of file paths. Ensure that the schema is consistent across all files.\n# List of file paths file_paths = [\"file1.csv\", \"file2.csv\", \"file3.csv\"] # Read multiple CSV files into a single DataFrame df = spark.read.csv(file_paths, header=True, inferSchema=True) 5. Load a CSV from FileStore Here is an example of loading a CSV file from Databricks FileStore:\ndf = spark.read.csv(\"/FileStore/tables/Order.csv\", header=True, inferSchema=True, sep=',') 6. Display the DataFrame Use the following commands to check the schema and display the DataFrame:\n# Print the schema of the DataFrame df.printSchema() # Show the first 20 rows of the DataFrame df.show() # Displays only the first 20 rows # Display the DataFrame in a tabular format display(df) # For Databricks notebooks Interview Question: How Does inferSchema Work? Behind the Scenes: When you use inferSchema, Spark runs a job that scans the CSV file from top to bottom to identify the best-suited data type for each column based on the values it encounters.\nDoes It Make Sense to Use inferSchema? Pros:\nUseful when the schema of the file keeps changing, as it allows Spark to automatically detect the data types. Cons:\nPerformance Impact: Spark must scan the entire file, which can take extra time, especially for large files. Loss of Control: You lose the ability to explicitly define the schema, which may lead to incorrect data types if the data is inconsistent. Conclusion Loading data from CSV files into a DataFrame is straightforward in PySpark. Understanding how to define a schema and the implications of using inferSchema is crucial for optimizing your data processing workflows.\nThis document provides a comprehensive overview of how to load CSV data into DataFrames in PySpark, along with considerations for using schema inference. Let me know if you need any more details or adjustments!\nPySpark DataFrame Schema Definition 1. Defining Schema Programmatically with StructType from pyspark.sql.types import * # Define the schema using StructType employeeSchema = StructType([ StructField(\"ID\", IntegerType(), True), StructField(\"Name\", StringType(), True), StructField(\"Age\", IntegerType(), True), StructField(\"Salary\", DoubleType(), True), StructField(\"Joining_Date\", StringType(), True), # Keeping as String for date issues StructField(\"Department\", StringType(), True), StructField(\"Performance_Rating\", IntegerType(), True), StructField(\"Email\", StringType(), True), StructField(\"Address\", StringType(), True), StructField(\"Phone\", StringType(), True) ]) # Load the DataFrame with the defined schema df = spark.read.load(\"/FileStore/tables/employees.csv\",format=\"csv\", header=True, schema=employeeSchema) # Print the schema of the DataFrame df.printSchema() # Optionally display the DataFrame # display(df) 2. Defining Schema as a String # Define the schema as a string employeeSchemaString = ''' ID Integer, Name String, Age Integer, Salary Double, Joining_Date String, Department String, Performance_Rating Integer, Email String, Address String, Phone String ''' # Load the DataFrame with the defined schema df = spark.read.load(\"dbfs:/FileStore/shared_uploads/imsvk11@gmail.com/employee_data.csv\", format=\"csv\", header=True, schema=employeeSchemaString) # Print the schema of the DataFrame df.printSchema() # Optionally display the DataFrame # display(df) Explanation Schema Definition: Both methods define a schema for the DataFrame, accommodating the dataset‚Äôs requirements, including handling null values where applicable. Data Types: The Joining_Date column is defined as StringType to accommodate potential date format issues or missing values. Loading the DataFrame: The spark.read.load method is used to load the CSV file into a DataFrame using the specified schema. Printing the Schema: The df.printSchema() function allows you to verify that the DataFrame is structured as intended.",
    "description": "Creating DataFrame Creating DataFrame from Lists/Tuples # Sample Data data = [(1, \"Alice\"), (2, \"Bob\"), (3, \"Charlie\"), (4, \"David\"), (5, \"Eve\")] columns = [\"ID\", \"Name\"] # Create DataFrame df = spark.createDataFrame(data, columns) # Show DataFrame df.show() Creating DataFrame from Pandas import pandas as pd # Sample Pandas DataFrame pandas_df = pd.DataFrame(data, columns=columns) # Convert to PySpark DataFrame df_from_pandas = spark.createDataFrame(pandas_df) df_from_pandas.show() Create DataFrame from Dictionary data_dict = [{\"ID\": 1, \"Name\": \"Alice\"}, {\"ID\": 2, \"Name\": \"Bob\"}] df_from_dict = spark.createDataFrame(data_dict) df_from_dict.show() Create Empty DataFrame You can create an empty DataFrame with just schema definitions.\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType # Define Schema schema = StructType([ StructField(\"ID\", IntegerType(), True), StructField(\"Name\", StringType(), True) ]) # Create Empty DataFrame empty_df = spark.createDataFrame([], schema) empty_df.show() Creating DataFrame from Structured Data (CSV, JSON, Parquet) # Reading CSV file into DataFrame df_csv = spark.read.csv(\"/path/to/file.csv\", header=True, inferSchema=True) df_csv.show() # Reading JSON file into DataFrame df_json = spark.read.json(\"/path/to/file.json\") df_json.show() # Reading Parquet file into DataFrame df_parquet = spark.read.parquet(\"/path/to/file.parquet\") df_parquet.show() show() Function in PySpark DataFrames The show() function in PySpark displays the contents of a DataFrame in a tabular format. It has several useful parameters for customization:",
    "tags": [],
    "title": "DF Basics",
    "uri": "/azure_data_bricks/df-creation/"
  },
  {
    "breadcrumb": "SQL",
    "content": "This guide covers various SELECT query techniques used for retrieving, filtering, sorting, and aggregating data efficiently.\nComments -- This is a single-line comment. /* This is a multiple-line comment */ 1. SELECT ALL COLUMNS -- Retrieve All Customer Data SELECT * FROM customers; -- Retrieve All Order Data SELECT * FROM orders; 2. SELECT SPECIFIC COLUMNS -- Retrieve each customer's name, country, and score SELECT first_name, country, score FROM customers; 3. WHERE CLAUSE -- Retrieve customers with a score not equal to 0 SELECT * FROM customers WHERE score != 0; -- Retrieve customers from Germany SELECT * FROM customers WHERE country = 'Germany'; -- Retrieve the name and country of customers from Germany SELECT first_name, country FROM customers WHERE country = 'Germany'; 4. ORDER BY -- Sort by highest score first SELECT * FROM customers ORDER BY score DESC; -- Sort by lowest score first SELECT * FROM customers ORDER BY score ASC; -- Sort by country SELECT * FROM customers ORDER BY country ASC; -- Sort by country, then highest score SELECT * FROM customers ORDER BY country ASC, score DESC; -- Customers with score != 0, sorted by highest score SELECT first_name, country, score FROM customers WHERE score != 0 ORDER BY score DESC; 5. GROUP BY -- Find total score for each country SELECT country, SUM(score) AS total_score FROM customers GROUP BY country; -- ‚ùå Invalid: first_name not grouped or aggregated SELECT country, first_name, SUM(score) AS total_score FROM customers GROUP BY country; -- Find total score \u0026 number of customers per country SELECT country, SUM(score) AS total_score, COUNT(id) AS total_customers FROM customers GROUP BY country; 6. HAVING -- Average score \u003e 430 for each country SELECT country, AVG(score) AS avg_score FROM customers GROUP BY country HAVING AVG(score) \u003e 430; -- Only consider customers with score != 0 SELECT country, AVG(score) AS avg_score FROM customers WHERE score != 0 GROUP BY country HAVING AVG(score) \u003e 430; 7. DISTINCT -- Unique list of countries SELECT DISTINCT country FROM customers; 8. TOP -- Retrieve only 3 customers SELECT TOP 3 * FROM customers; -- Top 3 customers with highest scores SELECT TOP 3 * FROM customers ORDER BY score DESC; -- Lowest 2 customers by score SELECT TOP 2 * FROM customers ORDER BY score ASC; -- Two most recent orders SELECT TOP 2 * FROM orders ORDER BY order_date DESC; 9. All Together -- Average score per country, excluding score=0, -- only \u003e 430, ordered by highest avg_score SELECT country, AVG(score) AS avg_score FROM customers WHERE score != 0 GROUP BY country HAVING AVG(score) \u003e 430 ORDER BY AVG(score) DESC; 10. COOL STUFF ‚Äì Additional SQL Features -- Execute multiple queries at once SELECT * FROM customers; SELECT * FROM orders; -- Select static values SELECT 123 AS static_number; SELECT 'Hello' AS static_string; -- Assign a constant value in query results SELECT id, first_name, 'New Customer' AS customer_type FROM customers;",
    "description": "This guide covers various SELECT query techniques used for retrieving, filtering, sorting, and aggregating data efficiently.\nComments -- This is a single-line comment. /* This is a multiple-line comment */ 1. SELECT ALL COLUMNS -- Retrieve All Customer Data SELECT * FROM customers; -- Retrieve All Order Data SELECT * FROM orders; 2. SELECT SPECIFIC COLUMNS -- Retrieve each customer's name, country, and score SELECT first_name, country, score FROM customers; 3. WHERE CLAUSE -- Retrieve customers with a score not equal to 0 SELECT * FROM customers WHERE score != 0; -- Retrieve customers from Germany SELECT * FROM customers WHERE country = 'Germany'; -- Retrieve the name and country of customers from Germany SELECT first_name, country FROM customers WHERE country = 'Germany'; 4. ORDER BY -- Sort by highest score first SELECT * FROM customers ORDER BY score DESC; -- Sort by lowest score first SELECT * FROM customers ORDER BY score ASC; -- Sort by country SELECT * FROM customers ORDER BY country ASC; -- Sort by country, then highest score SELECT * FROM customers ORDER BY country ASC, score DESC; -- Customers with score != 0, sorted by highest score SELECT first_name, country, score FROM customers WHERE score != 0 ORDER BY score DESC; 5. GROUP BY -- Find total score for each country SELECT country, SUM(score) AS total_score FROM customers GROUP BY country; -- ‚ùå Invalid: first_name not grouped or aggregated SELECT country, first_name, SUM(score) AS total_score FROM customers GROUP BY country; -- Find total score \u0026 number of customers per country SELECT country, SUM(score) AS total_score, COUNT(id) AS total_customers FROM customers GROUP BY country; 6. HAVING -- Average score \u003e 430 for each country SELECT country, AVG(score) AS avg_score FROM customers GROUP BY country HAVING AVG(score) \u003e 430; -- Only consider customers with score != 0 SELECT country, AVG(score) AS avg_score FROM customers WHERE score != 0 GROUP BY country HAVING AVG(score) \u003e 430; 7. DISTINCT -- Unique list of countries SELECT DISTINCT country FROM customers; 8. TOP -- Retrieve only 3 customers SELECT TOP 3 * FROM customers; -- Top 3 customers with highest scores SELECT TOP 3 * FROM customers ORDER BY score DESC; -- Lowest 2 customers by score SELECT TOP 2 * FROM customers ORDER BY score ASC; -- Two most recent orders SELECT TOP 2 * FROM orders ORDER BY order_date DESC; 9. All Together -- Average score per country, excluding score=0, -- only \u003e 430, ordered by highest avg_score SELECT country, AVG(score) AS avg_score FROM customers WHERE score != 0 GROUP BY country HAVING AVG(score) \u003e 430 ORDER BY AVG(score) DESC; 10. COOL STUFF ‚Äì Additional SQL Features -- Execute multiple queries at once SELECT * FROM customers; SELECT * FROM orders; -- Select static values SELECT 123 AS static_number; SELECT 'Hello' AS static_string; -- Assign a constant value in query results SELECT id, first_name, 'New Customer' AS customer_type FROM customers;",
    "tags": [],
    "title": "SELECT Query",
    "uri": "/sql/select/"
  },
  {
    "breadcrumb": "",
    "content": "Course Content 1. Introduction What is SQL? Types of Databases (Relational vs Non-Relational) SQL vs NoSQL SQL Standards (ANSI SQL, T-SQL, PL/SQL, MySQL SQL, PostgreSQL SQL) 2. SQL Basics Database, Schema, Table Data Types (Numeric, String, Date/Time, Boolean, JSON, XML) CREATE, DROP, ALTER INSERT, UPDATE, DELETE SELECT Statement WHERE Clause ORDER BY LIMIT / TOP / FETCH 3. Filtering \u0026 Operators Comparison Operators Logical Operators (AND, OR, NOT) BETWEEN, IN, LIKE IS NULL / IS NOT NULL Pattern Matching (Wildcards, Regex) 4. Functions String Functions (CONCAT, SUBSTRING, TRIM, UPPER, LOWER, LENGTH) Numeric Functions (ROUND, CEIL, FLOOR, ABS, MOD) Date/Time Functions (NOW, DATEADD, DATEDIFF, FORMAT) Conversion Functions (CAST, CONVERT) Aggregate Functions (COUNT, SUM, AVG, MIN, MAX) 5. Joins INNER JOIN LEFT JOIN RIGHT JOIN FULL OUTER JOIN CROSS JOIN SELF JOIN USING vs ON 6. Subqueries \u0026 Nested Queries Scalar Subqueries Table Subqueries Correlated Subqueries EXISTS / NOT EXISTS IN / ANY / ALL 7. Set Operations UNION vs UNION ALL INTERSECT EXCEPT / MINUS 8. Grouping \u0026 Aggregation GROUP BY HAVING GROUPING SETS ROLLUP CUBE 9. Constraints \u0026 Keys Primary Key Foreign Key Unique Key Check Constraint Default Constraint Not Null 10. Indexing \u0026 Performance Clustered vs Non-Clustered Index Composite Index Covering Index Index Maintenance Query Execution Plans SQL Performance Tuning 11. Transactions BEGIN, COMMIT, ROLLBACK Savepoints ACID Properties Isolation Levels (Read Uncommitted, Read Committed, Repeatable Read, Serializable, Snapshot) Deadlocks \u0026 Locking 12. Views \u0026 Materialized Views Creating Views Updatable Views Indexed Views Materialized Views Refresh Strategies 13. Stored Procedures \u0026 Functions User-Defined Functions (Scalar, Table-Valued) Stored Procedures Parameters (IN, OUT, INOUT) Error Handling in Procedures Recursive Procedures Dynamic SQL 14. Triggers AFTER Trigger INSTEAD OF Trigger Row-level vs Statement-level Triggers Use Cases 15. Advanced SQL Window Functions (ROW_NUMBER, RANK, DENSE_RANK, NTILE) LAG \u0026 LEAD FIRST_VALUE \u0026 LAST_VALUE Common Table Expressions (CTEs) Recursive CTEs Pivoting and Unpivoting JSON \u0026 XML in SQL Hierarchical Queries Full-Text Search 16. Security Users \u0026 Roles GRANT, REVOKE, DENY Row-Level Security Column-Level Security Data Masking Encryption at Rest \u0026 In-Transit 17. Advanced Database Concepts Normalization \u0026 Denormalization Star \u0026 Snowflake Schema Partitioning (Horizontal, Vertical) Sharding Federation Replication High Availability \u0026 Failover 18. SQL with Big Data SQL on Hadoop / Hive Spark SQL SQL in Cloud Platforms (AWS Athena, BigQuery, Azure Synapse) 19. SQL in Practice Data Warehousing with SQL ETL using SQL Reporting Queries Performance Benchmarking Debugging Complex Queries 20. Capstone Project Design \u0026 Implement Normalized Database Build ETL Pipeline using SQL Create Analytical Queries \u0026 Reports Optimize Large-Scale SQL Workloads",
    "description": "Course Content 1. Introduction What is SQL? Types of Databases (Relational vs Non-Relational) SQL vs NoSQL SQL Standards (ANSI SQL, T-SQL, PL/SQL, MySQL SQL, PostgreSQL SQL) 2. SQL Basics Database, Schema, Table Data Types (Numeric, String, Date/Time, Boolean, JSON, XML) CREATE, DROP, ALTER INSERT, UPDATE, DELETE SELECT Statement WHERE Clause ORDER BY LIMIT / TOP / FETCH 3. Filtering \u0026 Operators Comparison Operators Logical Operators (AND, OR, NOT) BETWEEN, IN, LIKE IS NULL / IS NOT NULL Pattern Matching (Wildcards, Regex) 4. Functions String Functions (CONCAT, SUBSTRING, TRIM, UPPER, LOWER, LENGTH) Numeric Functions (ROUND, CEIL, FLOOR, ABS, MOD) Date/Time Functions (NOW, DATEADD, DATEDIFF, FORMAT) Conversion Functions (CAST, CONVERT) Aggregate Functions (COUNT, SUM, AVG, MIN, MAX) 5. Joins INNER JOIN LEFT JOIN RIGHT JOIN FULL OUTER JOIN CROSS JOIN SELF JOIN USING vs ON 6. Subqueries \u0026 Nested Queries Scalar Subqueries Table Subqueries Correlated Subqueries EXISTS / NOT EXISTS IN / ANY / ALL 7. Set Operations UNION vs UNION ALL INTERSECT EXCEPT / MINUS 8. Grouping \u0026 Aggregation GROUP BY HAVING GROUPING SETS ROLLUP CUBE 9. Constraints \u0026 Keys Primary Key Foreign Key Unique Key Check Constraint Default Constraint Not Null 10. Indexing \u0026 Performance Clustered vs Non-Clustered Index Composite Index Covering Index Index Maintenance Query Execution Plans SQL Performance Tuning 11. Transactions BEGIN, COMMIT, ROLLBACK Savepoints ACID Properties Isolation Levels (Read Uncommitted, Read Committed, Repeatable Read, Serializable, Snapshot) Deadlocks \u0026 Locking 12. Views \u0026 Materialized Views Creating Views Updatable Views Indexed Views Materialized Views Refresh Strategies 13. Stored Procedures \u0026 Functions User-Defined Functions (Scalar, Table-Valued) Stored Procedures Parameters (IN, OUT, INOUT) Error Handling in Procedures Recursive Procedures Dynamic SQL 14. Triggers AFTER Trigger INSTEAD OF Trigger Row-level vs Statement-level Triggers Use Cases 15. Advanced SQL Window Functions (ROW_NUMBER, RANK, DENSE_RANK, NTILE) LAG \u0026 LEAD FIRST_VALUE \u0026 LAST_VALUE Common Table Expressions (CTEs) Recursive CTEs Pivoting and Unpivoting JSON \u0026 XML in SQL Hierarchical Queries Full-Text Search 16. Security Users \u0026 Roles GRANT, REVOKE, DENY Row-Level Security Column-Level Security Data Masking Encryption at Rest \u0026 In-Transit 17. Advanced Database Concepts Normalization \u0026 Denormalization Star \u0026 Snowflake Schema Partitioning (Horizontal, Vertical) Sharding Federation Replication High Availability \u0026 Failover 18. SQL with Big Data SQL on Hadoop / Hive Spark SQL SQL in Cloud Platforms (AWS Athena, BigQuery, Azure Synapse) 19. SQL in Practice Data Warehousing with SQL ETL using SQL Reporting Queries Performance Benchmarking Debugging Complex Queries 20. Capstone Project Design \u0026 Implement Normalized Database Build ETL Pipeline using SQL Create Analytical Queries \u0026 Reports Optimize Large-Scale SQL Workloads",
    "tags": [],
    "title": "SQL",
    "uri": "/sql/"
  },
  {
    "breadcrumb": "",
    "content": "1. Introduction Overview of Azure Data Factory Data Integration in Cloud ADF Architecture ADF Components (Pipelines, Activities, Datasets, Linked Services, Triggers) ADF vs SSIS vs Synapse Pipelines 2. ADF Basics Creating an ADF Instance ADF Studio Overview Linked Services Datasets Pipelines Activities Triggers (Schedule, Tumbling Window, Event-based, Manual) 3. Data Movement Copy Activity Integration Runtime (IR) Types Azure IR Self-hosted IR Azure-SSIS IR Data Movement Performance Parallelism \u0026 Batch Copy 4. Data Transformation Mapping Data Flows Wrangling Data Flows Data Flow Debugging Joins, Aggregations, Filters, Derived Columns Surrogate Keys \u0026 Window Functions Data Flow Performance Tuning 5. Orchestration Control Activities (Execute Pipeline, ForEach, If Condition, Until, Switch, Wait) Parameterization Variables in Pipelines Expressions \u0026 Functions Dynamic Content Error Handling \u0026 Retry Policies 6. Data Sources \u0026 Destinations Azure Blob Storage Azure Data Lake Storage (ADLS) Azure SQL Database Azure Synapse Analytics On-Premises SQL Server Cosmos DB REST API SAP, Oracle, Teradata, Snowflake Amazon S3 Google Cloud Storage 7. Integration with Other Azure Services Azure Key Vault Integration Azure Monitor \u0026 Log Analytics Power BI Integration Event Grid \u0026 Event Hub Logic Apps \u0026 Functions Azure Machine Learning 8. CI/CD \u0026 DevOps with ADF Source Control with Git (GitHub, Azure Repos) Branching \u0026 Collaboration Publishing \u0026 ARM Templates Continuous Integration \u0026 Deployment Automated Testing Environment Promotion (Dev ‚Üí Test ‚Üí Prod) 9. Security Managed Identity in ADF Service Principals Access Control (RBAC) Data Encryption Network Security (VNET Integration, Private Endpoints) Credential Management with Key Vault 10. Monitoring \u0026 Troubleshooting Pipeline Monitoring Activity Run Details Debugging Failures Alerts \u0026 Notifications Logging with Log Analytics Performance Optimization 11. Advanced Features Incremental Data Loading (Watermarking, Change Data Capture) Slowly Changing Dimensions (SCD) Implementation Parameterized Pipelines \u0026 Templates Global Parameters Data Lineage \u0026 Impact Analysis ADF REST API Managed VNET Data Integration 12. Best Practices Designing Efficient Pipelines Naming Conventions Error Handling Framework Cost Optimization in ADF Scalability \u0026 Performance Tuning Version Control Strategies 13. Real-World Use Cases Building an ETL Pipeline with ADF Data Lake to Synapse ETL Real-Time Event Processing Hybrid Data Integration (On-Prem + Cloud) Machine Learning Model Deployment Orchestration 14. Capstone Project End-to-End Data Warehouse Load using ADF Incremental Data Pipeline with Delta Processing Enterprise Data Integration with ADF and Synapse",
    "description": "1. Introduction Overview of Azure Data Factory Data Integration in Cloud ADF Architecture ADF Components (Pipelines, Activities, Datasets, Linked Services, Triggers) ADF vs SSIS vs Synapse Pipelines 2. ADF Basics Creating an ADF Instance ADF Studio Overview Linked Services Datasets Pipelines Activities Triggers (Schedule, Tumbling Window, Event-based, Manual) 3. Data Movement Copy Activity Integration Runtime (IR) Types Azure IR Self-hosted IR Azure-SSIS IR Data Movement Performance Parallelism \u0026 Batch Copy 4. Data Transformation Mapping Data Flows Wrangling Data Flows Data Flow Debugging Joins, Aggregations, Filters, Derived Columns Surrogate Keys \u0026 Window Functions Data Flow Performance Tuning 5. Orchestration Control Activities (Execute Pipeline, ForEach, If Condition, Until, Switch, Wait) Parameterization Variables in Pipelines Expressions \u0026 Functions Dynamic Content Error Handling \u0026 Retry Policies 6. Data Sources \u0026 Destinations Azure Blob Storage Azure Data Lake Storage (ADLS) Azure SQL Database Azure Synapse Analytics On-Premises SQL Server Cosmos DB REST API SAP, Oracle, Teradata, Snowflake Amazon S3 Google Cloud Storage 7. Integration with Other Azure Services Azure Key Vault Integration Azure Monitor \u0026 Log Analytics Power BI Integration Event Grid \u0026 Event Hub Logic Apps \u0026 Functions Azure Machine Learning 8. CI/CD \u0026 DevOps with ADF Source Control with Git (GitHub, Azure Repos) Branching \u0026 Collaboration Publishing \u0026 ARM Templates Continuous Integration \u0026 Deployment Automated Testing Environment Promotion (Dev ‚Üí Test ‚Üí Prod) 9. Security Managed Identity in ADF Service Principals Access Control (RBAC) Data Encryption Network Security (VNET Integration, Private Endpoints) Credential Management with Key Vault 10. Monitoring \u0026 Troubleshooting Pipeline Monitoring Activity Run Details Debugging Failures Alerts \u0026 Notifications Logging with Log Analytics Performance Optimization 11. Advanced Features Incremental Data Loading (Watermarking, Change Data Capture) Slowly Changing Dimensions (SCD) Implementation Parameterized Pipelines \u0026 Templates Global Parameters Data Lineage \u0026 Impact Analysis ADF REST API Managed VNET Data Integration 12. Best Practices Designing Efficient Pipelines Naming Conventions Error Handling Framework Cost Optimization in ADF Scalability \u0026 Performance Tuning Version Control Strategies 13. Real-World Use Cases Building an ETL Pipeline with ADF Data Lake to Synapse ETL Real-Time Event Processing Hybrid Data Integration (On-Prem + Cloud) Machine Learning Model Deployment Orchestration 14. Capstone Project End-to-End Data Warehouse Load using ADF Incremental Data Pipeline with Delta Processing Enterprise Data Integration with ADF and Synapse",
    "tags": [],
    "title": "Azure Data Factory",
    "uri": "/azure_data_factory/"
  },
  {
    "breadcrumb": "SQL",
    "content": "This guide covers the essential DDL (Data Definition Language) commands used for defining and managing database structures, including creating, modifying, and deleting tables.\n1. CREATE ‚Äì Creating Tables -- Create a new table called persons -- with columns: id, person_name, birth_date, and phone CREATE TABLE persons ( id INT NOT NULL, person_name VARCHAR(50) NOT NULL, birth_date DATE, phone VARCHAR(15) NOT NULL, CONSTRAINT pk_persons PRIMARY KEY (id) ); 2. ALTER ‚Äì Modifying Table Structure -- Add a new column called email to the persons table ALTER TABLE persons ADD email VARCHAR(50) NOT NULL; -- Remove the column phone from the persons table ALTER TABLE persons DROP COLUMN phone; 3. DROP ‚Äì Removing Tables -- Delete the table persons from the database DROP TABLE persons;",
    "description": "This guide covers the essential DDL (Data Definition Language) commands used for defining and managing database structures, including creating, modifying, and deleting tables.\n1. CREATE ‚Äì Creating Tables -- Create a new table called persons -- with columns: id, person_name, birth_date, and phone CREATE TABLE persons ( id INT NOT NULL, person_name VARCHAR(50) NOT NULL, birth_date DATE, phone VARCHAR(15) NOT NULL, CONSTRAINT pk_persons PRIMARY KEY (id) ); 2. ALTER ‚Äì Modifying Table Structure -- Add a new column called email to the persons table ALTER TABLE persons ADD email VARCHAR(50) NOT NULL; -- Remove the column phone from the persons table ALTER TABLE persons DROP COLUMN phone; 3. DROP ‚Äì Removing Tables -- Delete the table persons from the database DROP TABLE persons;",
    "tags": [],
    "title": "DDL",
    "uri": "/sql/ddl/"
  },
  {
    "breadcrumb": "ADB",
    "content": "Column Selection \u0026 Manipulation 1. Different Methods to Select Columns In PySpark, you can select specific columns in multiple ways:\n# Using col() function df.select(col(\"Name\")).show() # Using column() function df.select(column(\"Age\")).show() # Directly using string name df.select(\"Salary\").show() 2. Selecting Multiple Columns Together You can combine different methods to select multiple columns:\n# multiple column df2 = df.select(\"ID\", \"Name\", col(\"Salary\"), column(\"Department\"), df.Phone) df2.show() 3. Listing All Columns in a DataFrame To get a list of all the column names:\n# get all column name df.columns 4. Renaming Columns with alias() You can rename columns using the alias() method:\ndf.select( col(\"Name\").alias('EmployeeName'), # Rename \"Name\" to \"EmployeeName\" col(\"Salary\").alias('EmployeeSalary'), # Rename \"Salary\" to \"EmployeeSalary\" column(\"Department\"), # Select \"Department\" df.Joining_Date # Select \"Joining_Date\" ).show() 5. Using selectExpr() for Concise Column Selection selectExpr() allows you to use SQL expressions directly and rename columns concisely:\ndf.selectExpr(\"Name as EmployeeName\", \"Salary as EmployeeSalary\", \"Department\").show() Summary Use col(), column(), or string names to select columns. Use expr() and selectExpr() for SQL-like expressions and renaming. Use alias() to rename columns. Get the list of columns using df.columns. Adding, Renaming, and Dropping Columns 1. Adding New Columns with withColumn() In PySpark, the withColumn() function is widely used to add new columns to a DataFrame. You can either assign a constant value using lit() or perform transformations using existing columns.\nAdd a constant value column: newdf = df.withColumn(\"NewColumn\", lit(1)) Add a column based on an expression: newdf = df.withColumn(\"withinCountry\", expr(\"Country == 'India'\")) This function allows adding multiple columns, including calculated ones:\nExample:\nAssign a constant value with lit(). Perform calculations using existing columns like multiplying values. 2. Renaming Columns with withColumnRenamed() PySpark provides the withColumnRenamed() method to rename columns. This is especially useful when you want to change the names for clarity or to follow naming conventions:\nRenaming a column: python\nnew_df = df.withColumnRenamed(\"oldColumnName\", \"newColumnName\") Handling column names with special characters or spaces: If a column has special characters or spaces, you need to use backticks (`) to escape it:\npython\nnewdf.select(\"`New Column Name`\").show() 3. Dropping Columns with drop() To remove unwanted columns, you can use the drop() method:\nDrop a single column: df2 = df.drop(\"Country\") Drop multiple columns: df2 = df.drop(\"Country\", \"Region\") Dropping columns creates a new DataFrame, and the original DataFrame remains unchanged.\n4. Immutability of DataFrames In Spark, DataFrames are immutable by nature. This means that after creating a DataFrame, its contents cannot be changed. All transformations like adding, renaming, or dropping columns result in a new DataFrame, keeping the original one intact.\nFor instance, dropping columns creates a new DataFrame without altering the original: newdf = df.drop(\"ItemType\", \"SalesChannel\") This immutability ensures data consistency and supports Spark‚Äôs parallel processing, as transformations do not affect the source data.\nKey Points Use withColumn() for adding columns, with lit() for constant values and expressions for computed values. Use withColumnRenamed() to rename columns and backticks for special characters or spaces. Use drop() to remove one or more columns. DataFrames are immutable in Spark‚Äîtransformations result in new DataFrames, leaving the original unchanged. Data Types, Filtering, and Unique Values Here‚Äôs a structured set of notes with code to cover changing data types, filtering data, and handling unique/distinct values in PySpark using the employee data:\n1. Changing Data Types (Schema Transformation) In PySpark, you can change the data type of a column using the cast() method. This is helpful when you need to convert data types for columns like Salary or Phone.\nfrom pyspark.sql.functions import col # Change the 'Salary' column from integer to double df = df.withColumn(\"Salary\", col(\"Salary\").cast(\"double\")) # Convert 'Phone' column to string df = df.withColumn(\"Phone\", col(\"Phone\").cast(\"string\")) df.printSchema() 2. Filtering Data You can filter rows based on specific conditions. For instance, to filter employees with a salary greater than 50,000:\n# Filter rows where Salary is greater than 50,000 filtered_df = df.filter(col(\"Salary\") \u003e 50000) filtered_df.show() # Filtering rows where Age is not null filtered_df = df.filter(df[\"Age\"].isNotNull()) filtered_df.show() 3. Multiple Filters (Chaining Conditions) You can also apply multiple conditions using \u0026 or | (AND/OR) to filter data. For example, finding employees over 30 years old and in the IT department:\n# Filter rows where Age \u003e 30 and Department is 'IT' filtered_df = df.filter((df[\"Age\"] \u003e 30) \u0026 (df[\"Department\"] == \"IT\")) filtered_df.show() 4. Filtering on Null or Non-Null Values Filtering based on whether a column has NULL values or not is crucial for data cleaning:\n# Filter rows where 'Address' is NULL filtered_df = df.filter(df[\"Address\"].isNull()) filtered_df.show() # Filter rows where 'Email' is NOT NULL filtered_df = df.filter(df[\"Email\"].isNotNull()) filtered_df.show() 5. Handling Unique or Distinct Data To get distinct rows or unique values from your dataset:\n# Get distinct rows from the entire DataFrame unique_df = df.distinct() unique_df.show() # Get distinct values from the 'Department' column unique_departments_df = df.select(\"Department\").distinct() unique_departments_df.show() To remove duplicates based on specific columns, such as Email or Phone, use dropDuplicates():\n# Remove duplicates based on 'Email' column unique_df = df.dropDuplicates([\"Email\"]) unique_df.show() # Remove duplicates based on both 'Phone' and 'Email' unique_df = df.dropDuplicates([\"Phone\", \"Email\"]) unique_df.show() 6. Counting Distinct Values You can count distinct values in a particular column, or combinations of columns:\n# Count distinct values in the 'Department' column distinct_count_department = df.select(\"Department\").distinct().count() print(\"Distinct Department Count:\", distinct_count_department) # Count distinct combinations of 'Department' and 'Performance_Rating' distinct_combinations_count = df.select(\"Department\", \"Performance_Rating\").distinct().count() print(\"Distinct Department and Performance Rating Combinations:\", distinct_combinations_count) This set of operations will help you efficiently manage and transform your data in PySpark, ensuring data integrity and accuracy for your analysis!\nMastering PySpark DataFrame Operations Changing Data Types: Easily modify column types using .cast(). E.g., change ‚ÄòSalary‚Äô to double or ‚ÄòPhone‚Äô to string for better data handling. Filtering Data: Use .filter() or .where() to extract specific rows. For example, filter employees with a salary over 50,000 or non-null Age. Multiple Conditions: Chain filters with \u0026 and | to apply complex conditions, such as finding employees over 30 in the IT department. Handling NULLs: Use .isNull() and .isNotNull() to filter rows with missing or available values, such as missing addresses or valid emails. Unique/Distinct Values: Use .distinct() to get unique rows or distinct values in a column. Remove duplicates based on specific fields like Email or Phone using .dropDuplicates(). Count Distinct Values: Count distinct values in one or multiple columns to analyze data diversity, such as counting unique departments or combinations of Department and Performance_Rating.",
    "description": "Column Selection \u0026 Manipulation 1. Different Methods to Select Columns In PySpark, you can select specific columns in multiple ways:\n# Using col() function df.select(col(\"Name\")).show() # Using column() function df.select(column(\"Age\")).show() # Directly using string name df.select(\"Salary\").show() 2. Selecting Multiple Columns Together You can combine different methods to select multiple columns:\n# multiple column df2 = df.select(\"ID\", \"Name\", col(\"Salary\"), column(\"Department\"), df.Phone) df2.show() 3. Listing All Columns in a DataFrame To get a list of all the column names:",
    "tags": [],
    "title": "DF Operations",
    "uri": "/azure_data_bricks/df-operations/"
  },
  {
    "breadcrumb": "",
    "content": "PySpark Basics 1. Introduction Big Data Overview Introduction to Apache Spark Introduction to Azure Databricks Databricks Architecture Databricks Workspace Overview 2. Spark Core Concepts RDDs (Resilient Distributed Datasets) Transformations and Actions Lazy Evaluation Spark Execution Model Caching and Persistence Partitioning 3. Spark SQL DataFrames and Datasets Spark SQL Engine Schema Inference Catalog and Metadata Temporary and Global Views SQL Queries in Spark Performance Optimization (Catalyst Optimizer, Tungsten) 4. Spark Data Sources Reading/Writing CSV, JSON, Parquet, ORC Working with Delta Lake JDBC Data Sources Streaming Data Sources Handling Nested Data (Struct, Array, Map) 5. Delta Lake Introduction to Delta Lake ACID Transactions Schema Enforcement \u0026 Evolution Time Travel Upserts \u0026 Deletes (MERGE) Delta Lake Optimization (Z-order, Optimize, Vacuum) 6. Spark Streaming \u0026 Structured Streaming Introduction to Streaming DStreams vs Structured Streaming Streaming Sources (Kafka, Event Hubs, Socket) Watermarking \u0026 Late Data State Management Streaming Sinks (Delta, Console, Kafka, Event Hubs) 7. Spark MLlib (Machine Learning) MLlib Basics Feature Engineering Pipelines \u0026 Transformers Classification Regression Clustering Model Persistence 8. Spark GraphX Introduction to GraphX Graph Processing Basics Graph Algorithms Use Cases 9. Performance Tuning Spark Configurations \u0026 Parameters Memory Management Shuffle Optimizations Caching Strategies Adaptive Query Execution (AQE) Skew Handling 10. Azure Databricks Essentials Databricks Clusters (Types, Autoscaling, Pools) Databricks Notebooks Databricks Repos \u0026 Git Integration Databricks Jobs \u0026 Scheduling Databricks Secrets Databricks Utilities (DBUtils) Workspace Security \u0026 Permissions 11. Integration with Azure Services Azure Data Lake Storage (ADLS) Integration Azure Blob Storage Integration Azure Synapse Analytics Azure Event Hubs Azure Data Factory (ADF) with Databricks Power BI with Databricks 12. Advanced Databricks Unity Catalog Databricks SQL Delta Live Tables (DLT) Databricks REST API Databricks CLI Monitoring \u0026 Logging Cost Optimization in Databricks 13. Real-World Use Cases ETL Pipeline with Databricks Streaming Data Pipeline Batch Data Processing Machine Learning Pipeline End-to-End Data Lakehouse Implementation 14. Best Practices Cluster Sizing \u0026 Configuration Code Versioning \u0026 Collaboration Data Governance Security \u0026 Compliance CI/CD with Databricks Productionizing Workloads 15. Capstone Project Build a Scalable Data Lakehouse with Databricks Real-Time Streaming Data Pipeline ML Model Training and Deployment",
    "description": "PySpark Basics 1. Introduction Big Data Overview Introduction to Apache Spark Introduction to Azure Databricks Databricks Architecture Databricks Workspace Overview 2. Spark Core Concepts RDDs (Resilient Distributed Datasets) Transformations and Actions Lazy Evaluation Spark Execution Model Caching and Persistence Partitioning 3. Spark SQL DataFrames and Datasets Spark SQL Engine Schema Inference Catalog and Metadata Temporary and Global Views SQL Queries in Spark Performance Optimization (Catalyst Optimizer, Tungsten) 4. Spark Data Sources Reading/Writing CSV, JSON, Parquet, ORC Working with Delta Lake JDBC Data Sources Streaming Data Sources Handling Nested Data (Struct, Array, Map) 5. Delta Lake Introduction to Delta Lake ACID Transactions Schema Enforcement \u0026 Evolution Time Travel Upserts \u0026 Deletes (MERGE) Delta Lake Optimization (Z-order, Optimize, Vacuum) 6. Spark Streaming \u0026 Structured Streaming Introduction to Streaming DStreams vs Structured Streaming Streaming Sources (Kafka, Event Hubs, Socket) Watermarking \u0026 Late Data State Management Streaming Sinks (Delta, Console, Kafka, Event Hubs) 7. Spark MLlib (Machine Learning) MLlib Basics Feature Engineering Pipelines \u0026 Transformers Classification Regression Clustering Model Persistence 8. Spark GraphX Introduction to GraphX Graph Processing Basics Graph Algorithms Use Cases 9. Performance Tuning Spark Configurations \u0026 Parameters Memory Management Shuffle Optimizations Caching Strategies Adaptive Query Execution (AQE) Skew Handling 10. Azure Databricks Essentials Databricks Clusters (Types, Autoscaling, Pools) Databricks Notebooks Databricks Repos \u0026 Git Integration Databricks Jobs \u0026 Scheduling Databricks Secrets Databricks Utilities (DBUtils) Workspace Security \u0026 Permissions 11. Integration with Azure Services Azure Data Lake Storage (ADLS) Integration Azure Blob Storage Integration Azure Synapse Analytics Azure Event Hubs Azure Data Factory (ADF) with Databricks Power BI with Databricks 12. Advanced Databricks Unity Catalog Databricks SQL Delta Live Tables (DLT) Databricks REST API Databricks CLI Monitoring \u0026 Logging Cost Optimization in Databricks 13. Real-World Use Cases ETL Pipeline with Databricks Streaming Data Pipeline Batch Data Processing Machine Learning Pipeline End-to-End Data Lakehouse Implementation 14. Best Practices Cluster Sizing \u0026 Configuration Code Versioning \u0026 Collaboration Data Governance Security \u0026 Compliance CI/CD with Databricks Productionizing Workloads 15. Capstone Project Build a Scalable Data Lakehouse with Databricks Real-Time Streaming Data Pipeline ML Model Training and Deployment",
    "tags": [],
    "title": "Azure Data Bricks",
    "uri": "/azure_data_bricks/"
  },
  {
    "breadcrumb": "SQL",
    "content": "This guide covers the essential DML (Data Manipulation Language) commands used for inserting, updating, and deleting data in database tables.\n1. INSERT ‚Äì Adding Data to Tables Method 1: Manual INSERT using VALUES -- Insert new records into the customers table INSERT INTO customers (id, first_name, country, score) VALUES (6, 'Anna', 'USA', NULL), (7, 'Sam', NULL, 100); -- Incorrect column order INSERT INTO customers (id, first_name, country, score) VALUES (8, 'Max', 'USA', NULL); -- Incorrect data type in values INSERT INTO customers (id, first_name, country, score) VALUES ('Max', 9, 'Max', NULL); -- Insert a new record with full column values INSERT INTO customers (id, first_name, country, score) VALUES (8, 'Max', 'USA', 368); -- Insert without specifying column names (not recommended) INSERT INTO customers VALUES (9, 'Andreas', 'Germany', NULL); -- Insert a record with only id and first_name INSERT INTO customers (id, first_name) VALUES (10, 'Sahra'); Method 2: INSERT using SELECT (Copying Data) -- Copy data from customers table into persons INSERT INTO persons (id, person_name, birth_date, phone) SELECT id, first_name, NULL, 'Unknown' FROM customers; 2. UPDATE ‚Äì Modifying Existing Data -- Change the score of customer with ID 6 to 0 UPDATE customers SET score = 0 WHERE id = 6; -- Change score and country for customer with ID 10 UPDATE customers SET score = 0, country = 'UK' WHERE id = 10; -- Update all NULL scores to 0 UPDATE customers SET score = 0 WHERE score IS NULL; -- Verify the update SELECT * FROM customers WHERE score IS NULL; 3. DELETE ‚Äì Removing Data from Tables -- Select customers with ID greater than 5 SELECT * FROM customers WHERE id \u003e 5; -- Delete customers with ID greater than 5 DELETE FROM customers WHERE id \u003e 5; -- Delete all rows from persons DELETE FROM persons; -- Faster method to delete all rows TRUNCATE TABLE persons;",
    "description": "This guide covers the essential DML (Data Manipulation Language) commands used for inserting, updating, and deleting data in database tables.\n1. INSERT ‚Äì Adding Data to Tables Method 1: Manual INSERT using VALUES -- Insert new records into the customers table INSERT INTO customers (id, first_name, country, score) VALUES (6, 'Anna', 'USA', NULL), (7, 'Sam', NULL, 100); -- Incorrect column order INSERT INTO customers (id, first_name, country, score) VALUES (8, 'Max', 'USA', NULL); -- Incorrect data type in values INSERT INTO customers (id, first_name, country, score) VALUES ('Max', 9, 'Max', NULL); -- Insert a new record with full column values INSERT INTO customers (id, first_name, country, score) VALUES (8, 'Max', 'USA', 368); -- Insert without specifying column names (not recommended) INSERT INTO customers VALUES (9, 'Andreas', 'Germany', NULL); -- Insert a record with only id and first_name INSERT INTO customers (id, first_name) VALUES (10, 'Sahra'); Method 2: INSERT using SELECT (Copying Data) -- Copy data from customers table into persons INSERT INTO persons (id, person_name, birth_date, phone) SELECT id, first_name, NULL, 'Unknown' FROM customers; 2. UPDATE ‚Äì Modifying Existing Data -- Change the score of customer with ID 6 to 0 UPDATE customers SET score = 0 WHERE id = 6; -- Change score and country for customer with ID 10 UPDATE customers SET score = 0, country = 'UK' WHERE id = 10; -- Update all NULL scores to 0 UPDATE customers SET score = 0 WHERE score IS NULL; -- Verify the update SELECT * FROM customers WHERE score IS NULL; 3. DELETE ‚Äì Removing Data from Tables -- Select customers with ID greater than 5 SELECT * FROM customers WHERE id \u003e 5; -- Delete customers with ID greater than 5 DELETE FROM customers WHERE id \u003e 5; -- Delete all rows from persons DELETE FROM persons; -- Faster method to delete all rows TRUNCATE TABLE persons;",
    "tags": [],
    "title": "DML",
    "uri": "/sql/dml/"
  },
  {
    "breadcrumb": "ADB",
    "content": "Sorting and String Functions from pyspark.sql import SparkSession from pyspark.sql.functions import col, desc, asc, concat, concat_ws, initcap, lower, upper, instr, length, lit # Create a Spark session spark = SparkSession.builder.appName(\"SortingAndStringFunctions\").getOrCreate() # Sample data data = [ (\"USA\", \"North America\", 100, 50.5), (\"India\", \"Asia\", 300, 20.0), (\"Germany\", \"Europe\", 200, 30.5), (\"Australia\", \"Oceania\", 150, 60.0), (\"Japan\", \"Asia\", 120, 45.0), (\"Brazil\", \"South America\", 180, 25.0) ] # Define the schema columns = [\"Country\", \"Region\", \"UnitsSold\", \"UnitPrice\"] # Create DataFrame df = spark.createDataFrame(data, columns) # Display the original DataFrame df.show() Sorting the DataFrame 1. Sort by a single column (ascending order) df.orderBy(\"Country\").show(5)\nBy default, sorting is ascending. This shows the first 5 countries alphabetically.\n2. Sort by multiple columns df.orderBy(\"Country\", \"UnitsSold\").show(5)\nFirst sorts by Country, then within each country sorts by UnitsSold.\n3. Sort by column in descending order and limit df.orderBy(desc(\"Country\")).limit(3).show(5)\nSorts by Country in descending order and returns the top 3 rows.\n4. Sorting with null values last df.orderBy(col(\"Country\").desc(), nulls_last=True).show(5)\nEnsures null values appear at the end when sorting.\nKey Functions:\nUse .orderBy() or .sort() to sort DataFrames.\nControl order with asc() or desc().\nString Functions 1. Capitalize first letter of each word df.select(initcap(col(\"Country\"))).show()\nConverts \"united states\" ‚Üí \"United States\".\n2. Convert all text to lowercase df.select(lower(col(\"Country\"))).show()\n3. Convert all text to uppercase df.select(upper(col(\"Country\"))).show()\nKey Functions:\ninitcap() ‚Üí Capitalize first letter of each word.\nlower() ‚Üí Convert to lowercase.\nupper() ‚Üí Convert to uppercase.\nConcatenation Functions 1. Concatenate two columns df.select(concat(col(\"Region\"), col(\"Country\"))).show()\nJoins Region and Country without separator.\n2. Concatenate with a separator df.select(concat_ws(\" | \", col(\"Region\"), col(\"Country\"))).show()\nJoins with \" | \" as separator.\n3. Create a new concatenated column df.withColumn(\"concatenated\", concat(df[\"Region\"], lit(\" \"), df[\"Country\"])).show()\nAdds a new column combining Region and Country.\nKey Functions:\nconcat() ‚Üí Join columns directly.\nconcat_ws() ‚Üí Join columns with a separator.\nüìå Summary Sorting: Use .orderBy() or .sort() with asc() / desc(). String Manipulation: Use initcap(), lower(), upper(). Concatenation: Use concat() or concat_ws() for flexible joins. Split Function in DataFrame Let‚Äôs create a PySpark DataFrame for employee data with columns such as EmployeeID, Name, Department, and Skills. We‚Äôll explore split, explode, and other useful array functions.\nSample Data Creation for Employee Data from pyspark.sql import SparkSession from pyspark.sql.functions import split, explode, size, array_contains, col # Sample employee data data = [ (1, \"Alice\", \"HR\", \"Communication Management\"), (2, \"Bob\", \"IT\", \"Programming Networking\"), (3, \"Charlie\", \"Finance\", \"Accounting Analysis\"), (4, \"David\", \"HR\", \"Recruiting Communication\"), (5, \"Eve\", \"IT\", \"Cloud DevOps\") ] # Define the schema columns = [\"EmployeeID\", \"Name\", \"Department\", \"Skills\"] # Create DataFrame df = spark.createDataFrame(data, columns) # Display the original DataFrame df.show(truncate=False) Examples 1. Split the Skills column df.select(col(\"EmployeeID\"), col(\"Name\"), split(col(\"Skills\"), \" \").alias(\"Skills_Array\")).show(truncate=False)\nSplits the Skills column into an array of skills using space as a delimiter.\n2. Select the first skill from Skills_Array df2.select(col(\"EmployeeID\"), col(\"Name\"), col(\"Skills_Array\")[0].alias(\"First_Skill\")).show(truncate=False)\nUses index notation (Skills_Array[0]) to pick the first skill. Indexing starts from 0.\n3. Count the number of skills per employee df2.select(col(\"EmployeeID\"), col(\"Name\"), size(col(\"Skills_Array\")).alias(\"Number_of_Skills\")).show(truncate=False)\nThe size() function returns the number of elements in the array.\n4. Check if the employee has ‚ÄúCloud‚Äù skill df.select(col(\"EmployeeID\"), col(\"Name\"), array_contains(split(col(\"Skills\"), \" \"), \"Cloud\").alias(\"Has_Cloud_Skill\")).show(truncate=False) `\narray_contains() returns True if \"Cloud\" exists in the skill set.\n5. Explode the Skills_Array into multiple rows df3 = df2.withColumn(\"Skill\", explode(col(\"Skills_Array\")))\ndf3.select(\"EmployeeID\", \"Name\", \"Skill\").show(truncate=False)\nexplode() flattens the array into rows, where each skill becomes a separate row for that employee.\nüìå Summary of Key Functions split() ‚Üí Splits a string into an array. explode() ‚Üí Converts an array into multiple rows. size() ‚Üí Counts elements in an array. array_contains() ‚Üí Checks if an array contains a value. selectExpr() ‚Üí Lets you query arrays using SQL expressions like Skills_Array[0]. Trim Function in DataFrame Let‚Äôs create a sample dataset for employees and demonstrate string trimming and padding functions in PySpark:\nltrim() rtrim() trim() lpad() rpad() Sample Data Creation for Employees from pyspark.sql import SparkSession from pyspark.sql.functions import lit, ltrim, rtrim, rpad, lpad, trim, col # Sample employee data with leading and trailing spaces in the 'Name' column data = [ (1, \" Alice \", \"HR\"), (2, \" Bob\", \"IT\"), (3, \"Charlie \", \"Finance\"), (4, \" David \", \"HR\"), (5, \"Eve \", \"IT\") ] # Define the schema for the DataFrame columns = [\"EmployeeID\", \"Name\", \"Department\"] # Create DataFrame df = spark.createDataFrame(data, columns) # Show the original DataFrame df.show(truncate=False) Applying Trimming and Padding Functions 1. Trimming Functions ltrim() ‚Üí Removes leading spaces.\nrtrim() ‚Üí Removes trailing spaces.\ntrim() ‚Üí Removes both leading and trailing spaces.\n2. Padding Functions lpad() ‚Üí Pads the left side of a string with a character up to a given length.\nrpad() ‚Üí Pads the right side of a string with a character up to a given length.\nExample # Apply trimming and padding functions result_df = df.select( col(\"EmployeeID\"), col(\"Department\"), ltrim(col(\"Name\")).alias(\"ltrim_Name\"), # Remove leading spaces rtrim(col(\"Name\")).alias(\"rtrim_Name\"), # Remove trailing spaces trim(col(\"Name\")).alias(\"trim_Name\"), # Remove both leading \u0026 trailing spaces lpad(col(\"Name\"), 10, \"X\").alias(\"lpad_Name\"), # Left pad with \"X\" to length 10 rpad(col(\"Name\"), 10, \"Y\").alias(\"rpad_Name\") # Right pad with \"Y\" to length 10 ) # Show the resulting DataFrame result_df.show(truncate=False) Output Explanation ltrim_Name ‚Üí Leading spaces removed.\nrtrim_Name ‚Üí Trailing spaces removed.\ntrim_Name ‚Üí Both leading \u0026 trailing spaces removed.\nlpad_Name ‚Üí Padded left with \"X\" until length = 10.\nrpad_Name ‚Üí Padded right with \"Y\" until length = 10.\nüìå Summary Use trim functions (ltrim, rtrim, trim) to clean up unwanted spaces. Use padding functions (lpad, rpad) to format strings with fixed lengths.",
    "description": "Sorting and String Functions from pyspark.sql import SparkSession from pyspark.sql.functions import col, desc, asc, concat, concat_ws, initcap, lower, upper, instr, length, lit # Create a Spark session spark = SparkSession.builder.appName(\"SortingAndStringFunctions\").getOrCreate() # Sample data data = [ (\"USA\", \"North America\", 100, 50.5), (\"India\", \"Asia\", 300, 20.0), (\"Germany\", \"Europe\", 200, 30.5), (\"Australia\", \"Oceania\", 150, 60.0), (\"Japan\", \"Asia\", 120, 45.0), (\"Brazil\", \"South America\", 180, 25.0) ] # Define the schema columns = [\"Country\", \"Region\", \"UnitsSold\", \"UnitPrice\"] # Create DataFrame df = spark.createDataFrame(data, columns) # Display the original DataFrame df.show() Sorting the DataFrame 1. Sort by a single column (ascending order) df.orderBy(\"Country\").show(5)\nBy default, sorting is ascending. This shows the first 5 countries alphabetically.",
    "tags": [],
    "title": "Functions",
    "uri": "/azure_data_bricks/functions/"
  },
  {
    "breadcrumb": "ADB",
    "content": "In PySpark, you can use various date functions to manipulate and analyze date and timestamp columns.\nWe‚Äôll explore:\ncurrent_date current_timestamp date_add date_sub datediff months_between Sample Code from pyspark.sql import SparkSession from pyspark.sql.functions import current_date, current_timestamp, date_add, date_sub, col, datediff, months_between, to_date, lit # Generate a DataFrame with 10 rows, adding \"today\" and \"now\" columns dateDF = spark.range(10).withColumn(\"today\", current_date()).withColumn(\"now\", current_timestamp()) # Show the DataFrame with today and now columns dateDF.show(truncate=False) Key Functions current_date() ‚Üí Returns current date. current_timestamp() ‚Üí Returns current timestamp (date + time). date_sub(col(\"today\"), 5) ‚Üí Subtracts 5 days. date_add(col(\"today\"), 5) ‚Üí Adds 5 days. datediff(date1, date2) ‚Üí Returns difference in days. months_between(date1, date2) ‚Üí Returns difference in months. Working with dates and timestamps often requires converting formats and extracting components.\nWe‚Äôll explore:\nto_date to_timestamp year, month, dayofmonth hour, minute, second 1. to_date Converts a string to a date (default format: yyyy-MM-dd). If format doesn‚Äôt match, returns null. Example: to_date(lit(\"2017-12-11\"), \"yyyy-dd-MM\") 2. to_timestamp Converts a string with date \u0026 time into a timestamp. Allows extraction of time components. 3. Extracting Components year() ‚Üí Extracts year. month() ‚Üí Extracts month. dayofmonth() ‚Üí Extracts day. hour() ‚Üí Extracts hour. minute() ‚Üí Extracts minute. second() ‚Üí Extracts second. Example Output For input \"2017-12-11\" (format yyyy-dd-MM):\nYear: 2017 Month: 12 Day: 11 Hour: 0 Minute: 0 Second: 0 For invalid input (e.g., \"2017-20-12\"):\nResult: null",
    "description": "In PySpark, you can use various date functions to manipulate and analyze date and timestamp columns.\nWe‚Äôll explore:\ncurrent_date current_timestamp date_add date_sub datediff months_between Sample Code from pyspark.sql import SparkSession from pyspark.sql.functions import current_date, current_timestamp, date_add, date_sub, col, datediff, months_between, to_date, lit # Generate a DataFrame with 10 rows, adding \"today\" and \"now\" columns dateDF = spark.range(10).withColumn(\"today\", current_date()).withColumn(\"now\", current_timestamp()) # Show the DataFrame with today and now columns dateDF.show(truncate=False) Key Functions current_date() ‚Üí Returns current date. current_timestamp() ‚Üí Returns current timestamp (date + time). date_sub(col(\"today\"), 5) ‚Üí Subtracts 5 days. date_add(col(\"today\"), 5) ‚Üí Adds 5 days. datediff(date1, date2) ‚Üí Returns difference in days. months_between(date1, date2) ‚Üí Returns difference in months. Working with dates and timestamps often requires converting formats and extracting components.\nWe‚Äôll explore:",
    "tags": [],
    "title": "Date Functions",
    "uri": "/azure_data_bricks/date/"
  },
  {
    "breadcrumb": "SQL",
    "content": "This document provides an overview of SQL filtering techniques using WHERE and various operators for precise data retrieval.\n1. Comparison Operators (=, \u003c\u003e, \u003e, \u003e=, \u003c, \u003c=) -- Retrieve all customers from Germany SELECT * FROM customers WHERE country = 'Germany'; -- Retrieve all customers who are not from Germany SELECT * FROM customers WHERE country \u003c\u003e 'Germany'; -- Retrieve all customers with a score greater than 500 SELECT * FROM customers WHERE score \u003e 500; -- Retrieve all customers with a score of 500 or more SELECT * FROM customers WHERE score \u003e= 500; -- Retrieve all customers with a score less than 500 SELECT * FROM customers WHERE score \u003c 500; -- Retrieve all customers with a score of 500 or less SELECT * FROM customers WHERE score \u003c= 500; 2. Logical Operators (AND, OR, NOT) -- Customers from the USA and score \u003e 500 SELECT * FROM customers WHERE country = 'USA' AND score \u003e 500; -- Customers from the USA or score \u003e 500 SELECT * FROM customers WHERE country = 'USA' OR score \u003e 500; -- Customers with score not less than 500 SELECT * FROM customers WHERE NOT score \u003c 500; 3. Range Filtering ‚Äì BETWEEN -- Customers with score between 100 and 500 SELECT * FROM customers WHERE score BETWEEN 100 AND 500; -- Equivalent to BETWEEN SELECT * FROM customers WHERE score \u003e= 100 AND score \u003c= 500; 4. Set Filtering ‚Äì IN -- Customers from Germany or USA SELECT * FROM customers WHERE country IN ('Germany', 'USA'); 5. Pattern Matching ‚Äì LIKE -- First name starts with 'M' SELECT * FROM customers WHERE first_name LIKE 'M%'; -- First name ends with 'n' SELECT * FROM customers WHERE first_name LIKE '%n'; -- First name contains 'r' SELECT * FROM customers WHERE first_name LIKE '%r%'; -- First name has 'r' in the third position SELECT * FROM customers WHERE first_name LIKE '__r%';",
    "description": "This document provides an overview of SQL filtering techniques using WHERE and various operators for precise data retrieval.\n1. Comparison Operators (=, \u003c\u003e, \u003e, \u003e=, \u003c, \u003c=) -- Retrieve all customers from Germany SELECT * FROM customers WHERE country = 'Germany'; -- Retrieve all customers who are not from Germany SELECT * FROM customers WHERE country \u003c\u003e 'Germany'; -- Retrieve all customers with a score greater than 500 SELECT * FROM customers WHERE score \u003e 500; -- Retrieve all customers with a score of 500 or more SELECT * FROM customers WHERE score \u003e= 500; -- Retrieve all customers with a score less than 500 SELECT * FROM customers WHERE score \u003c 500; -- Retrieve all customers with a score of 500 or less SELECT * FROM customers WHERE score \u003c= 500; 2. Logical Operators (AND, OR, NOT) -- Customers from the USA and score \u003e 500 SELECT * FROM customers WHERE country = 'USA' AND score \u003e 500; -- Customers from the USA or score \u003e 500 SELECT * FROM customers WHERE country = 'USA' OR score \u003e 500; -- Customers with score not less than 500 SELECT * FROM customers WHERE NOT score \u003c 500; 3. Range Filtering ‚Äì BETWEEN -- Customers with score between 100 and 500 SELECT * FROM customers WHERE score BETWEEN 100 AND 500; -- Equivalent to BETWEEN SELECT * FROM customers WHERE score \u003e= 100 AND score \u003c= 500; 4. Set Filtering ‚Äì IN -- Customers from Germany or USA SELECT * FROM customers WHERE country IN ('Germany', 'USA'); 5. Pattern Matching ‚Äì LIKE -- First name starts with 'M' SELECT * FROM customers WHERE first_name LIKE 'M%'; -- First name ends with 'n' SELECT * FROM customers WHERE first_name LIKE '%n'; -- First name contains 'r' SELECT * FROM customers WHERE first_name LIKE '%r%'; -- First name has 'r' in the third position SELECT * FROM customers WHERE first_name LIKE '__r%';",
    "tags": [],
    "title": "Filtering Data",
    "uri": "/sql/filter/"
  },
  {
    "breadcrumb": "ADB",
    "content": "Sample Sales Data with Null Values # Sample data: sales data with nulls data = [ (\"John\", \"North\", 100, None), (\"Doe\", \"East\", None, 50), (None, \"West\", 150, 30), (\"Alice\", None, 200, 40), (\"Bob\", \"South\", None, None), (None, None, None, None) ] columns = [\"Name\", \"Region\", \"UnitsSold\", \"Revenue\"] # Create DataFrame df = spark.createDataFrame(data, columns) df.show() 1. Detecting Null Values Use isNull() to identify rows where a column contains null values. The output is a boolean flag indicating whether the value is null. 2. Dropping Rows with Null Values dropna() removes rows with nulls in any column (default mode). Use how=\"all\" to remove rows only if all columns are null. Use subset=[\"col1\", \"col2\"] to target specific columns. 3. Filling Null Values fillna() replaces nulls with specified default values. Replace across all columns or selectively. Example: Replace Region nulls with \"Unknown\". Replace UnitsSold and Revenue nulls with 0. 4. Coalesce Function coalesce() returns the first non-null value among multiple columns. Useful when providing fallback values if some columns contain nulls. 5. Handling Nulls in Aggregations Nulls can distort aggregations like mean(). Use coalesce() to substitute nulls with defaults (e.g., 0.0). This prevents inaccurate results. üìå Summary Detecting Nulls: Use isNull() to find null values. Dropping Nulls: Use dropna() to remove rows with nulls (all or specific columns). Filling Nulls: Use fillna() to replace nulls with defaults. Coalesce Function: Use coalesce() to return the first non-null value. Aggregations: Use coalesce() in aggregations to handle nulls safely.",
    "description": "Sample Sales Data with Null Values # Sample data: sales data with nulls data = [ (\"John\", \"North\", 100, None), (\"Doe\", \"East\", None, 50), (None, \"West\", 150, 30), (\"Alice\", None, 200, 40), (\"Bob\", \"South\", None, None), (None, None, None, None) ] columns = [\"Name\", \"Region\", \"UnitsSold\", \"Revenue\"] # Create DataFrame df = spark.createDataFrame(data, columns) df.show() 1. Detecting Null Values Use isNull() to identify rows where a column contains null values. The output is a boolean flag indicating whether the value is null. 2. Dropping Rows with Null Values dropna() removes rows with nulls in any column (default mode). Use how=\"all\" to remove rows only if all columns are null. Use subset=[\"col1\", \"col2\"] to target specific columns. 3. Filling Null Values fillna() replaces nulls with specified default values. Replace across all columns or selectively. Example: Replace Region nulls with \"Unknown\". Replace UnitsSold and Revenue nulls with 0. 4. Coalesce Function coalesce() returns the first non-null value among multiple columns. Useful when providing fallback values if some columns contain nulls. 5. Handling Nulls in Aggregations Nulls can distort aggregations like mean(). Use coalesce() to substitute nulls with defaults (e.g., 0.0). This prevents inaccurate results. üìå Summary Detecting Nulls: Use isNull() to find null values. Dropping Nulls: Use dropna() to remove rows with nulls (all or specific columns). Filling Nulls: Use fillna() to replace nulls with defaults. Coalesce Function: Use coalesce() to return the first non-null value. Aggregations: Use coalesce() in aggregations to handle nulls safely.",
    "tags": [],
    "title": "Handling Nulls",
    "uri": "/azure_data_bricks/nulls/"
  },
  {
    "breadcrumb": "SQL",
    "content": "This document provides an overview of SQL Joins, which allow combining data from multiple tables to retrieve meaningful insights.\n1. Basic Joins No Join -- Retrieve all data from customers and orders as separate results SELECT * FROM customers; SELECT * FROM orders; INNER JOIN -- Get all customers along with their orders (only those who placed an order) SELECT c.id, c.first_name, o.order_id, o.sales FROM customers AS c INNER JOIN orders AS o ON c.id = o.customer_id; LEFT JOIN -- Get all customers along with their orders (including customers without orders) SELECT c.id, c.first_name, o.order_id, o.sales FROM customers AS c LEFT JOIN orders AS o ON c.id = o.customer_id; RIGHT JOIN -- Get all customers along with their orders (including orders without customers) SELECT c.id, c.first_name, o.order_id, o.customer_id, o.sales FROM customers AS c RIGHT JOIN orders AS o ON c.id = o.customer_id; Alternative to RIGHT JOIN (using LEFT JOIN) SELECT c.id, c.first_name, o.order_id, o.sales FROM orders AS o LEFT JOIN customers AS c ON c.id = o.customer_id; FULL JOIN -- Get all customers and all orders, even if there‚Äôs no match SELECT c.id, c.first_name, o.order_id, o.customer_id, o.sales FROM customers AS c FULL JOIN orders AS o ON c.id = o.customer_id; 2. Advanced Joins LEFT ANTI JOIN -- Customers who haven't placed any order SELECT * FROM customers AS c LEFT JOIN orders AS o ON c.id = o.customer_id WHERE o.customer_id IS NULL; RIGHT ANTI JOIN -- Orders without matching customers SELECT * FROM customers AS c RIGHT JOIN orders AS o ON c.id = o.customer_id WHERE c.id IS NULL; Alternative to RIGHT ANTI JOIN (using LEFT JOIN) SELECT * FROM orders AS o LEFT JOIN customers AS c ON c.id = o.customer_id WHERE c.id IS NULL; Alternative to INNER JOIN (using LEFT JOIN) SELECT * FROM customers AS c LEFT JOIN orders AS o ON c.id = o.customer_id WHERE o.customer_id IS NOT NULL; FULL ANTI JOIN -- Find customers without orders and orders without customers SELECT c.id, c.first_name, o.order_id, o.customer_id, o.sales FROM customers AS c FULL JOIN orders AS o ON c.id = o.customer_id WHERE o.customer_id IS NULL OR c.id IS NULL; CROSS JOIN -- Generate all possible combinations of customers and orders SELECT * FROM customers CROSS JOIN orders; 3. Multiple Table Joins (4 Tables) Task: Using SalesDB, retrieve a list of all orders along with related customer, product, and employee details. For each order, display:\nOrder ID\nCustomer‚Äôs name\nProduct name\nSales amount\nProduct price\nSalesperson‚Äôs name\nUSE SalesDB; SELECT o.OrderID, o.Sales, c.FirstName AS CustomerFirstName, c.LastName AS CustomerLastName, p.Product AS ProductName, p.Price, e.FirstName AS EmployeeFirstName, e.LastName AS EmployeeLastName FROM Sales.Orders AS o LEFT JOIN Sales.Customers AS c ON o.CustomerID = c.CustomerID LEFT JOIN Sales.Products AS p ON o.ProductID = p.ProductID LEFT JOIN Sales.Employees AS e ON o.SalesPersonID = e.EmployeeID;",
    "description": "This document provides an overview of SQL Joins, which allow combining data from multiple tables to retrieve meaningful insights.\n1. Basic Joins No Join -- Retrieve all data from customers and orders as separate results SELECT * FROM customers; SELECT * FROM orders; INNER JOIN -- Get all customers along with their orders (only those who placed an order) SELECT c.id, c.first_name, o.order_id, o.sales FROM customers AS c INNER JOIN orders AS o ON c.id = o.customer_id; LEFT JOIN -- Get all customers along with their orders (including customers without orders) SELECT c.id, c.first_name, o.order_id, o.sales FROM customers AS c LEFT JOIN orders AS o ON c.id = o.customer_id; RIGHT JOIN -- Get all customers along with their orders (including orders without customers) SELECT c.id, c.first_name, o.order_id, o.customer_id, o.sales FROM customers AS c RIGHT JOIN orders AS o ON c.id = o.customer_id; Alternative to RIGHT JOIN (using LEFT JOIN) SELECT c.id, c.first_name, o.order_id, o.sales FROM orders AS o LEFT JOIN customers AS c ON c.id = o.customer_id; FULL JOIN -- Get all customers and all orders, even if there‚Äôs no match SELECT c.id, c.first_name, o.order_id, o.customer_id, o.sales FROM customers AS c FULL JOIN orders AS o ON c.id = o.customer_id; 2. Advanced Joins LEFT ANTI JOIN -- Customers who haven't placed any order SELECT * FROM customers AS c LEFT JOIN orders AS o ON c.id = o.customer_id WHERE o.customer_id IS NULL; RIGHT ANTI JOIN -- Orders without matching customers SELECT * FROM customers AS c RIGHT JOIN orders AS o ON c.id = o.customer_id WHERE c.id IS NULL; Alternative to RIGHT ANTI JOIN (using LEFT JOIN) SELECT * FROM orders AS o LEFT JOIN customers AS c ON c.id = o.customer_id WHERE c.id IS NULL; Alternative to INNER JOIN (using LEFT JOIN) SELECT * FROM customers AS c LEFT JOIN orders AS o ON c.id = o.customer_id WHERE o.customer_id IS NOT NULL; FULL ANTI JOIN -- Find customers without orders and orders without customers SELECT c.id, c.first_name, o.order_id, o.customer_id, o.sales FROM customers AS c FULL JOIN orders AS o ON c.id = o.customer_id WHERE o.customer_id IS NULL OR c.id IS NULL; CROSS JOIN -- Generate all possible combinations of customers and orders SELECT * FROM customers CROSS JOIN orders; 3. Multiple Table Joins (4 Tables) Task: Using SalesDB, retrieve a list of all orders along with related customer, product, and employee details. For each order, display:",
    "tags": [],
    "title": "Joins",
    "uri": "/sql/joins/"
  },
  {
    "breadcrumb": "ADB",
    "content": "Basic Aggregate Functions Sample Data from pyspark.sql import Row # Create sample data data = [ Row(id=1, value=10), Row(id=2, value=20), Row(id=3, value=30), Row(id=4, value=None), Row(id=5, value=40), Row(id=6, value=20) ] # Create DataFrame df = spark.createDataFrame(data) # Show the DataFrame df.show() Aggregate Functions in PySpark Summation (sum) ‚Äì Adds up the values in a column. Average (avg) ‚Äì Computes the average of values in a column. Count (count) ‚Äì Counts the number of non-null values in a column. Maximum (max) / Minimum (min) ‚Äì Finds the highest and lowest values. Distinct Count (countDistinct) ‚Äì Counts unique values in a column. Notes Handling Nulls:\ncount() counts only non-null values. sum(), avg(), max(), and min() ignore null values. Performance:\nAggregate functions can be expensive on large datasets; partitioning improves performance.\nUse Cases:\nSummation: Total sales, total revenue. Average: Average sales per day. Count: Number of transactions. Max/Min: Highest and lowest values (e.g., max sales in a day). Distinct Count: Unique customers, unique products. Advanced Aggregation Functions Sample Data from pyspark.sql import SparkSession from pyspark.sql import functions as F from pyspark.sql.types import StructType, StructField, StringType, IntegerType # Create Spark session spark = SparkSession.builder.appName(\"AggregationExamples\").getOrCreate() # Sample data data = [ (\"HR\", 10000, 500, \"John\"), (\"Finance\", 20000, 1500, \"Doe\"), (\"HR\", 15000, 1000, \"Alice\"), (\"Finance\", 25000, 2000, \"Eve\"), (\"HR\", 20000, 1500, \"Mark\") ] # Define schema schema = StructType([ StructField(\"department\", StringType(), True), StructField(\"salary\", IntegerType(), True), StructField(\"bonus\", IntegerType(), True), StructField(\"employee_name\", StringType(), True) ]) # Create DataFrame df = spark.createDataFrame(data, schema) df.show() 1. Grouped Aggregation Perform aggregation within groups based on a column.\nsum() ‚Üí Adds values within the group. avg() ‚Üí Computes group average. max() ‚Üí Finds maximum value. min() ‚Üí Finds minimum value. 2. Multiple Aggregations Perform several aggregations in one step.\ncount() ‚Üí Number of rows in each group. avg() ‚Üí Average of values. max() ‚Üí Maximum value in group. 3. Concatenate Strings concat_ws() ‚Üí Concatenates string values within a column, separated by a delimiter (,). 4. First and Last first() ‚Üí Retrieves the first value of a column in a group. last() ‚Üí Retrieves the last value of a column in a group. 5. Standard Deviation and Variance stddev() ‚Üí Standard deviation of values. variance() ‚Üí Variance of values. 6. Aggregation with Alias .alias() ‚Üí Rename the result columns after aggregation. 7. Sum of Distinct Values sumDistinct() ‚Üí Sums only unique values in a column (avoids double-counting duplicates). üìå Summary Use basic aggregations (sum, avg, count, max, min, countDistinct) for general metrics.\nApply advanced aggregations (grouped, concat_ws, first, last, stddev, variance, sumDistinct) for deeper analysis.\nAlways consider null handling and performance optimizations when using aggregate functions in PySpark.",
    "description": "Basic Aggregate Functions Sample Data from pyspark.sql import Row # Create sample data data = [ Row(id=1, value=10), Row(id=2, value=20), Row(id=3, value=30), Row(id=4, value=None), Row(id=5, value=40), Row(id=6, value=20) ] # Create DataFrame df = spark.createDataFrame(data) # Show the DataFrame df.show() Aggregate Functions in PySpark Summation (sum) ‚Äì Adds up the values in a column. Average (avg) ‚Äì Computes the average of values in a column. Count (count) ‚Äì Counts the number of non-null values in a column. Maximum (max) / Minimum (min) ‚Äì Finds the highest and lowest values. Distinct Count (countDistinct) ‚Äì Counts unique values in a column. Notes Handling Nulls:",
    "tags": [],
    "title": "Aggregate functions",
    "uri": "/azure_data_bricks/aggregations/"
  },
  {
    "breadcrumb": "SQL",
    "content": "SQL set operations enable you to combine results from multiple queries into a single result set.\nThis guide demonstrates the rules and usage of UNION, UNION ALL, EXCEPT, and INTERSECT.\n1. SQL Operation Rules Rule: Data Types The data types of columns in each query should match.\nSELECT FirstName, LastName, Country FROM Sales.Customers UNION SELECT FirstName, LastName FROM Sales.Employees; Rule: Data Types (Example) SELECT CustomerID, LastName FROM Sales.Customers UNION SELECT FirstName, LastName FROM Sales.Employees; Rule: Column Order The order of the columns in each query must be the same.\nSELECT LastName, CustomerID FROM Sales.Customers UNION SELECT EmployeeID, LastName FROM Sales.Employees; Rule: Column Aliases The column names in the result set are determined by the column names specified in the first SELECT statement.\nSELECT CustomerID AS ID, LastName AS Last_Name FROM Sales.Customers UNION SELECT EmployeeID, LastName FROM Sales.Employees; Rule: Correct Columns Ensure that the correct columns are used to maintain data consistency.\nSELECT FirstName, LastName FROM Sales.Customers UNION SELECT LastName, FirstName FROM Sales.Employees; 2. UNION, UNION ALL, EXCEPT, INTERSECT Task 1: UNION Combine the data from Employees and Customers into one table.\nSELECT FirstName, LastName FROM Sales.Customers UNION SELECT FirstName, LastName FROM Sales.Employees; Task 2: UNION ALL Combine the data from Employees and Customers into one table, including duplicates.\nSELECT FirstName, LastName FROM Sales.Customers UNION ALL SELECT FirstName, LastName FROM Sales.Employees; Task 3: EXCEPT Find employees who are NOT customers.\nSELECT FirstName, LastName FROM Sales.Employees EXCEPT SELECT FirstName, LastName FROM Sales.Customers; Task 4: INTERSECT Find employees who are also customers.\nSELECT FirstName, LastName FROM Sales.Employees INTERSECT SELECT FirstName, LastName FROM Sales.Customers; Task 5: UNION with Orders Combine order data from Orders and OrdersArchive into one report without duplicates.\nSELECT 'Orders' AS SourceTable, OrderID, ProductID, CustomerID, SalesPersonID, OrderDate, ShipDate, OrderStatus, ShipAddress, BillAddress, Quantity, Sales, CreationTime FROM Sales.Orders UNION SELECT 'OrdersArchive' AS SourceTable, OrderID, ProductID, CustomerID, SalesPersonID, OrderDate, ShipDate, OrderStatus, ShipAddress, BillAddress, Quantity, Sales, CreationTime FROM Sales.OrdersArchive ORDER BY OrderID;",
    "description": "SQL set operations enable you to combine results from multiple queries into a single result set.\nThis guide demonstrates the rules and usage of UNION, UNION ALL, EXCEPT, and INTERSECT.\n1. SQL Operation Rules Rule: Data Types The data types of columns in each query should match.\nSELECT FirstName, LastName, Country FROM Sales.Customers UNION SELECT FirstName, LastName FROM Sales.Employees; Rule: Data Types (Example) SELECT CustomerID, LastName FROM Sales.Customers UNION SELECT FirstName, LastName FROM Sales.Employees; Rule: Column Order The order of the columns in each query must be the same.",
    "tags": [],
    "title": "Set Operations",
    "uri": "/sql/set-operations/"
  },
  {
    "breadcrumb": "ADB",
    "content": "Joins in PySpark In PySpark Joins are used to combine two DataFrames based on a common column or condition.\nTypes of Joins in PySpark Inner Join: Matches rows from both DataFrames. df1.join(df2, df1.common_column == df2.common_column, \"inner\") Left/Right Join: Keeps all rows from the left or right DataFrame and matches where possible. df1.join(df2, df1.common_column == df2.common_column, \"left\") df1.join(df2, df1.common_column == df2.common_column, \"right\") Full Join: Keeps all rows from both DataFrames. df1.join(df2, df1.common_column == df2.common_column, \"outer\") Left Semi: Filters df1 to rows that match df2 without including columns from df2. df1.join(df2, df1.common_column == df2.common_column, \"left_semi\") Left Anti: Filters df1 to rows that do not match df2. df1.join(df2, df1.common_column == df2.common_column, \"left_anti\") Cross Join: Returns the Cartesian product, combining all rows of both DataFrames. df1.crossJoin(df2) Explicit Condition Join: Allows complex join conditions, including columns with different names. df1.join(df2, df1.columnA == df2.columnB, \"inner\") df1.join(df2, df1.common_column == df2.common_column, \"inner\") df1.join(df2, df1.common_column == df2.common_column, \"left\") df1.join(df2, df1.common_column == df2.common_column, \"right\") df1.join(df2, df1.common_column == df2.common_column, \"outer\") df1.join(df2, df1.common_column == df2.common_column, \"left_semi\") df1.join(df2, df1.common_column == df2.common_column, \"left_anti\") df1.crossJoin(df2) df1.join(df2, df1.columnA == df2.columnB, \"inner\") Practice-1 from pyspark.sql import SparkSession from pyspark.sql import Row from pyspark.sql.functions import broadcast # Initialize Spark session spark = SparkSession.builder.appName(\"JoinsExample\").getOrCreate() # Sample DataFrames data1 = [Row(id=0), Row(id=1), Row(id=1), Row(id=None), Row(id=None)] data2 = [Row(id=1), Row(id=0), Row(id=None)] df1 = spark.createDataFrame(data1) df2 = spark.createDataFrame(data2) # Inner Join inner_join = df1.join(df2, on=\"id\", how=\"inner\") print(\"Inner Join:\") inner_join.show() # Right Join right_join = df1.join(df2, on=\"id\", how=\"right\") print(\"Right Join:\") right_join.show() # Full (Outer) Join full_join = df1.join(df2, on=\"id\", how=\"outer\") print(\"Full (Outer) Join:\") full_join.show() # Left Anti Join left_anti_join = df1.join(df2, on=\"id\", how=\"left_anti\") print(\"Left Anti Join:\") left_anti_join.show() # Right Anti Join (Equivalent to swapping DataFrames and performing Left Anti Join) right_anti_join = df2.join(df1, on=\"id\", how=\"left_anti\") print(\"Right Anti Join:\") right_anti_join.show() # Broadcast Join (Optimizing a join with a smaller DataFrame) broadcast_join = df1.join(broadcast(df2), on=\"id\", how=\"inner\") print(\"Broadcast Join:\") broadcast_join.show() Practice 2 PySpark Coding Questions Find employees whose location matches the location of their department\nDisplay: emp_id, emp_name, emp_location, dept_name, dept_location. Find departments that have no employees assigned to them\nDisplay: dept_id, dept_name, dept_head. Get the average salary of employees in each department\nDisplay: dept_name, average_salary. List the employees who earn more than the average salary of their department\nDisplay: emp_id, emp_name, emp_salary, dept_name, dept_location. # Sample DataFrames emp_data = [ Row(emp_id=1, emp_name=\"Alice\", emp_salary=50000, emp_dept_id=101, emp_location=\"New York\"), Row(emp_id=2, emp_name=\"Bob\", emp_salary=60000, emp_dept_id=102, emp_location=\"Los Angeles\"), Row(emp_id=3, emp_name=\"Charlie\", emp_salary=55000, emp_dept_id=101, emp_location=\"Chicago\"), Row(emp_id=4, emp_name=\"David\", emp_salary=70000, emp_dept_id=103, emp_location=\"San Francisco\"), Row(emp_id=5, emp_name=\"Eve\", emp_salary=48000, emp_dept_id=102, emp_location=\"Houston\") ] dept_data = [ Row(dept_id=101, dept_name=\"Engineering\", dept_head=\"John\", dept_location=\"New York\"), Row(dept_id=102, dept_name=\"Marketing\", dept_head=\"Mary\", dept_location=\"Los Angeles\"), Row(dept_id=103, dept_name=\"Finance\", dept_head=\"Frank\", dept_location=\"Chicago\") ] emp_columns = [\"emp_id\", \"emp_name\", \"emp_salary\", \"emp_dept_id\", \"emp_location\"] dept_columns = [\"dept_id\", \"dept_name\", \"dept_head\", \"dept_location\"] emp_df = spark.createDataFrame(emp_data, emp_columns) dept_df = spark.createDataFrame(dept_data, dept_columns) # Display emp data print(\"emp_data:\") emp_df.show() # Display dept data print(\"dept_data:\") dept_df.show() # Inner Join on emp_dept_id and dept_id inner_join = emp_df.join(dept_df, emp_df[\"emp_dept_id\"] == dept_df[\"dept_id\"], \"inner\") # Display the result print(\"Inner Join Result:\") inner_join.show() # Inner Join with Filtering Columns and WHERE Condition inner_join = emp_df.join(dept_df, emp_df[\"emp_dept_id\"] == dept_df[\"dept_id\"], \"inner\") \\ .select(\"emp_id\", \"emp_name\", \"emp_salary\", \"dept_name\", \"dept_location\") \\ .filter(\"emp_salary \u003e 55000\") # Add a WHERE condition # Display the result print(\"Inner Join with Filter and WHERE Condition:\") inner_join.show() # Left Join with Filtering Columns and WHERE Condition left_join_filtered = emp_df.join(dept_df, emp_df[\"emp_dept_id\"] == dept_df[\"dept_id\"], \"left\") \\ .select(\"emp_id\", \"emp_name\", \"dept_name\", \"dept_location\") \\ .filter(\"emp_salary \u003e 55000\") # Add a WHERE condition # Display the result print(\"Left Join with Filter and WHERE Condition:\") left_join_filtered.show() # Left Anti Join left_anti_join = emp_df.join( dept_df, emp_df[\"emp_dept_id\"] == dept_df[\"dept_id\"], \"left_anti\" ) # Display the result print(\"Left Anti Join Result:\") left_anti_join.show() Practice 3 PySpark Coding Questions List each employee along with their manager‚Äôs name\nDisplay: employee, manager. Find employees who do not have a manager (CEO-level employees)\nDisplay: employee, manager. Find all employees who directly report to ‚ÄúManager A‚Äù\nDisplay: empid, ename, mrgid. Determine the hierarchy level of each employee\nCEO ‚Üí Level 1, direct reports to CEO ‚Üí Level 2, and so on. Display: empid, ename, mrgid, level. from pyspark.sql import SparkSession from pyspark.sql.functions import col, expr # Create a Spark session spark = SparkSession.builder.appName(\"EmployeeHierarchy\").getOrCreate() # Sample data data = [ (1, None, \"CEO\"), (2, 1, \"Manager A\"), (3, 1, \"Manager B\"), (4, 2, \"Employee X\"), (5, 3, \"Employee Y\"), ] columns = [\"empid\", \"mrgid\", \"ename\"] employee_df = spark.createDataFrame(data, columns) # Display the result print(\"emp_data:\") employee_df.show() # Self-join to find the manager and CEO manager_df = employee_df.alias(\"e\") \\ .join(employee_df.alias(\"m\"), col(\"e.mrgid\") == col(\"m.empid\"), \"left\") \\ .select( col(\"e.ename\").alias(\"employee\"), col(\"m.ename\").alias(\"manager\") ) # Display the result print(\"mgr:\") manager_df.show() # Filter for employees without a manager (CEO) manager_df2 = employee_df.alias(\"e1\") \\ .join(employee_df.alias(\"m1\"), col(\"e1.mrgid\") == col(\"m1.empid\"), \"left\") \\ .select( col(\"e1.ename\").alias(\"employee\"), col(\"m1.ename\").alias(\"manager\") ) \\ .filter(col(\"manager\").isNull()) # Display the result manager_df2.show()",
    "description": "Joins in PySpark In PySpark Joins are used to combine two DataFrames based on a common column or condition.\nTypes of Joins in PySpark Inner Join: Matches rows from both DataFrames. df1.join(df2, df1.common_column == df2.common_column, \"inner\") Left/Right Join: Keeps all rows from the left or right DataFrame and matches where possible. df1.join(df2, df1.common_column == df2.common_column, \"left\") df1.join(df2, df1.common_column == df2.common_column, \"right\") Full Join: Keeps all rows from both DataFrames. df1.join(df2, df1.common_column == df2.common_column, \"outer\") Left Semi: Filters df1 to rows that match df2 without including columns from df2. df1.join(df2, df1.common_column == df2.common_column, \"left_semi\") Left Anti: Filters df1 to rows that do not match df2. df1.join(df2, df1.common_column == df2.common_column, \"left_anti\") Cross Join: Returns the Cartesian product, combining all rows of both DataFrames. df1.crossJoin(df2) Explicit Condition Join: Allows complex join conditions, including columns with different names. df1.join(df2, df1.columnA == df2.columnB, \"inner\") df1.join(df2, df1.common_column == df2.common_column, \"inner\") df1.join(df2, df1.common_column == df2.common_column, \"left\") df1.join(df2, df1.common_column == df2.common_column, \"right\") df1.join(df2, df1.common_column == df2.common_column, \"outer\") df1.join(df2, df1.common_column == df2.common_column, \"left_semi\") df1.join(df2, df1.common_column == df2.common_column, \"left_anti\") df1.crossJoin(df2) df1.join(df2, df1.columnA == df2.columnB, \"inner\") Practice-1 from pyspark.sql import SparkSession from pyspark.sql import Row from pyspark.sql.functions import broadcast # Initialize Spark session spark = SparkSession.builder.appName(\"JoinsExample\").getOrCreate() # Sample DataFrames data1 = [Row(id=0), Row(id=1), Row(id=1), Row(id=None), Row(id=None)] data2 = [Row(id=1), Row(id=0), Row(id=None)] df1 = spark.createDataFrame(data1) df2 = spark.createDataFrame(data2) # Inner Join inner_join = df1.join(df2, on=\"id\", how=\"inner\") print(\"Inner Join:\") inner_join.show() # Right Join right_join = df1.join(df2, on=\"id\", how=\"right\") print(\"Right Join:\") right_join.show() # Full (Outer) Join full_join = df1.join(df2, on=\"id\", how=\"outer\") print(\"Full (Outer) Join:\") full_join.show() # Left Anti Join left_anti_join = df1.join(df2, on=\"id\", how=\"left_anti\") print(\"Left Anti Join:\") left_anti_join.show() # Right Anti Join (Equivalent to swapping DataFrames and performing Left Anti Join) right_anti_join = df2.join(df1, on=\"id\", how=\"left_anti\") print(\"Right Anti Join:\") right_anti_join.show() # Broadcast Join (Optimizing a join with a smaller DataFrame) broadcast_join = df1.join(broadcast(df2), on=\"id\", how=\"inner\") print(\"Broadcast Join:\") broadcast_join.show() Practice 2 PySpark Coding Questions Find employees whose location matches the location of their department",
    "tags": [],
    "title": "Joins",
    "uri": "/azure_data_bricks/joins/"
  },
  {
    "breadcrumb": "SQL",
    "content": "This document provides an overview of SQL string functions, which allow manipulation, transformation, and extraction of text data efficiently.\n1. Manipulations CONCAT() ‚Äì String Concatenation -- Concatenate first name and country into one column SELECT CONCAT(first_name, '-', country) AS full_info FROM customers; LOWER() \u0026 UPPER() ‚Äì Case Transformation -- Convert the first name to lowercase SELECT LOWER(first_name) AS lower_case_name FROM customers; -- Convert the first name to uppercase SELECT UPPER(first_name) AS upper_case_name FROM customers; TRIM() ‚Äì Remove White Spaces -- Find customers whose first name contains leading or trailing spaces SELECT first_name, LEN(first_name) AS len_name, LEN(TRIM(first_name)) AS len_trim_name, LEN(first_name) - LEN(TRIM(first_name)) AS flag FROM customers WHERE LEN(first_name) != LEN(TRIM(first_name)); -- Alternative: -- WHERE first_name != TRIM(first_name) REPLACE() ‚Äì Replace or Remove Values -- Remove dashes (-) from a phone number SELECT '123-456-7890' AS phone, REPLACE('123-456-7890', '-', '/') AS clean_phone; -- Replace file extension from .txt to .csv SELECT 'report.txt' AS old_filename, REPLACE('report.txt', '.txt', '.csv') AS new_filename; 2. Calculation LEN() ‚Äì String Length -- Calculate the length of each customer's first name SELECT first_name, LEN(first_name) AS name_length FROM customers; 3. Substring Extraction LEFT() \u0026 RIGHT() -- Retrieve the first two characters of each first name SELECT first_name, LEFT(TRIM(first_name), 2) AS first_2_chars FROM customers; -- Retrieve the last two characters of each first name SELECT first_name, RIGHT(first_name, 2) AS last_2_chars FROM customers; SUBSTRING() -- Retrieve a list of customers' first names after removing the first character SELECT first_name, SUBSTRING(TRIM(first_name), 2, LEN(first_name)) AS trimmed_name FROM customers; 4. Nesting Functions ‚Äì Nesting example SELECT first_name, UPPER(LOWER(first_name)) AS nesting FROM customers;",
    "description": "This document provides an overview of SQL string functions, which allow manipulation, transformation, and extraction of text data efficiently.\n1. Manipulations CONCAT() ‚Äì String Concatenation -- Concatenate first name and country into one column SELECT CONCAT(first_name, '-', country) AS full_info FROM customers; LOWER() \u0026 UPPER() ‚Äì Case Transformation -- Convert the first name to lowercase SELECT LOWER(first_name) AS lower_case_name FROM customers; -- Convert the first name to uppercase SELECT UPPER(first_name) AS upper_case_name FROM customers; TRIM() ‚Äì Remove White Spaces -- Find customers whose first name contains leading or trailing spaces SELECT first_name, LEN(first_name) AS len_name, LEN(TRIM(first_name)) AS len_trim_name, LEN(first_name) - LEN(TRIM(first_name)) AS flag FROM customers WHERE LEN(first_name) != LEN(TRIM(first_name)); -- Alternative: -- WHERE first_name != TRIM(first_name) REPLACE() ‚Äì Replace or Remove Values -- Remove dashes (-) from a phone number SELECT '123-456-7890' AS phone, REPLACE('123-456-7890', '-', '/') AS clean_phone; -- Replace file extension from .txt to .csv SELECT 'report.txt' AS old_filename, REPLACE('report.txt', '.txt', '.csv') AS new_filename; 2. Calculation LEN() ‚Äì String Length -- Calculate the length of each customer's first name SELECT first_name, LEN(first_name) AS name_length FROM customers; 3. Substring Extraction LEFT() \u0026 RIGHT() -- Retrieve the first two characters of each first name SELECT first_name, LEFT(TRIM(first_name), 2) AS first_2_chars FROM customers; -- Retrieve the last two characters of each first name SELECT first_name, RIGHT(first_name, 2) AS last_2_chars FROM customers; SUBSTRING() -- Retrieve a list of customers' first names after removing the first character SELECT first_name, SUBSTRING(TRIM(first_name), 2, LEN(first_name)) AS trimmed_name FROM customers; 4. Nesting Functions ‚Äì Nesting example SELECT first_name, UPPER(LOWER(first_name)) AS nesting FROM customers;",
    "tags": [],
    "title": "String Functions",
    "uri": "/sql/string-functions/"
  },
  {
    "breadcrumb": "SQL",
    "content": "This document provides an overview of SQL number functions, which allow performing mathematical operations and formatting numerical values.\n1. Rounding Functions ROUND() ‚Äì Rounding Numbers -- Demonstrate rounding a number to different decimal places SELECT 3.516 AS original_number, ROUND(3.516, 2) AS round_2, ROUND(3.516, 1) AS round_1, ROUND(3.516, 0) AS round_0; 2. Absolute Value Function ABS() ‚Äì Absolute Value -- Demonstrate absolute value function SELECT -10 AS original_number, ABS(-10) AS absolute_value_negative, ABS(10) AS absolute_value_positive;",
    "description": "This document provides an overview of SQL number functions, which allow performing mathematical operations and formatting numerical values.\n1. Rounding Functions ROUND() ‚Äì Rounding Numbers -- Demonstrate rounding a number to different decimal places SELECT 3.516 AS original_number, ROUND(3.516, 2) AS round_2, ROUND(3.516, 1) AS round_1, ROUND(3.516, 0) AS round_0; 2. Absolute Value Function ABS() ‚Äì Absolute Value -- Demonstrate absolute value function SELECT -10 AS original_number, ABS(-10) AS absolute_value_negative, ABS(10) AS absolute_value_positive;",
    "tags": [],
    "title": "Number Functions",
    "uri": "/sql/number-functions/"
  },
  {
    "breadcrumb": "ADB",
    "content": "when and otherwise The when and otherwise functions in PySpark provide a way to create conditional expressions within a DataFrame, allowing you to specify different values for new or existing columns based on specific conditions.\nwhen:\nThe when function in PySpark is used to define a condition.\nIf the condition is met, it returns the specified value.\nYou can chain multiple when conditions to handle various cases.\notherwise:\nThe otherwise function specifies a default value to return if none of the conditions in the when statements are met.\nfrom pyspark.sql import SparkSession from pyspark.sql.functions import when from pyspark.sql.types import StructType, StructField, IntegerType, StringType # Initialize Spark session spark = SparkSession.builder.appName(\"WhenOtherwiseExample\").getOrCreate() # Define the schema for the dataset schema = StructType([ StructField(\"name\", StringType(), True), StructField(\"age\", IntegerType(), True), StructField(\"salary\", IntegerType(), True) ]) # Create a sample dataset data = [ (\"Alice\", 25, 3000), (\"Bob\", 35, 4000), (\"Charlie\", 40, 5000), (\"David\", 28, 4500), (\"Eve\", 32, 3500) ] # Create DataFrame df = spark.createDataFrame(data, schema) df.show() # Apply 'when' and 'otherwise' to add new columns based on conditions df = ( df.withColumn(\"status\", when(df.age \u003c 30, \"Young\").otherwise(\"Adult\")) .withColumn(\"income_bracket\", when(df.salary \u003c 4000, \"Low\") .when((df.salary \u003e= 4000) \u0026 (df.salary \u003c= 4500), \"Medium\") .otherwise(\"High\")) ) # Show the result df.show() Explanation status column\nAssigns \"Young\" if age \u003c 30. Otherwise assigns \"Adult\". income_bracket column\nAssigns \"Low\" if salary \u003c 4000. Assigns \"Medium\" if 4000 \u003c= salary \u003c= 4500. Assigns \"High\" for any other salary values. This approach allows for flexible handling of multiple conditions in PySpark DataFrames using when and otherwise.\ncast() and printSchema() In PySpark, the cast() function is used to change the data type of a column within a DataFrame.\nThis is helpful when you need to standardize column data types for data processing, schema consistency, or compatibility with other operations.\nPurpose:\nThe cast() function allows you to change the data type of a column, useful in situations like standardizing formats (e.g., converting strings to dates or integers).\nSyntax:\nThe cast() function is applied on individual columns and requires specifying the target data type in quotes.\nMultiple Columns:\nYou can cast multiple columns at once by using a list of cast expressions and passing them to select().\nSupported Data Types:\nPySpark supports various data types for casting, including:\nStringType IntegerType (or \"int\") DoubleType (or \"double\") DateType TimestampType BooleanType Others, based on the data types available in PySpark. from pyspark.sql.functions import col # Single column cast df = df.withColumn(\"column_name\", col(\"column_name\").cast(\"target_data_type\")) # Multiple columns cast with select cast_expr = [ col(\"column1_name\").cast(\"target_data_type1\"), col(\"column2_name\").cast(\"target_data_type2\"), # More columns and data types as needed ] df = df.select(*cast_expr) Example Let‚Äôs create a dataset and apply cast() to change the data types of multiple columns:\nfrom pyspark.sql import SparkSession from pyspark.sql.functions import col from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType # Initialize Spark session spark = SparkSession.builder.appName(\"CastExample\").getOrCreate() # Define the schema for the dataset schema = StructType([ StructField(\"name\", StringType(), True), StructField(\"age\", StringType(), True), # Stored as StringType initially StructField(\"height\", StringType(), True) # Stored as StringType initially ]) # Create a sample dataset data = [ (\"Alice\", \"25\", \"5.5\"), (\"Bob\", \"35\", \"6.1\"), (\"Charlie\", \"40\", \"5.8\"), ] # Create DataFrame df = spark.createDataFrame(data, schema) # Show schema and data before casting df.printSchema() df.show() # Define cast expressions for multiple columns cast_expr = [ col(\"name\").cast(\"string\"), col(\"age\").cast(\"int\"), # Casting age to IntegerType col(\"height\").cast(\"double\") # Casting height to DoubleType ] # Apply the cast expressions to the DataFrame df = df.select(*cast_expr) # Show the result df.printSchema() df.show() Explanation age column: Initially stored as StringType, it‚Äôs cast to IntegerType (or \"int\"). height column: Initially stored as StringType, it‚Äôs cast to DoubleType (or \"double\"). Advantages of Using cast() Schema Alignment: Ensures data types in different tables or DataFrames are compatible for joining or union operations. Data Consistency: Ensures all columns conform to expected data types for downstream data processing. Error Reduction: Minimizes issues arising from mismatched data types in computations or transformations. This approach using cast() provides a flexible and powerful way to manage data types in PySpark.\nprintSchema() Method in PySpark Purpose:\nTo display the schema of a DataFrame, which includes the column names, data types, and nullability of each column. Output Structure:\nThe schema is presented in a tree-like structure showing:\nColumn Name: The name of the column. Data Type: The data type of the column (e.g., string, integer, double, boolean, etc.). Nullability: Indicates whether the column can contain null values (e.g., nullable = true). Usage:\nCall df.printSchema() on a DataFrame df to see its structure. Useful for verifying the structure of the DataFrame after operations like select(), withColumn(), or cast(). union and unionAll Overview Purpose: Both union and unionAll are used to combine two DataFrames into a single DataFrame. DataFrame Compatibility: The two DataFrames must have the same schema (i.e., the same column names and data types) to perform the union operation. union() Functionality:\nCombines two DataFrames and retains all rows, including duplicates. Behavior:\nThe union() method does not remove duplicate rows, resulting in a DataFrame that may contain duplicates. unionAll() Functionality:\nCombines two DataFrames and retains all rows, including duplicates. Behavior:\nThe unionAll() method performs the union operation but does not eliminate duplicate rows (similar to union). Syntax # Using union to retain all rows including duplicates unioned_df = df1.union(df2) # Using unionAll to retain all rows including duplicates unionAll_df = df1.unionAll(df2) Example: Using union and unionAll in PySpark from pyspark.sql import SparkSession # Initialize Spark session spark = SparkSession.builder.appName(\"UnionExample\").getOrCreate() # Sample DataFrames data1 = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)] data2 = [(\"David\", 40), (\"Eve\", 45), (\"Alice\", 25)] columns = [\"name\", \"age\"] df1 = spark.createDataFrame(data1, columns) df2 = spark.createDataFrame(data2, columns) # Using union to retain all rows including duplicates unioned_df = df1.union(df2) # Using unionAll to retain all rows unionAll_df = df1.unionAll(df2) # Show the results print(\"unioned_df (No duplicates removed):\") unioned_df.show() print(\"unionAll_df (duplicates retained):\") unionAll_df.show() ## Removing Duplicate Rows in PySpark # Remove duplicate rows and create a new DataFrame unique_df = unioned_df.dropDuplicates() # or unique_df = unioned_df.distinct() print(\"unique_df (after removing duplicates):\") unique_df.show() Union and UnionByName In PySpark, both union and unionByName are operations that allow you to combine two or more DataFrames. However, they do this in slightly different ways, particularly regarding how they handle column names.\n1. union Definition:\nThe union() function is used to combine two DataFrames with the same schema (i.e., the same number of columns with the same data types).\nIt appends the rows of one DataFrame to the other.\nKey Characteristics:\nThe DataFrames must have the same number of columns. The columns must have compatible data types. It does not automatically handle column names that differ between DataFrames. Syntax DataFrame.union(otherDataFrame) from pyspark.sql import SparkSession # Create a Spark session spark = SparkSession.builder.appName(\"Union Example\").getOrCreate() # Create two DataFrames with the same schema data1 = [(\"Alice\", 1), (\"Bob\", 2)] data2 = [(\"Cathy\", 3), (\"David\", 4)] columns = [\"Name\", \"Id\"] df1 = spark.createDataFrame(data1, columns) df2 = spark.createDataFrame(data2, columns) # Perform union result_union = df1.union(df2) # Show the result result_union.show() 2. unionByName Definition:\nThe unionByName() function allows you to combine two DataFrames by matching column names.\nIf the DataFrames do not have the same schema, it will fill in missing columns with null.\nKey Characteristics:\nMatches DataFrames by column names rather than position. If the DataFrames have different columns, it will include all columns and fill in null for missing values in any DataFrame. You can specify allowMissingColumns=True to ignore missing columns. Syntax DataFrame.unionByName(otherDataFrame, allowMissingColumns=False) # Create two DataFrames with different schemas data3 = [(\"Eve\", 5), (\"Frank\", 6)] data4 = [(\"Grace\", \"New York\"), (\"Hannah\", \"Los Angeles\")] columns1 = [\"Name\", \"Id\"] columns2 = [\"Name\", \"City\"] df3 = spark.createDataFrame(data3, columns1) df4 = spark.createDataFrame(data4, columns2) # Perform unionByName (with allowMissingColumns=True to handle schema differences) result_union_by_name = df3.unionByName(df4, allowMissingColumns=True) # Show the result result_union_by_name.show() result_union_by_name.show() Summary of Differences union(): Requires DataFrames to have the same schema (same number of columns and compatible data types). It combines rows without checking column names. unionByName(): Matches DataFrames by column names. It can handle different schemas and fill missing columns with null (when allowMissingColumns=True). Feature Union UnionByName Column Matching Positional By Name Missing Columns Handling Does not allow Allows with null for missing Schema Requirement Must be identical Can differ Conclusion In PySpark:\nUse union() when you have DataFrames with the same schema and need a straightforward concatenation. Use unionByName() when your DataFrames have different schemas and you want to combine them by matching column names while handling missing columns.",
    "description": "when and otherwise The when and otherwise functions in PySpark provide a way to create conditional expressions within a DataFrame, allowing you to specify different values for new or existing columns based on specific conditions.\nwhen:\nThe when function in PySpark is used to define a condition.\nIf the condition is met, it returns the specified value.\nYou can chain multiple when conditions to handle various cases.\notherwise:\nThe otherwise function specifies a default value to return if none of the conditions in the when statements are met.",
    "tags": [],
    "title": "When|Cast|Union",
    "uri": "/azure_data_bricks/when/"
  },
  {
    "breadcrumb": "SQL",
    "content": "This guide demonstrates various date and time functions in SQL.\nIt covers GETDATE, DATETRUNC, DATENAME, DATEPART, YEAR, MONTH, DAY, EOMONTH, FORMAT, CONVERT, CAST, DATEADD, DATEDIFF, and ISDATE.\n1. GETDATE() | Date Values -- Display OrderID, CreationTime, a hard-coded date, and the current system date SELECT OrderID, CreationTime, '2025-08-20' AS HardCoded, GETDATE() AS Today FROM Sales.Orders; 2. Date Part Extractions (DATETRUNC, DATENAME, DATEPART, YEAR, MONTH, DAY) -- Extract various parts of CreationTime SELECT OrderID, CreationTime, DATETRUNC(year, CreationTime) AS Year_dt, DATETRUNC(day, CreationTime) AS Day_dt, DATETRUNC(minute, CreationTime) AS Minute_dt, DATENAME(month, CreationTime) AS Month_dn, DATENAME(weekday, CreationTime) AS Weekday_dn, DATENAME(day, CreationTime) AS Day_dn, DATENAME(year, CreationTime) AS Year_dn, DATEPART(year, CreationTime) AS Year_dp, DATEPART(month, CreationTime) AS Month_dp, DATEPART(day, CreationTime) AS Day_dp, DATEPART(hour, CreationTime) AS Hour_dp, DATEPART(quarter, CreationTime) AS Quarter_dp, DATEPART(week, CreationTime) AS Week_dp, YEAR(CreationTime) AS Year, MONTH(CreationTime) AS Month, DAY(CreationTime) AS Day FROM Sales.Orders; 3. DATETRUNC() Data Aggregation -- Aggregate orders by year SELECT DATETRUNC(year, CreationTime) AS Creation, COUNT(*) AS OrderCount FROM Sales.Orders GROUP BY DATETRUNC(year, CreationTime); 4. EOMONTH() -- Display end-of-month date for each order SELECT OrderID, CreationTime, EOMONTH(CreationTime) AS EndOfMonth FROM Sales.Orders; 5. Date Parts | Use Cases -- Orders per year SELECT YEAR(OrderDate) AS OrderYear, COUNT(*) AS TotalOrders FROM Sales.Orders GROUP BY YEAR(OrderDate); -- Orders per month SELECT MONTH(OrderDate) AS OrderMonth, COUNT(*) AS TotalOrders FROM Sales.Orders GROUP BY MONTH(OrderDate); -- Orders per month (friendly names) SELECT DATENAME(month, OrderDate) AS OrderMonth, COUNT(*) AS TotalOrders FROM Sales.Orders GROUP BY DATENAME(month, OrderDate); -- Orders placed in February SELECT * FROM Sales.Orders WHERE MONTH(OrderDate) = 2; 6. FORMAT() -- Format CreationTime into different formats SELECT OrderID, CreationTime, FORMAT(CreationTime, 'MM-dd-yyyy') AS USA_Format, FORMAT(CreationTime, 'dd-MM-yyyy') AS EURO_Format, FORMAT(CreationTime, 'dd') AS dd, FORMAT(CreationTime, 'ddd') AS ddd, FORMAT(CreationTime, 'dddd') AS dddd, FORMAT(CreationTime, 'MM') AS MM, FORMAT(CreationTime, 'MMM') AS MMM, FORMAT(CreationTime, 'MMMM') AS MMMM FROM Sales.Orders; -- Custom format example SELECT OrderID, CreationTime, 'Day ' + FORMAT(CreationTime, 'ddd MMM') + ' Q' + DATENAME(quarter, CreationTime) + ' ' + FORMAT(CreationTime, 'yyyy hh:mm:ss tt') AS CustomFormat FROM Sales.Orders; -- Orders per month (formatted \"MMM yy\") SELECT FORMAT(CreationTime, 'MMM yy') AS OrderDate, COUNT(*) AS TotalOrders FROM Sales.Orders GROUP BY FORMAT(CreationTime, 'MMM yy'); 7. CONVERT() -- Conversion using CONVERT SELECT CONVERT(INT, '123') AS [String to Int CONVERT], CONVERT(DATE, '2025-08-20') AS [String to Date CONVERT], CreationTime, CONVERT(DATE, CreationTime) AS [Datetime to Date CONVERT], CONVERT(VARCHAR, CreationTime, 32) AS [USA Std. Style:32], CONVERT(VARCHAR, CreationTime, 34) AS [EURO Std. Style:34] FROM Sales.Orders; 8. CAST() -- Conversion using CAST SELECT CAST('123' AS INT) AS [String to Int], CAST(123 AS VARCHAR) AS [Int to String], CAST('2025-08-20' AS DATE) AS [String to Date], CAST('2025-08-20' AS DATETIME2) AS [String to Datetime], CreationTime, CAST(CreationTime AS DATE) AS [Datetime to Date] FROM Sales.Orders; 9. DATEADD / DATEDIFF -- Date arithmetic SELECT OrderID, OrderDate, DATEADD(day, -10, OrderDate) AS TenDaysBefore, DATEADD(month, 3, OrderDate) AS ThreeMonthsLater, DATEADD(year, 2, OrderDate) AS TwoYearsLater FROM Sales.Orders; -- Employee ages SELECT EmployeeID, BirthDate, DATEDIFF(year, BirthDate, GETDATE()) AS Age FROM Sales.Employees; -- Avg shipping duration SELECT MONTH(OrderDate) AS OrderMonth, AVG(DATEDIFF(day, OrderDate, ShipDate)) AS AvgShip FROM Sales.Orders GROUP BY MONTH(OrderDate); -- Time gap analysis SELECT OrderID, OrderDate AS CurrentOrderDate, LAG(OrderDate) OVER (ORDER BY OrderDate) AS PreviousOrderDate, DATEDIFF(day, LAG(OrderDate) OVER (ORDER BY OrderDate), OrderDate) AS NrOfDays FROM Sales.Orders; 10. ISDATE() -- Validate OrderDate using ISDATE SELECT OrderDate, ISDATE(OrderDate) AS IsValidDate, CASE WHEN ISDATE(OrderDate) = 1 THEN CAST(OrderDate AS DATE) ELSE '9999-01-01' END AS NewOrderDate FROM ( SELECT '2025-08-20' AS OrderDate UNION SELECT '2025-08-21' UNION SELECT '2025-08-23' UNION SELECT '2025-08' ) AS t; -- WHERE ISDATE(OrderDate) = 0",
    "description": "This guide demonstrates various date and time functions in SQL.\nIt covers GETDATE, DATETRUNC, DATENAME, DATEPART, YEAR, MONTH, DAY, EOMONTH, FORMAT, CONVERT, CAST, DATEADD, DATEDIFF, and ISDATE.\n1. GETDATE() | Date Values -- Display OrderID, CreationTime, a hard-coded date, and the current system date SELECT OrderID, CreationTime, '2025-08-20' AS HardCoded, GETDATE() AS Today FROM Sales.Orders; 2. Date Part Extractions (DATETRUNC, DATENAME, DATEPART, YEAR, MONTH, DAY) -- Extract various parts of CreationTime SELECT OrderID, CreationTime, DATETRUNC(year, CreationTime) AS Year_dt, DATETRUNC(day, CreationTime) AS Day_dt, DATETRUNC(minute, CreationTime) AS Minute_dt, DATENAME(month, CreationTime) AS Month_dn, DATENAME(weekday, CreationTime) AS Weekday_dn, DATENAME(day, CreationTime) AS Day_dn, DATENAME(year, CreationTime) AS Year_dn, DATEPART(year, CreationTime) AS Year_dp, DATEPART(month, CreationTime) AS Month_dp, DATEPART(day, CreationTime) AS Day_dp, DATEPART(hour, CreationTime) AS Hour_dp, DATEPART(quarter, CreationTime) AS Quarter_dp, DATEPART(week, CreationTime) AS Week_dp, YEAR(CreationTime) AS Year, MONTH(CreationTime) AS Month, DAY(CreationTime) AS Day FROM Sales.Orders; 3. DATETRUNC() Data Aggregation -- Aggregate orders by year SELECT DATETRUNC(year, CreationTime) AS Creation, COUNT(*) AS OrderCount FROM Sales.Orders GROUP BY DATETRUNC(year, CreationTime); 4. EOMONTH() -- Display end-of-month date for each order SELECT OrderID, CreationTime, EOMONTH(CreationTime) AS EndOfMonth FROM Sales.Orders; 5. Date Parts | Use Cases -- Orders per year SELECT YEAR(OrderDate) AS OrderYear, COUNT(*) AS TotalOrders FROM Sales.Orders GROUP BY YEAR(OrderDate); -- Orders per month SELECT MONTH(OrderDate) AS OrderMonth, COUNT(*) AS TotalOrders FROM Sales.Orders GROUP BY MONTH(OrderDate); -- Orders per month (friendly names) SELECT DATENAME(month, OrderDate) AS OrderMonth, COUNT(*) AS TotalOrders FROM Sales.Orders GROUP BY DATENAME(month, OrderDate); -- Orders placed in February SELECT * FROM Sales.Orders WHERE MONTH(OrderDate) = 2; 6. FORMAT() -- Format CreationTime into different formats SELECT OrderID, CreationTime, FORMAT(CreationTime, 'MM-dd-yyyy') AS USA_Format, FORMAT(CreationTime, 'dd-MM-yyyy') AS EURO_Format, FORMAT(CreationTime, 'dd') AS dd, FORMAT(CreationTime, 'ddd') AS ddd, FORMAT(CreationTime, 'dddd') AS dddd, FORMAT(CreationTime, 'MM') AS MM, FORMAT(CreationTime, 'MMM') AS MMM, FORMAT(CreationTime, 'MMMM') AS MMMM FROM Sales.Orders; -- Custom format example SELECT OrderID, CreationTime, 'Day ' + FORMAT(CreationTime, 'ddd MMM') + ' Q' + DATENAME(quarter, CreationTime) + ' ' + FORMAT(CreationTime, 'yyyy hh:mm:ss tt') AS CustomFormat FROM Sales.Orders; -- Orders per month (formatted \"MMM yy\") SELECT FORMAT(CreationTime, 'MMM yy') AS OrderDate, COUNT(*) AS TotalOrders FROM Sales.Orders GROUP BY FORMAT(CreationTime, 'MMM yy'); 7. CONVERT() -- Conversion using CONVERT SELECT CONVERT(INT, '123') AS [String to Int CONVERT], CONVERT(DATE, '2025-08-20') AS [String to Date CONVERT], CreationTime, CONVERT(DATE, CreationTime) AS [Datetime to Date CONVERT], CONVERT(VARCHAR, CreationTime, 32) AS [USA Std. Style:32], CONVERT(VARCHAR, CreationTime, 34) AS [EURO Std. Style:34] FROM Sales.Orders; 8. CAST() -- Conversion using CAST SELECT CAST('123' AS INT) AS [String to Int], CAST(123 AS VARCHAR) AS [Int to String], CAST('2025-08-20' AS DATE) AS [String to Date], CAST('2025-08-20' AS DATETIME2) AS [String to Datetime], CreationTime, CAST(CreationTime AS DATE) AS [Datetime to Date] FROM Sales.Orders; 9. DATEADD / DATEDIFF -- Date arithmetic SELECT OrderID, OrderDate, DATEADD(day, -10, OrderDate) AS TenDaysBefore, DATEADD(month, 3, OrderDate) AS ThreeMonthsLater, DATEADD(year, 2, OrderDate) AS TwoYearsLater FROM Sales.Orders; -- Employee ages SELECT EmployeeID, BirthDate, DATEDIFF(year, BirthDate, GETDATE()) AS Age FROM Sales.Employees; -- Avg shipping duration SELECT MONTH(OrderDate) AS OrderMonth, AVG(DATEDIFF(day, OrderDate, ShipDate)) AS AvgShip FROM Sales.Orders GROUP BY MONTH(OrderDate); -- Time gap analysis SELECT OrderID, OrderDate AS CurrentOrderDate, LAG(OrderDate) OVER (ORDER BY OrderDate) AS PreviousOrderDate, DATEDIFF(day, LAG(OrderDate) OVER (ORDER BY OrderDate), OrderDate) AS NrOfDays FROM Sales.Orders; 10. ISDATE() -- Validate OrderDate using ISDATE SELECT OrderDate, ISDATE(OrderDate) AS IsValidDate, CASE WHEN ISDATE(OrderDate) = 1 THEN CAST(OrderDate AS DATE) ELSE '9999-01-01' END AS NewOrderDate FROM ( SELECT '2025-08-20' AS OrderDate UNION SELECT '2025-08-21' UNION SELECT '2025-08-23' UNION SELECT '2025-08' ) AS t; -- WHERE ISDATE(OrderDate) = 0",
    "tags": [],
    "title": "Date \u0026 Time Functions",
    "uri": "/sql/date-time1/"
  },
  {
    "breadcrumb": "ADB",
    "content": "Windows Function in PySpark 1. Introduction to Window Functions Window functions allow you to perform calculations across a set of rows related to the current row within a specified partition.\nUnlike groupBy functions, window functions do not reduce the number of rows in the result; instead, they calculate a value for each row based on the specified window.\n2. Importing Required Libraries To use window functions, import the necessary modules from PySpark:\nfrom pyspark.sql import SparkSession from pyspark.sql import functions as F from pyspark.sql.window import Window 3. Creating a Window Specification A window specification defines how rows will be grouped (partitioned) and ordered within each group.\nExample ‚Äì Basic Window Specification:\nwindow_spec = Window.partitionBy(\"category\").orderBy(\"timestamp\")\nExample ‚Äì Advanced Window Specification:\nwindow_spec = Window.partitionBy(\"category\", \"sub_category\").orderBy(F.col(\"timestamp\"), F.col(\"score\"))\n4. Common Window Functions a. Row Number Function: row_number()\nDescription: Assigns a unique integer to each row within the partition (starting from 1).\ndf = df.withColumn(\"row_number\", F.row_number().over(window_spec))\nb. Rank\nFunction: rank()\nDescription: Assigns the same rank to rows with the same values in the order criteria. The next rank has a gap.\ndf = df.withColumn(\"rank\", F.rank().over(window_spec))\nc. Dense Rank\nFunction: dense_rank()\nDescription: Similar to rank, but does not leave gaps in ranking.\ndf = df.withColumn(\"dense_rank\", F.dense_rank().over(window_spec))\nd. Lead and Lag\nFunctions: lead(), lag()\nDescription:\nlead() ‚Üí value of the next row within the window.\nlag() ‚Üí value of the previous row within the window.\ndf = df.withColumn(\"next_value\", F.lead(\"value\").over(window_spec)) df = df.withColumn(\"previous_value\", F.lag(\"value\").over(window_spec))\ne. Aggregation Functions\nWindow functions can also compute aggregated values across the specified window.\ndf = df.withColumn(\"avg_value\", F.avg(\"value\").over(window_spec))\nOther common aggregations:\nSum: F.sum(\"column_name\").over(window_spec)\nMin: F.min(\"column_name\").over(window_spec)\nMax: F.max(\"column_name\").over(window_spec)\n5. Putting It All Together ‚Äì Example from pyspark.sql import SparkSession from pyspark.sql import functions as F from pyspark.sql.window import Window # Initialize Spark session spark = SparkSession.builder.appName(\"WindowFunctionsExample\").getOrCreate() # Sample DataFrame data = [ (\"A\", \"X\", 1, \"2023-01-01\"), (\"A\", \"X\", 2, \"2023-01-02\"), (\"A\", \"Y\", 3, \"2023-01-01\"), (\"A\", \"Y\", 3, \"2023-01-02\"), (\"B\", \"X\", 5, \"2023-01-01\"), (\"B\", \"X\", 4, \"2023-01-02\"), ] columns = [\"category\", \"sub_category\", \"value\", \"timestamp\"] df = spark.createDataFrame(data, columns) # Define window specification window_spec = Window.partitionBy(\"category\", \"sub_category\") \\ .orderBy(F.col(\"timestamp\"), F.col(\"value\")) # Apply window functions df = df.withColumn(\"row_number\", F.row_number().over(window_spec)) df = df.withColumn(\"rank\", F.rank().over(window_spec)) df = df.withColumn(\"dense_rank\", F.dense_rank().over(window_spec)) df = df.withColumn(\"next_value\", F.lead(\"value\").over(window_spec)) df = df.withColumn(\"previous_value\", F.lag(\"value\").over(window_spec)) df = df.withColumn(\"avg_value\", F.avg(\"value\").over(window_spec)) df.show() Conclusion Window functions in PySpark are powerful tools for analyzing data within groups while retaining row-level details.\nBy defining window specifications and applying functions like rank, dense_rank, lead, lag, and aggregations, you can perform complex analytics efficiently.\nüîπ Windows Function in PySpark ‚Äì Part 2\nfrom pyspark.sql import SparkSession from pyspark.sql.window import Window import pyspark.sql.functions as F # Sample data data = [ (\"Alice\", 100), (\"Bob\", 200), (\"Charlie\", 200), (\"David\", 300), (\"Eve\", 400), (\"Frank\", 500), (\"Grace\", 500), (\"Hank\", 600), (\"Ivy\", 700), (\"Jack\", 800) ] columns = [\"Name\", \"Score\"] df = spark.createDataFrame(data, columns) # Define window window_spec = Window.orderBy(\"Score\") # Ranking functions df1 = df.withColumn(\"Rank\", F.rank().over(window_spec)) df2 = df.withColumn(\"DenseRank\", F.dense_rank().over(window_spec)) df3 = df.withColumn(\"RowNumber\", F.row_number().over(window_spec)) # Lead \u0026 Lag df4 = df.withColumn(\"ScoreDifferenceWithNext\", F.lead(\"Score\").over(window_spec) - df[\"Score\"]) df5 = df.withColumn(\"ScoreDifferenceWithPrevious\", df[\"Score\"] - F.lag(\"Score\").over(window_spec)) üîπ Windows Function in PySpark ‚Äì Part 3 (Student Marks Analysis)\n# Updated sample data data = [ (\"Alice\", \"Math\", 90, 1), (\"Alice\", \"Science\", 85, 1), (\"Alice\", \"History\", 78, 1), (\"Bob\", \"Math\", 80, 1), (\"Bob\", \"Science\", 81, 1), (\"Bob\", \"History\", 77, 1), (\"Charlie\", \"Math\", 75, 1), (\"Charlie\", \"Science\", 82, 1), (\"Charlie\", \"History\", 79, 1), (\"Alice\", \"Physics\", 86, 2), (\"Alice\", \"Chemistry\", 92, 2), (\"Alice\", \"Biology\", 80, 2), (\"Bob\", \"Physics\", 94, 2), (\"Bob\", \"Chemistry\", 91, 2), (\"Bob\", \"Biology\", 96, 2), (\"Charlie\", \"Physics\", 89, 2), (\"Charlie\", \"Chemistry\", 88, 2), (\"Charlie\", \"Biology\", 85, 2), (\"Alice\", \"Computer Science\", 95, 3), (\"Alice\", \"Electronics\", 91, 3), (\"Alice\", \"Geography\", 97, 3), (\"Bob\", \"Computer Science\", 88, 3), (\"Bob\", \"Electronics\", 66, 3), (\"Bob\", \"Geography\", 92, 3), (\"Charlie\", \"Computer Science\", 92, 3), (\"Charlie\", \"Electronics\", 97, 3), (\"Charlie\", \"Geography\", 99, 3) ] columns = [\"First Name\", \"Subject\", \"Marks\", \"Semester\"] df = spark.createDataFrame(data, columns) # 1. Max marks in each semester window_spec_max_marks = Window.partitionBy(\"Semester\").orderBy(F.desc(\"Marks\")) max_marks_df = df.withColumn(\"Rank\", F.rank().over(window_spec_max_marks)) top_scorer = max_marks_df.filter(max_marks_df[\"Rank\"] == 1) # 2. Percentage of each student window_spec_total_marks = Window.partitionBy(\"First Name\", \"Semester\") df = df.withColumn(\"TotalMarks\", F.sum(\"Marks\").over(window_spec_total_marks)) df = df.withColumn(\"Percentage\", (F.col(\"TotalMarks\") / (3*100)).cast(\"decimal(5,2)\")*100) df2 = df.groupBy(\"First Name\",\"Semester\").agg(F.max(\"TotalMarks\").alias(\"TotalMarks\"), F.max(\"Percentage\").alias(\"Percentage\")) # 3. Top rank holder in each semester window_spec_rank = Window.partitionBy(\"Semester\").orderBy(F.desc(\"Percentage\")) rank_df = df.withColumn(\"Rank\", F.rank().over(window_spec_rank)) top_rank_holder = rank_df.filter(rank_df[\"Rank\"] == 1).select(\"First Name\",\"Semester\",\"Rank\",\"Percentage\").distinct() # 4. Max marks in each subject in each semester window_spec_max_subject_marks = Window.partitionBy(\"Semester\",\"Subject\").orderBy(F.desc(\"Marks\")) max_subject_marks_df = df.withColumn(\"Rank\", F.rank().over(window_spec_max_subject_marks)) max_subject_scorer = max_subject_marks_df.filter(max_subject_marks_df[\"Rank\"] == 1) Windows Function in PySpark ‚Äì Part 4 (Highest Salary per Department) from pyspark.sql import functions as F from pyspark.sql.window import Window # Employee Data emp_data = [ (1, \"Alice\", 1, 6300), (2, \"Bob\", 1, 6200), (3, \"Charlie\", 2, 7000), (4, \"David\", 2, 7200), (5, \"Eve\", 1, 6300), (6, \"Frank\", 2, 7100) ] # Department Data dept_data = [ (1, \"HR\"), (2, \"Finance\") ] # Create DataFrames emp_df = spark.createDataFrame(emp_data, [\"EmpId\",\"EmpName\",\"DeptId\",\"Salary\"]) dept_df = spark.createDataFrame(dept_data, [\"DeptId\",\"DeptName\"]) # Window for salary ranking window_spec = Window.partitionBy(\"DeptId\").orderBy(F.desc(\"Salary\")) # Add rank \u0026 filter top salary ranked_salary_df = emp_df.withColumn(\"Rank\", F.rank().over(window_spec)) result_df = ranked_salary_df.filter(F.col(\"Rank\") == 1) # Join department names result_df = result_df.join(dept_df, [\"DeptId\"], \"left\") # Final Output result_df.select(\"EmpName\",\"DeptName\",\"Salary\").show()",
    "description": "Windows Function in PySpark 1. Introduction to Window Functions Window functions allow you to perform calculations across a set of rows related to the current row within a specified partition.\nUnlike groupBy functions, window functions do not reduce the number of rows in the result; instead, they calculate a value for each row based on the specified window.\n2. Importing Required Libraries To use window functions, import the necessary modules from PySpark:",
    "tags": [],
    "title": "Window Functions",
    "uri": "/azure_data_bricks/window-functions/"
  },
  {
    "breadcrumb": "SQL",
    "content": "This guide demonstrates all possible date parts, number formats, and culture-specific styles available in SQL Server.\n1. Numeric Format Specifiers SELECT 'N' AS FormatType, FORMAT(1234.56, 'N') AS FormattedValue UNION ALL SELECT 'P', FORMAT(1234.56, 'P') UNION ALL SELECT 'C', FORMAT(1234.56, 'C') UNION ALL SELECT 'E', FORMAT(1234.56, 'E') UNION ALL SELECT 'F', FORMAT(1234.56, 'F') UNION ALL SELECT 'N0', FORMAT(1234.56, 'N0') UNION ALL SELECT 'N1', FORMAT(1234.56, 'N1') UNION ALL SELECT 'N2', FORMAT(1234.56, 'N2') UNION ALL SELECT 'N_de-DE', FORMAT(1234.56, 'N', 'de-DE') UNION ALL SELECT 'N_en-US', FORMAT(1234.56, 'N', 'en-US'); 2. Date Format Specifiers SELECT 'D' AS FormatType, FORMAT(GETDATE(), 'D') AS FormattedValue, 'Full date pattern' AS Description UNION ALL SELECT 'd', FORMAT(GETDATE(), 'd'), 'Short date pattern' UNION ALL SELECT 'dd', FORMAT(GETDATE(), 'dd'), 'Day of month with leading zero' UNION ALL SELECT 'ddd', FORMAT(GETDATE(), 'ddd'), 'Abbreviated name of day' UNION ALL SELECT 'dddd', FORMAT(GETDATE(), 'dddd'), 'Full name of day' UNION ALL SELECT 'M', FORMAT(GETDATE(), 'M'), 'Month without leading zero' UNION ALL SELECT 'MM', FORMAT(GETDATE(), 'MM'), 'Month with leading zero' UNION ALL SELECT 'MMM', FORMAT(GETDATE(), 'MMM'), 'Abbreviated name of month' UNION ALL SELECT 'MMMM', FORMAT(GETDATE(), 'MMMM'), 'Full name of month' UNION ALL SELECT 'yy', FORMAT(GETDATE(), 'yy'), 'Two-digit year' UNION ALL SELECT 'yyyy', FORMAT(GETDATE(), 'yyyy'), 'Four-digit year' UNION ALL SELECT 'hh', FORMAT(GETDATE(), 'hh'), '12-hour clock with leading zero' UNION ALL SELECT 'HH', FORMAT(GETDATE(), 'HH'), '24-hour clock with leading zero' UNION ALL SELECT 'm', FORMAT(GETDATE(), 'm'), 'Minutes without leading zero' UNION ALL SELECT 'mm', FORMAT(GETDATE(), 'mm'), 'Minutes with leading zero' UNION ALL SELECT 's', FORMAT(GETDATE(), 's'), 'Seconds without leading zero' UNION ALL SELECT 'ss', FORMAT(GETDATE(), 'ss'), 'Seconds with leading zero' UNION ALL SELECT 'f', FORMAT(GETDATE(), 'f'), 'Tenths of a second' UNION ALL SELECT 'ff', FORMAT(GETDATE(), 'ff'), 'Hundredths of a second' UNION ALL SELECT 'fff', FORMAT(GETDATE(), 'fff'), 'Milliseconds' UNION ALL SELECT 'T', FORMAT(GETDATE(), 'T'), 'Full AM/PM designator' UNION ALL SELECT 't', FORMAT(GETDATE(), 't'), 'Single char AM/PM designator' UNION ALL SELECT 'tt', FORMAT(GETDATE(), 'tt'), 'Two char AM/PM designator'; 3. DatePart / DateName / DateTrunc Comparisons SELECT 'Year' AS DatePart, DATEPART(year, GETDATE()), DATENAME(year, GETDATE()), DATETRUNC(year, GETDATE()) UNION ALL SELECT 'Quarter', DATEPART(quarter, GETDATE()), DATENAME(quarter, GETDATE()), DATETRUNC(quarter, GETDATE()) UNION ALL SELECT 'Month', DATEPART(month, GETDATE()), DATENAME(month, GETDATE()), DATETRUNC(month, GETDATE()) UNION ALL SELECT 'DayOfYear', DATEPART(dayofyear, GETDATE()), DATENAME(dayofyear, GETDATE()), DATETRUNC(dayofyear, GETDATE()) UNION ALL SELECT 'Day', DATEPART(day, GETDATE()), DATENAME(day, GETDATE()), DATETRUNC(day, GETDATE()) UNION ALL SELECT 'Week', DATEPART(week, GETDATE()), DATENAME(week, GETDATE()), DATETRUNC(week, GETDATE()) UNION ALL SELECT 'Weekday', DATEPART(weekday, GETDATE()), DATENAME(weekday, GETDATE()), NULL UNION ALL SELECT 'Hour', DATEPART(hour, GETDATE()), DATENAME(hour, GETDATE()), DATETRUNC(hour, GETDATE()) UNION ALL SELECT 'Minute', DATEPART(minute, GETDATE()), DATENAME(minute, GETDATE()), DATETRUNC(minute, GETDATE()) UNION ALL SELECT 'Second', DATEPART(second, GETDATE()), DATENAME(second, GETDATE()), DATETRUNC(second, GETDATE()) UNION ALL SELECT 'Millisecond', DATEPART(millisecond, GETDATE()), DATENAME(millisecond, GETDATE()), DATETRUNC(millisecond, GETDATE()) UNION ALL SELECT 'Microsecond', DATEPART(microsecond, GETDATE()), DATENAME(microsecond, GETDATE()), NULL UNION ALL SELECT 'Nanosecond', DATEPART(nanosecond, GETDATE()), DATENAME(nanosecond, GETDATE()), NULL UNION ALL SELECT 'ISOWeek', DATEPART(iso_week, GETDATE()), DATENAME(iso_week, GETDATE()), DATETRUNC(iso_week, GETDATE()); 4. Culture Code Formatting SELECT 'en-US' AS CultureCode, FORMAT(1234567.89, 'N', 'en-US'), FORMAT(GETDATE(), 'D', 'en-US') UNION ALL SELECT 'en-GB', FORMAT(1234567.89, 'N', 'en-GB'), FORMAT(GETDATE(), 'D', 'en-GB') UNION ALL SELECT 'fr-FR', FORMAT(1234567.89, 'N', 'fr-FR'), FORMAT(GETDATE(), 'D', 'fr-FR') UNION ALL SELECT 'de-DE', FORMAT(1234567.89, 'N', 'de-DE'), FORMAT(GETDATE(), 'D', 'de-DE') UNION ALL SELECT 'es-ES', FORMAT(1234567.89, 'N', 'es-ES'), FORMAT(GETDATE(), 'D', 'es-ES') UNION ALL SELECT 'zh-CN', FORMAT(1234567.89, 'N', 'zh-CN'), FORMAT(GETDATE(), 'D', 'zh-CN') UNION ALL SELECT 'ja-JP', FORMAT(1234567.89, 'N', 'ja-JP'), FORMAT(GETDATE(), 'D', 'ja-JP') UNION ALL SELECT 'ko-KR', FORMAT(1234567.89, 'N', 'ko-KR'), FORMAT(GETDATE(), 'D', 'ko-KR') UNION ALL SELECT 'pt-BR', FORMAT(1234567.89, 'N', 'pt-BR'), FORMAT(GETDATE(), 'D', 'pt-BR') UNION ALL SELECT 'it-IT', FORMAT(1234567.89, 'N', 'it-IT'), FORMAT(GETDATE(), 'D', 'it-IT') UNION ALL SELECT 'nl-NL', FORMAT(1234567.89, 'N', 'nl-NL'), FORMAT(GETDATE(), 'D', 'nl-NL') UNION ALL SELECT 'ru-RU', FORMAT(1234567.89, 'N', 'ru-RU'), FORMAT(GETDATE(), 'D', 'ru-RU') UNION ALL SELECT 'ar-SA', FORMAT(1234567.89, 'N', 'ar-SA'), FORMAT(GETDATE(), 'D', 'ar-SA') UNION ALL SELECT 'el-GR', FORMAT(1234567.89, 'N', 'el-GR'), FORMAT(GETDATE(), 'D', 'el-GR') UNION ALL SELECT 'tr-TR', FORMAT(1234567.89, 'N', 'tr-TR'), FORMAT(GETDATE(), 'D', 'tr-TR') UNION ALL SELECT 'he-IL', FORMAT(1234567.89, 'N', 'he-IL'), FORMAT(GETDATE(), 'D', 'he-IL') UNION ALL SELECT 'hi-IN', FORMAT(1234567.89, 'N', 'hi-IN'), FORMAT(GETDATE(), 'D', 'hi-IN');",
    "description": "This guide demonstrates all possible date parts, number formats, and culture-specific styles available in SQL Server.\n1. Numeric Format Specifiers SELECT 'N' AS FormatType, FORMAT(1234.56, 'N') AS FormattedValue UNION ALL SELECT 'P', FORMAT(1234.56, 'P') UNION ALL SELECT 'C', FORMAT(1234.56, 'C') UNION ALL SELECT 'E', FORMAT(1234.56, 'E') UNION ALL SELECT 'F', FORMAT(1234.56, 'F') UNION ALL SELECT 'N0', FORMAT(1234.56, 'N0') UNION ALL SELECT 'N1', FORMAT(1234.56, 'N1') UNION ALL SELECT 'N2', FORMAT(1234.56, 'N2') UNION ALL SELECT 'N_de-DE', FORMAT(1234.56, 'N', 'de-DE') UNION ALL SELECT 'N_en-US', FORMAT(1234.56, 'N', 'en-US'); 2. Date Format Specifiers SELECT 'D' AS FormatType, FORMAT(GETDATE(), 'D') AS FormattedValue, 'Full date pattern' AS Description UNION ALL SELECT 'd', FORMAT(GETDATE(), 'd'), 'Short date pattern' UNION ALL SELECT 'dd', FORMAT(GETDATE(), 'dd'), 'Day of month with leading zero' UNION ALL SELECT 'ddd', FORMAT(GETDATE(), 'ddd'), 'Abbreviated name of day' UNION ALL SELECT 'dddd', FORMAT(GETDATE(), 'dddd'), 'Full name of day' UNION ALL SELECT 'M', FORMAT(GETDATE(), 'M'), 'Month without leading zero' UNION ALL SELECT 'MM', FORMAT(GETDATE(), 'MM'), 'Month with leading zero' UNION ALL SELECT 'MMM', FORMAT(GETDATE(), 'MMM'), 'Abbreviated name of month' UNION ALL SELECT 'MMMM', FORMAT(GETDATE(), 'MMMM'), 'Full name of month' UNION ALL SELECT 'yy', FORMAT(GETDATE(), 'yy'), 'Two-digit year' UNION ALL SELECT 'yyyy', FORMAT(GETDATE(), 'yyyy'), 'Four-digit year' UNION ALL SELECT 'hh', FORMAT(GETDATE(), 'hh'), '12-hour clock with leading zero' UNION ALL SELECT 'HH', FORMAT(GETDATE(), 'HH'), '24-hour clock with leading zero' UNION ALL SELECT 'm', FORMAT(GETDATE(), 'm'), 'Minutes without leading zero' UNION ALL SELECT 'mm', FORMAT(GETDATE(), 'mm'), 'Minutes with leading zero' UNION ALL SELECT 's', FORMAT(GETDATE(), 's'), 'Seconds without leading zero' UNION ALL SELECT 'ss', FORMAT(GETDATE(), 'ss'), 'Seconds with leading zero' UNION ALL SELECT 'f', FORMAT(GETDATE(), 'f'), 'Tenths of a second' UNION ALL SELECT 'ff', FORMAT(GETDATE(), 'ff'), 'Hundredths of a second' UNION ALL SELECT 'fff', FORMAT(GETDATE(), 'fff'), 'Milliseconds' UNION ALL SELECT 'T', FORMAT(GETDATE(), 'T'), 'Full AM/PM designator' UNION ALL SELECT 't', FORMAT(GETDATE(), 't'), 'Single char AM/PM designator' UNION ALL SELECT 'tt', FORMAT(GETDATE(), 'tt'), 'Two char AM/PM designator'; 3. DatePart / DateName / DateTrunc Comparisons SELECT 'Year' AS DatePart, DATEPART(year, GETDATE()), DATENAME(year, GETDATE()), DATETRUNC(year, GETDATE()) UNION ALL SELECT 'Quarter', DATEPART(quarter, GETDATE()), DATENAME(quarter, GETDATE()), DATETRUNC(quarter, GETDATE()) UNION ALL SELECT 'Month', DATEPART(month, GETDATE()), DATENAME(month, GETDATE()), DATETRUNC(month, GETDATE()) UNION ALL SELECT 'DayOfYear', DATEPART(dayofyear, GETDATE()), DATENAME(dayofyear, GETDATE()), DATETRUNC(dayofyear, GETDATE()) UNION ALL SELECT 'Day', DATEPART(day, GETDATE()), DATENAME(day, GETDATE()), DATETRUNC(day, GETDATE()) UNION ALL SELECT 'Week', DATEPART(week, GETDATE()), DATENAME(week, GETDATE()), DATETRUNC(week, GETDATE()) UNION ALL SELECT 'Weekday', DATEPART(weekday, GETDATE()), DATENAME(weekday, GETDATE()), NULL UNION ALL SELECT 'Hour', DATEPART(hour, GETDATE()), DATENAME(hour, GETDATE()), DATETRUNC(hour, GETDATE()) UNION ALL SELECT 'Minute', DATEPART(minute, GETDATE()), DATENAME(minute, GETDATE()), DATETRUNC(minute, GETDATE()) UNION ALL SELECT 'Second', DATEPART(second, GETDATE()), DATENAME(second, GETDATE()), DATETRUNC(second, GETDATE()) UNION ALL SELECT 'Millisecond', DATEPART(millisecond, GETDATE()), DATENAME(millisecond, GETDATE()), DATETRUNC(millisecond, GETDATE()) UNION ALL SELECT 'Microsecond', DATEPART(microsecond, GETDATE()), DATENAME(microsecond, GETDATE()), NULL UNION ALL SELECT 'Nanosecond', DATEPART(nanosecond, GETDATE()), DATENAME(nanosecond, GETDATE()), NULL UNION ALL SELECT 'ISOWeek', DATEPART(iso_week, GETDATE()), DATENAME(iso_week, GETDATE()), DATETRUNC(iso_week, GETDATE()); 4. Culture Code Formatting SELECT 'en-US' AS CultureCode, FORMAT(1234567.89, 'N', 'en-US'), FORMAT(GETDATE(), 'D', 'en-US') UNION ALL SELECT 'en-GB', FORMAT(1234567.89, 'N', 'en-GB'), FORMAT(GETDATE(), 'D', 'en-GB') UNION ALL SELECT 'fr-FR', FORMAT(1234567.89, 'N', 'fr-FR'), FORMAT(GETDATE(), 'D', 'fr-FR') UNION ALL SELECT 'de-DE', FORMAT(1234567.89, 'N', 'de-DE'), FORMAT(GETDATE(), 'D', 'de-DE') UNION ALL SELECT 'es-ES', FORMAT(1234567.89, 'N', 'es-ES'), FORMAT(GETDATE(), 'D', 'es-ES') UNION ALL SELECT 'zh-CN', FORMAT(1234567.89, 'N', 'zh-CN'), FORMAT(GETDATE(), 'D', 'zh-CN') UNION ALL SELECT 'ja-JP', FORMAT(1234567.89, 'N', 'ja-JP'), FORMAT(GETDATE(), 'D', 'ja-JP') UNION ALL SELECT 'ko-KR', FORMAT(1234567.89, 'N', 'ko-KR'), FORMAT(GETDATE(), 'D', 'ko-KR') UNION ALL SELECT 'pt-BR', FORMAT(1234567.89, 'N', 'pt-BR'), FORMAT(GETDATE(), 'D', 'pt-BR') UNION ALL SELECT 'it-IT', FORMAT(1234567.89, 'N', 'it-IT'), FORMAT(GETDATE(), 'D', 'it-IT') UNION ALL SELECT 'nl-NL', FORMAT(1234567.89, 'N', 'nl-NL'), FORMAT(GETDATE(), 'D', 'nl-NL') UNION ALL SELECT 'ru-RU', FORMAT(1234567.89, 'N', 'ru-RU'), FORMAT(GETDATE(), 'D', 'ru-RU') UNION ALL SELECT 'ar-SA', FORMAT(1234567.89, 'N', 'ar-SA'), FORMAT(GETDATE(), 'D', 'ar-SA') UNION ALL SELECT 'el-GR', FORMAT(1234567.89, 'N', 'el-GR'), FORMAT(GETDATE(), 'D', 'el-GR') UNION ALL SELECT 'tr-TR', FORMAT(1234567.89, 'N', 'tr-TR'), FORMAT(GETDATE(), 'D', 'tr-TR') UNION ALL SELECT 'he-IL', FORMAT(1234567.89, 'N', 'he-IL'), FORMAT(GETDATE(), 'D', 'he-IL') UNION ALL SELECT 'hi-IN', FORMAT(1234567.89, 'N', 'hi-IN'), FORMAT(GETDATE(), 'D', 'hi-IN');",
    "tags": [],
    "title": "Date \u0026 Time Formats",
    "uri": "/sql/date-time2/"
  },
  {
    "breadcrumb": "ADB",
    "content": "Explode vs Explode_outer in PySpark In PySpark, explode and explode_outer are functions used to work with nested data structures, like arrays or maps, by ‚Äúexploding‚Äù (flattening) each element of an array or key-value pair in a map into separate rows.\nThe key difference between explode and explode_outer is in handling null or empty arrays, which makes them useful in different scenarios.\n1. explode() The explode() function takes a column with array or map data and creates a new row for each element in the array (or each key-value pair in the map).\nIf the array is empty or null, explode() will drop the row entirely.\nKey Characteristics Converts each element in an array or each entry in a map into its own row. Drops rows with null or empty arrays. Example: Using explode() in PySpark from pyspark.sql import SparkSession from pyspark.sql.functions import explode # Initialize Spark session spark = SparkSession.builder.appName(\"ExplodeExample\").getOrCreate() # Sample DataFrame with arrays data = [ (\"Alice\", [\"Math\", \"Science\"]), (\"Bob\", [\"History\"]), (\"Cathy\", []), # Empty array (\"David\", None) # Null array ] df = spark.createDataFrame(data, [\"Name\", \"Subjects\"]) df.show() # Use explode to flatten the array exploded_df = df.select(\"Name\", explode(\"Subjects\").alias(\"Subject\")) # Show the result exploded_df.show() 1. explode() The explode() function expands the Subjects array into individual rows.\nRows with empty ([]) or null (None) arrays are removed, which is why Cathy and David do not appear in the output.\nKey Characteristics Converts each element in an array or each entry in a map into its own row. Drops rows with null or empty arrays. 2. explode_outer() The explode_outer() function works similarly to explode(), but it keeps rows with null or empty arrays.\nWhen explode_outer() encounters a null or empty array, it still generates a row for that entry, with null as the value in the resulting column.\nKey Characteristics Converts each element in an array or each entry in a map into its own row. Retains rows with null or empty arrays, using null values in the exploded column. # Use explode_outer to flatten the array while keeping null or empty rows exploded_outer_df = df.select( \"Name\", F.explode_outer(\"Subjects\").alias(\"Subject\") ) # Show the result exploded_outer_df.show() Explanation: explode_outer() explode_outer() expands the Subjects array into individual rows. Unlike explode(), rows with empty ([]) or null arrays (None) are kept in the result, with null values in the Subject column for these cases. Summary Table of Differences Function Description Null/Empty Arrays Behavior explode() Expands each element of an array or map into rows Drops rows with null or empty arrays explode_outer() Similar to explode(), but retains null/empty Keeps rows with null/empty arrays, fills with null These functions are very useful when working with complex, nested data structures, especially when dealing with JSON or other hierarchical data.",
    "description": "Explode vs Explode_outer in PySpark In PySpark, explode and explode_outer are functions used to work with nested data structures, like arrays or maps, by ‚Äúexploding‚Äù (flattening) each element of an array or key-value pair in a map into separate rows.\nThe key difference between explode and explode_outer is in handling null or empty arrays, which makes them useful in different scenarios.\n1. explode() The explode() function takes a column with array or map data and creates a new row for each element in the array (or each key-value pair in the map).\nIf the array is empty or null, explode() will drop the row entirely.",
    "tags": [],
    "title": "Explode",
    "uri": "/azure_data_bricks/explode/"
  },
  {
    "breadcrumb": "SQL",
    "content": "This guide covers essential SQL functions for handling NULL values in different scenarios such as aggregation, mathematical operations, sorting, joins, and comparisons.\n1. Handle NULL - Data Aggregation Replace NULL values using COALESCE to ensure accurate averages.\nSELECT CustomerID, Score, COALESCE(Score, 0) AS Score2, AVG(Score) OVER () AS AvgScores, AVG(COALESCE(Score, 0)) OVER () AS AvgScores2 FROM Sales.Customers; 2. Handle NULL - Mathematical Operators Concatenate first and last names safely and add bonus points with COALESCE.\nSELECT CustomerID, FirstName, LastName, FirstName + ' ' + COALESCE(LastName, '') AS FullName, Score, COALESCE(Score, 0) + 10 AS ScoreWithBonus FROM Sales.Customers; 3. Handle NULL - Sorting Data Order results so that NULL values appear last.\nSELECT CustomerID, Score FROM Sales.Customers ORDER BY CASE WHEN Score IS NULL THEN 1 ELSE 0 END, Score; 4. NULLIF - Division by Zero Prevent division errors by using NULLIF.\nSELECT OrderID, Sales, Quantity, Sales / NULLIF(Quantity, 0) AS Price FROM Sales.Orders; 5. IS NULL / IS NOT NULL Identify rows with or without NULL values.\n-- Customers with no scores SELECT * FROM Sales.Customers WHERE Score IS NULL; -- Customers with scores SELECT * FROM Sales.Customers WHERE Score IS NOT NULL; 6. LEFT ANTI JOIN Retrieve customers who have not placed any orders.\nSELECT c.*, o.OrderID FROM Sales.Customers AS c LEFT JOIN Sales.Orders AS o ON c.CustomerID = o.CustomerID WHERE o.CustomerID IS NULL; 7. NULLs vs Empty String vs Blank Spaces Differentiate between NULL, empty strings (‚Äô‚Äô), and blank spaces (‚Äô ‚Äò).\nWITH Orders AS ( SELECT 1 AS Id, 'A' AS Category UNION SELECT 2, NULL UNION SELECT 3, '' UNION SELECT 4, ' ' ) SELECT *, DATALENGTH(Category) AS LenCategory, TRIM(Category) AS Policy1, NULLIF(TRIM(Category), '') AS Policy2, COALESCE(NULLIF(TRIM(Category), ''), 'unknown') AS Policy3 FROM Orders;",
    "description": "This guide covers essential SQL functions for handling NULL values in different scenarios such as aggregation, mathematical operations, sorting, joins, and comparisons.\n1. Handle NULL - Data Aggregation Replace NULL values using COALESCE to ensure accurate averages.\nSELECT CustomerID, Score, COALESCE(Score, 0) AS Score2, AVG(Score) OVER () AS AvgScores, AVG(COALESCE(Score, 0)) OVER () AS AvgScores2 FROM Sales.Customers; 2. Handle NULL - Mathematical Operators Concatenate first and last names safely and add bonus points with COALESCE.",
    "tags": [],
    "title": "NULL Functions",
    "uri": "/sql/null-functions/"
  },
  {
    "breadcrumb": "ADB",
    "content": "The pivot operation in PySpark is used to transpose rows into columns based on a specified column‚Äôs unique values.\nIt‚Äôs particularly useful for creating wide-format data, where values in one column become new column headers, and corresponding values from another column fill those headers.\nKey Concepts groupBy and pivot\nThe pivot method is typically used in combination with groupBy. You group by certain columns and pivot one column to create new columns. Aggregation Function\nYou need to specify an aggregation function (like sum, avg, count, etc.) to fill the values in the pivoted columns. Performance Consideration\nPivoting can be computationally expensive, especially with a high number of unique values in the pivot column. For better performance, explicitly specify the values to pivot if possible. Syntax\ndataframe.groupBy(\"group_column\").pivot(\"pivot_column\").agg(aggregation_function) Example Code: Pivot in PySpark Sample Data Imagine we have a DataFrame of sales data with the following schema:\nProduct Region Sales A North 100 B North 150 A South 200 B South 300 We want to pivot the data so that regions (North, South) become columns and the sales values are aggregated.\nCode Implementation Example: Pivot in PySpark from pyspark.sql import SparkSession from pyspark.sql.functions import sum # Create a Spark session spark = SparkSession.builder.appName(\"PivotExample\").getOrCreate() # Create a sample DataFrame data = [ (\"A\", \"North\", 100), (\"B\", \"North\", 150), (\"A\", \"South\", 200), (\"B\", \"South\", 300) ] columns = [\"Product\", \"Region\", \"Sales\"] df = spark.createDataFrame(data, columns) # Pivot the DataFrame pivoted_df = df.groupBy(\"Product\").pivot(\"Region\").agg(sum(\"Sales\")) # Show the results pivoted_df.show() Output Product North South A 100 200 B 150 300 Explanation of Code groupBy(‚ÄúProduct‚Äù)\nGroups the data by the Product column. pivot(‚ÄúRegion‚Äù)\nTransforms unique values in the Region column (North, South) into new columns. agg(sum(‚ÄúSales‚Äù))\nComputes the sum of Sales for each combination of Product and the new columns created by the pivot. Notes Explicit Pivot Values: To improve performance, you can specify the pivot values explicitly. df.groupBy(\"Product\").pivot(\"Region\", [\"North\", \"South\"]).agg(sum(\"Sales\"))\nHandling Null Values: If some combinations of groupBy and pivot values have no corresponding rows, the resulting cells will contain null. Alternative Aggregations: You can use other aggregation functions like avg, max, min, etc. This approach is commonly used in creating summary reports or preparing data for machine learning models where wide-format data is required.\nUnpivot in PySpark The unpivot operation (also called melting) is used to transform a wide-format table into a long-format table. This means columns are turned into rows, effectively reversing the pivot operation. PySpark doesn‚Äôt have a direct unpivot function like Pandas‚Äô melt, but you can achieve it using the selectExpr method or a combination of stack and other DataFrame transformations.\nKey Concepts 1. Purpose of Unpivot: Simplifies data analysis by converting column headers into a single column (e.g., categorical variables).\nIdeal for scenarios where you need to aggregate data further or visualize it in a long format.\n2. Syntax Overview: Use the stack function inside a selectExpr to unpivot.\nStack reshapes the DataFrame by creating multiple rows for specified columns.\n3. Performance: Unpivoting can generate many rows, especially if the original DataFrame is wide with numerous columns. Ensure your environment can handle the resulting data volume. Example: Unpivot in PySpark Sample Data Suppose we have the following DataFrame:\nProduct North South East West A 100 200 150 130 B 150 300 200 180 from pyspark.sql import SparkSession # Create a Spark session spark = SparkSession.builder.appName(\"UnpivotExample\").getOrCreate() # Sample data data = [ (\"A\", 100, 200, 150, 130), (\"B\", 150, 300, 200, 180) ] columns = [\"Product\", \"North\", \"South\", \"East\", \"West\"] # Create the DataFrame df = spark.createDataFrame(data, columns) # Unpivot the DataFrame using stack unpivoted_df = df.selectExpr( \"Product\", \"stack(4, 'North', North, 'South', South, 'East', East, 'West', West) as (Region, Sales)\" ) # Show the results unpivoted_df.show() Explanation of Code Input DataFrame:\nEach column (North, South, East, West) represents a region‚Äôs sales for each product. selectExpr with stack:\nThe stack function takes two arguments: The number of columns being unpivoted (4 in this case). A sequence of column-value pairs: ‚ÄòColumnName1‚Äô, ColumnValue1, ‚ÄòColumnName2‚Äô, ColumnValue2, ‚Ä¶. The result is two new columns: the first contains the column names (now rows, Region), and the second contains the corresponding values (Sales). Aliasing Columns:\nThe stack result is aliased as (Region, Sales) to give meaningful names to the new columns. Alternative Methods Using withColumn and union:\nIf stack isn‚Äôt flexible enough, you can manually combine rows for each column:\nfrom pyspark.sql import functions as F # Create a DataFrame with union operations for unpivoting north = df.select(\"Product\", F.lit(\"North\").alias(\"Region\"), F.col(\"North\").alias(\"Sales\")) south = df.select(\"Product\", F.lit(\"South\").alias(\"Region\"), F.col(\"South\").alias(\"Sales\")) east = df.select(\"Product\", F.lit(\"East\").alias(\"Region\"), F.col(\"East\").alias(\"Sales\")) west = df.select(\"Product\", F.lit(\"West\").alias(\"Region\"), F.col(\"West\").alias(\"Sales\")) # Combine all rows using union unpivoted_df = north.union(south).union(east).union(west) # Show results unpivoted_df.show() Notes Performance Considerations\nstack is efficient for unpivoting a large number of columns. The union method may become unwieldy for many columns, but it offers more control over the transformation process. Dynamic Column Unpivoting\nIf the column names are not fixed (dynamic), you can: Collect the column names dynamically using df.columns. Construct the selectExpr or union queries programmatically. Resulting Format\nAfter unpivoting, the data will have more rows but fewer columns. Ensure downstream processes are optimized to handle the increased row count. Unpivoting is a powerful operation for restructuring data and is frequently used in data preprocessing, reporting, and machine learning pipelines.",
    "description": "The pivot operation in PySpark is used to transpose rows into columns based on a specified column‚Äôs unique values.\nIt‚Äôs particularly useful for creating wide-format data, where values in one column become new column headers, and corresponding values from another column fill those headers.\nKey Concepts groupBy and pivot\nThe pivot method is typically used in combination with groupBy. You group by certain columns and pivot one column to create new columns. Aggregation Function",
    "tags": [],
    "title": "Pivot",
    "uri": "/azure_data_bricks/pivot/"
  },
  {
    "breadcrumb": "SQL",
    "content": "1. Categorize Data Create sales categories based on value ranges.\nSELECT Category, SUM(Sales) AS TotalSales FROM ( SELECT OrderID, Sales, CASE WHEN Sales \u003e 50 THEN 'High' WHEN Sales \u003e 20 THEN 'Medium' ELSE 'Low' END AS Category FROM Sales.Orders ) AS t GROUP BY Category ORDER BY TotalSales DESC; 2. Mapping Map country names to abbreviations.\nSELECT CustomerID, FirstName, LastName, Country, CASE WHEN Country = 'Germany' THEN 'DE' WHEN Country = 'USA' THEN 'US' ELSE 'n/a' END AS CountryAbbr FROM Sales.Customers; 3. Quick Form of CASE Statement Use the shorthand CASE syntax for direct equality comparisons.\nSELECT CustomerID, FirstName, LastName, Country, CASE WHEN Country = 'Germany' THEN 'DE' WHEN Country = 'USA' THEN 'US' ELSE 'n/a' END AS CountryAbbr, CASE Country WHEN 'Germany' THEN 'DE' WHEN 'USA' THEN 'US' ELSE 'n/a' END AS CountryAbbr2 FROM Sales.Customers; 4. Handling Nulls Replace NULL values with defaults and compute averages.\nSELECT CustomerID, LastName, Score, CASE WHEN Score IS NULL THEN 0 ELSE Score END AS ScoreClean, AVG( CASE WHEN Score IS NULL THEN 0 ELSE Score END ) OVER () AS AvgCustomerClean, AVG(Score) OVER () AS AvgCustomer FROM Sales.Customers; 5. Conditional Aggregation Count orders with sales above a threshold.\nSELECT CustomerID, SUM( CASE WHEN Sales \u003e 30 THEN 1 ELSE 0 END ) AS TotalOrdersHighSales, COUNT(*) AS TotalOrders FROM Sales.Orders GROUP BY CustomerID;",
    "description": "1. Categorize Data Create sales categories based on value ranges.\nSELECT Category, SUM(Sales) AS TotalSales FROM ( SELECT OrderID, Sales, CASE WHEN Sales \u003e 50 THEN 'High' WHEN Sales \u003e 20 THEN 'Medium' ELSE 'Low' END AS Category FROM Sales.Orders ) AS t GROUP BY Category ORDER BY TotalSales DESC; 2. Mapping Map country names to abbreviations.\nSELECT CustomerID, FirstName, LastName, Country, CASE WHEN Country = 'Germany' THEN 'DE' WHEN Country = 'USA' THEN 'US' ELSE 'n/a' END AS CountryAbbr FROM Sales.Customers; 3. Quick Form of CASE Statement Use the shorthand CASE syntax for direct equality comparisons.",
    "tags": [],
    "title": "CASE Statement",
    "uri": "/sql/case-statement/"
  },
  {
    "breadcrumb": "ADB",
    "content": "Hadoop vs. Spark Architecture Aspect Hadoop Spark Storage Uses HDFS for storage Uses in-memory processing for speed Processing MapReduce is disk-based In-memory processing improves performance Integration Runs independently or with Hadoop ecosystem Can run on top of Hadoop; more flexible Complexity More complex setup and deployment Simpler to deploy and configure Performance Slower for iterative tasks due to disk I/O Better performance for iterative tasks RDD vs. DataFrame vs. Dataset Aspect RDD DataFrame Dataset API Level Low-level, more control High-level, optimized with Catalyst High-level, type-safe Schema No schema, unstructured Uses schema for structured data Strongly typed, compile-time type safety Optimization No built-in optimization Optimized using Catalyst Optimized using Catalyst, with type safety Type Safety No type safety No compile-time type safety Provides compile-time type safety Performance Less optimized for performance Better performance due to optimizations Combines type safety with optimization Action vs. Transformation in Spark Aspect Action Transformation Execution Triggers execution of the Spark job Builds up a logical plan of data operations Return Type Returns results or output Returns a new RDD/DataFrame Evaluation Eager evaluation; executes immediately Lazy evaluation; executed when an action is triggered Computation Involves actual computation (e.g., collect()) Defines data transformations (e.g., map()) Performance Can cause data processing; affects performance Does not affect performance until an action is called Map vs. FlatMap Aspect Map FlatMap Output Returns one output element per input element Can return zero or more output elements per input Flattening Does not flatten output Flattens the output into a single level Use Case Suitable for one-to-one transformations Suitable for one-to-many transformations Complexity Simpler, straightforward More complex due to variable number of outputs Examples map(x =\u003e x * 2) flatMap(x =\u003e x.split(\" \")) GroupByKey vs. ReduceByKey Aspect GroupByKey ReduceByKey Operation Groups all values by key Aggregates values with the same key Efficiency Can lead to high shuffling More efficient due to partial aggregation Data Movement Requires shuffling of all values Minimizes data movement through local aggregation Use Case Useful for simple grouping Preferred for aggregations and reductions Performance Less efficient with large datasets Better performance for large datasets Repartition vs. Coalesce Aspect Repartition Coalesce Partitioning Can increase or decrease the number of partitions Only decreases the number of partitions Shuffling Involves full shuffle Avoids full shuffle, more efficient Efficiency More expensive due to shuffling More efficient for reducing partitions Use Case Used for increasing partitions or balancing load Used for reducing partitions, typically after filtering Performance Can be costly for large datasets More cost-effective for reducing partitions Cache vs. Persist Aspect Cache Persist Storage Level Defaults to MEMORY_ONLY Can use various storage levels (e.g., MEMORY_AND_DISK) Flexibility Simplified, with default storage level Offers more options for storage levels Use Case Suitable for simple caching scenarios Suitable for complex caching scenarios requiring different storage levels Implementation Easier to use, shorthand for MEMORY_ONLY More flexible, allows custom storage options Performance Suitable when memory suffices More efficient when dealing with larger datasets and limited memory Narrow vs. Wide Transformation Aspect Narrow Transformation Wide Transformation Partitioning Each parent partition is used by one child partition Requires data from multiple partitions Shuffling No shuffling required Involves shuffling of data Performance More efficient and less costly Less efficient due to data movement Examples map(), filter() groupByKey(), join() Complexity Simpler and faster More complex and slower due to data movement Collect vs. Take Aspect Collect Take Output Retrieves all data from the RDD/DataFrame Retrieves a specified number of elements Memory Usage Can be expensive and use a lot of memory More memory-efficient Use Case Used when you need the entire dataset Useful for sampling or debugging Performance Can cause performance issues with large data Faster and more controlled Action Type Triggers full data retrieval Triggers partial data retrieval Broadcast Variable vs. Accumulator Aspect Broadcast Variable Accumulator Purpose Efficiently shares read-only data across tasks Tracks metrics and aggregates values Data Type Data that is shared and read-only Counters and sums, often numerical Use Case Useful for large lookup tables or configurations Useful for aggregating metrics like counts Efficiency Reduces data transfer by broadcasting data once Efficient for aggregating values across tasks Mutability Immutable, read-only Mutable, can be updated during computation Spark SQL vs. DataFrame API Aspect Spark SQL DataFrame API Interface Executes SQL queries Provides a programmatic interface Syntax Uses SQL-like syntax Uses function-based syntax Optimization Optimized with Catalyst Optimized with Catalyst Spark Streaming vs. Structured Streaming Aspect Spark Streaming Structured Streaming Processing Micro-batch processing Micro-batch and continuous processing API RDD-based API SQL-based API with DataFrame/Dataset support Complexity More complex and lower-level Simplified with high-level APIs Consistency Can be less consistent due to micro-batches Provides stronger consistency guarantees Performance Can be slower for complex queries Better performance with optimizations Shuffle vs. MapReduce Aspect Shuffle MapReduce Operation Data reorganization across partitions Data processing model for distributed computing Efficiency Can be costly due to data movement Designed for batch processing with high I/O Performance Affects performance based on the amount of data movement Optimized for large-scale data processing but less efficient for iterative tasks Use Case Used in Spark for data redistribution Used in Hadoop for data processing tasks Implementation Integrated into Spark operations Core component of the Hadoop ecosystem Union vs. Join Aspect Union Join Operation Combines two DataFrames/RDDs into one Combines rows from two DataFrames/RDDs based on a key Data Requirements Requires same schema for both DataFrames/RDDs Requires a common key for joining Performance Generally faster as it does not require key matching Can be slower due to key matching and shuffling Output Stacks data vertically Merges data horizontally based on keys Use Case Appending data or combining datasets Merging related data based on keys Executor vs. Driver Aspect Executor Driver Role Executes tasks and processes data Coordinates and manages the Spark application Memory Memory allocated per executor for data processing Memory used for managing application execution Lifecycle Exists throughout the application execution Starts and stops the Spark application Tasks Runs the tasks assigned by the driver Schedules and coordinates tasks and jobs Parallelism Multiple executors run in parallel Single driver coordinates multiple executors Checkpointing vs. Caching Aspect Checkpointing Caching Purpose Provides fault tolerance and reliability Improves performance by storing intermediate data Storage Writes data to stable storage (e.g., HDFS) Stores data in memory or on disk (depends on storage level) Use Case Used for recovery in case of failures Used for optimizing repeated operations Impact Can be more costly and slow Generally faster but not suitable for fault tolerance Data Data is written to external storage Data is kept in memory or disk storage for quick access ReduceByKey vs. AggregateByKey Aspect ReduceByKey AggregateByKey Operation Combines values with the same key using a function Performs custom aggregation and combinatory operations Efficiency More efficient for simple aggregations Flexible for complex aggregation scenarios Shuffling Involves shuffling but can be optimized Can be more complex due to custom aggregation Use Case Suitable for straightforward aggregations Ideal for advanced and custom aggregations Performance Generally faster for simple operations Performance varies with complexity SQLContext vs. HiveContext vs. SparkSession Aspect SQLContext HiveContext SparkSession Purpose Provides SQL query capabilities Provides integration with Hive for SQL queries Unified entry point for Spark functionality Integration Basic SQL capabilities Integrates with Hive Metastore Combines SQL, DataFrame, and Streaming APIs Usage Legacy, less functionality Supports HiveQL and Hive UDFs Supports all Spark functionalities including Hive Configuration Less flexible and older Requires Hive setup and configuration Modern and flexible, manages configurations Capabilities Limited to SQL queries Extends SQL capabilities with Hive integration Comprehensive access to all Spark features Broadcast Join vs. Shuffle Join Aspect Broadcast Join Shuffle Join Operation Broadcasts a small dataset to all nodes Shuffles data across nodes for joining Data Size Suitable for small datasets Suitable for larger datasets Efficiency More efficient for small tables More suited for large datasets Performance Faster due to reduced shuffling Can be slower due to extensive shuffling Use Case Use when one dataset is small relative to others Use when both datasets are large SparkContext vs. SparkSession Aspect SparkContext SparkSession Purpose Entry point for Spark functionality Unified entry point for Spark functionalities Lifecycle Created before Spark jobs start Manages the Spark application lifecycle Functionality Provides access to RDD and basic Spark functionality Provides access to RDD, DataFrame, SQL, and Streaming APIs Configuration Configuration is less flexible More flexible and easier to configure Usage Older, used for legacy applications Modern and recommended for new applications Structured Streaming vs. Spark Streaming Aspect Structured Streaming Spark Streaming Processing Micro-batch and continuous processing Micro-batch processing API SQL-based API with DataFrame/Dataset support RDD-based API Complexity Simplified and high-level More complex and low-level Consistency Provides stronger consistency guarantees Can be less consistent due to micro-batches Performance Better performance with built-in optimizations Can be slower for complex queries Partitioning vs. Bucketing Aspect Partitioning Bucketing Purpose Divides data into multiple partitions based on a key Divides data into buckets based on a hash function Usage Used to optimize queries by reducing data scanned Used to improve join performance and maintain sorted data Shuffling Reduces shuffling by placing related data together Reduces shuffle during joins and aggregations Data Layout Data is physically separated based on partition key Data is organized into fixed-size buckets Performance Improves performance for queries involving partition keys Enhances performance for join operations",
    "description": "Hadoop vs. Spark Architecture Aspect Hadoop Spark Storage Uses HDFS for storage Uses in-memory processing for speed Processing MapReduce is disk-based In-memory processing improves performance Integration Runs independently or with Hadoop ecosystem Can run on top of Hadoop; more flexible Complexity More complex setup and deployment Simpler to deploy and configure Performance Slower for iterative tasks due to disk I/O Better performance for iterative tasks RDD vs. DataFrame vs. Dataset Aspect RDD DataFrame Dataset API Level Low-level, more control High-level, optimized with Catalyst High-level, type-safe Schema No schema, unstructured Uses schema for structured data Strongly typed, compile-time type safety Optimization No built-in optimization Optimized using Catalyst Optimized using Catalyst, with type safety Type Safety No type safety No compile-time type safety Provides compile-time type safety Performance Less optimized for performance Better performance due to optimizations Combines type safety with optimization Action vs. Transformation in Spark Aspect Action Transformation Execution Triggers execution of the Spark job Builds up a logical plan of data operations Return Type Returns results or output Returns a new RDD/DataFrame Evaluation Eager evaluation; executes immediately Lazy evaluation; executed when an action is triggered Computation Involves actual computation (e.g., collect()) Defines data transformations (e.g., map()) Performance Can cause data processing; affects performance Does not affect performance until an action is called Map vs. FlatMap Aspect Map FlatMap Output Returns one output element per input element Can return zero or more output elements per input Flattening Does not flatten output Flattens the output into a single level Use Case Suitable for one-to-one transformations Suitable for one-to-many transformations Complexity Simpler, straightforward More complex due to variable number of outputs Examples map(x =\u003e x * 2) flatMap(x =\u003e x.split(\" \")) GroupByKey vs. ReduceByKey Aspect GroupByKey ReduceByKey Operation Groups all values by key Aggregates values with the same key Efficiency Can lead to high shuffling More efficient due to partial aggregation Data Movement Requires shuffling of all values Minimizes data movement through local aggregation Use Case Useful for simple grouping Preferred for aggregations and reductions Performance Less efficient with large datasets Better performance for large datasets Repartition vs. Coalesce Aspect Repartition Coalesce Partitioning Can increase or decrease the number of partitions Only decreases the number of partitions Shuffling Involves full shuffle Avoids full shuffle, more efficient Efficiency More expensive due to shuffling More efficient for reducing partitions Use Case Used for increasing partitions or balancing load Used for reducing partitions, typically after filtering Performance Can be costly for large datasets More cost-effective for reducing partitions Cache vs. Persist Aspect Cache Persist Storage Level Defaults to MEMORY_ONLY Can use various storage levels (e.g., MEMORY_AND_DISK) Flexibility Simplified, with default storage level Offers more options for storage levels Use Case Suitable for simple caching scenarios Suitable for complex caching scenarios requiring different storage levels Implementation Easier to use, shorthand for MEMORY_ONLY More flexible, allows custom storage options Performance Suitable when memory suffices More efficient when dealing with larger datasets and limited memory Narrow vs. Wide Transformation Aspect Narrow Transformation Wide Transformation Partitioning Each parent partition is used by one child partition Requires data from multiple partitions Shuffling No shuffling required Involves shuffling of data Performance More efficient and less costly Less efficient due to data movement Examples map(), filter() groupByKey(), join() Complexity Simpler and faster More complex and slower due to data movement Collect vs. Take Aspect Collect Take Output Retrieves all data from the RDD/DataFrame Retrieves a specified number of elements Memory Usage Can be expensive and use a lot of memory More memory-efficient Use Case Used when you need the entire dataset Useful for sampling or debugging Performance Can cause performance issues with large data Faster and more controlled Action Type Triggers full data retrieval Triggers partial data retrieval Broadcast Variable vs. Accumulator Aspect Broadcast Variable Accumulator Purpose Efficiently shares read-only data across tasks Tracks metrics and aggregates values Data Type Data that is shared and read-only Counters and sums, often numerical Use Case Useful for large lookup tables or configurations Useful for aggregating metrics like counts Efficiency Reduces data transfer by broadcasting data once Efficient for aggregating values across tasks Mutability Immutable, read-only Mutable, can be updated during computation Spark SQL vs. DataFrame API Aspect Spark SQL DataFrame API Interface Executes SQL queries Provides a programmatic interface Syntax Uses SQL-like syntax Uses function-based syntax Optimization Optimized with Catalyst Optimized with Catalyst Spark Streaming vs. Structured Streaming Aspect Spark Streaming Structured Streaming Processing Micro-batch processing Micro-batch and continuous processing API RDD-based API SQL-based API with DataFrame/Dataset support Complexity More complex and lower-level Simplified with high-level APIs Consistency Can be less consistent due to micro-batches Provides stronger consistency guarantees Performance Can be slower for complex queries Better performance with optimizations Shuffle vs. MapReduce Aspect Shuffle MapReduce Operation Data reorganization across partitions Data processing model for distributed computing Efficiency Can be costly due to data movement Designed for batch processing with high I/O Performance Affects performance based on the amount of data movement Optimized for large-scale data processing but less efficient for iterative tasks Use Case Used in Spark for data redistribution Used in Hadoop for data processing tasks Implementation Integrated into Spark operations Core component of the Hadoop ecosystem Union vs. Join Aspect Union Join Operation Combines two DataFrames/RDDs into one Combines rows from two DataFrames/RDDs based on a key Data Requirements Requires same schema for both DataFrames/RDDs Requires a common key for joining Performance Generally faster as it does not require key matching Can be slower due to key matching and shuffling Output Stacks data vertically Merges data horizontally based on keys Use Case Appending data or combining datasets Merging related data based on keys Executor vs. Driver Aspect Executor Driver Role Executes tasks and processes data Coordinates and manages the Spark application Memory Memory allocated per executor for data processing Memory used for managing application execution Lifecycle Exists throughout the application execution Starts and stops the Spark application Tasks Runs the tasks assigned by the driver Schedules and coordinates tasks and jobs Parallelism Multiple executors run in parallel Single driver coordinates multiple executors Checkpointing vs. Caching Aspect Checkpointing Caching Purpose Provides fault tolerance and reliability Improves performance by storing intermediate data Storage Writes data to stable storage (e.g., HDFS) Stores data in memory or on disk (depends on storage level) Use Case Used for recovery in case of failures Used for optimizing repeated operations Impact Can be more costly and slow Generally faster but not suitable for fault tolerance Data Data is written to external storage Data is kept in memory or disk storage for quick access ReduceByKey vs. AggregateByKey Aspect ReduceByKey AggregateByKey Operation Combines values with the same key using a function Performs custom aggregation and combinatory operations Efficiency More efficient for simple aggregations Flexible for complex aggregation scenarios Shuffling Involves shuffling but can be optimized Can be more complex due to custom aggregation Use Case Suitable for straightforward aggregations Ideal for advanced and custom aggregations Performance Generally faster for simple operations Performance varies with complexity SQLContext vs. HiveContext vs. SparkSession Aspect SQLContext HiveContext SparkSession Purpose Provides SQL query capabilities Provides integration with Hive for SQL queries Unified entry point for Spark functionality Integration Basic SQL capabilities Integrates with Hive Metastore Combines SQL, DataFrame, and Streaming APIs Usage Legacy, less functionality Supports HiveQL and Hive UDFs Supports all Spark functionalities including Hive Configuration Less flexible and older Requires Hive setup and configuration Modern and flexible, manages configurations Capabilities Limited to SQL queries Extends SQL capabilities with Hive integration Comprehensive access to all Spark features Broadcast Join vs. Shuffle Join Aspect Broadcast Join Shuffle Join Operation Broadcasts a small dataset to all nodes Shuffles data across nodes for joining Data Size Suitable for small datasets Suitable for larger datasets Efficiency More efficient for small tables More suited for large datasets Performance Faster due to reduced shuffling Can be slower due to extensive shuffling Use Case Use when one dataset is small relative to others Use when both datasets are large SparkContext vs. SparkSession Aspect SparkContext SparkSession Purpose Entry point for Spark functionality Unified entry point for Spark functionalities Lifecycle Created before Spark jobs start Manages the Spark application lifecycle Functionality Provides access to RDD and basic Spark functionality Provides access to RDD, DataFrame, SQL, and Streaming APIs Configuration Configuration is less flexible More flexible and easier to configure Usage Older, used for legacy applications Modern and recommended for new applications Structured Streaming vs. Spark Streaming Aspect Structured Streaming Spark Streaming Processing Micro-batch and continuous processing Micro-batch processing API SQL-based API with DataFrame/Dataset support RDD-based API Complexity Simplified and high-level More complex and low-level Consistency Provides stronger consistency guarantees Can be less consistent due to micro-batches Performance Better performance with built-in optimizations Can be slower for complex queries Partitioning vs. Bucketing Aspect Partitioning Bucketing Purpose Divides data into multiple partitions based on a key Divides data into buckets based on a hash function Usage Used to optimize queries by reducing data scanned Used to improve join performance and maintain sorted data Shuffling Reduces shuffling by placing related data together Reduces shuffle during joins and aggregations Data Layout Data is physically separated based on partition key Data is organized into fixed-size buckets Performance Improves performance for queries involving partition keys Enhances performance for join operations",
    "tags": [],
    "title": "Comparisons",
    "uri": "/azure_data_bricks/comparisons/"
  },
  {
    "breadcrumb": "SQL",
    "content": "SQL aggregate functions perform calculations on multiple rows of data and return summary results.\n1. Basic Aggregate Functions COUNT ‚Äì Count rows SELECT COUNT(*) AS total_customers FROM customers; SUM ‚Äì Total of values SELECT SUM(sales) AS total_sales FROM orders; AVG ‚Äì Average of values SELECT AVG(sales) AS avg_sales FROM orders; MAX ‚Äì Maximum value SELECT MAX(score) AS max_score FROM customers; MIN ‚Äì Minimum value SELECT MIN(score) AS min_score FROM customers; 2. Grouped Aggregations ‚Äì GROUP BY Aggregate results per group.\nSELECT customer_id, COUNT(*) AS total_orders, SUM(sales) AS total_sales, AVG(sales) AS avg_sales, MAX(sales) AS highest_sales, MIN(sales) AS lowest_sales FROM orders GROUP BY customer_id;",
    "description": "SQL aggregate functions perform calculations on multiple rows of data and return summary results.\n1. Basic Aggregate Functions COUNT ‚Äì Count rows SELECT COUNT(*) AS total_customers FROM customers; SUM ‚Äì Total of values SELECT SUM(sales) AS total_sales FROM orders; AVG ‚Äì Average of values SELECT AVG(sales) AS avg_sales FROM orders; MAX ‚Äì Maximum value SELECT MAX(score) AS max_score FROM customers; MIN ‚Äì Minimum value SELECT MIN(score) AS min_score FROM customers; 2. Grouped Aggregations ‚Äì GROUP BY Aggregate results per group.",
    "tags": [],
    "title": "Aggregate Functions",
    "uri": "/sql/aggregate-functions/"
  },
  {
    "breadcrumb": "SQL",
    "content": "SQL window functions enable advanced calculations across sets of rows related to the current row without needing complex subqueries or joins. They support clauses like OVER, PARTITION, ORDER, FRAME, along with important rules and group-based use cases.\nTable of Contents SQL Window Basics SQL Window OVER Clause SQL Window PARTITION Clause SQL Window ORDER Clause SQL Window FRAME Clause SQL Window Rules SQL Window with GROUP BY 1. SQL Window Basics TASK 1 ‚Äì Calculate the Total Sales Across All Orders SELECT SUM(Sales) AS Total_Sales FROM Sales.Orders; TASK 2 ‚Äì Calculate the Total Sales for Each Product SELECT ProductID, SUM(Sales) AS Total_Sales FROM Sales.Orders GROUP BY ProductID; 2. SQL Window OVER Clause TASK 3 ‚Äì Total sales across all orders with order details SELECT OrderID, OrderDate, ProductID, Sales, SUM(Sales) OVER () AS Total_Sales FROM Sales.Orders; 3. SQL Window PARTITION Clause TASK 4 ‚Äì Total sales overall and per product SELECT OrderID, OrderDate, ProductID, Sales, SUM(Sales) OVER () AS Total_Sales, SUM(Sales) OVER (PARTITION BY ProductID) AS Sales_By_Product FROM Sales.Orders; TASK 5 ‚Äì Total sales overall, per product, and per product-status combination SELECT OrderID, OrderDate, ProductID, OrderStatus, Sales, SUM(Sales) OVER () AS Total_Sales, SUM(Sales) OVER (PARTITION BY ProductID) AS Sales_By_Product, SUM(Sales) OVER (PARTITION BY ProductID, OrderStatus) AS Sales_By_Product_Status FROM Sales.Orders; 4. SQL Window ORDER Clause TASK 6 ‚Äì Rank each order by sales (highest to lowest) SELECT OrderID, OrderDate, Sales, RANK() OVER (ORDER BY Sales DESC) AS Rank_Sales FROM Sales.Orders; 5. SQL Window FRAME Clause TASK 7 ‚Äì Total sales by order status (current + next two orders) SELECT OrderID, OrderDate, ProductID, OrderStatus, Sales, SUM(Sales) OVER ( PARTITION BY OrderStatus ORDER BY OrderDate ROWS BETWEEN CURRENT ROW AND 2 FOLLOWING ) AS Total_Sales FROM Sales.Orders; TASK 8 ‚Äì Total sales by order status (current + previous two orders) SELECT OrderID, OrderDate, ProductID, OrderStatus, Sales, SUM(Sales) OVER ( PARTITION BY OrderStatus ORDER BY OrderDate ROWS BETWEEN 2 PRECEDING AND CURRENT ROW ) AS Total_Sales FROM Sales.Orders; TASK 9 ‚Äì Total sales by order status (previous two orders only) SELECT OrderID, OrderDate, ProductID, OrderStatus, Sales, SUM(Sales) OVER ( PARTITION BY OrderStatus ORDER BY OrderDate ROWS 2 PRECEDING ) AS Total_Sales FROM Sales.Orders; TASK 10 ‚Äì Cumulative sales up to the current order SELECT OrderID, OrderDate, ProductID, OrderStatus, Sales, SUM(Sales) OVER ( PARTITION BY OrderStatus ORDER BY OrderDate ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW ) AS Total_Sales FROM Sales.Orders; TASK 11 ‚Äì Cumulative sales from start to current row SELECT OrderID, OrderDate, ProductID, OrderStatus, Sales, SUM(Sales) OVER ( PARTITION BY OrderStatus ORDER BY OrderDate ROWS UNBOUNDED PRECEDING ) AS Total_Sales FROM Sales.Orders; 6. SQL Window Rules RULE 1 ‚Äì Window functions can only be used in SELECT or ORDER BY SELECT OrderID, OrderDate, ProductID, OrderStatus, Sales, SUM(Sales) OVER (PARTITION BY OrderStatus) AS Total_Sales FROM Sales.Orders WHERE SUM(Sales) OVER (PARTITION BY OrderStatus) \u003e 100; -- ‚ùå Invalid RULE 2 ‚Äì Window functions cannot be nested SELECT OrderID, OrderDate, ProductID, OrderStatus, Sales, SUM(SUM(Sales) OVER (PARTITION BY OrderStatus)) OVER (PARTITION BY OrderStatus) AS Total_Sales -- ‚ùå Invalid nesting FROM Sales.Orders; 7. SQL Window with GROUP BY TASK 12 ‚Äì Rank customers by total sales SELECT CustomerID, SUM(Sales) AS Total_Sales, RANK() OVER (ORDER BY SUM(Sales) DESC) AS Rank_Customers FROM Sales.Orders GROUP BY CustomerID;",
    "description": "SQL window functions enable advanced calculations across sets of rows related to the current row without needing complex subqueries or joins. They support clauses like OVER, PARTITION, ORDER, FRAME, along with important rules and group-based use cases.\nTable of Contents SQL Window Basics SQL Window OVER Clause SQL Window PARTITION Clause SQL Window ORDER Clause SQL Window FRAME Clause SQL Window Rules SQL Window with GROUP BY 1. SQL Window Basics TASK 1 ‚Äì Calculate the Total Sales Across All Orders SELECT SUM(Sales) AS Total_Sales FROM Sales.Orders; TASK 2 ‚Äì Calculate the Total Sales for Each Product SELECT ProductID, SUM(Sales) AS Total_Sales FROM Sales.Orders GROUP BY ProductID; 2. SQL Window OVER Clause TASK 3 ‚Äì Total sales across all orders with order details SELECT OrderID, OrderDate, ProductID, Sales, SUM(Sales) OVER () AS Total_Sales FROM Sales.Orders; 3. SQL Window PARTITION Clause TASK 4 ‚Äì Total sales overall and per product SELECT OrderID, OrderDate, ProductID, Sales, SUM(Sales) OVER () AS Total_Sales, SUM(Sales) OVER (PARTITION BY ProductID) AS Sales_By_Product FROM Sales.Orders; TASK 5 ‚Äì Total sales overall, per product, and per product-status combination SELECT OrderID, OrderDate, ProductID, OrderStatus, Sales, SUM(Sales) OVER () AS Total_Sales, SUM(Sales) OVER (PARTITION BY ProductID) AS Sales_By_Product, SUM(Sales) OVER (PARTITION BY ProductID, OrderStatus) AS Sales_By_Product_Status FROM Sales.Orders; 4. SQL Window ORDER Clause TASK 6 ‚Äì Rank each order by sales (highest to lowest) SELECT OrderID, OrderDate, Sales, RANK() OVER (ORDER BY Sales DESC) AS Rank_Sales FROM Sales.Orders; 5. SQL Window FRAME Clause TASK 7 ‚Äì Total sales by order status (current + next two orders) SELECT OrderID, OrderDate, ProductID, OrderStatus, Sales, SUM(Sales) OVER ( PARTITION BY OrderStatus ORDER BY OrderDate ROWS BETWEEN CURRENT ROW AND 2 FOLLOWING ) AS Total_Sales FROM Sales.Orders; TASK 8 ‚Äì Total sales by order status (current + previous two orders) SELECT OrderID, OrderDate, ProductID, OrderStatus, Sales, SUM(Sales) OVER ( PARTITION BY OrderStatus ORDER BY OrderDate ROWS BETWEEN 2 PRECEDING AND CURRENT ROW ) AS Total_Sales FROM Sales.Orders; TASK 9 ‚Äì Total sales by order status (previous two orders only) SELECT OrderID, OrderDate, ProductID, OrderStatus, Sales, SUM(Sales) OVER ( PARTITION BY OrderStatus ORDER BY OrderDate ROWS 2 PRECEDING ) AS Total_Sales FROM Sales.Orders; TASK 10 ‚Äì Cumulative sales up to the current order SELECT OrderID, OrderDate, ProductID, OrderStatus, Sales, SUM(Sales) OVER ( PARTITION BY OrderStatus ORDER BY OrderDate ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW ) AS Total_Sales FROM Sales.Orders; TASK 11 ‚Äì Cumulative sales from start to current row SELECT OrderID, OrderDate, ProductID, OrderStatus, Sales, SUM(Sales) OVER ( PARTITION BY OrderStatus ORDER BY OrderDate ROWS UNBOUNDED PRECEDING ) AS Total_Sales FROM Sales.Orders; 6. SQL Window Rules RULE 1 ‚Äì Window functions can only be used in SELECT or ORDER BY SELECT OrderID, OrderDate, ProductID, OrderStatus, Sales, SUM(Sales) OVER (PARTITION BY OrderStatus) AS Total_Sales FROM Sales.Orders WHERE SUM(Sales) OVER (PARTITION BY OrderStatus) \u003e 100; -- ‚ùå Invalid RULE 2 ‚Äì Window functions cannot be nested SELECT OrderID, OrderDate, ProductID, OrderStatus, Sales, SUM(SUM(Sales) OVER (PARTITION BY OrderStatus)) OVER (PARTITION BY OrderStatus) AS Total_Sales -- ‚ùå Invalid nesting FROM Sales.Orders; 7. SQL Window with GROUP BY TASK 12 ‚Äì Rank customers by total sales SELECT CustomerID, SUM(Sales) AS Total_Sales, RANK() OVER (ORDER BY SUM(Sales) DESC) AS Rank_Customers FROM Sales.Orders GROUP BY CustomerID;",
    "tags": [],
    "title": "Window Functions",
    "uri": "/sql/window-functions/"
  },
  {
    "breadcrumb": "SQL",
    "content": "These functions allow you to perform aggregate calculations over a set of rows without the need for complex subqueries. They enable you to compute counts, sums, averages, minimums, and maximums while still retaining access to individual row details.\nTable of Contents COUNT SUM AVG MAX / MIN ROLLING SUM \u0026 AVERAGE Use Case COUNT Task 1: Find the Total Number of Orders and the Total Number of Orders for Each Customer SELECT OrderID, OrderDate, CustomerID, COUNT(*) OVER() AS TotalOrders, COUNT(*) OVER(PARTITION BY CustomerID) AS OrdersByCustomers FROM Sales.Orders Task 2: Find the Total Number of Customers, Scores, and Countries SELECT *, COUNT(*) OVER () AS TotalCustomersStar, COUNT(1) OVER () AS TotalCustomersOne, COUNT(Score) OVER() AS TotalScores, COUNT(Country) OVER() AS TotalCountries FROM Sales.Customers Task 3: Check whether the table ‚ÄòOrdersArchive‚Äô contains any duplicate rows SELECT * FROM ( SELECT *, COUNT(*) OVER(PARTITION BY OrderID) AS CheckDuplicates FROM Sales.OrdersArchive ) t WHERE CheckDuplicates \u003e 1 SUM Task 4: Find the Total Sales Across All Orders and per Product SELECT OrderID, OrderDate, Sales, ProductID, SUM(Sales) OVER () AS TotalSales, SUM(Sales) OVER (PARTITION BY ProductID) AS SalesByProduct FROM Sales.Orders Task 5: Find the Percentage Contribution of Each Product‚Äôs Sales to the Total Sales SELECT OrderID, ProductID, Sales, SUM(Sales) OVER () AS TotalSales, ROUND(CAST(Sales AS FLOAT) / SUM(Sales) OVER () * 100, 2) AS PercentageOfTotal FROM Sales.Orders AVG Task 6: Find the Average Sales Across All Orders and per Product SELECT OrderID, OrderDate, Sales, ProductID, AVG(Sales) OVER () AS AvgSales, AVG(Sales) OVER (PARTITION BY ProductID) AS AvgSalesByProduct FROM Sales.Orders Task 7: Find the Average Scores of Customers SELECT CustomerID, LastName, Score, COALESCE(Score, 0) AS CustomerScore, AVG(Score) OVER () AS AvgScore, AVG(COALESCE(Score, 0)) OVER () AS AvgScoreWithoutNull FROM Sales.Customers Task 8: Find all orders where Sales exceed the average Sales across all orders SELECT * FROM ( SELECT OrderID, ProductID, Sales, AVG(Sales) OVER () AS Avg_Sales FROM Sales.Orders ) t WHERE Sales \u003e Avg_Sales MAX / MIN Task 9: Find the Highest and Lowest Sales across all orders SELECT MIN(Sales) AS MinSales, MAX(Sales) AS MaxSales FROM Sales.Orders Task 10: Find the Lowest Sales across all orders and by Product SELECT OrderID, ProductID, OrderDate, Sales, MIN(Sales) OVER () AS LowestSales, MIN(Sales) OVER (PARTITION BY ProductID) AS LowestSalesByProduct FROM Sales.Orders Task 11: Show the employees who have the highest salaries SELECT * FROM ( SELECT *, MAX(Salary) OVER() AS HighestSalary FROM Sales.Employees ) t WHERE Salary = HighestSalary Task 12: Find the deviation of each Sale from the minimum and maximum Sales SELECT OrderID, OrderDate, ProductID, Sales, MAX(Sales) OVER () AS HighestSales, MIN(Sales) OVER () AS LowestSales, Sales - MIN(Sales) OVER () AS DeviationFromMin, MAX(Sales) OVER () - Sales AS DeviationFromMax FROM Sales.Orders ROLLING SUM \u0026 AVERAGE Use Case Task 13: Calculate the moving average of Sales for each Product over time SELECT OrderID, ProductID, OrderDate, Sales, AVG(Sales) OVER (PARTITION BY ProductID) AS AvgByProduct, AVG(Sales) OVER (PARTITION BY ProductID ORDER BY OrderDate) AS MovingAvg FROM Sales.Orders Task 14: Calculate the moving average of Sales for each Product over time, including only the next order SELECT OrderID, ProductID, OrderDate, Sales, AVG(Sales) OVER (PARTITION BY ProductID ORDER BY OrderDate ROWS BETWEEN CURRENT ROW AND 1 FOLLOWING) AS RollingAvg FROM Sales.Orders",
    "description": "These functions allow you to perform aggregate calculations over a set of rows without the need for complex subqueries. They enable you to compute counts, sums, averages, minimums, and maximums while still retaining access to individual row details.\nTable of Contents COUNT SUM AVG MAX / MIN ROLLING SUM \u0026 AVERAGE Use Case COUNT Task 1: Find the Total Number of Orders and the Total Number of Orders for Each Customer SELECT OrderID, OrderDate, CustomerID, COUNT(*) OVER() AS TotalOrders, COUNT(*) OVER(PARTITION BY CustomerID) AS OrdersByCustomers FROM Sales.Orders Task 2: Find the Total Number of Customers, Scores, and Countries SELECT *, COUNT(*) OVER () AS TotalCustomersStar, COUNT(1) OVER () AS TotalCustomersOne, COUNT(Score) OVER() AS TotalScores, COUNT(Country) OVER() AS TotalCountries FROM Sales.Customers Task 3: Check whether the table ‚ÄòOrdersArchive‚Äô contains any duplicate rows SELECT * FROM ( SELECT *, COUNT(*) OVER(PARTITION BY OrderID) AS CheckDuplicates FROM Sales.OrdersArchive ) t WHERE CheckDuplicates \u003e 1 SUM Task 4: Find the Total Sales Across All Orders and per Product SELECT OrderID, OrderDate, Sales, ProductID, SUM(Sales) OVER () AS TotalSales, SUM(Sales) OVER (PARTITION BY ProductID) AS SalesByProduct FROM Sales.Orders Task 5: Find the Percentage Contribution of Each Product‚Äôs Sales to the Total Sales SELECT OrderID, ProductID, Sales, SUM(Sales) OVER () AS TotalSales, ROUND(CAST(Sales AS FLOAT) / SUM(Sales) OVER () * 100, 2) AS PercentageOfTotal FROM Sales.Orders AVG Task 6: Find the Average Sales Across All Orders and per Product SELECT OrderID, OrderDate, Sales, ProductID, AVG(Sales) OVER () AS AvgSales, AVG(Sales) OVER (PARTITION BY ProductID) AS AvgSalesByProduct FROM Sales.Orders Task 7: Find the Average Scores of Customers SELECT CustomerID, LastName, Score, COALESCE(Score, 0) AS CustomerScore, AVG(Score) OVER () AS AvgScore, AVG(COALESCE(Score, 0)) OVER () AS AvgScoreWithoutNull FROM Sales.Customers Task 8: Find all orders where Sales exceed the average Sales across all orders SELECT * FROM ( SELECT OrderID, ProductID, Sales, AVG(Sales) OVER () AS Avg_Sales FROM Sales.Orders ) t WHERE Sales \u003e Avg_Sales MAX / MIN Task 9: Find the Highest and Lowest Sales across all orders SELECT MIN(Sales) AS MinSales, MAX(Sales) AS MaxSales FROM Sales.Orders Task 10: Find the Lowest Sales across all orders and by Product SELECT OrderID, ProductID, OrderDate, Sales, MIN(Sales) OVER () AS LowestSales, MIN(Sales) OVER (PARTITION BY ProductID) AS LowestSalesByProduct FROM Sales.Orders Task 11: Show the employees who have the highest salaries SELECT * FROM ( SELECT *, MAX(Salary) OVER() AS HighestSalary FROM Sales.Employees ) t WHERE Salary = HighestSalary Task 12: Find the deviation of each Sale from the minimum and maximum Sales SELECT OrderID, OrderDate, ProductID, Sales, MAX(Sales) OVER () AS HighestSales, MIN(Sales) OVER () AS LowestSales, Sales - MIN(Sales) OVER () AS DeviationFromMin, MAX(Sales) OVER () - Sales AS DeviationFromMax FROM Sales.Orders ROLLING SUM \u0026 AVERAGE Use Case Task 13: Calculate the moving average of Sales for each Product over time SELECT OrderID, ProductID, OrderDate, Sales, AVG(Sales) OVER (PARTITION BY ProductID) AS AvgByProduct, AVG(Sales) OVER (PARTITION BY ProductID ORDER BY OrderDate) AS MovingAvg FROM Sales.Orders Task 14: Calculate the moving average of Sales for each Product over time, including only the next order SELECT OrderID, ProductID, OrderDate, Sales, AVG(Sales) OVER (PARTITION BY ProductID ORDER BY OrderDate ROWS BETWEEN CURRENT ROW AND 1 FOLLOWING) AS RollingAvg FROM Sales.Orders",
    "tags": [],
    "title": "Window Aggregate Functions",
    "uri": "/sql/window_aggregations/"
  },
  {
    "breadcrumb": "SQL",
    "content": "These functions allow you to rank and order rows within a result set without the need for complex joins or subqueries. They enable you to assign unique or non-unique rankings, group rows into buckets, and analyze data distributions on ordered data.\nSQL WINDOW RANKING | ROW_NUMBER, RANK, DENSE_RANK Task 1: Rank Orders Based on Sales from Highest to Lowest SELECT OrderID, ProductID, Sales, ROW_NUMBER() OVER (ORDER BY Sales DESC) AS SalesRank_Row, RANK() OVER (ORDER BY Sales DESC) AS SalesRank_Rank, DENSE_RANK() OVER (ORDER BY Sales DESC) AS SalesRank_Dense FROM Sales.Orders; Task 2: Top-N Analysis ‚Üí Find the Highest Sale for Each Product SELECT * FROM ( SELECT OrderID, ProductID, Sales, ROW_NUMBER() OVER (PARTITION BY ProductID ORDER BY Sales DESC) AS RankByProduct FROM Sales.Orders ) AS TopProductSales WHERE RankByProduct = 1; Task 3: Bottom-N Analysis ‚Üí Find the Lowest 2 Customers Based on Total Sales SELECT * FROM ( SELECT CustomerID, SUM(Sales) AS TotalSales, ROW_NUMBER() OVER (ORDER BY SUM(Sales)) AS RankCustomers FROM Sales.Orders GROUP BY CustomerID ) AS BottomCustomerSales WHERE RankCustomers \u003c= 2; Task 4: Assign Unique IDs to Rows of the ‚ÄòOrder Archive‚Äô SELECT ROW_NUMBER() OVER (ORDER BY OrderID, OrderDate) AS UniqueID, * FROM Sales.OrdersArchive; Task 5: Identify Duplicates in ‚ÄòOrder Archive‚Äô SELECT * FROM ( SELECT ROW_NUMBER() OVER (PARTITION BY OrderID ORDER BY CreationTime DESC) AS rn, * FROM Sales.OrdersArchive ) AS UniqueOrdersArchive WHERE rn = 1; SQL WINDOW RANKING | NTILE Task 6: Divide Orders into Groups Based on Sales SELECT OrderID, Sales, NTILE(1) OVER (ORDER BY Sales) AS OneBucket, NTILE(2) OVER (ORDER BY Sales) AS TwoBuckets, NTILE(3) OVER (ORDER BY Sales) AS ThreeBuckets, NTILE(4) OVER (ORDER BY Sales) AS FourBuckets, NTILE(2) OVER (PARTITION BY ProductID ORDER BY Sales) AS TwoBucketByProducts FROM Sales.Orders; Task 7: Segment Orders into High, Medium, and Low Sales SELECT OrderID, Sales, Buckets, CASE WHEN Buckets = 1 THEN 'High' WHEN Buckets = 2 THEN 'Medium' WHEN Buckets = 3 THEN 'Low' END AS SalesSegmentations FROM ( SELECT OrderID, Sales, NTILE(3) OVER (ORDER BY Sales DESC) AS Buckets FROM Sales.Orders ) AS SalesBuckets; Task 8: Divide Orders into Groups for Processing SELECT NTILE(5) OVER (ORDER BY OrderID) AS Buckets, * FROM Sales.Orders; SQL WINDOW RANKING | CUME_DIST Task 9: Find Products Within the Highest 40% of Prices SELECT Product, Price, DistRank, CONCAT(DistRank * 100, '%') AS DistRankPerc FROM ( SELECT Product, Price, CUME_DIST() OVER (ORDER BY Price DESC) AS DistRank FROM Sales.Products ) AS PriceDistribution WHERE DistRank \u003c= 0.4;",
    "description": "These functions allow you to rank and order rows within a result set without the need for complex joins or subqueries. They enable you to assign unique or non-unique rankings, group rows into buckets, and analyze data distributions on ordered data.\nSQL WINDOW RANKING | ROW_NUMBER, RANK, DENSE_RANK Task 1: Rank Orders Based on Sales from Highest to Lowest SELECT OrderID, ProductID, Sales, ROW_NUMBER() OVER (ORDER BY Sales DESC) AS SalesRank_Row, RANK() OVER (ORDER BY Sales DESC) AS SalesRank_Rank, DENSE_RANK() OVER (ORDER BY Sales DESC) AS SalesRank_Dense FROM Sales.Orders; Task 2: Top-N Analysis ‚Üí Find the Highest Sale for Each Product SELECT * FROM ( SELECT OrderID, ProductID, Sales, ROW_NUMBER() OVER (PARTITION BY ProductID ORDER BY Sales DESC) AS RankByProduct FROM Sales.Orders ) AS TopProductSales WHERE RankByProduct = 1; Task 3: Bottom-N Analysis ‚Üí Find the Lowest 2 Customers Based on Total Sales SELECT * FROM ( SELECT CustomerID, SUM(Sales) AS TotalSales, ROW_NUMBER() OVER (ORDER BY SUM(Sales)) AS RankCustomers FROM Sales.Orders GROUP BY CustomerID ) AS BottomCustomerSales WHERE RankCustomers \u003c= 2; Task 4: Assign Unique IDs to Rows of the ‚ÄòOrder Archive‚Äô SELECT ROW_NUMBER() OVER (ORDER BY OrderID, OrderDate) AS UniqueID, * FROM Sales.OrdersArchive; Task 5: Identify Duplicates in ‚ÄòOrder Archive‚Äô SELECT * FROM ( SELECT ROW_NUMBER() OVER (PARTITION BY OrderID ORDER BY CreationTime DESC) AS rn, * FROM Sales.OrdersArchive ) AS UniqueOrdersArchive WHERE rn = 1; SQL WINDOW RANKING | NTILE Task 6: Divide Orders into Groups Based on Sales SELECT OrderID, Sales, NTILE(1) OVER (ORDER BY Sales) AS OneBucket, NTILE(2) OVER (ORDER BY Sales) AS TwoBuckets, NTILE(3) OVER (ORDER BY Sales) AS ThreeBuckets, NTILE(4) OVER (ORDER BY Sales) AS FourBuckets, NTILE(2) OVER (PARTITION BY ProductID ORDER BY Sales) AS TwoBucketByProducts FROM Sales.Orders; Task 7: Segment Orders into High, Medium, and Low Sales SELECT OrderID, Sales, Buckets, CASE WHEN Buckets = 1 THEN 'High' WHEN Buckets = 2 THEN 'Medium' WHEN Buckets = 3 THEN 'Low' END AS SalesSegmentations FROM ( SELECT OrderID, Sales, NTILE(3) OVER (ORDER BY Sales DESC) AS Buckets FROM Sales.Orders ) AS SalesBuckets; Task 8: Divide Orders into Groups for Processing SELECT NTILE(5) OVER (ORDER BY OrderID) AS Buckets, * FROM Sales.Orders; SQL WINDOW RANKING | CUME_DIST Task 9: Find Products Within the Highest 40% of Prices SELECT Product, Price, DistRank, CONCAT(DistRank * 100, '%') AS DistRankPerc FROM ( SELECT Product, Price, CUME_DIST() OVER (ORDER BY Price DESC) AS DistRank FROM Sales.Products ) AS PriceDistribution WHERE DistRank \u003c= 0.4;",
    "tags": [],
    "title": "Window Ranking Functions",
    "uri": "/sql/window-ranking/"
  },
  {
    "breadcrumb": "SQL",
    "content": "These functions let you reference and compare values from other rows in a result set without complex joins or subqueries, enabling advanced analysis on ordered data.\nSQL WINDOW VALUE | LEAD, LAG Task 1: Analyze Month-over-Month Performance Find the percentage change in sales between the current and previous months.\nSELECT *, CurrentMonthSales - PreviousMonthSales AS MoM_Change, ROUND( CAST((CurrentMonthSales - PreviousMonthSales) AS FLOAT) / PreviousMonthSales * 100, 1 ) AS MoM_Perc FROM ( SELECT MONTH(OrderDate) AS OrderMonth, SUM(Sales) AS CurrentMonthSales, LAG(SUM(Sales)) OVER (ORDER BY MONTH(OrderDate)) AS PreviousMonthSales FROM Sales.Orders GROUP BY MONTH(OrderDate) ) AS MonthlySales; Task 2: Customer Loyalty Analysis Rank customers based on the average days between their orders.\nSELECT CustomerID, AVG(DaysUntilNextOrder) AS AvgDays, RANK() OVER (ORDER BY COALESCE(AVG(DaysUntilNextOrder), 999999)) AS RankAvg FROM ( SELECT OrderID, CustomerID, OrderDate AS CurrentOrder, LEAD(OrderDate) OVER (PARTITION BY CustomerID ORDER BY OrderDate) AS NextOrder, DATEDIFF( day, OrderDate, LEAD(OrderDate) OVER (PARTITION BY CustomerID ORDER BY OrderDate) ) AS DaysUntilNextOrder FROM Sales.Orders ) AS CustomerOrdersWithNext GROUP BY CustomerID; SQL WINDOW VALUE | FIRST \u0026 LAST VALUE Task 3: Lowest and Highest Sales Per Product Find the lowest and highest sales for each product and compute the difference between the current sale and the lowest sale.\nSELECT OrderID, ProductID, Sales, FIRST_VALUE(Sales) OVER (PARTITION BY ProductID ORDER BY Sales) AS LowestSales, LAST_VALUE(Sales) OVER ( PARTITION BY ProductID ORDER BY Sales ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING ) AS HighestSales, Sales - FIRST_VALUE(Sales) OVER (PARTITION BY ProductID ORDER BY Sales) AS SalesDifference FROM Sales.Orders;",
    "description": "These functions let you reference and compare values from other rows in a result set without complex joins or subqueries, enabling advanced analysis on ordered data.\nSQL WINDOW VALUE | LEAD, LAG Task 1: Analyze Month-over-Month Performance Find the percentage change in sales between the current and previous months.\nSELECT *, CurrentMonthSales - PreviousMonthSales AS MoM_Change, ROUND( CAST((CurrentMonthSales - PreviousMonthSales) AS FLOAT) / PreviousMonthSales * 100, 1 ) AS MoM_Perc FROM ( SELECT MONTH(OrderDate) AS OrderMonth, SUM(Sales) AS CurrentMonthSales, LAG(SUM(Sales)) OVER (ORDER BY MONTH(OrderDate)) AS PreviousMonthSales FROM Sales.Orders GROUP BY MONTH(OrderDate) ) AS MonthlySales; Task 2: Customer Loyalty Analysis Rank customers based on the average days between their orders.",
    "tags": [],
    "title": "Window Value Functions",
    "uri": "/sql/window-value-functions/"
  },
  {
    "breadcrumb": "SQL",
    "content": "This script demonstrates various subquery techniques in SQL.\nIt covers result types, subqueries in the FROM clause, in SELECT, in JOIN clauses, with comparison operators, IN, ANY, correlated subqueries, and EXISTS.\nTable of Contents SUBQUERY - RESULT TYPES SUBQUERY - FROM CLAUSE SUBQUERY - SELECT SUBQUERY - JOIN CLAUSE SUBQUERY - COMPARISON OPERATORS SUBQUERY - IN OPERATOR SUBQUERY - ANY OPERATOR SUBQUERY - CORRELATED SUBQUERY - EXISTS OPERATOR SUBQUERY | RESULT TYPES Scalar Query SELECT AVG(Sales) FROM Sales.Orders; Row Query SELECT CustomerID FROM Sales.Orders; Table Query SELECT OrderID, OrderDate FROM Sales.Orders; SUBQUERY | FROM CLAUSE Task 1: Products with Price Higher than Average Price SELECT * FROM ( SELECT ProductID, Price, AVG(Price) OVER () AS AvgPrice FROM Sales.Products ) AS t WHERE Price \u003e AvgPrice; Task 2: Rank Customers by Total Sales SELECT *, RANK() OVER (ORDER BY TotalSales DESC) AS CustomerRank FROM ( SELECT CustomerID, SUM(Sales) AS TotalSales FROM Sales.Orders GROUP BY CustomerID ) AS t; SUBQUERY | SELECT Task 3: Product Details with Total Number of Orders SELECT ProductID, Product, Price, (SELECT COUNT(*) FROM Sales.Orders) AS TotalOrders FROM Sales.Products; SUBQUERY | JOIN CLAUSE Task 4: Customer Details with Total Sales SELECT c.*, t.TotalSales FROM Sales.Customers AS c LEFT JOIN ( SELECT CustomerID, SUM(Sales) AS TotalSales FROM Sales.Orders GROUP BY CustomerID ) AS t ON c.CustomerID = t.CustomerID; Task 5: Customer Details with Total Orders SELECT c.*, o.TotalOrders FROM Sales.Customers AS c LEFT JOIN ( SELECT CustomerID, COUNT(*) AS TotalOrders FROM Sales.Orders GROUP BY CustomerID ) AS o ON c.CustomerID = o.CustomerID; SUBQUERY | COMPARISON OPERATORS Task 6: Products with Price Higher than Average Price SELECT ProductID, Price, (SELECT AVG(Price) FROM Sales.Products) AS AvgPrice FROM Sales.Products WHERE Price \u003e (SELECT AVG(Price) FROM Sales.Products); SUBQUERY | IN OPERATOR Task 7: Orders Made by Customers in Germany SELECT * FROM Sales.Orders WHERE CustomerID IN ( SELECT CustomerID FROM Sales.Customers WHERE Country = 'Germany' ); Task 8: Orders Made by Customers Not in Germany SELECT * FROM Sales.Orders WHERE CustomerID NOT IN ( SELECT CustomerID FROM Sales.Customers WHERE Country = 'Germany' ); SUBQUERY | ANY OPERATOR Task 9: Female Employees with Salaries Greater than Any Male Employee SELECT EmployeeID, FirstName, Salary FROM Sales.Employees WHERE Gender = 'F' AND Salary \u003e ANY ( SELECT Salary FROM Sales.Employees WHERE Gender = 'M' ); CORRELATED SUBQUERY Task 10: Customer Details with Total Orders (Correlated) SELECT *, (SELECT COUNT(*) FROM Sales.Orders o WHERE o.CustomerID = c.CustomerID) AS TotalSales FROM Sales.Customers AS c; SUBQUERY | EXISTS OPERATOR Task 11: Orders Made by Customers in Germany SELECT * FROM Sales.Orders AS o WHERE EXISTS ( SELECT 1 FROM Sales.Customers AS c WHERE Country = 'Germany' AND o.CustomerID = c.CustomerID ); Task 12: Orders Made by Customers Not in Germany SELECT * FROM Sales.Orders AS o WHERE NOT EXISTS ( SELECT 1 FROM Sales.Customers AS c WHERE Country = 'Germany' AND o.CustomerID = c.CustomerID );",
    "description": "This script demonstrates various subquery techniques in SQL.\nIt covers result types, subqueries in the FROM clause, in SELECT, in JOIN clauses, with comparison operators, IN, ANY, correlated subqueries, and EXISTS.\nTable of Contents SUBQUERY - RESULT TYPES SUBQUERY - FROM CLAUSE SUBQUERY - SELECT SUBQUERY - JOIN CLAUSE SUBQUERY - COMPARISON OPERATORS SUBQUERY - IN OPERATOR SUBQUERY - ANY OPERATOR SUBQUERY - CORRELATED SUBQUERY - EXISTS OPERATOR SUBQUERY | RESULT TYPES Scalar Query SELECT AVG(Sales) FROM Sales.Orders; Row Query SELECT CustomerID FROM Sales.Orders; Table Query SELECT OrderID, OrderDate FROM Sales.Orders; SUBQUERY | FROM CLAUSE Task 1: Products with Price Higher than Average Price SELECT * FROM ( SELECT ProductID, Price, AVG(Price) OVER () AS AvgPrice FROM Sales.Products ) AS t WHERE Price \u003e AvgPrice; Task 2: Rank Customers by Total Sales SELECT *, RANK() OVER (ORDER BY TotalSales DESC) AS CustomerRank FROM ( SELECT CustomerID, SUM(Sales) AS TotalSales FROM Sales.Orders GROUP BY CustomerID ) AS t; SUBQUERY | SELECT Task 3: Product Details with Total Number of Orders SELECT ProductID, Product, Price, (SELECT COUNT(*) FROM Sales.Orders) AS TotalOrders FROM Sales.Products; SUBQUERY | JOIN CLAUSE Task 4: Customer Details with Total Sales SELECT c.*, t.TotalSales FROM Sales.Customers AS c LEFT JOIN ( SELECT CustomerID, SUM(Sales) AS TotalSales FROM Sales.Orders GROUP BY CustomerID ) AS t ON c.CustomerID = t.CustomerID; Task 5: Customer Details with Total Orders SELECT c.*, o.TotalOrders FROM Sales.Customers AS c LEFT JOIN ( SELECT CustomerID, COUNT(*) AS TotalOrders FROM Sales.Orders GROUP BY CustomerID ) AS o ON c.CustomerID = o.CustomerID; SUBQUERY | COMPARISON OPERATORS Task 6: Products with Price Higher than Average Price SELECT ProductID, Price, (SELECT AVG(Price) FROM Sales.Products) AS AvgPrice FROM Sales.Products WHERE Price \u003e (SELECT AVG(Price) FROM Sales.Products); SUBQUERY | IN OPERATOR Task 7: Orders Made by Customers in Germany SELECT * FROM Sales.Orders WHERE CustomerID IN ( SELECT CustomerID FROM Sales.Customers WHERE Country = 'Germany' ); Task 8: Orders Made by Customers Not in Germany SELECT * FROM Sales.Orders WHERE CustomerID NOT IN ( SELECT CustomerID FROM Sales.Customers WHERE Country = 'Germany' ); SUBQUERY | ANY OPERATOR Task 9: Female Employees with Salaries Greater than Any Male Employee SELECT EmployeeID, FirstName, Salary FROM Sales.Employees WHERE Gender = 'F' AND Salary \u003e ANY ( SELECT Salary FROM Sales.Employees WHERE Gender = 'M' ); CORRELATED SUBQUERY Task 10: Customer Details with Total Orders (Correlated) SELECT *, (SELECT COUNT(*) FROM Sales.Orders o WHERE o.CustomerID = c.CustomerID) AS TotalSales FROM Sales.Customers AS c; SUBQUERY | EXISTS OPERATOR Task 11: Orders Made by Customers in Germany SELECT * FROM Sales.Orders AS o WHERE EXISTS ( SELECT 1 FROM Sales.Customers AS c WHERE Country = 'Germany' AND o.CustomerID = c.CustomerID ); Task 12: Orders Made by Customers Not in Germany SELECT * FROM Sales.Orders AS o WHERE NOT EXISTS ( SELECT 1 FROM Sales.Customers AS c WHERE Country = 'Germany' AND o.CustomerID = c.CustomerID );",
    "tags": [],
    "title": "Subquery Functions",
    "uri": "/sql/subqueries/"
  },
  {
    "breadcrumb": "SQL",
    "content": "This script demonstrates the use of Common Table Expressions (CTEs) in SQL Server.\nIt includes examples of non-recursive CTEs for data aggregation and segmentation, as well as recursive CTEs for generating sequences and building hierarchical data.\nNON-RECURSIVE CTE -- Step 1 ‚Üí Total Sales Per Customer WITH CTE_Total_Sales AS ( SELECT CustomerID, SUM(Sales) AS TotalSales FROM Sales.Orders GROUP BY CustomerID ) -- Step 2 ‚Üí Last Order Date for Each Customer , CTE_Last_Order AS ( SELECT CustomerID, MAX(OrderDate) AS Last_Order FROM Sales.Orders GROUP BY CustomerID ) -- Step 3 ‚Üí Rank Customers by Total Sales , CTE_Customer_Rank AS ( SELECT CustomerID, TotalSales, RANK() OVER (ORDER BY TotalSales DESC) AS CustomerRank FROM CTE_Total_Sales ) -- Step 4 ‚Üí Segment Customers by Sales , CTE_Customer_Segments AS ( SELECT CustomerID, TotalSales, CASE WHEN TotalSales \u003e 100 THEN 'High' WHEN TotalSales \u003e 80 THEN 'Medium' ELSE 'Low' END AS CustomerSegments FROM CTE_Total_Sales ) -- Final Query Combining All CTEs SELECT c.CustomerID, c.FirstName, c.LastName, cts.TotalSales, clo.Last_Order, ccr.CustomerRank, ccs.CustomerSegments FROM Sales.Customers AS c LEFT JOIN CTE_Total_Sales AS cts ON cts.CustomerID = c.CustomerID LEFT JOIN CTE_Last_Order AS clo ON clo.CustomerID = c.CustomerID LEFT JOIN CTE_Customer_Rank AS ccr ON ccr.CustomerID = c.CustomerID LEFT JOIN CTE_Customer_Segments AS ccs ON ccs.CustomerID = c.CustomerID; RECURSIVE CTE | GENERATE SEQUENCE Task 2: Generate Numbers 1 to 20 WITH Series AS ( SELECT 1 AS MyNumber UNION ALL SELECT MyNumber + 1 FROM Series WHERE MyNumber \u003c 20 ) SELECT * FROM Series; Task 3: Generate Numbers 1 to 1000 WITH Series AS ( SELECT 1 AS MyNumber UNION ALL SELECT MyNumber + 1 FROM Series WHERE MyNumber \u003c 1000 ) SELECT * FROM Series OPTION (MAXRECURSION 5000); RECURSIVE CTE | BUILD HIERARCHY Task 4: Build Employee Hierarchy Display each employee‚Äôs level within the organization.\nAnchor Query: Select employees with no manager.\nRecursive Query: Select subordinates and increment the level.\nWITH CTE_Emp_Hierarchy AS ( -- Anchor Query: Top-level employees (no manager) SELECT EmployeeID, FirstName, ManagerID, 1 AS Level FROM Sales.Employees WHERE ManagerID IS NULL UNION ALL -- Recursive Query: Get subordinate employees and increment level SELECT e.EmployeeID, e.FirstName, e.ManagerID, Level + 1 FROM Sales.Employees AS e INNER JOIN CTE_Emp_Hierarchy AS ceh ON e.ManagerID = ceh.EmployeeID ) SELECT * FROM CTE_Emp_Hierarchy;",
    "description": "This script demonstrates the use of Common Table Expressions (CTEs) in SQL Server.\nIt includes examples of non-recursive CTEs for data aggregation and segmentation, as well as recursive CTEs for generating sequences and building hierarchical data.\nNON-RECURSIVE CTE -- Step 1 ‚Üí Total Sales Per Customer WITH CTE_Total_Sales AS ( SELECT CustomerID, SUM(Sales) AS TotalSales FROM Sales.Orders GROUP BY CustomerID ) -- Step 2 ‚Üí Last Order Date for Each Customer , CTE_Last_Order AS ( SELECT CustomerID, MAX(OrderDate) AS Last_Order FROM Sales.Orders GROUP BY CustomerID ) -- Step 3 ‚Üí Rank Customers by Total Sales , CTE_Customer_Rank AS ( SELECT CustomerID, TotalSales, RANK() OVER (ORDER BY TotalSales DESC) AS CustomerRank FROM CTE_Total_Sales ) -- Step 4 ‚Üí Segment Customers by Sales , CTE_Customer_Segments AS ( SELECT CustomerID, TotalSales, CASE WHEN TotalSales \u003e 100 THEN 'High' WHEN TotalSales \u003e 80 THEN 'Medium' ELSE 'Low' END AS CustomerSegments FROM CTE_Total_Sales ) -- Final Query Combining All CTEs SELECT c.CustomerID, c.FirstName, c.LastName, cts.TotalSales, clo.Last_Order, ccr.CustomerRank, ccs.CustomerSegments FROM Sales.Customers AS c LEFT JOIN CTE_Total_Sales AS cts ON cts.CustomerID = c.CustomerID LEFT JOIN CTE_Last_Order AS clo ON clo.CustomerID = c.CustomerID LEFT JOIN CTE_Customer_Rank AS ccr ON ccr.CustomerID = c.CustomerID LEFT JOIN CTE_Customer_Segments AS ccs ON ccs.CustomerID = c.CustomerID; RECURSIVE CTE | GENERATE SEQUENCE Task 2: Generate Numbers 1 to 20 WITH Series AS ( SELECT 1 AS MyNumber UNION ALL SELECT MyNumber + 1 FROM Series WHERE MyNumber \u003c 20 ) SELECT * FROM Series; Task 3: Generate Numbers 1 to 1000 WITH Series AS ( SELECT 1 AS MyNumber UNION ALL SELECT MyNumber + 1 FROM Series WHERE MyNumber \u003c 1000 ) SELECT * FROM Series OPTION (MAXRECURSION 5000); RECURSIVE CTE | BUILD HIERARCHY Task 4: Build Employee Hierarchy Display each employee‚Äôs level within the organization.",
    "tags": [],
    "title": "Common Table Expressions (CTEs)",
    "uri": "/sql/cte/"
  },
  {
    "breadcrumb": "SQL",
    "content": "This script demonstrates various view use cases in SQL Server.\nIt includes examples for creating, dropping, and modifying views, hiding query complexity, and implementing data security by controlling data access.\nCREATE, DROP, MODIFY VIEW Task: Create a Monthly Sales Summary View Aggregate by OrderMonth with total sales, total orders, and total quantities.\n-- Create View CREATE VIEW Sales.V_Monthly_Summary AS ( SELECT DATETRUNC(month, OrderDate) AS OrderMonth, SUM(Sales) AS TotalSales, COUNT(OrderID) AS TotalOrders, SUM(Quantity) AS TotalQuantities FROM Sales.Orders GROUP BY DATETRUNC(month, OrderDate) ); GO -- Query the View SELECT * FROM Sales.V_Monthly_Summary; -- Drop View if it exists IF OBJECT_ID('Sales.V_Monthly_Summary', 'V') IS NOT NULL DROP VIEW Sales.V_Monthly_Summary; GO -- Re-create the View with modified logic CREATE VIEW Sales.V_Monthly_Summary AS SELECT DATETRUNC(month, OrderDate) AS OrderMonth, SUM(Sales) AS TotalSales, COUNT(OrderID) AS TotalOrders FROM Sales.Orders GROUP BY DATETRUNC(month, OrderDate); GO VIEW USE CASE | HIDE COMPLEXITY Task: Abstract Multi-Table Joins with a View Join Orders, Products, Customers, and Employees into a single view.\nCREATE VIEW Sales.V_Order_Details AS ( SELECT o.OrderID, o.OrderDate, p.Product, p.Category, COALESCE(c.FirstName, '') + ' ' + COALESCE(c.LastName, '') AS CustomerName, c.Country AS CustomerCountry, COALESCE(e.FirstName, '') + ' ' + COALESCE(e.LastName, '') AS SalesName, e.Department, o.Sales, o.Quantity FROM Sales.Orders AS o LEFT JOIN Sales.Products AS p ON p.ProductID = o.ProductID LEFT JOIN Sales.Customers AS c ON c.CustomerID = o.CustomerID LEFT JOIN Sales.Employees AS e ON e.EmployeeID = o.SalesPersonID ); GO VIEW USE CASE | DATA SECURITY Task: Restrict Access for EU Sales Team Exclude USA data from order details.\nCREATE VIEW Sales.V_Order_Details_EU AS ( SELECT o.OrderID, o.OrderDate, p.Product, p.Category, COALESCE(c.FirstName, '') + ' ' + COALESCE(c.LastName, '') AS CustomerName, c.Country AS CustomerCountry, COALESCE(e.FirstName, '') + ' ' + COALESCE(e.LastName, '') AS SalesName, e.Department, o.Sales, o.Quantity FROM Sales.Orders AS o LEFT JOIN Sales.Products AS p ON p.ProductID = o.ProductID LEFT JOIN Sales.Customers AS c ON c.CustomerID = o.CustomerID LEFT JOIN Sales.Employees AS e ON e.EmployeeID = o.SalesPersonID WHERE c.Country != 'USA' ); GO",
    "description": "This script demonstrates various view use cases in SQL Server.\nIt includes examples for creating, dropping, and modifying views, hiding query complexity, and implementing data security by controlling data access.\nCREATE, DROP, MODIFY VIEW Task: Create a Monthly Sales Summary View Aggregate by OrderMonth with total sales, total orders, and total quantities.\n-- Create View CREATE VIEW Sales.V_Monthly_Summary AS ( SELECT DATETRUNC(month, OrderDate) AS OrderMonth, SUM(Sales) AS TotalSales, COUNT(OrderID) AS TotalOrders, SUM(Quantity) AS TotalQuantities FROM Sales.Orders GROUP BY DATETRUNC(month, OrderDate) ); GO -- Query the View SELECT * FROM Sales.V_Monthly_Summary; -- Drop View if it exists IF OBJECT_ID('Sales.V_Monthly_Summary', 'V') IS NOT NULL DROP VIEW Sales.V_Monthly_Summary; GO -- Re-create the View with modified logic CREATE VIEW Sales.V_Monthly_Summary AS SELECT DATETRUNC(month, OrderDate) AS OrderMonth, SUM(Sales) AS TotalSales, COUNT(OrderID) AS TotalOrders FROM Sales.Orders GROUP BY DATETRUNC(month, OrderDate); GO VIEW USE CASE | HIDE COMPLEXITY Task: Abstract Multi-Table Joins with a View Join Orders, Products, Customers, and Employees into a single view.",
    "tags": [],
    "title": "Views",
    "uri": "/sql/views/"
  },
  {
    "breadcrumb": "SQL",
    "content": "This script provides a generic example of data migration using a temporary table.\nStep 1: Create Temporary Table (#Orders) SELECT * INTO #Orders FROM Sales.Orders; Step 2: Clean Data in Temporary Table DELETE FROM #Orders WHERE OrderStatus = 'Delivered'; Step 3: Load Cleaned Data into Permanent Table SELECT * INTO Sales.OrdersTest FROM #Orders;",
    "description": "This script provides a generic example of data migration using a temporary table.\nStep 1: Create Temporary Table (#Orders) SELECT * INTO #Orders FROM Sales.Orders; Step 2: Clean Data in Temporary Table DELETE FROM #Orders WHERE OrderStatus = 'Delivered'; Step 3: Load Cleaned Data into Permanent Table SELECT * INTO Sales.OrdersTest FROM #Orders;",
    "tags": [],
    "title": "Temporary Tables",
    "uri": "/sql/temporary-tables/"
  },
  {
    "breadcrumb": "SQL",
    "content": "This script shows how to work with stored procedures in SQL Server, starting from basic implementations and advancing to more sophisticated techniques.\n1. Basic Stored Procedure -- Define the Stored Procedure CREATE PROCEDURE GetCustomerSummary AS BEGIN SELECT COUNT(*) AS TotalCustomers, AVG(Score) AS AvgScore FROM Sales.Customers WHERE Country = 'USA'; END GO -- Execute Stored Procedure EXEC GetCustomerSummary; 2. Parameters in Stored Procedure -- Edit the Stored Procedure ALTER PROCEDURE GetCustomerSummary @Country NVARCHAR(50) = 'USA' AS BEGIN -- Reports: Summary from Customers and Orders SELECT COUNT(*) AS TotalCustomers, AVG(Score) AS AvgScore FROM Sales.Customers WHERE Country = @Country; END GO -- Execute Stored Procedure EXEC GetCustomerSummary @Country = 'Germany'; EXEC GetCustomerSummary @Country = 'USA'; EXEC GetCustomerSummary; 3. Multiple Queries in Stored Procedure -- Edit the Stored Procedure ALTER PROCEDURE GetCustomerSummary @Country NVARCHAR(50) = 'USA' AS BEGIN -- Query 1: Find the Total Nr. of Customers and the Average Score SELECT COUNT(*) AS TotalCustomers, AVG(Score) AS AvgScore FROM Sales.Customers WHERE Country = @Country; -- Query 2: Find the Total Nr. of Orders and Total Sales SELECT COUNT(OrderID) AS TotalOrders, SUM(Sales) AS TotalSales FROM Sales.Orders AS o JOIN Sales.Customers AS c ON c.CustomerID = o.CustomerID WHERE c.Country = @Country; END GO -- Execute Stored Procedure EXEC GetCustomerSummary @Country = 'Germany'; EXEC GetCustomerSummary @Country = 'USA'; EXEC GetCustomerSummary; 4. Variables in Stored Procedure -- Edit the Stored Procedure ALTER PROCEDURE GetCustomerSummary @Country NVARCHAR(50) = 'USA' AS BEGIN -- Declare Variables DECLARE @TotalCustomers INT, @AvgScore FLOAT; -- Query 1: Find the Total Nr. of Customers and the Average Score SELECT @TotalCustomers = COUNT(*), @AvgScore = AVG(Score) FROM Sales.Customers WHERE Country = @Country; PRINT('Total Customers from ' + @Country + ':' + CAST(@TotalCustomers AS NVARCHAR)); PRINT('Average Score from ' + @Country + ':' + CAST(@AvgScore AS NVARCHAR)); -- Query 2: Find the Total Nr. of Orders and Total Sales SELECT COUNT(OrderID) AS TotalOrders, SUM(Sales) AS TotalSales FROM Sales.Orders AS o JOIN Sales.Customers AS c ON c.CustomerID = o.CustomerID WHERE c.Country = @Country; END GO -- Execute Stored Procedure EXEC GetCustomerSummary @Country = 'Germany'; EXEC GetCustomerSummary @Country = 'USA'; EXEC GetCustomerSummary; 5. Control Flow with IF/ELSE ALTER PROCEDURE GetCustomerSummary @Country NVARCHAR(50) = 'USA' AS BEGIN -- Declare Variables DECLARE @TotalCustomers INT, @AvgScore FLOAT; /* -------------------------------------------------------------------------- Prepare \u0026 Cleanup Data -------------------------------------------------------------------------- */ IF EXISTS (SELECT 1 FROM Sales.Customers WHERE Score IS NULL AND Country = @Country) BEGIN PRINT('Updating NULL Scores to 0'); UPDATE Sales.Customers SET Score = 0 WHERE Score IS NULL AND Country = @Country; END ELSE BEGIN PRINT('No NULL Scores found'); END; /* -------------------------------------------------------------------------- Generating Reports -------------------------------------------------------------------------- */ SELECT @TotalCustomers = COUNT(*), @AvgScore = AVG(Score) FROM Sales.Customers WHERE Country = @Country; PRINT('Total Customers from ' + @Country + ':' + CAST(@TotalCustomers AS NVARCHAR)); PRINT('Average Score from ' + @Country + ':' + CAST(@AvgScore AS NVARCHAR)); SELECT COUNT(OrderID) AS TotalOrders, SUM(Sales) AS TotalSales, 1/0 AS FaultyCalculation -- Intentional error for demonstration FROM Sales.Orders AS o JOIN Sales.Customers AS c ON c.CustomerID = o.CustomerID WHERE c.Country = @Country; END GO -- Execute Stored Procedure EXEC GetCustomerSummary @Country = 'Germany'; EXEC GetCustomerSummary @Country = 'USA'; EXEC GetCustomerSummary; 6. Error Handling with TRY/CATCH ALTER PROCEDURE GetCustomerSummary @Country NVARCHAR(50) = 'USA' AS BEGIN BEGIN TRY -- Declare Variables DECLARE @TotalCustomers INT, @AvgScore FLOAT; /* -------------------------------------------------------------------------- Prepare \u0026 Cleanup Data -------------------------------------------------------------------------- */ IF EXISTS (SELECT 1 FROM Sales.Customers WHERE Score IS NULL AND Country = @Country) BEGIN PRINT('Updating NULL Scores to 0'); UPDATE Sales.Customers SET Score = 0 WHERE Score IS NULL AND Country = @Country; END ELSE BEGIN PRINT('No NULL Scores found'); END; /* -------------------------------------------------------------------------- Generating Reports -------------------------------------------------------------------------- */ SELECT @TotalCustomers = COUNT(*), @AvgScore = AVG(Score) FROM Sales.Customers WHERE Country = @Country; PRINT('Total Customers from ' + @Country + ':' + CAST(@TotalCustomers AS NVARCHAR)); PRINT('Average Score from ' + @Country + ':' + CAST(@AvgScore AS NVARCHAR)); SELECT COUNT(OrderID) AS TotalOrders, SUM(Sales) AS TotalSales, 1/0 AS FaultyCalculation -- Intentional error for demonstration FROM Sales.Orders AS o JOIN Sales.Customers AS c ON c.CustomerID = o.CustomerID WHERE c.Country = @Country; END TRY BEGIN CATCH /* -------------------------------------------------------------------------- Error Handling -------------------------------------------------------------------------- */ PRINT('An error occurred.'); PRINT('Error Message: ' + ERROR_MESSAGE()); PRINT('Error Number: ' + CAST(ERROR_NUMBER() AS NVARCHAR)); PRINT('Error Severity: ' + CAST(ERROR_SEVERITY() AS NVARCHAR)); PRINT('Error State: ' + CAST(ERROR_STATE() AS NVARCHAR)); PRINT('Error Line: ' + CAST(ERROR_LINE() AS NVARCHAR)); PRINT('Error Procedure: ' + ISNULL(ERROR_PROCEDURE(), 'N/A')); END CATCH; END GO -- Execute Stored Procedure EXEC GetCustomerSummary @Country = 'Germany'; EXEC GetCustomerSummary @Country = 'USA'; EXEC GetCustomerSummary;",
    "description": "This script shows how to work with stored procedures in SQL Server, starting from basic implementations and advancing to more sophisticated techniques.\n1. Basic Stored Procedure -- Define the Stored Procedure CREATE PROCEDURE GetCustomerSummary AS BEGIN SELECT COUNT(*) AS TotalCustomers, AVG(Score) AS AvgScore FROM Sales.Customers WHERE Country = 'USA'; END GO -- Execute Stored Procedure EXEC GetCustomerSummary; 2. Parameters in Stored Procedure -- Edit the Stored Procedure ALTER PROCEDURE GetCustomerSummary @Country NVARCHAR(50) = 'USA' AS BEGIN -- Reports: Summary from Customers and Orders SELECT COUNT(*) AS TotalCustomers, AVG(Score) AS AvgScore FROM Sales.Customers WHERE Country = @Country; END GO -- Execute Stored Procedure EXEC GetCustomerSummary @Country = 'Germany'; EXEC GetCustomerSummary @Country = 'USA'; EXEC GetCustomerSummary; 3. Multiple Queries in Stored Procedure -- Edit the Stored Procedure ALTER PROCEDURE GetCustomerSummary @Country NVARCHAR(50) = 'USA' AS BEGIN -- Query 1: Find the Total Nr. of Customers and the Average Score SELECT COUNT(*) AS TotalCustomers, AVG(Score) AS AvgScore FROM Sales.Customers WHERE Country = @Country; -- Query 2: Find the Total Nr. of Orders and Total Sales SELECT COUNT(OrderID) AS TotalOrders, SUM(Sales) AS TotalSales FROM Sales.Orders AS o JOIN Sales.Customers AS c ON c.CustomerID = o.CustomerID WHERE c.Country = @Country; END GO -- Execute Stored Procedure EXEC GetCustomerSummary @Country = 'Germany'; EXEC GetCustomerSummary @Country = 'USA'; EXEC GetCustomerSummary; 4. Variables in Stored Procedure -- Edit the Stored Procedure ALTER PROCEDURE GetCustomerSummary @Country NVARCHAR(50) = 'USA' AS BEGIN -- Declare Variables DECLARE @TotalCustomers INT, @AvgScore FLOAT; -- Query 1: Find the Total Nr. of Customers and the Average Score SELECT @TotalCustomers = COUNT(*), @AvgScore = AVG(Score) FROM Sales.Customers WHERE Country = @Country; PRINT('Total Customers from ' + @Country + ':' + CAST(@TotalCustomers AS NVARCHAR)); PRINT('Average Score from ' + @Country + ':' + CAST(@AvgScore AS NVARCHAR)); -- Query 2: Find the Total Nr. of Orders and Total Sales SELECT COUNT(OrderID) AS TotalOrders, SUM(Sales) AS TotalSales FROM Sales.Orders AS o JOIN Sales.Customers AS c ON c.CustomerID = o.CustomerID WHERE c.Country = @Country; END GO -- Execute Stored Procedure EXEC GetCustomerSummary @Country = 'Germany'; EXEC GetCustomerSummary @Country = 'USA'; EXEC GetCustomerSummary; 5. Control Flow with IF/ELSE ALTER PROCEDURE GetCustomerSummary @Country NVARCHAR(50) = 'USA' AS BEGIN -- Declare Variables DECLARE @TotalCustomers INT, @AvgScore FLOAT; /* -------------------------------------------------------------------------- Prepare \u0026 Cleanup Data -------------------------------------------------------------------------- */ IF EXISTS (SELECT 1 FROM Sales.Customers WHERE Score IS NULL AND Country = @Country) BEGIN PRINT('Updating NULL Scores to 0'); UPDATE Sales.Customers SET Score = 0 WHERE Score IS NULL AND Country = @Country; END ELSE BEGIN PRINT('No NULL Scores found'); END; /* -------------------------------------------------------------------------- Generating Reports -------------------------------------------------------------------------- */ SELECT @TotalCustomers = COUNT(*), @AvgScore = AVG(Score) FROM Sales.Customers WHERE Country = @Country; PRINT('Total Customers from ' + @Country + ':' + CAST(@TotalCustomers AS NVARCHAR)); PRINT('Average Score from ' + @Country + ':' + CAST(@AvgScore AS NVARCHAR)); SELECT COUNT(OrderID) AS TotalOrders, SUM(Sales) AS TotalSales, 1/0 AS FaultyCalculation -- Intentional error for demonstration FROM Sales.Orders AS o JOIN Sales.Customers AS c ON c.CustomerID = o.CustomerID WHERE c.Country = @Country; END GO -- Execute Stored Procedure EXEC GetCustomerSummary @Country = 'Germany'; EXEC GetCustomerSummary @Country = 'USA'; EXEC GetCustomerSummary; 6. Error Handling with TRY/CATCH ALTER PROCEDURE GetCustomerSummary @Country NVARCHAR(50) = 'USA' AS BEGIN BEGIN TRY -- Declare Variables DECLARE @TotalCustomers INT, @AvgScore FLOAT; /* -------------------------------------------------------------------------- Prepare \u0026 Cleanup Data -------------------------------------------------------------------------- */ IF EXISTS (SELECT 1 FROM Sales.Customers WHERE Score IS NULL AND Country = @Country) BEGIN PRINT('Updating NULL Scores to 0'); UPDATE Sales.Customers SET Score = 0 WHERE Score IS NULL AND Country = @Country; END ELSE BEGIN PRINT('No NULL Scores found'); END; /* -------------------------------------------------------------------------- Generating Reports -------------------------------------------------------------------------- */ SELECT @TotalCustomers = COUNT(*), @AvgScore = AVG(Score) FROM Sales.Customers WHERE Country = @Country; PRINT('Total Customers from ' + @Country + ':' + CAST(@TotalCustomers AS NVARCHAR)); PRINT('Average Score from ' + @Country + ':' + CAST(@AvgScore AS NVARCHAR)); SELECT COUNT(OrderID) AS TotalOrders, SUM(Sales) AS TotalSales, 1/0 AS FaultyCalculation -- Intentional error for demonstration FROM Sales.Orders AS o JOIN Sales.Customers AS c ON c.CustomerID = o.CustomerID WHERE c.Country = @Country; END TRY BEGIN CATCH /* -------------------------------------------------------------------------- Error Handling -------------------------------------------------------------------------- */ PRINT('An error occurred.'); PRINT('Error Message: ' + ERROR_MESSAGE()); PRINT('Error Number: ' + CAST(ERROR_NUMBER() AS NVARCHAR)); PRINT('Error Severity: ' + CAST(ERROR_SEVERITY() AS NVARCHAR)); PRINT('Error State: ' + CAST(ERROR_STATE() AS NVARCHAR)); PRINT('Error Line: ' + CAST(ERROR_LINE() AS NVARCHAR)); PRINT('Error Procedure: ' + ISNULL(ERROR_PROCEDURE(), 'N/A')); END CATCH; END GO -- Execute Stored Procedure EXEC GetCustomerSummary @Country = 'Germany'; EXEC GetCustomerSummary @Country = 'USA'; EXEC GetCustomerSummary;",
    "tags": [],
    "title": "Stored Procedures",
    "uri": "/sql/stored-procedures/"
  },
  {
    "breadcrumb": "SQL",
    "content": "This script demonstrates:\nThe creation of a logging table. A trigger on the Sales.Employees table. An insert operation that fires the trigger. The trigger logs details of newly added employees into the Sales.EmployeeLogs table.\nStep 1: Create Log Table CREATE TABLE Sales.EmployeeLogs ( LogID INT IDENTITY(1,1) PRIMARY KEY, EmployeeID INT, LogMessage VARCHAR(255), LogDate DATE ); GO Step 2: Create Trigger on Employees Table CREATE TRIGGER trg_AfterInsertEmployee ON Sales.Employees AFTER INSERT AS BEGIN INSERT INTO Sales.EmployeeLogs (EmployeeID, LogMessage, LogDate) SELECT EmployeeID, 'New Employee Added = ' + CAST(EmployeeID AS VARCHAR), GETDATE() FROM INSERTED; END; GO Step 3: Insert New Data Into Employees INSERT INTO Sales.Employees VALUES (6, 'Maria', 'Doe', 'HR', '1988-01-12', 'F', 80000, 3); GO Step 4: Check the Logs SELECT * FROM Sales.EmployeeLogs; GO",
    "description": "This script demonstrates:\nThe creation of a logging table. A trigger on the Sales.Employees table. An insert operation that fires the trigger. The trigger logs details of newly added employees into the Sales.EmployeeLogs table.\nStep 1: Create Log Table CREATE TABLE Sales.EmployeeLogs ( LogID INT IDENTITY(1,1) PRIMARY KEY, EmployeeID INT, LogMessage VARCHAR(255), LogDate DATE ); GO Step 2: Create Trigger on Employees Table CREATE TRIGGER trg_AfterInsertEmployee ON Sales.Employees AFTER INSERT AS BEGIN INSERT INTO Sales.EmployeeLogs (EmployeeID, LogMessage, LogDate) SELECT EmployeeID, 'New Employee Added = ' + CAST(EmployeeID AS VARCHAR), GETDATE() FROM INSERTED; END; GO Step 3: Insert New Data Into Employees INSERT INTO Sales.Employees VALUES (6, 'Maria', 'Doe', 'HR', '1988-01-12', 'F', 80000, 3); GO Step 4: Check the Logs SELECT * FROM Sales.EmployeeLogs; GO",
    "tags": [],
    "title": "Triggers",
    "uri": "/sql/triggers/"
  },
  {
    "breadcrumb": "SQL",
    "content": "This script demonstrates various index types in SQL Server including clustered, non-clustered, columnstore, unique, and filtered indexes.\nIt also covers index monitoring techniques like usage stats, missing/duplicate indexes, updating statistics, and fragmentation.\nüìë Table of Contents Index Types Clustered and Non-Clustered Indexes Leftmost Prefix Rule Explanation Columnstore Indexes Unique Indexes Filtered Indexes Index Monitoring Monitor Index Usage Monitor Missing Indexes Monitor Duplicate Indexes Update Statistics Fragmentations Index Types Clustered and Non-Clustered Indexes -- Create a Heap Table as a copy of Sales.Customers SELECT * INTO Sales.DBCustomers FROM Sales.Customers; -- Test Query SELECT * FROM Sales.DBCustomers WHERE CustomerID = 1; -- Create a Clustered Index CREATE CLUSTERED INDEX idx_DBCustomers_CustomerID ON Sales.DBCustomers (CustomerID); -- Attempt to create a second Clustered Index (will fail) CREATE CLUSTERED INDEX idx_DBCustomers_CustomerID ON Sales.DBCustomers (CustomerID); -- Drop the Clustered Index DROP INDEX idx_DBCustomers_CustomerID ON Sales.DBCustomers; -- Query using LastName filter SELECT * FROM Sales.DBCustomers WHERE LastName = 'Brown'; -- Create a Non-Clustered Index on LastName CREATE NONCLUSTERED INDEX idx_DBCustomers_LastName ON Sales.DBCustomers (LastName); -- Additional Non-Clustered Index on FirstName CREATE INDEX idx_DBCustomers_FirstName ON Sales.DBCustomers (FirstName); -- Composite Index on Country and Score CREATE INDEX idx_DBCustomers_CountryScore ON Sales.DBCustomers (Country, Score); -- Query that uses Composite Index SELECT * FROM Sales.DBCustomers WHERE Country = 'USA' AND Score \u003e 500; -- Query that may not use Composite Index due to column order SELECT * FROM Sales.DBCustomers WHERE Score \u003e 500 AND Country = 'USA'; Leftmost Prefix Rule Explanation For a composite index (A, B, C, D) the index is useful when filtering on:\nA only\nA, B\nA, B, C\nIt is not effective when filtering on:\nB only\nA, C\nA, B, D\nColumnstore Indexes -- Create Clustered Columnstore Index CREATE CLUSTERED COLUMNSTORE INDEX idx_DBCustomers_CS ON Sales.DBCustomers; GO -- Non-Clustered Columnstore Index on FirstName CREATE NONCLUSTERED COLUMNSTORE INDEX idx_DBCustomers_CS_FirstName ON Sales.DBCustomers (FirstName); GO -- Switch context to AdventureWorksDW2022 USE AdventureWorksDW2022; -- Create Heap Table SELECT * INTO FactInternetSales_HP FROM FactInternetSales; -- Create RowStore Table SELECT * INTO FactInternetSales_RS FROM FactInternetSales; -- Clustered Index on RowStore CREATE CLUSTERED INDEX idx_FactInternetSales_RS_PK ON FactInternetSales_RS (SalesOrderNumber, SalesOrderLineNumber); -- Create Columnstore Table SELECT * INTO FactInternetSales_CS FROM FactInternetSales; -- Clustered Columnstore Index CREATE CLUSTERED COLUMNSTORE INDEX idx_FactInternetSales_CS_PK ON FactInternetSales_CS; Unique Indexes -- Attempt Unique Index on Category (fails if duplicates exist) CREATE UNIQUE INDEX idx_Products_Category ON Sales.Products (Category); -- Unique Index on Product CREATE UNIQUE INDEX idx_Products_Product ON Sales.Products (Product); -- Test Insert (should fail if duplicate constraint holds) INSERT INTO Sales.Products (ProductID, Product) VALUES (106, 'Caps'); Filtered Indexes -- Test Query SELECT * FROM Sales.Customers WHERE Country = 'USA'; -- Filtered Index CREATE NONCLUSTERED INDEX idx_Customers_Country ON Sales.Customers (Country) WHERE Country = 'USA'; Index Monitoring Monitor Index Usage -- List all indexes on a table sp_helpindex 'Sales.DBCustomers'; -- Monitor Index Usage SELECT tbl.name AS TableName, idx.name AS IndexName, idx.type_desc AS IndexType, idx.is_primary_key AS IsPrimaryKey, idx.is_unique AS IsUnique, idx.is_disabled AS IsDisabled, s.user_seeks AS UserSeeks, s.user_scans AS UserScans, s.user_lookups AS UserLookups, s.user_updates AS UserUpdates, COALESCE(s.last_user_seek, s.last_user_scan) AS LastUpdate FROM sys.indexes idx JOIN sys.tables tbl ON idx.object_id = tbl.object_id LEFT JOIN sys.dm_db_index_usage_stats s ON s.object_id = idx.object_id AND s.index_id = idx.index_id ORDER BY tbl.name, idx.name; Monitor Missing Indexes SELECT * FROM sys.dm_db_missing_index_details; Monitor Duplicate Indexes SELECT tbl.name AS TableName, col.name AS IndexColumn, idx.name AS IndexName, idx.type_desc AS IndexType, COUNT(*) OVER (PARTITION BY tbl.name, col.name) ColumnCount FROM sys.indexes idx JOIN sys.tables tbl ON idx.object_id = tbl.object_id JOIN sys.index_columns ic ON idx.object_id = ic.object_id AND idx.index_id = ic.index_id JOIN sys.columns col ON ic.object_id = col.object_id AND ic.column_id = col.column_id ORDER BY ColumnCount DESC; Update Statistics SELECT SCHEMA_NAME(t.schema_id) AS SchemaName, t.name AS TableName, s.name AS StatisticName, sp.last_updated AS LastUpdate, DATEDIFF(day, sp.last_updated, GETDATE()) AS LastUpdateDay, sp.rows AS 'Rows', sp.modification_counter AS ModificationsSinceLastUpdate FROM sys.stats AS s JOIN sys.tables AS t ON s.object_id = t.object_id CROSS APPLY sys.dm_db_stats_properties(s.object_id, s.stats_id) AS sp ORDER BY sp.modification_counter DESC; -- Update specific system statistic UPDATE STATISTICS Sales.DBCustomers _WA_Sys_00000001_6EF57B66; GO -- Update all statistics for a table UPDATE STATISTICS Sales.DBCustomers; GO -- Update all statistics in the database EXEC sp_updatestats; GO Fragmentations -- Retrieve fragmentation stats SELECT tbl.name AS TableName, idx.name AS IndexName, s.avg_fragmentation_in_percent, s.page_count FROM sys.dm_db_index_physical_stats(DB_ID(), NULL, NULL, NULL, 'LIMITED') AS s INNER JOIN sys.tables tbl ON s.object_id = tbl.object_id INNER JOIN sys.indexes AS idx ON idx.object_id = s.object_id AND idx.index_id = s.index_id ORDER BY s.avg_fragmentation_in_percent DESC; -- Reorganize index (lightweight) ALTER INDEX idx_Customers_CS_Country ON Sales.Customers REORGANIZE; GO -- Rebuild index (full) ALTER INDEX idx_Customers_Country ON Sales.Customers REBUILD; GO",
    "description": "This script demonstrates various index types in SQL Server including clustered, non-clustered, columnstore, unique, and filtered indexes.\nIt also covers index monitoring techniques like usage stats, missing/duplicate indexes, updating statistics, and fragmentation.\nüìë Table of Contents Index Types Clustered and Non-Clustered Indexes Leftmost Prefix Rule Explanation Columnstore Indexes Unique Indexes Filtered Indexes Index Monitoring Monitor Index Usage Monitor Missing Indexes Monitor Duplicate Indexes Update Statistics Fragmentations Index Types Clustered and Non-Clustered Indexes -- Create a Heap Table as a copy of Sales.Customers SELECT * INTO Sales.DBCustomers FROM Sales.Customers; -- Test Query SELECT * FROM Sales.DBCustomers WHERE CustomerID = 1; -- Create a Clustered Index CREATE CLUSTERED INDEX idx_DBCustomers_CustomerID ON Sales.DBCustomers (CustomerID); -- Attempt to create a second Clustered Index (will fail) CREATE CLUSTERED INDEX idx_DBCustomers_CustomerID ON Sales.DBCustomers (CustomerID); -- Drop the Clustered Index DROP INDEX idx_DBCustomers_CustomerID ON Sales.DBCustomers; -- Query using LastName filter SELECT * FROM Sales.DBCustomers WHERE LastName = 'Brown'; -- Create a Non-Clustered Index on LastName CREATE NONCLUSTERED INDEX idx_DBCustomers_LastName ON Sales.DBCustomers (LastName); -- Additional Non-Clustered Index on FirstName CREATE INDEX idx_DBCustomers_FirstName ON Sales.DBCustomers (FirstName); -- Composite Index on Country and Score CREATE INDEX idx_DBCustomers_CountryScore ON Sales.DBCustomers (Country, Score); -- Query that uses Composite Index SELECT * FROM Sales.DBCustomers WHERE Country = 'USA' AND Score \u003e 500; -- Query that may not use Composite Index due to column order SELECT * FROM Sales.DBCustomers WHERE Score \u003e 500 AND Country = 'USA'; Leftmost Prefix Rule Explanation For a composite index (A, B, C, D) the index is useful when filtering on:",
    "tags": [],
    "title": "Indexes",
    "uri": "/sql/indexes/"
  },
  {
    "breadcrumb": "SQL",
    "content": "This script demonstrates SQL Server partitioning features including:\nPartition functions, filegroups, data files, partition schemes Partitioned tables and data insertion Verification queries and execution plan comparisons Step 1: Create a Partition Function -- Create Left Range Partition Functions based on Years CREATE PARTITION FUNCTION PartitionByYear (DATE) AS RANGE LEFT FOR VALUES ('2023-12-31', '2024-12-31', '2025-12-31'); -- List all existing Partition Functions SELECT name, function_id, type, type_desc, boundary_value_on_right FROM sys.partition_functions; Step 2: Create Filegroups -- Create Filegroups in SalesDB ALTER DATABASE SalesDB ADD FILEGROUP FG_2023; ALTER DATABASE SalesDB ADD FILEGROUP FG_2024; ALTER DATABASE SalesDB ADD FILEGROUP FG_2025; ALTER DATABASE SalesDB ADD FILEGROUP FG_2026; -- Optional: Remove a Filegroup ALTER DATABASE SalesDB REMOVE FILEGROUP FG_2023; -- List existing Filegroups SELECT * FROM sys.filegroups WHERE type = 'FG'; Step 3: Create Data Files -- Create Data Files and map them to Filegroups ALTER DATABASE SalesDB ADD FILE ( NAME = P_2023, FILENAME = 'C:\\Program Files\\Microsoft SQL Server\\MSSQL16.SQLEXPRESS\\MSSQL\\DATA\\P_2023.ndf' ) TO FILEGROUP FG_2023; ALTER DATABASE SalesDB ADD FILE ( NAME = P_2024, FILENAME = 'C:\\Program Files\\Microsoft SQL Server\\MSSQL16.SQLEXPRESS\\MSSQL\\DATA\\P_2024.ndf' ) TO FILEGROUP FG_2024; ALTER DATABASE SalesDB ADD FILE ( NAME = P_2025, FILENAME = 'C:\\Program Files\\Microsoft SQL Server\\MSSQL16.SQLEXPRESS\\MSSQL\\DATA\\P_2025.ndf' ) TO FILEGROUP FG_2025; ALTER DATABASE SalesDB ADD FILE ( NAME = P_2026, FILENAME = 'C:\\Program Files\\Microsoft SQL Server\\MSSQL16.SQLEXPRESS\\MSSQL\\DATA\\P_2026.ndf' ) TO FILEGROUP FG_2026; -- List all files in SalesDB SELECT fg.name AS FilegroupName, mf.name AS LogicalFileName, mf.physical_name AS PhysicalFilePath, mf.size / 128 AS SizeInMB FROM sys.filegroups fg JOIN sys.master_files mf ON fg.data_space_id = mf.data_space_id WHERE mf.database_id = DB_ID('SalesDB'); Step 4: Create Partition Scheme -- Partition Scheme using the Partition Function CREATE PARTITION SCHEME SchemePartitionByYear AS PARTITION PartitionByYear TO (FG_2023, FG_2024, FG_2025, FG_2026); -- List Partition Schemes SELECT ps.name AS PartitionSchemeName, pf.name AS PartitionFunctionName, ds.destination_id AS PartitionNumber, fg.name AS FilegroupName FROM sys.partition_schemes ps JOIN sys.partition_functions pf ON ps.function_id = pf.function_id JOIN sys.destination_data_spaces ds ON ps.data_space_id = ds.partition_scheme_id JOIN sys.filegroups fg ON ds.data_space_id = fg.data_space_id; Step 5: Create the Partitioned Table CREATE TABLE Sales.Orders_Partitioned ( OrderID INT, OrderDate DATE, Sales INT ) ON SchemePartitionByYear (OrderDate); Step 6: Insert Data Into the Partitioned Table INSERT INTO Sales.Orders_Partitioned VALUES (1, '2023-05-15', 100); INSERT INTO Sales.Orders_Partitioned VALUES (2, '2024-07-20', 50); INSERT INTO Sales.Orders_Partitioned VALUES (3, '2025-12-31', 20); INSERT INTO Sales.Orders_Partitioned VALUES (4, '2026-01-01', 100); Step 7: Verify Partitioning and Compare Execution Plans -- Verify that data is correctly partitioned SELECT p.partition_number AS PartitionNumber, f.name AS PartitionFilegroup, p.rows AS NumberOfRows FROM sys.partitions p JOIN sys.destination_data_spaces dds ON p.partition_number = dds.destination_id JOIN sys.filegroups f ON dds.data_space_id = f.data_space_id WHERE OBJECT_NAME(p.object_id) = 'Orders_Partitioned'; -- Non-partitioned copy for comparison SELECT * INTO Sales.Orders_NoPartition FROM Sales.Orders_Partitioned; -- Query on Partitioned Table SELECT * FROM Sales.Orders_Partitioned WHERE OrderDate IN ('2026-01-01', '2025-12-31'); -- Query on Non-Partitioned Table SELECT * FROM Sales.Orders_NoPartition WHERE OrderDate IN ('2026-01-01', '2025-12-31');",
    "description": "This script demonstrates SQL Server partitioning features including:\nPartition functions, filegroups, data files, partition schemes Partitioned tables and data insertion Verification queries and execution plan comparisons Step 1: Create a Partition Function -- Create Left Range Partition Functions based on Years CREATE PARTITION FUNCTION PartitionByYear (DATE) AS RANGE LEFT FOR VALUES ('2023-12-31', '2024-12-31', '2025-12-31'); -- List all existing Partition Functions SELECT name, function_id, type, type_desc, boundary_value_on_right FROM sys.partition_functions; Step 2: Create Filegroups -- Create Filegroups in SalesDB ALTER DATABASE SalesDB ADD FILEGROUP FG_2023; ALTER DATABASE SalesDB ADD FILEGROUP FG_2024; ALTER DATABASE SalesDB ADD FILEGROUP FG_2025; ALTER DATABASE SalesDB ADD FILEGROUP FG_2026; -- Optional: Remove a Filegroup ALTER DATABASE SalesDB REMOVE FILEGROUP FG_2023; -- List existing Filegroups SELECT * FROM sys.filegroups WHERE type = 'FG'; Step 3: Create Data Files -- Create Data Files and map them to Filegroups ALTER DATABASE SalesDB ADD FILE ( NAME = P_2023, FILENAME = 'C:\\Program Files\\Microsoft SQL Server\\MSSQL16.SQLEXPRESS\\MSSQL\\DATA\\P_2023.ndf' ) TO FILEGROUP FG_2023; ALTER DATABASE SalesDB ADD FILE ( NAME = P_2024, FILENAME = 'C:\\Program Files\\Microsoft SQL Server\\MSSQL16.SQLEXPRESS\\MSSQL\\DATA\\P_2024.ndf' ) TO FILEGROUP FG_2024; ALTER DATABASE SalesDB ADD FILE ( NAME = P_2025, FILENAME = 'C:\\Program Files\\Microsoft SQL Server\\MSSQL16.SQLEXPRESS\\MSSQL\\DATA\\P_2025.ndf' ) TO FILEGROUP FG_2025; ALTER DATABASE SalesDB ADD FILE ( NAME = P_2026, FILENAME = 'C:\\Program Files\\Microsoft SQL Server\\MSSQL16.SQLEXPRESS\\MSSQL\\DATA\\P_2026.ndf' ) TO FILEGROUP FG_2026; -- List all files in SalesDB SELECT fg.name AS FilegroupName, mf.name AS LogicalFileName, mf.physical_name AS PhysicalFilePath, mf.size / 128 AS SizeInMB FROM sys.filegroups fg JOIN sys.master_files mf ON fg.data_space_id = mf.data_space_id WHERE mf.database_id = DB_ID('SalesDB'); Step 4: Create Partition Scheme -- Partition Scheme using the Partition Function CREATE PARTITION SCHEME SchemePartitionByYear AS PARTITION PartitionByYear TO (FG_2023, FG_2024, FG_2025, FG_2026); -- List Partition Schemes SELECT ps.name AS PartitionSchemeName, pf.name AS PartitionFunctionName, ds.destination_id AS PartitionNumber, fg.name AS FilegroupName FROM sys.partition_schemes ps JOIN sys.partition_functions pf ON ps.function_id = pf.function_id JOIN sys.destination_data_spaces ds ON ps.data_space_id = ds.partition_scheme_id JOIN sys.filegroups fg ON ds.data_space_id = fg.data_space_id; Step 5: Create the Partitioned Table CREATE TABLE Sales.Orders_Partitioned ( OrderID INT, OrderDate DATE, Sales INT ) ON SchemePartitionByYear (OrderDate); Step 6: Insert Data Into the Partitioned Table INSERT INTO Sales.Orders_Partitioned VALUES (1, '2023-05-15', 100); INSERT INTO Sales.Orders_Partitioned VALUES (2, '2024-07-20', 50); INSERT INTO Sales.Orders_Partitioned VALUES (3, '2025-12-31', 20); INSERT INTO Sales.Orders_Partitioned VALUES (4, '2026-01-01', 100); Step 7: Verify Partitioning and Compare Execution Plans -- Verify that data is correctly partitioned SELECT p.partition_number AS PartitionNumber, f.name AS PartitionFilegroup, p.rows AS NumberOfRows FROM sys.partitions p JOIN sys.destination_data_spaces dds ON p.partition_number = dds.destination_id JOIN sys.filegroups f ON dds.data_space_id = f.data_space_id WHERE OBJECT_NAME(p.object_id) = 'Orders_Partitioned'; -- Non-partitioned copy for comparison SELECT * INTO Sales.Orders_NoPartition FROM Sales.Orders_Partitioned; -- Query on Partitioned Table SELECT * FROM Sales.Orders_Partitioned WHERE OrderDate IN ('2026-01-01', '2025-12-31'); -- Query on Non-Partitioned Table SELECT * FROM Sales.Orders_NoPartition WHERE OrderDate IN ('2026-01-01', '2025-12-31');",
    "tags": [],
    "title": "Partitioning",
    "uri": "/sql/partitions/"
  },
  {
    "breadcrumb": "SQL",
    "content": "This guide demonstrates best practices for:\nFetching data Filtering Joins UNION Aggregations Subqueries/CTE DDL Indexing It covers techniques such as selecting only necessary columns, proper filtering methods, explicit joins, avoiding redundant logic, and efficient indexing strategies.\nFetching Data Tip 1: Select Only What You Need -- Bad Practice SELECT * FROM Sales.Customers; -- Good Practice SELECT CustomerID, FirstName, LastName FROM Sales.Customers; Tip 2: Avoid unnecessary DISTINCT \u0026 ORDER BY -- Bad Practice SELECT DISTINCT FirstName FROM Sales.Customers ORDER BY FirstName; -- Good Practice SELECT FirstName FROM Sales.Customers; Tip 3: For Exploration Purpose, Limit Rows! -- Bad Practice SELECT OrderID, Sales FROM Sales.Orders; -- Good Practice SELECT TOP 10 OrderID, Sales FROM Sales.Orders; Filtering Tip 4: Create nonclustered index on frequently used columns SELECT * FROM Sales.Orders WHERE OrderStatus = 'Delivered'; CREATE NONCLUSTERED INDEX Idx_Orders_OrderStatus ON Sales.Orders(OrderStatus); Tip 5: Avoid applying functions in WHERE -- Bad SELECT * FROM Sales.Orders WHERE LOWER(OrderStatus) = 'delivered'; -- Good SELECT * FROM Sales.Orders WHERE OrderStatus = 'Delivered'; -- Bad SELECT * FROM Sales.Customers WHERE SUBSTRING(FirstName, 1, 1) = 'A'; -- Good SELECT * FROM Sales.Customers WHERE FirstName LIKE 'A%'; -- Bad SELECT * FROM Sales.Orders WHERE YEAR(OrderDate) = 2025; -- Good SELECT * FROM Sales.Orders WHERE OrderDate BETWEEN '2025-01-01' AND '2025-12-31'; Tip 6: Avoid leading wildcards -- Bad SELECT * FROM Sales.Customers WHERE LastName LIKE '%Gold%'; -- Good SELECT * FROM Sales.Customers WHERE LastName LIKE 'Gold%'; Tip 7: Use IN instead of multiple OR -- Bad SELECT * FROM Sales.Orders WHERE CustomerID = 1 OR CustomerID = 2 OR CustomerID = 3; -- Good SELECT * FROM Sales.Orders WHERE CustomerID IN (1, 2, 3); Joins Tip 8: Use INNER JOIN when possible -- Best SELECT c.FirstName, o.OrderID FROM Sales.Customers c INNER JOIN Sales.Orders o ON c.CustomerID = o.CustomerID; -- Slower SELECT c.FirstName, o.OrderID FROM Sales.Customers c LEFT JOIN Sales.Orders o ON c.CustomerID = o.CustomerID; -- Worst SELECT c.FirstName, o.OrderID FROM Sales.Customers c OUTER JOIN Sales.Orders o ON c.CustomerID = o.CustomerID; Tip 9: Use explicit (ANSI) joins -- Bad SELECT o.OrderID, c.FirstName FROM Sales.Customers c, Sales.Orders o WHERE c.CustomerID = o.CustomerID; -- Good SELECT o.OrderID, c.FirstName FROM Sales.Customers c INNER JOIN Sales.Orders o ON c.CustomerID = o.CustomerID; Tip 10: Index columns used in ON clause SELECT c.FirstName, o.OrderID FROM Sales.Orders o INNER JOIN Sales.Customers c ON c.CustomerID = o.CustomerID; CREATE NONCLUSTERED INDEX IX_Orders_CustomerID ON Sales.Orders(CustomerID); Tip 11: Filter before joining big tables -- Best for big tables SELECT c.FirstName, o.OrderID FROM Sales.Customers c INNER JOIN ( SELECT OrderID, CustomerID FROM Sales.Orders WHERE OrderStatus = 'Delivered' ) o ON c.CustomerID = o.CustomerID; Tip 12: Aggregate before joining -- Best for big tables SELECT c.CustomerID, c.FirstName, o.OrderCount FROM Sales.Customers c INNER JOIN ( SELECT CustomerID, COUNT(OrderID) AS OrderCount FROM Sales.Orders GROUP BY CustomerID ) o ON c.CustomerID = o.CustomerID; Tip 13: Use UNION instead of OR in joins -- Bad SELECT o.OrderID, c.FirstName FROM Sales.Customers c INNER JOIN Sales.Orders o ON c.CustomerID = o.CustomerID OR c.CustomerID = o.SalesPersonID; -- Good SELECT o.OrderID, c.FirstName FROM Sales.Customers c INNER JOIN Sales.Orders o ON c.CustomerID = o.CustomerID UNION SELECT o.OrderID, c.FirstName FROM Sales.Customers c INNER JOIN Sales.Orders o ON c.CustomerID = o.SalesPersonID; Tip 14: Use SQL hints for optimization SELECT o.OrderID, c.FirstName FROM Sales.Customers c INNER JOIN Sales.Orders o ON c.CustomerID = o.CustomerID OPTION (HASH JOIN); Union Tip 15: Use UNION ALL when duplicates are acceptable -- Bad SELECT CustomerID FROM Sales.Orders UNION SELECT CustomerID FROM Sales.OrdersArchive; -- Good SELECT CustomerID FROM Sales.Orders UNION ALL SELECT CustomerID FROM Sales.OrdersArchive; Tip 16: Use UNION ALL + DISTINCT when duplicates not allowed SELECT DISTINCT CustomerID FROM ( SELECT CustomerID FROM Sales.Orders UNION ALL SELECT CustomerID FROM Sales.OrdersArchive ) CombinedData; Aggregations Tip 17: Use columnstore indexes SELECT CustomerID, COUNT(OrderID) AS OrderCount FROM Sales.Orders GROUP BY CustomerID; CREATE CLUSTERED COLUMNSTORE INDEX Idx_Orders_Columnstore ON Sales.Orders; Tip 18: Pre-aggregate data SELECT MONTH(OrderDate) AS OrderYear, SUM(Sales) AS TotalSales INTO Sales.SalesSummary FROM Sales.Orders GROUP BY MONTH(OrderDate); SELECT OrderYear, TotalSales FROM Sales.SalesSummary; Subqueries \u0026 CTE Tip 19: Prefer JOIN or EXISTS over IN -- Good SELECT o.OrderID, o.Sales FROM Sales.Orders o WHERE EXISTS ( SELECT 1 FROM Sales.Customers c WHERE c.CustomerID = o.CustomerID AND c.Country = 'USA' ); -- Bad SELECT o.OrderID, o.Sales FROM Sales.Orders o WHERE o.CustomerID IN ( SELECT CustomerID FROM Sales.Customers WHERE Country = 'USA' ); Tip 20: Avoid redundant logic -- Good SELECT EmployeeID, FirstName, CASE WHEN Salary \u003e AVG(Salary) OVER () THEN 'Above Average' WHEN Salary \u003c AVG(Salary) OVER () THEN 'Below Average' ELSE 'Average' END AS Status FROM Sales.Employees; DDL Tip 21: Avoid VARCHAR(MAX) unless necessary. Tip 22: Avoid overly large lengths. Tip 23: Use NOT NULL when possible. Tip 24: All tables should have a clustered primary key. Tip 25: Create nonclustered indexes on foreign keys when frequently used. -- Good Practice CREATE TABLE CustomersInfo ( CustomerID INT PRIMARY KEY CLUSTERED, FirstName VARCHAR(50) NOT NULL, LastName VARCHAR(50) NOT NULL, Country VARCHAR(50) NOT NULL, TotalPurchases FLOAT, Score INT, BirthDate DATE, EmployeeID INT, CONSTRAINT FK_CustomersInfo_EmployeeID FOREIGN KEY (EmployeeID) REFERENCES Sales.Employees(EmployeeID) ); CREATE NONCLUSTERED INDEX IX_CustomersInfo_EmployeeID ON CustomersInfo(EmployeeID); Indexing Tip 26: Avoid over-indexing (slows down writes). Tip 27: Drop unused indexes regularly. Tip 28: Update table statistics weekly. Tip 29: Reorganize/rebuild fragmented indexes weekly. Tip 30: For very large tables, partition + columnstore index.",
    "description": "This guide demonstrates best practices for:\nFetching data Filtering Joins UNION Aggregations Subqueries/CTE DDL Indexing It covers techniques such as selecting only necessary columns, proper filtering methods, explicit joins, avoiding redundant logic, and efficient indexing strategies.\nFetching Data Tip 1: Select Only What You Need -- Bad Practice SELECT * FROM Sales.Customers; -- Good Practice SELECT CustomerID, FirstName, LastName FROM Sales.Customers; Tip 2: Avoid unnecessary DISTINCT \u0026 ORDER BY -- Bad Practice SELECT DISTINCT FirstName FROM Sales.Customers ORDER BY FirstName; -- Good Practice SELECT FirstName FROM Sales.Customers; Tip 3: For Exploration Purpose, Limit Rows! -- Bad Practice SELECT OrderID, Sales FROM Sales.Orders; -- Good Practice SELECT TOP 10 OrderID, Sales FROM Sales.Orders; Filtering Tip 4: Create nonclustered index on frequently used columns SELECT * FROM Sales.Orders WHERE OrderStatus = 'Delivered'; CREATE NONCLUSTERED INDEX Idx_Orders_OrderStatus ON Sales.Orders(OrderStatus); Tip 5: Avoid applying functions in WHERE -- Bad SELECT * FROM Sales.Orders WHERE LOWER(OrderStatus) = 'delivered'; -- Good SELECT * FROM Sales.Orders WHERE OrderStatus = 'Delivered'; -- Bad SELECT * FROM Sales.Customers WHERE SUBSTRING(FirstName, 1, 1) = 'A'; -- Good SELECT * FROM Sales.Customers WHERE FirstName LIKE 'A%'; -- Bad SELECT * FROM Sales.Orders WHERE YEAR(OrderDate) = 2025; -- Good SELECT * FROM Sales.Orders WHERE OrderDate BETWEEN '2025-01-01' AND '2025-12-31'; Tip 6: Avoid leading wildcards -- Bad SELECT * FROM Sales.Customers WHERE LastName LIKE '%Gold%'; -- Good SELECT * FROM Sales.Customers WHERE LastName LIKE 'Gold%'; Tip 7: Use IN instead of multiple OR -- Bad SELECT * FROM Sales.Orders WHERE CustomerID = 1 OR CustomerID = 2 OR CustomerID = 3; -- Good SELECT * FROM Sales.Orders WHERE CustomerID IN (1, 2, 3); Joins Tip 8: Use INNER JOIN when possible -- Best SELECT c.FirstName, o.OrderID FROM Sales.Customers c INNER JOIN Sales.Orders o ON c.CustomerID = o.CustomerID; -- Slower SELECT c.FirstName, o.OrderID FROM Sales.Customers c LEFT JOIN Sales.Orders o ON c.CustomerID = o.CustomerID; -- Worst SELECT c.FirstName, o.OrderID FROM Sales.Customers c OUTER JOIN Sales.Orders o ON c.CustomerID = o.CustomerID; Tip 9: Use explicit (ANSI) joins -- Bad SELECT o.OrderID, c.FirstName FROM Sales.Customers c, Sales.Orders o WHERE c.CustomerID = o.CustomerID; -- Good SELECT o.OrderID, c.FirstName FROM Sales.Customers c INNER JOIN Sales.Orders o ON c.CustomerID = o.CustomerID; Tip 10: Index columns used in ON clause SELECT c.FirstName, o.OrderID FROM Sales.Orders o INNER JOIN Sales.Customers c ON c.CustomerID = o.CustomerID; CREATE NONCLUSTERED INDEX IX_Orders_CustomerID ON Sales.Orders(CustomerID); Tip 11: Filter before joining big tables -- Best for big tables SELECT c.FirstName, o.OrderID FROM Sales.Customers c INNER JOIN ( SELECT OrderID, CustomerID FROM Sales.Orders WHERE OrderStatus = 'Delivered' ) o ON c.CustomerID = o.CustomerID; Tip 12: Aggregate before joining -- Best for big tables SELECT c.CustomerID, c.FirstName, o.OrderCount FROM Sales.Customers c INNER JOIN ( SELECT CustomerID, COUNT(OrderID) AS OrderCount FROM Sales.Orders GROUP BY CustomerID ) o ON c.CustomerID = o.CustomerID; Tip 13: Use UNION instead of OR in joins -- Bad SELECT o.OrderID, c.FirstName FROM Sales.Customers c INNER JOIN Sales.Orders o ON c.CustomerID = o.CustomerID OR c.CustomerID = o.SalesPersonID; -- Good SELECT o.OrderID, c.FirstName FROM Sales.Customers c INNER JOIN Sales.Orders o ON c.CustomerID = o.CustomerID UNION SELECT o.OrderID, c.FirstName FROM Sales.Customers c INNER JOIN Sales.Orders o ON c.CustomerID = o.SalesPersonID; Tip 14: Use SQL hints for optimization SELECT o.OrderID, c.FirstName FROM Sales.Customers c INNER JOIN Sales.Orders o ON c.CustomerID = o.CustomerID OPTION (HASH JOIN); Union Tip 15: Use UNION ALL when duplicates are acceptable -- Bad SELECT CustomerID FROM Sales.Orders UNION SELECT CustomerID FROM Sales.OrdersArchive; -- Good SELECT CustomerID FROM Sales.Orders UNION ALL SELECT CustomerID FROM Sales.OrdersArchive; Tip 16: Use UNION ALL + DISTINCT when duplicates not allowed SELECT DISTINCT CustomerID FROM ( SELECT CustomerID FROM Sales.Orders UNION ALL SELECT CustomerID FROM Sales.OrdersArchive ) CombinedData; Aggregations Tip 17: Use columnstore indexes SELECT CustomerID, COUNT(OrderID) AS OrderCount FROM Sales.Orders GROUP BY CustomerID; CREATE CLUSTERED COLUMNSTORE INDEX Idx_Orders_Columnstore ON Sales.Orders; Tip 18: Pre-aggregate data SELECT MONTH(OrderDate) AS OrderYear, SUM(Sales) AS TotalSales INTO Sales.SalesSummary FROM Sales.Orders GROUP BY MONTH(OrderDate); SELECT OrderYear, TotalSales FROM Sales.SalesSummary; Subqueries \u0026 CTE Tip 19: Prefer JOIN or EXISTS over IN -- Good SELECT o.OrderID, o.Sales FROM Sales.Orders o WHERE EXISTS ( SELECT 1 FROM Sales.Customers c WHERE c.CustomerID = o.CustomerID AND c.Country = 'USA' ); -- Bad SELECT o.OrderID, o.Sales FROM Sales.Orders o WHERE o.CustomerID IN ( SELECT CustomerID FROM Sales.Customers WHERE Country = 'USA' ); Tip 20: Avoid redundant logic -- Good SELECT EmployeeID, FirstName, CASE WHEN Salary \u003e AVG(Salary) OVER () THEN 'Above Average' WHEN Salary \u003c AVG(Salary) OVER () THEN 'Below Average' ELSE 'Average' END AS Status FROM Sales.Employees; DDL Tip 21: Avoid VARCHAR(MAX) unless necessary. Tip 22: Avoid overly large lengths. Tip 23: Use NOT NULL when possible. Tip 24: All tables should have a clustered primary key. Tip 25: Create nonclustered indexes on foreign keys when frequently used. -- Good Practice CREATE TABLE CustomersInfo ( CustomerID INT PRIMARY KEY CLUSTERED, FirstName VARCHAR(50) NOT NULL, LastName VARCHAR(50) NOT NULL, Country VARCHAR(50) NOT NULL, TotalPurchases FLOAT, Score INT, BirthDate DATE, EmployeeID INT, CONSTRAINT FK_CustomersInfo_EmployeeID FOREIGN KEY (EmployeeID) REFERENCES Sales.Employees(EmployeeID) ); CREATE NONCLUSTERED INDEX IX_CustomersInfo_EmployeeID ON CustomersInfo(EmployeeID); Indexing Tip 26: Avoid over-indexing (slows down writes). Tip 27: Drop unused indexes regularly. Tip 28: Update table statistics weekly. Tip 29: Reorganize/rebuild fragmented indexes weekly. Tip 30: For very large tables, partition + columnstore index.",
    "tags": [],
    "title": "Performance Tips",
    "uri": "/sql/performance-optimization/"
  },
  {
    "breadcrumb": "SQL",
    "content": "SQL AI Prompts for SQL This document contains a series of AI-powered prompts designed to help SQL developers and learners improve skills in writing, optimizing, and understanding SQL queries.\nThe prompts cover tasks, readability, performance, debugging, interview/exam prep, and more.\nEach section provides clear instructions and sample code to support self-learning and real-world application.\nTable of Contents Solve an SQL Task Improve the Readability Optimize the Performance Query Optimize Execution Plan Debugging Explain the Result Styling \u0026 Formatting Documentations \u0026 Comments Improve Database DDL Generate Test Dataset Create SQL Course Understand SQL Concept Comparing SQL Concepts SQL Questions with Options Prepare for a SQL Interview Prepare for a SQL Exam 1. Solve an SQL Task Prompt:\nIn my SQL Server database, we have two tables:\norders(order_id, sales, customer_id, product_id) customers(customer_id, first_name, last_name, country) Tasks:\nWrite a query to rank customers based on their sales. Result must include: customer_id, full name, country, total sales, rank. Write 3 different versions of the query. Include comments (avoid obvious ones). Evaluate which version is best in terms of readability and performance. 2. Improve the Readability The following SQL Server query is long and hard to understand.\nTask:\nImprove its readability. Remove any redundancy and consolidate it. Include comments (avoid obvious ones). Explain each improvement. Original Query:\nWITH CTE_Total_Sales_By_Customer AS ( SELECT c.CustomerID, c.FirstName + ' ' + c.LastName AS FullName, SUM(o.Sales) AS TotalSales FROM Sales.Customers c INNER JOIN Sales.Orders o ON c.CustomerID = o.CustomerID GROUP BY c.CustomerID, c.FirstName, c.LastName ),CTE_Highest_Order_Product AS ( SELECT o.CustomerID, p.Product, ROW_NUMBER() OVER (PARTITION BY o.CustomerID ORDER BY o.Sales DESC) AS rn FROM Sales.Orders o INNER JOIN Sales.Products p ON o.ProductID = p.ProductID ), CTE_Highest_Category AS ( SELECT o.CustomerID, p.Category, ROW_NUMBER() OVER (PARTITION BY o.CustomerID ORDER BY SUM(o.Sales) DESC) AS rn FROM Sales.Orders o INNER JOIN Sales.Products p ON o.ProductID = p.ProductID GROUP BY o.CustomerID, p.Category ), CTE_Last_Order_Date AS ( SELECT CustomerID, MAX(OrderDate) AS LastOrderDate FROM Sales.Orders GROUP BY CustomerID ), CTE_Total_Discounts_By_Customer AS ( SELECT o.CustomerID, SUM(o.Quantity * p.Price * 0.1) AS TotalDiscounts FROM Sales.Orders o INNER JOIN Sales.Products p ON o.ProductID = p.ProductID GROUP BY o.CustomerID ) SELECT ts.CustomerID, ts.FullName, ts.TotalSales,hop.Product AS HighestOrderProduct,hc.Category AS HighestCategory, lod.LastOrderDate, td.TotalDiscounts FROM CTE_Total_Sales_By_Customer ts LEFT JOIN (SELECT CustomerID, Product FROM CTE_Highest_Order_Product WHERE rn = 1) hop ON ts.CustomerID = hop.CustomerID LEFT JOIN (SELECT CustomerID, Category FROM CTE_Highest_Category WHERE rn = 1) hc ON ts.CustomerID = hc.CustomerID LEFT JOIN CTE_Last_Order_Date lod ON ts.CustomerID = lod.CustomerID LEFT JOIN CTE_Total_Discounts_By_Customer td ON ts.CustomerID = td.CustomerID WHERE ts.TotalSales \u003e 0 ORDER BY ts.TotalSales DESC 3. Optimize the Performance Query The following query is slow.\nTask:\nPropose optimizations.\nProvide an improved query.\nExplain each improvement.\nOriginal Query:\nSELECT o.OrderID, o.CustomerID, c.FirstName AS CustomerFirstName, (SELECT COUNT(o2.OrderID) FROM Sales.Orders o2 WHERE o2.CustomerID = c.CustomerID) AS OrderCount FROM Sales.Orders o LEFT JOIN Sales.Customers c ON o.CustomerID = c.CustomerID WHERE LOWER(o.OrderStatus) = 'delivered' OR YEAR(o.OrderDate) = 2025 OR o.CustomerID =1 OR o.CustomerID =2 OR o.CustomerID =3 OR o.CustomerID IN ( SELECT CustomerID FROM Sales.Customers WHERE Country LIKE '%USA%' ) 4. Optimize Execution Plan Task:\nDescribe the execution plan step by step.\nIdentify performance bottlenecks.\nSuggest optimizations.\n5. Debugging The following query causes an error: Msg 8120, Level 16, State 1, Line 5\nTask:\nExplain the error.\nFind the root cause.\nSuggest a fix.\nSELECT C.CustomerID, C.Country, SUM(O.Sales) AS TotalSales, RANK() OVER (PARTITION BY C.Country ORDER BY O.Sales DESC) AS RankInCountry FROM Sales.Customers C LEFT JOIN Sales.Orders O ON C.CustomerID = O.CustomerID GROUP BY C.CustomerID, C.Country 6. Explain the Result Query:\nWITH Series AS ( -- Anchor Query SELECT 1 AS MyNumber UNION ALL -- Recursive Query SELECT MyNumber + 1 FROM Series WHERE MyNumber \u003c 20 ) SELECT * FROM Series Task:\nBreak down how SQL processes this query step by step.\nExplain how the result is formed.\n7. Styling \u0026 Formatting The following query is hard to read.\nTask:\nRestyle for readability.\nAlign column aliases.\nKeep compact.\nOriginal Query:\nwith CTE_Total_Sales as (Select CustomerID, sum(Sales) as TotalSales from Sales.Orders group by CustomerID), cte_customer_segments as (SELECT CustomerID, case when TotalSales \u003e 100 then 'High Value' when TotalSales between 50 and 100 then 'Medium Value' else 'Low Value' end as CustomerSegment from CTE_Total_Sales) select c.CustomerID, c.FirstName, c.LastName, cts.TotalSales, ccs.CustomerSegment FROM sales.customers c left join CTE_Total_Sales cts ON cts.CustomerID = c.CustomerID left JOIN cte_customer_segments ccs ON ccs.CustomerID = c.CustomerID 8. Documentations \u0026 Comments The following query lacks comments.\nTask:\nAdd a leading comment.\nInsert clarifying comments where needed.\nCreate two documents:\nBusiness rules implemented.\nHow the query works.\nWITH CTE_Total_Sales AS ( SELECT CustomerID, SUM(Sales) AS TotalSales FROM Sales.Orders GROUP BY CustomerID ), CTE_Customer_Segements AS ( SELECT CustomerID, CASE WHEN TotalSales \u003e 100 THEN 'High Value' WHEN TotalSales BETWEEN 50 AND 100 THEN 'Medium Value' ELSE 'Low Value' END CustomerSegment FROM CTE_Total_Sales ) SELECT c.CustomerID, c.FirstName, c.LastName, cts.TotalSales, ccs.CustomerSegment FROM Sales.Customers c LEFT JOIN CTE_Total_Sales cts ON cts.CustomerID = c.CustomerID LEFT JOIN CTE_Customer_Segements ccs ON ccs.CustomerID = c.CustomerID 9. Improve Database DDL Task:\nCheck naming consistency.\nOptimize data types.\nVerify PK/FK integrity.\nReview indexes.\nEnsure normalization.\n10. Generate Test Dataset Task:\nGenerate realistic test dataset with INSERT statements.\nKeep it small.\nEnsure valid PK/FK relationships.\nAvoid NULL.\n11. Create SQL Course Task:\nBuild a beginner-to-advanced SQL roadmap.\nInclude analytics-focused topics.\nUse real-world scenarios.\n12. Understand SQL Concept Task:\nExplain SQL Window Functions.\nProvide an analogy.\nDescribe when/why to use them.\nShow syntax \u0026 examples.\nList top 3 use cases.\n13. Comparing SQL Concepts Task:\nCompare Window Functions vs GROUP BY.\nExplain key differences.\nShow examples.\nProvide pros/cons.\nSummarize in a side-by-side table.\n14. SQL Questions with Options Task:\nAct as SQL trainer.\nProvide sample dataset.\nCreate progressive tasks.\nSimulate SQL Server results.\nReview solutions \u0026 suggest improvements.\n15. Prepare for a SQL Interview Task:\nAct as interviewer.\nAsk common SQL questions.\nProgress to advanced topics.\nEvaluate answers \u0026 give feedback.\n16. Prepare for a SQL Exam Task:\nAsk SQL exam-style questions.\nProgress gradually.\nEvaluate answers \u0026 give feedback.",
    "description": "SQL AI Prompts for SQL This document contains a series of AI-powered prompts designed to help SQL developers and learners improve skills in writing, optimizing, and understanding SQL queries.\nThe prompts cover tasks, readability, performance, debugging, interview/exam prep, and more.\nEach section provides clear instructions and sample code to support self-learning and real-world application.\nTable of Contents Solve an SQL Task Improve the Readability Optimize the Performance Query Optimize Execution Plan Debugging Explain the Result Styling \u0026 Formatting Documentations \u0026 Comments Improve Database DDL Generate Test Dataset Create SQL Course Understand SQL Concept Comparing SQL Concepts SQL Questions with Options Prepare for a SQL Interview Prepare for a SQL Exam 1. Solve an SQL Task Prompt:\nIn my SQL Server database, we have two tables:",
    "tags": [],
    "title": "AI and SQL",
    "uri": "/sql/ai-and-sql/"
  },
  {
    "breadcrumb": "",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/categories/"
  },
  {
    "breadcrumb": "",
    "content": "Contents Azure DataBricks SQL Azure Data Factory",
    "description": "Contents Azure DataBricks SQL Azure Data Factory",
    "tags": [],
    "title": "Data Engineering Notes",
    "uri": "/"
  },
  {
    "breadcrumb": "",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/tags/"
  }
]
