<!DOCTYPE html>
<html lang="en-us" dir="ltr" itemscope itemtype="http://schema.org/Article" data-r-output-format="html">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta name="viewport" content="height=device-height, width=device-width, initial-scale=1.0, minimum-scale=1.0">
    <meta name="generator" content="Hugo 0.148.2">
    <meta name="generator" content="Relearn 8.0.0+9803d5122ebb3276acea823f476e9eb44f607862">
    <meta name="description" content="Hadoop vs. Spark Architecture Aspect Hadoop Spark Storage Uses HDFS for storage Uses in-memory processing for speed Processing MapReduce is disk-based In-memory processing improves performance Integration Runs independently or with Hadoop ecosystem Can run on top of Hadoop; more flexible Complexity More complex setup and deployment Simpler to deploy and configure Performance Slower for iterative tasks due to disk I/O Better performance for iterative tasks RDD vs. DataFrame vs. Dataset Aspect RDD DataFrame Dataset API Level Low-level, more control High-level, optimized with Catalyst High-level, type-safe Schema No schema, unstructured Uses schema for structured data Strongly typed, compile-time type safety Optimization No built-in optimization Optimized using Catalyst Optimized using Catalyst, with type safety Type Safety No type safety No compile-time type safety Provides compile-time type safety Performance Less optimized for performance Better performance due to optimizations Combines type safety with optimization Action vs. Transformation in Spark Aspect Action Transformation Execution Triggers execution of the Spark job Builds up a logical plan of data operations Return Type Returns results or output Returns a new RDD/DataFrame Evaluation Eager evaluation; executes immediately Lazy evaluation; executed when an action is triggered Computation Involves actual computation (e.g., collect()) Defines data transformations (e.g., map()) Performance Can cause data processing; affects performance Does not affect performance until an action is called Map vs. FlatMap Aspect Map FlatMap Output Returns one output element per input element Can return zero or more output elements per input Flattening Does not flatten output Flattens the output into a single level Use Case Suitable for one-to-one transformations Suitable for one-to-many transformations Complexity Simpler, straightforward More complex due to variable number of outputs Examples map(x =&gt; x * 2) flatMap(x =&gt; x.split(&#34; &#34;)) GroupByKey vs. ReduceByKey Aspect GroupByKey ReduceByKey Operation Groups all values by key Aggregates values with the same key Efficiency Can lead to high shuffling More efficient due to partial aggregation Data Movement Requires shuffling of all values Minimizes data movement through local aggregation Use Case Useful for simple grouping Preferred for aggregations and reductions Performance Less efficient with large datasets Better performance for large datasets Repartition vs. Coalesce Aspect Repartition Coalesce Partitioning Can increase or decrease the number of partitions Only decreases the number of partitions Shuffling Involves full shuffle Avoids full shuffle, more efficient Efficiency More expensive due to shuffling More efficient for reducing partitions Use Case Used for increasing partitions or balancing load Used for reducing partitions, typically after filtering Performance Can be costly for large datasets More cost-effective for reducing partitions Cache vs. Persist Aspect Cache Persist Storage Level Defaults to MEMORY_ONLY Can use various storage levels (e.g., MEMORY_AND_DISK) Flexibility Simplified, with default storage level Offers more options for storage levels Use Case Suitable for simple caching scenarios Suitable for complex caching scenarios requiring different storage levels Implementation Easier to use, shorthand for MEMORY_ONLY More flexible, allows custom storage options Performance Suitable when memory suffices More efficient when dealing with larger datasets and limited memory Narrow vs. Wide Transformation Aspect Narrow Transformation Wide Transformation Partitioning Each parent partition is used by one child partition Requires data from multiple partitions Shuffling No shuffling required Involves shuffling of data Performance More efficient and less costly Less efficient due to data movement Examples map(), filter() groupByKey(), join() Complexity Simpler and faster More complex and slower due to data movement Collect vs. Take Aspect Collect Take Output Retrieves all data from the RDD/DataFrame Retrieves a specified number of elements Memory Usage Can be expensive and use a lot of memory More memory-efficient Use Case Used when you need the entire dataset Useful for sampling or debugging Performance Can cause performance issues with large data Faster and more controlled Action Type Triggers full data retrieval Triggers partial data retrieval Broadcast Variable vs. Accumulator Aspect Broadcast Variable Accumulator Purpose Efficiently shares read-only data across tasks Tracks metrics and aggregates values Data Type Data that is shared and read-only Counters and sums, often numerical Use Case Useful for large lookup tables or configurations Useful for aggregating metrics like counts Efficiency Reduces data transfer by broadcasting data once Efficient for aggregating values across tasks Mutability Immutable, read-only Mutable, can be updated during computation Spark SQL vs. DataFrame API Aspect Spark SQL DataFrame API Interface Executes SQL queries Provides a programmatic interface Syntax Uses SQL-like syntax Uses function-based syntax Optimization Optimized with Catalyst Optimized with Catalyst Spark Streaming vs. Structured Streaming Aspect Spark Streaming Structured Streaming Processing Micro-batch processing Micro-batch and continuous processing API RDD-based API SQL-based API with DataFrame/Dataset support Complexity More complex and lower-level Simplified with high-level APIs Consistency Can be less consistent due to micro-batches Provides stronger consistency guarantees Performance Can be slower for complex queries Better performance with optimizations Shuffle vs. MapReduce Aspect Shuffle MapReduce Operation Data reorganization across partitions Data processing model for distributed computing Efficiency Can be costly due to data movement Designed for batch processing with high I/O Performance Affects performance based on the amount of data movement Optimized for large-scale data processing but less efficient for iterative tasks Use Case Used in Spark for data redistribution Used in Hadoop for data processing tasks Implementation Integrated into Spark operations Core component of the Hadoop ecosystem Union vs. Join Aspect Union Join Operation Combines two DataFrames/RDDs into one Combines rows from two DataFrames/RDDs based on a key Data Requirements Requires same schema for both DataFrames/RDDs Requires a common key for joining Performance Generally faster as it does not require key matching Can be slower due to key matching and shuffling Output Stacks data vertically Merges data horizontally based on keys Use Case Appending data or combining datasets Merging related data based on keys Executor vs. Driver Aspect Executor Driver Role Executes tasks and processes data Coordinates and manages the Spark application Memory Memory allocated per executor for data processing Memory used for managing application execution Lifecycle Exists throughout the application execution Starts and stops the Spark application Tasks Runs the tasks assigned by the driver Schedules and coordinates tasks and jobs Parallelism Multiple executors run in parallel Single driver coordinates multiple executors Checkpointing vs. Caching Aspect Checkpointing Caching Purpose Provides fault tolerance and reliability Improves performance by storing intermediate data Storage Writes data to stable storage (e.g., HDFS) Stores data in memory or on disk (depends on storage level) Use Case Used for recovery in case of failures Used for optimizing repeated operations Impact Can be more costly and slow Generally faster but not suitable for fault tolerance Data Data is written to external storage Data is kept in memory or disk storage for quick access ReduceByKey vs. AggregateByKey Aspect ReduceByKey AggregateByKey Operation Combines values with the same key using a function Performs custom aggregation and combinatory operations Efficiency More efficient for simple aggregations Flexible for complex aggregation scenarios Shuffling Involves shuffling but can be optimized Can be more complex due to custom aggregation Use Case Suitable for straightforward aggregations Ideal for advanced and custom aggregations Performance Generally faster for simple operations Performance varies with complexity SQLContext vs. HiveContext vs. SparkSession Aspect SQLContext HiveContext SparkSession Purpose Provides SQL query capabilities Provides integration with Hive for SQL queries Unified entry point for Spark functionality Integration Basic SQL capabilities Integrates with Hive Metastore Combines SQL, DataFrame, and Streaming APIs Usage Legacy, less functionality Supports HiveQL and Hive UDFs Supports all Spark functionalities including Hive Configuration Less flexible and older Requires Hive setup and configuration Modern and flexible, manages configurations Capabilities Limited to SQL queries Extends SQL capabilities with Hive integration Comprehensive access to all Spark features Broadcast Join vs. Shuffle Join Aspect Broadcast Join Shuffle Join Operation Broadcasts a small dataset to all nodes Shuffles data across nodes for joining Data Size Suitable for small datasets Suitable for larger datasets Efficiency More efficient for small tables More suited for large datasets Performance Faster due to reduced shuffling Can be slower due to extensive shuffling Use Case Use when one dataset is small relative to others Use when both datasets are large SparkContext vs. SparkSession Aspect SparkContext SparkSession Purpose Entry point for Spark functionality Unified entry point for Spark functionalities Lifecycle Created before Spark jobs start Manages the Spark application lifecycle Functionality Provides access to RDD and basic Spark functionality Provides access to RDD, DataFrame, SQL, and Streaming APIs Configuration Configuration is less flexible More flexible and easier to configure Usage Older, used for legacy applications Modern and recommended for new applications Structured Streaming vs. Spark Streaming Aspect Structured Streaming Spark Streaming Processing Micro-batch and continuous processing Micro-batch processing API SQL-based API with DataFrame/Dataset support RDD-based API Complexity Simplified and high-level More complex and low-level Consistency Provides stronger consistency guarantees Can be less consistent due to micro-batches Performance Better performance with built-in optimizations Can be slower for complex queries Partitioning vs. Bucketing Aspect Partitioning Bucketing Purpose Divides data into multiple partitions based on a key Divides data into buckets based on a hash function Usage Used to optimize queries by reducing data scanned Used to improve join performance and maintain sorted data Shuffling Reduces shuffling by placing related data together Reduces shuffle during joins and aggregations Data Layout Data is physically separated based on partition key Data is organized into fixed-size buckets Performance Improves performance for queries involving partition keys Enhances performance for join operations">
    <meta name="author" content="">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Comparisons :: Data Engineering Notes">
    <meta name="twitter:description" content="Hadoop vs. Spark Architecture Aspect Hadoop Spark Storage Uses HDFS for storage Uses in-memory processing for speed Processing MapReduce is disk-based In-memory processing improves performance Integration Runs independently or with Hadoop ecosystem Can run on top of Hadoop; more flexible Complexity More complex setup and deployment Simpler to deploy and configure Performance Slower for iterative tasks due to disk I/O Better performance for iterative tasks RDD vs. DataFrame vs. Dataset Aspect RDD DataFrame Dataset API Level Low-level, more control High-level, optimized with Catalyst High-level, type-safe Schema No schema, unstructured Uses schema for structured data Strongly typed, compile-time type safety Optimization No built-in optimization Optimized using Catalyst Optimized using Catalyst, with type safety Type Safety No type safety No compile-time type safety Provides compile-time type safety Performance Less optimized for performance Better performance due to optimizations Combines type safety with optimization Action vs. Transformation in Spark Aspect Action Transformation Execution Triggers execution of the Spark job Builds up a logical plan of data operations Return Type Returns results or output Returns a new RDD/DataFrame Evaluation Eager evaluation; executes immediately Lazy evaluation; executed when an action is triggered Computation Involves actual computation (e.g., collect()) Defines data transformations (e.g., map()) Performance Can cause data processing; affects performance Does not affect performance until an action is called Map vs. FlatMap Aspect Map FlatMap Output Returns one output element per input element Can return zero or more output elements per input Flattening Does not flatten output Flattens the output into a single level Use Case Suitable for one-to-one transformations Suitable for one-to-many transformations Complexity Simpler, straightforward More complex due to variable number of outputs Examples map(x =&gt; x * 2) flatMap(x =&gt; x.split(&#34; &#34;)) GroupByKey vs. ReduceByKey Aspect GroupByKey ReduceByKey Operation Groups all values by key Aggregates values with the same key Efficiency Can lead to high shuffling More efficient due to partial aggregation Data Movement Requires shuffling of all values Minimizes data movement through local aggregation Use Case Useful for simple grouping Preferred for aggregations and reductions Performance Less efficient with large datasets Better performance for large datasets Repartition vs. Coalesce Aspect Repartition Coalesce Partitioning Can increase or decrease the number of partitions Only decreases the number of partitions Shuffling Involves full shuffle Avoids full shuffle, more efficient Efficiency More expensive due to shuffling More efficient for reducing partitions Use Case Used for increasing partitions or balancing load Used for reducing partitions, typically after filtering Performance Can be costly for large datasets More cost-effective for reducing partitions Cache vs. Persist Aspect Cache Persist Storage Level Defaults to MEMORY_ONLY Can use various storage levels (e.g., MEMORY_AND_DISK) Flexibility Simplified, with default storage level Offers more options for storage levels Use Case Suitable for simple caching scenarios Suitable for complex caching scenarios requiring different storage levels Implementation Easier to use, shorthand for MEMORY_ONLY More flexible, allows custom storage options Performance Suitable when memory suffices More efficient when dealing with larger datasets and limited memory Narrow vs. Wide Transformation Aspect Narrow Transformation Wide Transformation Partitioning Each parent partition is used by one child partition Requires data from multiple partitions Shuffling No shuffling required Involves shuffling of data Performance More efficient and less costly Less efficient due to data movement Examples map(), filter() groupByKey(), join() Complexity Simpler and faster More complex and slower due to data movement Collect vs. Take Aspect Collect Take Output Retrieves all data from the RDD/DataFrame Retrieves a specified number of elements Memory Usage Can be expensive and use a lot of memory More memory-efficient Use Case Used when you need the entire dataset Useful for sampling or debugging Performance Can cause performance issues with large data Faster and more controlled Action Type Triggers full data retrieval Triggers partial data retrieval Broadcast Variable vs. Accumulator Aspect Broadcast Variable Accumulator Purpose Efficiently shares read-only data across tasks Tracks metrics and aggregates values Data Type Data that is shared and read-only Counters and sums, often numerical Use Case Useful for large lookup tables or configurations Useful for aggregating metrics like counts Efficiency Reduces data transfer by broadcasting data once Efficient for aggregating values across tasks Mutability Immutable, read-only Mutable, can be updated during computation Spark SQL vs. DataFrame API Aspect Spark SQL DataFrame API Interface Executes SQL queries Provides a programmatic interface Syntax Uses SQL-like syntax Uses function-based syntax Optimization Optimized with Catalyst Optimized with Catalyst Spark Streaming vs. Structured Streaming Aspect Spark Streaming Structured Streaming Processing Micro-batch processing Micro-batch and continuous processing API RDD-based API SQL-based API with DataFrame/Dataset support Complexity More complex and lower-level Simplified with high-level APIs Consistency Can be less consistent due to micro-batches Provides stronger consistency guarantees Performance Can be slower for complex queries Better performance with optimizations Shuffle vs. MapReduce Aspect Shuffle MapReduce Operation Data reorganization across partitions Data processing model for distributed computing Efficiency Can be costly due to data movement Designed for batch processing with high I/O Performance Affects performance based on the amount of data movement Optimized for large-scale data processing but less efficient for iterative tasks Use Case Used in Spark for data redistribution Used in Hadoop for data processing tasks Implementation Integrated into Spark operations Core component of the Hadoop ecosystem Union vs. Join Aspect Union Join Operation Combines two DataFrames/RDDs into one Combines rows from two DataFrames/RDDs based on a key Data Requirements Requires same schema for both DataFrames/RDDs Requires a common key for joining Performance Generally faster as it does not require key matching Can be slower due to key matching and shuffling Output Stacks data vertically Merges data horizontally based on keys Use Case Appending data or combining datasets Merging related data based on keys Executor vs. Driver Aspect Executor Driver Role Executes tasks and processes data Coordinates and manages the Spark application Memory Memory allocated per executor for data processing Memory used for managing application execution Lifecycle Exists throughout the application execution Starts and stops the Spark application Tasks Runs the tasks assigned by the driver Schedules and coordinates tasks and jobs Parallelism Multiple executors run in parallel Single driver coordinates multiple executors Checkpointing vs. Caching Aspect Checkpointing Caching Purpose Provides fault tolerance and reliability Improves performance by storing intermediate data Storage Writes data to stable storage (e.g., HDFS) Stores data in memory or on disk (depends on storage level) Use Case Used for recovery in case of failures Used for optimizing repeated operations Impact Can be more costly and slow Generally faster but not suitable for fault tolerance Data Data is written to external storage Data is kept in memory or disk storage for quick access ReduceByKey vs. AggregateByKey Aspect ReduceByKey AggregateByKey Operation Combines values with the same key using a function Performs custom aggregation and combinatory operations Efficiency More efficient for simple aggregations Flexible for complex aggregation scenarios Shuffling Involves shuffling but can be optimized Can be more complex due to custom aggregation Use Case Suitable for straightforward aggregations Ideal for advanced and custom aggregations Performance Generally faster for simple operations Performance varies with complexity SQLContext vs. HiveContext vs. SparkSession Aspect SQLContext HiveContext SparkSession Purpose Provides SQL query capabilities Provides integration with Hive for SQL queries Unified entry point for Spark functionality Integration Basic SQL capabilities Integrates with Hive Metastore Combines SQL, DataFrame, and Streaming APIs Usage Legacy, less functionality Supports HiveQL and Hive UDFs Supports all Spark functionalities including Hive Configuration Less flexible and older Requires Hive setup and configuration Modern and flexible, manages configurations Capabilities Limited to SQL queries Extends SQL capabilities with Hive integration Comprehensive access to all Spark features Broadcast Join vs. Shuffle Join Aspect Broadcast Join Shuffle Join Operation Broadcasts a small dataset to all nodes Shuffles data across nodes for joining Data Size Suitable for small datasets Suitable for larger datasets Efficiency More efficient for small tables More suited for large datasets Performance Faster due to reduced shuffling Can be slower due to extensive shuffling Use Case Use when one dataset is small relative to others Use when both datasets are large SparkContext vs. SparkSession Aspect SparkContext SparkSession Purpose Entry point for Spark functionality Unified entry point for Spark functionalities Lifecycle Created before Spark jobs start Manages the Spark application lifecycle Functionality Provides access to RDD and basic Spark functionality Provides access to RDD, DataFrame, SQL, and Streaming APIs Configuration Configuration is less flexible More flexible and easier to configure Usage Older, used for legacy applications Modern and recommended for new applications Structured Streaming vs. Spark Streaming Aspect Structured Streaming Spark Streaming Processing Micro-batch and continuous processing Micro-batch processing API SQL-based API with DataFrame/Dataset support RDD-based API Complexity Simplified and high-level More complex and low-level Consistency Provides stronger consistency guarantees Can be less consistent due to micro-batches Performance Better performance with built-in optimizations Can be slower for complex queries Partitioning vs. Bucketing Aspect Partitioning Bucketing Purpose Divides data into multiple partitions based on a key Divides data into buckets based on a hash function Usage Used to optimize queries by reducing data scanned Used to improve join performance and maintain sorted data Shuffling Reduces shuffling by placing related data together Reduces shuffle during joins and aggregations Data Layout Data is physically separated based on partition key Data is organized into fixed-size buckets Performance Improves performance for queries involving partition keys Enhances performance for join operations">
    <meta property="og:url" content="http://localhost:1313/azure_data_bricks/comparisons/">
    <meta property="og:site_name" content="Data Engineering Notes">
    <meta property="og:title" content="Comparisons :: Data Engineering Notes">
    <meta property="og:description" content="Hadoop vs. Spark Architecture Aspect Hadoop Spark Storage Uses HDFS for storage Uses in-memory processing for speed Processing MapReduce is disk-based In-memory processing improves performance Integration Runs independently or with Hadoop ecosystem Can run on top of Hadoop; more flexible Complexity More complex setup and deployment Simpler to deploy and configure Performance Slower for iterative tasks due to disk I/O Better performance for iterative tasks RDD vs. DataFrame vs. Dataset Aspect RDD DataFrame Dataset API Level Low-level, more control High-level, optimized with Catalyst High-level, type-safe Schema No schema, unstructured Uses schema for structured data Strongly typed, compile-time type safety Optimization No built-in optimization Optimized using Catalyst Optimized using Catalyst, with type safety Type Safety No type safety No compile-time type safety Provides compile-time type safety Performance Less optimized for performance Better performance due to optimizations Combines type safety with optimization Action vs. Transformation in Spark Aspect Action Transformation Execution Triggers execution of the Spark job Builds up a logical plan of data operations Return Type Returns results or output Returns a new RDD/DataFrame Evaluation Eager evaluation; executes immediately Lazy evaluation; executed when an action is triggered Computation Involves actual computation (e.g., collect()) Defines data transformations (e.g., map()) Performance Can cause data processing; affects performance Does not affect performance until an action is called Map vs. FlatMap Aspect Map FlatMap Output Returns one output element per input element Can return zero or more output elements per input Flattening Does not flatten output Flattens the output into a single level Use Case Suitable for one-to-one transformations Suitable for one-to-many transformations Complexity Simpler, straightforward More complex due to variable number of outputs Examples map(x =&gt; x * 2) flatMap(x =&gt; x.split(&#34; &#34;)) GroupByKey vs. ReduceByKey Aspect GroupByKey ReduceByKey Operation Groups all values by key Aggregates values with the same key Efficiency Can lead to high shuffling More efficient due to partial aggregation Data Movement Requires shuffling of all values Minimizes data movement through local aggregation Use Case Useful for simple grouping Preferred for aggregations and reductions Performance Less efficient with large datasets Better performance for large datasets Repartition vs. Coalesce Aspect Repartition Coalesce Partitioning Can increase or decrease the number of partitions Only decreases the number of partitions Shuffling Involves full shuffle Avoids full shuffle, more efficient Efficiency More expensive due to shuffling More efficient for reducing partitions Use Case Used for increasing partitions or balancing load Used for reducing partitions, typically after filtering Performance Can be costly for large datasets More cost-effective for reducing partitions Cache vs. Persist Aspect Cache Persist Storage Level Defaults to MEMORY_ONLY Can use various storage levels (e.g., MEMORY_AND_DISK) Flexibility Simplified, with default storage level Offers more options for storage levels Use Case Suitable for simple caching scenarios Suitable for complex caching scenarios requiring different storage levels Implementation Easier to use, shorthand for MEMORY_ONLY More flexible, allows custom storage options Performance Suitable when memory suffices More efficient when dealing with larger datasets and limited memory Narrow vs. Wide Transformation Aspect Narrow Transformation Wide Transformation Partitioning Each parent partition is used by one child partition Requires data from multiple partitions Shuffling No shuffling required Involves shuffling of data Performance More efficient and less costly Less efficient due to data movement Examples map(), filter() groupByKey(), join() Complexity Simpler and faster More complex and slower due to data movement Collect vs. Take Aspect Collect Take Output Retrieves all data from the RDD/DataFrame Retrieves a specified number of elements Memory Usage Can be expensive and use a lot of memory More memory-efficient Use Case Used when you need the entire dataset Useful for sampling or debugging Performance Can cause performance issues with large data Faster and more controlled Action Type Triggers full data retrieval Triggers partial data retrieval Broadcast Variable vs. Accumulator Aspect Broadcast Variable Accumulator Purpose Efficiently shares read-only data across tasks Tracks metrics and aggregates values Data Type Data that is shared and read-only Counters and sums, often numerical Use Case Useful for large lookup tables or configurations Useful for aggregating metrics like counts Efficiency Reduces data transfer by broadcasting data once Efficient for aggregating values across tasks Mutability Immutable, read-only Mutable, can be updated during computation Spark SQL vs. DataFrame API Aspect Spark SQL DataFrame API Interface Executes SQL queries Provides a programmatic interface Syntax Uses SQL-like syntax Uses function-based syntax Optimization Optimized with Catalyst Optimized with Catalyst Spark Streaming vs. Structured Streaming Aspect Spark Streaming Structured Streaming Processing Micro-batch processing Micro-batch and continuous processing API RDD-based API SQL-based API with DataFrame/Dataset support Complexity More complex and lower-level Simplified with high-level APIs Consistency Can be less consistent due to micro-batches Provides stronger consistency guarantees Performance Can be slower for complex queries Better performance with optimizations Shuffle vs. MapReduce Aspect Shuffle MapReduce Operation Data reorganization across partitions Data processing model for distributed computing Efficiency Can be costly due to data movement Designed for batch processing with high I/O Performance Affects performance based on the amount of data movement Optimized for large-scale data processing but less efficient for iterative tasks Use Case Used in Spark for data redistribution Used in Hadoop for data processing tasks Implementation Integrated into Spark operations Core component of the Hadoop ecosystem Union vs. Join Aspect Union Join Operation Combines two DataFrames/RDDs into one Combines rows from two DataFrames/RDDs based on a key Data Requirements Requires same schema for both DataFrames/RDDs Requires a common key for joining Performance Generally faster as it does not require key matching Can be slower due to key matching and shuffling Output Stacks data vertically Merges data horizontally based on keys Use Case Appending data or combining datasets Merging related data based on keys Executor vs. Driver Aspect Executor Driver Role Executes tasks and processes data Coordinates and manages the Spark application Memory Memory allocated per executor for data processing Memory used for managing application execution Lifecycle Exists throughout the application execution Starts and stops the Spark application Tasks Runs the tasks assigned by the driver Schedules and coordinates tasks and jobs Parallelism Multiple executors run in parallel Single driver coordinates multiple executors Checkpointing vs. Caching Aspect Checkpointing Caching Purpose Provides fault tolerance and reliability Improves performance by storing intermediate data Storage Writes data to stable storage (e.g., HDFS) Stores data in memory or on disk (depends on storage level) Use Case Used for recovery in case of failures Used for optimizing repeated operations Impact Can be more costly and slow Generally faster but not suitable for fault tolerance Data Data is written to external storage Data is kept in memory or disk storage for quick access ReduceByKey vs. AggregateByKey Aspect ReduceByKey AggregateByKey Operation Combines values with the same key using a function Performs custom aggregation and combinatory operations Efficiency More efficient for simple aggregations Flexible for complex aggregation scenarios Shuffling Involves shuffling but can be optimized Can be more complex due to custom aggregation Use Case Suitable for straightforward aggregations Ideal for advanced and custom aggregations Performance Generally faster for simple operations Performance varies with complexity SQLContext vs. HiveContext vs. SparkSession Aspect SQLContext HiveContext SparkSession Purpose Provides SQL query capabilities Provides integration with Hive for SQL queries Unified entry point for Spark functionality Integration Basic SQL capabilities Integrates with Hive Metastore Combines SQL, DataFrame, and Streaming APIs Usage Legacy, less functionality Supports HiveQL and Hive UDFs Supports all Spark functionalities including Hive Configuration Less flexible and older Requires Hive setup and configuration Modern and flexible, manages configurations Capabilities Limited to SQL queries Extends SQL capabilities with Hive integration Comprehensive access to all Spark features Broadcast Join vs. Shuffle Join Aspect Broadcast Join Shuffle Join Operation Broadcasts a small dataset to all nodes Shuffles data across nodes for joining Data Size Suitable for small datasets Suitable for larger datasets Efficiency More efficient for small tables More suited for large datasets Performance Faster due to reduced shuffling Can be slower due to extensive shuffling Use Case Use when one dataset is small relative to others Use when both datasets are large SparkContext vs. SparkSession Aspect SparkContext SparkSession Purpose Entry point for Spark functionality Unified entry point for Spark functionalities Lifecycle Created before Spark jobs start Manages the Spark application lifecycle Functionality Provides access to RDD and basic Spark functionality Provides access to RDD, DataFrame, SQL, and Streaming APIs Configuration Configuration is less flexible More flexible and easier to configure Usage Older, used for legacy applications Modern and recommended for new applications Structured Streaming vs. Spark Streaming Aspect Structured Streaming Spark Streaming Processing Micro-batch and continuous processing Micro-batch processing API SQL-based API with DataFrame/Dataset support RDD-based API Complexity Simplified and high-level More complex and low-level Consistency Provides stronger consistency guarantees Can be less consistent due to micro-batches Performance Better performance with built-in optimizations Can be slower for complex queries Partitioning vs. Bucketing Aspect Partitioning Bucketing Purpose Divides data into multiple partitions based on a key Divides data into buckets based on a hash function Usage Used to optimize queries by reducing data scanned Used to improve join performance and maintain sorted data Shuffling Reduces shuffling by placing related data together Reduces shuffle during joins and aggregations Data Layout Data is physically separated based on partition key Data is organized into fixed-size buckets Performance Improves performance for queries involving partition keys Enhances performance for join operations">
    <meta property="og:locale" content="en_us">
    <meta property="og:type" content="article">
    <meta property="article:section" content="Azure Data Bricks">
    <meta itemprop="name" content="Comparisons :: Data Engineering Notes">
    <meta itemprop="description" content="Hadoop vs. Spark Architecture Aspect Hadoop Spark Storage Uses HDFS for storage Uses in-memory processing for speed Processing MapReduce is disk-based In-memory processing improves performance Integration Runs independently or with Hadoop ecosystem Can run on top of Hadoop; more flexible Complexity More complex setup and deployment Simpler to deploy and configure Performance Slower for iterative tasks due to disk I/O Better performance for iterative tasks RDD vs. DataFrame vs. Dataset Aspect RDD DataFrame Dataset API Level Low-level, more control High-level, optimized with Catalyst High-level, type-safe Schema No schema, unstructured Uses schema for structured data Strongly typed, compile-time type safety Optimization No built-in optimization Optimized using Catalyst Optimized using Catalyst, with type safety Type Safety No type safety No compile-time type safety Provides compile-time type safety Performance Less optimized for performance Better performance due to optimizations Combines type safety with optimization Action vs. Transformation in Spark Aspect Action Transformation Execution Triggers execution of the Spark job Builds up a logical plan of data operations Return Type Returns results or output Returns a new RDD/DataFrame Evaluation Eager evaluation; executes immediately Lazy evaluation; executed when an action is triggered Computation Involves actual computation (e.g., collect()) Defines data transformations (e.g., map()) Performance Can cause data processing; affects performance Does not affect performance until an action is called Map vs. FlatMap Aspect Map FlatMap Output Returns one output element per input element Can return zero or more output elements per input Flattening Does not flatten output Flattens the output into a single level Use Case Suitable for one-to-one transformations Suitable for one-to-many transformations Complexity Simpler, straightforward More complex due to variable number of outputs Examples map(x =&gt; x * 2) flatMap(x =&gt; x.split(&#34; &#34;)) GroupByKey vs. ReduceByKey Aspect GroupByKey ReduceByKey Operation Groups all values by key Aggregates values with the same key Efficiency Can lead to high shuffling More efficient due to partial aggregation Data Movement Requires shuffling of all values Minimizes data movement through local aggregation Use Case Useful for simple grouping Preferred for aggregations and reductions Performance Less efficient with large datasets Better performance for large datasets Repartition vs. Coalesce Aspect Repartition Coalesce Partitioning Can increase or decrease the number of partitions Only decreases the number of partitions Shuffling Involves full shuffle Avoids full shuffle, more efficient Efficiency More expensive due to shuffling More efficient for reducing partitions Use Case Used for increasing partitions or balancing load Used for reducing partitions, typically after filtering Performance Can be costly for large datasets More cost-effective for reducing partitions Cache vs. Persist Aspect Cache Persist Storage Level Defaults to MEMORY_ONLY Can use various storage levels (e.g., MEMORY_AND_DISK) Flexibility Simplified, with default storage level Offers more options for storage levels Use Case Suitable for simple caching scenarios Suitable for complex caching scenarios requiring different storage levels Implementation Easier to use, shorthand for MEMORY_ONLY More flexible, allows custom storage options Performance Suitable when memory suffices More efficient when dealing with larger datasets and limited memory Narrow vs. Wide Transformation Aspect Narrow Transformation Wide Transformation Partitioning Each parent partition is used by one child partition Requires data from multiple partitions Shuffling No shuffling required Involves shuffling of data Performance More efficient and less costly Less efficient due to data movement Examples map(), filter() groupByKey(), join() Complexity Simpler and faster More complex and slower due to data movement Collect vs. Take Aspect Collect Take Output Retrieves all data from the RDD/DataFrame Retrieves a specified number of elements Memory Usage Can be expensive and use a lot of memory More memory-efficient Use Case Used when you need the entire dataset Useful for sampling or debugging Performance Can cause performance issues with large data Faster and more controlled Action Type Triggers full data retrieval Triggers partial data retrieval Broadcast Variable vs. Accumulator Aspect Broadcast Variable Accumulator Purpose Efficiently shares read-only data across tasks Tracks metrics and aggregates values Data Type Data that is shared and read-only Counters and sums, often numerical Use Case Useful for large lookup tables or configurations Useful for aggregating metrics like counts Efficiency Reduces data transfer by broadcasting data once Efficient for aggregating values across tasks Mutability Immutable, read-only Mutable, can be updated during computation Spark SQL vs. DataFrame API Aspect Spark SQL DataFrame API Interface Executes SQL queries Provides a programmatic interface Syntax Uses SQL-like syntax Uses function-based syntax Optimization Optimized with Catalyst Optimized with Catalyst Spark Streaming vs. Structured Streaming Aspect Spark Streaming Structured Streaming Processing Micro-batch processing Micro-batch and continuous processing API RDD-based API SQL-based API with DataFrame/Dataset support Complexity More complex and lower-level Simplified with high-level APIs Consistency Can be less consistent due to micro-batches Provides stronger consistency guarantees Performance Can be slower for complex queries Better performance with optimizations Shuffle vs. MapReduce Aspect Shuffle MapReduce Operation Data reorganization across partitions Data processing model for distributed computing Efficiency Can be costly due to data movement Designed for batch processing with high I/O Performance Affects performance based on the amount of data movement Optimized for large-scale data processing but less efficient for iterative tasks Use Case Used in Spark for data redistribution Used in Hadoop for data processing tasks Implementation Integrated into Spark operations Core component of the Hadoop ecosystem Union vs. Join Aspect Union Join Operation Combines two DataFrames/RDDs into one Combines rows from two DataFrames/RDDs based on a key Data Requirements Requires same schema for both DataFrames/RDDs Requires a common key for joining Performance Generally faster as it does not require key matching Can be slower due to key matching and shuffling Output Stacks data vertically Merges data horizontally based on keys Use Case Appending data or combining datasets Merging related data based on keys Executor vs. Driver Aspect Executor Driver Role Executes tasks and processes data Coordinates and manages the Spark application Memory Memory allocated per executor for data processing Memory used for managing application execution Lifecycle Exists throughout the application execution Starts and stops the Spark application Tasks Runs the tasks assigned by the driver Schedules and coordinates tasks and jobs Parallelism Multiple executors run in parallel Single driver coordinates multiple executors Checkpointing vs. Caching Aspect Checkpointing Caching Purpose Provides fault tolerance and reliability Improves performance by storing intermediate data Storage Writes data to stable storage (e.g., HDFS) Stores data in memory or on disk (depends on storage level) Use Case Used for recovery in case of failures Used for optimizing repeated operations Impact Can be more costly and slow Generally faster but not suitable for fault tolerance Data Data is written to external storage Data is kept in memory or disk storage for quick access ReduceByKey vs. AggregateByKey Aspect ReduceByKey AggregateByKey Operation Combines values with the same key using a function Performs custom aggregation and combinatory operations Efficiency More efficient for simple aggregations Flexible for complex aggregation scenarios Shuffling Involves shuffling but can be optimized Can be more complex due to custom aggregation Use Case Suitable for straightforward aggregations Ideal for advanced and custom aggregations Performance Generally faster for simple operations Performance varies with complexity SQLContext vs. HiveContext vs. SparkSession Aspect SQLContext HiveContext SparkSession Purpose Provides SQL query capabilities Provides integration with Hive for SQL queries Unified entry point for Spark functionality Integration Basic SQL capabilities Integrates with Hive Metastore Combines SQL, DataFrame, and Streaming APIs Usage Legacy, less functionality Supports HiveQL and Hive UDFs Supports all Spark functionalities including Hive Configuration Less flexible and older Requires Hive setup and configuration Modern and flexible, manages configurations Capabilities Limited to SQL queries Extends SQL capabilities with Hive integration Comprehensive access to all Spark features Broadcast Join vs. Shuffle Join Aspect Broadcast Join Shuffle Join Operation Broadcasts a small dataset to all nodes Shuffles data across nodes for joining Data Size Suitable for small datasets Suitable for larger datasets Efficiency More efficient for small tables More suited for large datasets Performance Faster due to reduced shuffling Can be slower due to extensive shuffling Use Case Use when one dataset is small relative to others Use when both datasets are large SparkContext vs. SparkSession Aspect SparkContext SparkSession Purpose Entry point for Spark functionality Unified entry point for Spark functionalities Lifecycle Created before Spark jobs start Manages the Spark application lifecycle Functionality Provides access to RDD and basic Spark functionality Provides access to RDD, DataFrame, SQL, and Streaming APIs Configuration Configuration is less flexible More flexible and easier to configure Usage Older, used for legacy applications Modern and recommended for new applications Structured Streaming vs. Spark Streaming Aspect Structured Streaming Spark Streaming Processing Micro-batch and continuous processing Micro-batch processing API SQL-based API with DataFrame/Dataset support RDD-based API Complexity Simplified and high-level More complex and low-level Consistency Provides stronger consistency guarantees Can be less consistent due to micro-batches Performance Better performance with built-in optimizations Can be slower for complex queries Partitioning vs. Bucketing Aspect Partitioning Bucketing Purpose Divides data into multiple partitions based on a key Divides data into buckets based on a hash function Usage Used to optimize queries by reducing data scanned Used to improve join performance and maintain sorted data Shuffling Reduces shuffling by placing related data together Reduces shuffle during joins and aggregations Data Layout Data is physically separated based on partition key Data is organized into fixed-size buckets Performance Improves performance for queries involving partition keys Enhances performance for join operations">
    <meta itemprop="wordCount" content="1531">
    <title>Comparisons :: Data Engineering Notes</title>
    <link href="/azure_data_bricks/comparisons/index.md" rel="alternate" type="text/markdown" title="Comparisons :: Data Engineering Notes">
    <link href="/azure_data_bricks/comparisons/index.print.html" rel="alternate" type="text/html" title="Comparisons :: Data Engineering Notes">
    <link href="/images/favicon.ico?1761498837" rel="icon" type="image/x-icon" sizes="any">
    <link href="/css/auto-complete/auto-complete.min.css?1761498837" rel="stylesheet">
    <script src="/js/auto-complete/auto-complete.min.js?1761498837" defer></script>
    <script src="/js/search-lunr.js?1761498837" defer></script>
    <script src="/js/search.js?1761498837" defer></script>
    <script>
      window.relearn = window.relearn || {};
      window.relearn.index_js_url="/searchindex.en.js?1761498837";
    </script>
    <script src="/js/lunr/lunr.min.js?1761498837" defer></script>
    <script src="/js/lunr/lunr.stemmer.support.min.js?1761498837" defer></script>
    <script src="/js/lunr/lunr.multi.min.js?1761498837" defer></script>
    <script src="/js/lunr/lunr.en.min.js?1761498837" defer></script>
    <script>
      window.relearn = window.relearn || {};
      window.relearn.contentLangs=['en'];
    </script>
    <link href="/fonts/fontawesome/css/fontawesome-all.min.css?1761498837" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="/fonts/fontawesome/css/fontawesome-all.min.css?1761498837" rel="stylesheet"></noscript>
    <link href="/css/perfect-scrollbar/perfect-scrollbar.min.css?1761498837" rel="stylesheet">
    <link href="/css/theme.css?1761498837" rel="stylesheet">
    <link href="/css/format-html.css?1761498837" rel="stylesheet" id="R-format-style">
    <script>
      window.relearn = window.relearn || {};
      // configuration
      window.relearn.min = ``;
      window.relearn.path='\/azure_data_bricks\/comparisons\/';
      window.relearn.relBasePath='..\/..';
      window.relearn.relBaseUri='..\/..';
      window.relearn.absBaseUri='http:\/\/localhost:1313';
      window.relearn.disableAnchorCopy=false;
      window.relearn.disableAnchorScrolling=false;
      window.relearn.disableInlineCopyToClipboard=false;
      window.relearn.enableBlockCodeWrap=true;
      // legal
      window.relearn.getItem = (s,n) => {return s.getItem(n)};
      window.relearn.setItem = (s,n,v) => {return s.setItem(n,v)};
      window.relearn.removeItem = (s,n) => {return s.removeItem(n)};
      // translations
      window.T_Copy_to_clipboard = `Copy to clipboard`;
      window.T_Copied_to_clipboard = `Copied to clipboard!`;
      window.T_Copy_link_to_clipboard = `Copy link to clipboard`;
      window.T_Link_copied_to_clipboard = `Copied link to clipboard!`;
      window.T_Reset_view = `Reset view`;
      window.T_View_reset = `View reset!`;
      window.T_No_results_found = `No results found for "{0}"`;
      window.T_N_results_found = `{1} results found for "{0}"`;
      // variant stuff
      window.relearn.themevariants = [ 'auto', 'zen-light', 'zen-dark' ];
      window.relearn.customvariantname = "my-custom-variant";
      window.relearn.changeVariant = function(variant) {
        var oldVariant = document.documentElement.dataset.rThemeVariant;
        window.relearn.setItem(window.localStorage, window.relearn.absBaseUri + "/variant", variant);
        document.documentElement.dataset.rThemeVariant = variant;
        if (oldVariant != variant) {
          document.dispatchEvent( new CustomEvent('themeVariantLoaded', { detail: { variant, oldVariant } }) );
          window.relearn.markVariant();
        }
      }
      window.relearn.markVariant = function() {
        var variant = window.relearn.getItem(window.localStorage, window.relearn.absBaseUri + "/variant");
        document.querySelectorAll(".R-variantswitcher select").forEach((select) => {select.value = variant;});
      }
      window.relearn.initVariant = function() {
        var variant = window.relearn.getItem(window.localStorage, window.relearn.absBaseUri + "/variant") ?? "";
        if( variant == window.relearn.customvariantname ){
        }else if( !variant || !window.relearn.themevariants.includes(variant) ){
          variant = window.relearn.themevariants[0];
          window.relearn.setItem(window.localStorage, window.relearn.absBaseUri + "/variant", variant);
        }
        document.documentElement.dataset.rThemeVariant = variant;
      }
      window.relearn.initVariant();
      window.relearn.markVariant();
    </script><style>
:root {
    --MENU-WIDTH-S: 14.375rem;
    --MENU-WIDTH-M: 14.375rem;
    --MENU-WIDTH-L: 18.75rem;
    --MAIN-WIDTH-MAX: 1000rem;
}
</style>
  </head>
  <body class="mobile-support html" data-url="/azure_data_bricks/comparisons/">
    <div id="R-body" class="default-animation">
      <div id="R-body-overlay"></div>
      <nav id="R-topbar">
        <div class="topbar-wrapper">
          <div class="topbar-sidebar-divider"></div>
          <div class="topbar-area topbar-area-start" data-area="start">
            <div class="topbar-button topbar-button-sidebar" data-content-empty="disable" data-width-s="show" data-width-m="hide" data-width-l="hide"><button class="topbar-control" onclick="toggleNav()" type="button" title="Menu (CTRL&#43;ALT&#43;n)"><i class="fa-fw fas fa-bars"></i></button>
            </div>
            <div class="topbar-button topbar-button-toc" data-content-empty="hide" data-width-s="show" data-width-m="show" data-width-l="show"><button class="topbar-control" onclick="toggleTopbarFlyout(this)" type="button" title="Table of Contents (CTRL&#43;ALT&#43;t)"><i class="fa-fw fas fa-list-alt"></i></button>
              <div class="topbar-content">
                <div class="topbar-content-wrapper">
<nav class="TableOfContents">
  <ul>
    <li><a href="#hadoop-vs-spark-architecture">Hadoop vs. Spark Architecture</a></li>
    <li><a href="#rdd-vs-dataframe-vs-dataset">RDD vs. DataFrame vs. Dataset</a></li>
    <li><a href="#action-vs-transformation-in-spark">Action vs. Transformation in Spark</a>
      <ul>
        <li><a href="#map-vs-flatmap">Map vs. FlatMap</a></li>
        <li><a href="#groupbykey-vs-reducebykey">GroupByKey vs. ReduceByKey</a></li>
        <li><a href="#repartition-vs-coalesce">Repartition vs. Coalesce</a></li>
        <li><a href="#cache-vs-persist">Cache vs. Persist</a></li>
        <li><a href="#narrow-vs-wide-transformation">Narrow vs. Wide Transformation</a></li>
        <li><a href="#collect-vs-take">Collect vs. Take</a></li>
        <li><a href="#broadcast-variable-vs-accumulator">Broadcast Variable vs. Accumulator</a></li>
        <li><a href="#spark-sql-vs-dataframe-api">Spark SQL vs. DataFrame API</a></li>
        <li><a href="#spark-streaming-vs-structured-streaming">Spark Streaming vs. Structured Streaming</a></li>
        <li><a href="#shuffle-vs-mapreduce">Shuffle vs. MapReduce</a></li>
        <li><a href="#union-vs-join">Union vs. Join</a></li>
        <li><a href="#executor-vs-driver">Executor vs. Driver</a></li>
        <li><a href="#checkpointing-vs-caching">Checkpointing vs. Caching</a></li>
        <li><a href="#reducebykey-vs-aggregatebykey">ReduceByKey vs. AggregateByKey</a></li>
        <li><a href="#sqlcontext-vs-hivecontext-vs-sparksession">SQLContext vs. HiveContext vs. SparkSession</a></li>
        <li><a href="#broadcast-join-vs-shuffle-join">Broadcast Join vs. Shuffle Join</a></li>
        <li><a href="#sparkcontext-vs-sparksession">SparkContext vs. SparkSession</a></li>
        <li><a href="#structured-streaming-vs-spark-streaming">Structured Streaming vs. Spark Streaming</a></li>
        <li><a href="#partitioning-vs-bucketing">Partitioning vs. Bucketing</a></li>
      </ul>
    </li>
  </ul>
</nav>
                </div>
              </div>
            </div>
          </div>
          <ol class="topbar-breadcrumbs breadcrumbs highlightable" itemscope itemtype="http://schema.org/BreadcrumbList"><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class="a11y-only"><a itemprop="item" href="/"><span itemprop="name">Data Engineering Notes</span></a><meta itemprop="position" content="1">&nbsp;/&nbsp;</li><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><a itemprop="item" href="/azure_data_bricks/"><span itemprop="name">ADB</span></a><meta itemprop="position" content="2">&nbsp;/&nbsp;</li><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><span itemprop="name">Comparisons</span><meta itemprop="position" content="3"></li>
          </ol>
          <div class="topbar-area topbar-area-end" data-area="end">
            <div class="topbar-button topbar-button-edit" data-content-empty="disable" data-width-s="area-more" data-width-m="show" data-width-l="show"><a class="topbar-control" href="https://github.com/jampalabharath/relearn/tree/main/content/Azure_Data_Bricks/Comparisons.md" rel="external" target="_blank" title="Edit (CTRL+ALT+w)"><i class="fa-fw fas fa-pen"></i></a>
            </div>
            <div class="topbar-button topbar-button-markdown" data-content-empty="disable" data-width-s="area-more" data-width-m="show" data-width-l="show"><a class="topbar-control" href="/azure_data_bricks/comparisons/index.md" title="Show Markdown"><i class="fa-fw fab fa-markdown"></i></a>
            </div>
            <div class="topbar-button topbar-button-print" data-content-empty="disable" data-width-s="area-more" data-width-m="show" data-width-l="show"><a class="topbar-control" href="/azure_data_bricks/comparisons/index.print.html" title="Print whole chapter (CTRL+ALT+p)"><i class="fa-fw fas fa-print"></i></a>
            </div>
            <div class="topbar-button topbar-button-prev" data-content-empty="disable" data-width-s="show" data-width-m="show" data-width-l="show"><a class="topbar-control" href="/azure_data_bricks/pivot/" title="Pivot ()"><i class="fa-fw fas fa-chevron-left"></i></a>
            </div>
            <div class="topbar-button topbar-button-next" data-content-empty="disable" data-width-s="show" data-width-m="show" data-width-l="show"><a class="topbar-control" href="/practice/" title="Practice Sets ()"><i class="fa-fw fas fa-chevron-right"></i></a>
            </div>
            <div class="topbar-button topbar-button-more" data-content-empty="hide" data-width-s="show" data-width-m="show" data-width-l="show"><button class="topbar-control" onclick="toggleTopbarFlyout(this)" type="button" title="More"><i class="fa-fw fas fa-ellipsis-v"></i></button>
              <div class="topbar-content">
                <div class="topbar-content-wrapper">
                  <div class="topbar-area topbar-area-more" data-area="more">
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </nav>
      <div id="R-main-overlay"></div>
      <main id="R-body-inner" class="highlightable azure_data_bricks" tabindex="-1">
        <div class="flex-block-wrapper">
<article class="default">
  <header class="headline">
  </header>

<h1 id="comparisons">Comparisons</h1>

<h2 id="hadoop-vs-spark-architecture">Hadoop vs. Spark Architecture</h2>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>Hadoop</th>
          <th>Spark</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Storage</strong></td>
          <td>Uses HDFS for storage</td>
          <td>Uses in-memory processing for speed</td>
      </tr>
      <tr>
          <td><strong>Processing</strong></td>
          <td>MapReduce is disk-based</td>
          <td>In-memory processing improves performance</td>
      </tr>
      <tr>
          <td><strong>Integration</strong></td>
          <td>Runs independently or with Hadoop ecosystem</td>
          <td>Can run on top of Hadoop; more flexible</td>
      </tr>
      <tr>
          <td><strong>Complexity</strong></td>
          <td>More complex setup and deployment</td>
          <td>Simpler to deploy and configure</td>
      </tr>
      <tr>
          <td><strong>Performance</strong></td>
          <td>Slower for iterative tasks due to disk I/O</td>
          <td>Better performance for iterative tasks</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="rdd-vs-dataframe-vs-dataset">RDD vs. DataFrame vs. Dataset</h2>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>RDD</th>
          <th>DataFrame</th>
          <th>Dataset</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>API Level</strong></td>
          <td>Low-level, more control</td>
          <td>High-level, optimized with Catalyst</td>
          <td>High-level, type-safe</td>
      </tr>
      <tr>
          <td><strong>Schema</strong></td>
          <td>No schema, unstructured</td>
          <td>Uses schema for structured data</td>
          <td>Strongly typed, compile-time type safety</td>
      </tr>
      <tr>
          <td><strong>Optimization</strong></td>
          <td>No built-in optimization</td>
          <td>Optimized using Catalyst</td>
          <td>Optimized using Catalyst, with type safety</td>
      </tr>
      <tr>
          <td><strong>Type Safety</strong></td>
          <td>No type safety</td>
          <td>No compile-time type safety</td>
          <td>Provides compile-time type safety</td>
      </tr>
      <tr>
          <td><strong>Performance</strong></td>
          <td>Less optimized for performance</td>
          <td>Better performance due to optimizations</td>
          <td>Combines type safety with optimization</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="action-vs-transformation-in-spark">Action vs. Transformation in Spark</h2>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>Action</th>
          <th>Transformation</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Execution</strong></td>
          <td>Triggers execution of the Spark job</td>
          <td>Builds up a logical plan of data operations</td>
      </tr>
      <tr>
          <td><strong>Return Type</strong></td>
          <td>Returns results or output</td>
          <td>Returns a new RDD/DataFrame</td>
      </tr>
      <tr>
          <td><strong>Evaluation</strong></td>
          <td>Eager evaluation; executes immediately</td>
          <td>Lazy evaluation; executed when an action is triggered</td>
      </tr>
      <tr>
          <td><strong>Computation</strong></td>
          <td>Involves actual computation (e.g., <code>collect()</code>)</td>
          <td>Defines data transformations (e.g., <code>map()</code>)</td>
      </tr>
      <tr>
          <td><strong>Performance</strong></td>
          <td>Can cause data processing; affects performance</td>
          <td>Does not affect performance until an action is called</td>
      </tr>
  </tbody>
</table>
<h3 id="map-vs-flatmap">Map vs. FlatMap</h3>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>Map</th>
          <th>FlatMap</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Output</td>
          <td>Returns one output element per input element</td>
          <td>Can return zero or more output elements per input</td>
      </tr>
      <tr>
          <td>Flattening</td>
          <td>Does not flatten output</td>
          <td>Flattens the output into a single level</td>
      </tr>
      <tr>
          <td>Use Case</td>
          <td>Suitable for one-to-one transformations</td>
          <td>Suitable for one-to-many transformations</td>
      </tr>
      <tr>
          <td>Complexity</td>
          <td>Simpler, straightforward</td>
          <td>More complex due to variable number of outputs</td>
      </tr>
      <tr>
          <td>Examples</td>
          <td><code>map(x =&gt; x * 2)</code></td>
          <td><code>flatMap(x =&gt; x.split(&quot; &quot;))</code></td>
      </tr>
  </tbody>
</table>
<h3 id="groupbykey-vs-reducebykey">GroupByKey vs. ReduceByKey</h3>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>GroupByKey</th>
          <th>ReduceByKey</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Operation</td>
          <td>Groups all values by key</td>
          <td>Aggregates values with the same key</td>
      </tr>
      <tr>
          <td>Efficiency</td>
          <td>Can lead to high shuffling</td>
          <td>More efficient due to partial aggregation</td>
      </tr>
      <tr>
          <td>Data Movement</td>
          <td>Requires shuffling of all values</td>
          <td>Minimizes data movement through local aggregation</td>
      </tr>
      <tr>
          <td>Use Case</td>
          <td>Useful for simple grouping</td>
          <td>Preferred for aggregations and reductions</td>
      </tr>
      <tr>
          <td>Performance</td>
          <td>Less efficient with large datasets</td>
          <td>Better performance for large datasets</td>
      </tr>
  </tbody>
</table>
<h3 id="repartition-vs-coalesce">Repartition vs. Coalesce</h3>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>Repartition</th>
          <th>Coalesce</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Partitioning</td>
          <td>Can increase or decrease the number of partitions</td>
          <td>Only decreases the number of partitions</td>
      </tr>
      <tr>
          <td>Shuffling</td>
          <td>Involves full shuffle</td>
          <td>Avoids full shuffle, more efficient</td>
      </tr>
      <tr>
          <td>Efficiency</td>
          <td>More expensive due to shuffling</td>
          <td>More efficient for reducing partitions</td>
      </tr>
      <tr>
          <td>Use Case</td>
          <td>Used for increasing partitions or balancing load</td>
          <td>Used for reducing partitions, typically after filtering</td>
      </tr>
      <tr>
          <td>Performance</td>
          <td>Can be costly for large datasets</td>
          <td>More cost-effective for reducing partitions</td>
      </tr>
  </tbody>
</table>
<h3 id="cache-vs-persist">Cache vs. Persist</h3>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>Cache</th>
          <th>Persist</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Storage Level</td>
          <td>Defaults to MEMORY_ONLY</td>
          <td>Can use various storage levels (e.g., MEMORY_AND_DISK)</td>
      </tr>
      <tr>
          <td>Flexibility</td>
          <td>Simplified, with default storage level</td>
          <td>Offers more options for storage levels</td>
      </tr>
      <tr>
          <td>Use Case</td>
          <td>Suitable for simple caching scenarios</td>
          <td>Suitable for complex caching scenarios requiring different storage levels</td>
      </tr>
      <tr>
          <td>Implementation</td>
          <td>Easier to use, shorthand for MEMORY_ONLY</td>
          <td>More flexible, allows custom storage options</td>
      </tr>
      <tr>
          <td>Performance</td>
          <td>Suitable when memory suffices</td>
          <td>More efficient when dealing with larger datasets and limited memory</td>
      </tr>
  </tbody>
</table>
<h3 id="narrow-vs-wide-transformation">Narrow vs. Wide Transformation</h3>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>Narrow Transformation</th>
          <th>Wide Transformation</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Partitioning</td>
          <td>Each parent partition is used by one child partition</td>
          <td>Requires data from multiple partitions</td>
      </tr>
      <tr>
          <td>Shuffling</td>
          <td>No shuffling required</td>
          <td>Involves shuffling of data</td>
      </tr>
      <tr>
          <td>Performance</td>
          <td>More efficient and less costly</td>
          <td>Less efficient due to data movement</td>
      </tr>
      <tr>
          <td>Examples</td>
          <td><code>map()</code>, <code>filter()</code></td>
          <td><code>groupByKey()</code>, <code>join()</code></td>
      </tr>
      <tr>
          <td>Complexity</td>
          <td>Simpler and faster</td>
          <td>More complex and slower due to data movement</td>
      </tr>
  </tbody>
</table>
<h3 id="collect-vs-take">Collect vs. Take</h3>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>Collect</th>
          <th>Take</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Output</td>
          <td>Retrieves all data from the RDD/DataFrame</td>
          <td>Retrieves a specified number of elements</td>
      </tr>
      <tr>
          <td>Memory Usage</td>
          <td>Can be expensive and use a lot of memory</td>
          <td>More memory-efficient</td>
      </tr>
      <tr>
          <td>Use Case</td>
          <td>Used when you need the entire dataset</td>
          <td>Useful for sampling or debugging</td>
      </tr>
      <tr>
          <td>Performance</td>
          <td>Can cause performance issues with large data</td>
          <td>Faster and more controlled</td>
      </tr>
      <tr>
          <td>Action Type</td>
          <td>Triggers full data retrieval</td>
          <td>Triggers partial data retrieval</td>
      </tr>
  </tbody>
</table>
<h3 id="broadcast-variable-vs-accumulator">Broadcast Variable vs. Accumulator</h3>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>Broadcast Variable</th>
          <th>Accumulator</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Purpose</td>
          <td>Efficiently shares read-only data across tasks</td>
          <td>Tracks metrics and aggregates values</td>
      </tr>
      <tr>
          <td>Data Type</td>
          <td>Data that is shared and read-only</td>
          <td>Counters and sums, often numerical</td>
      </tr>
      <tr>
          <td>Use Case</td>
          <td>Useful for large lookup tables or configurations</td>
          <td>Useful for aggregating metrics like counts</td>
      </tr>
      <tr>
          <td>Efficiency</td>
          <td>Reduces data transfer by broadcasting data once</td>
          <td>Efficient for aggregating values across tasks</td>
      </tr>
      <tr>
          <td>Mutability</td>
          <td>Immutable, read-only</td>
          <td>Mutable, can be updated during computation</td>
      </tr>
  </tbody>
</table>
<h3 id="spark-sql-vs-dataframe-api">Spark SQL vs. DataFrame API</h3>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>Spark SQL</th>
          <th>DataFrame API</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Interface</td>
          <td>Executes SQL queries</td>
          <td>Provides a programmatic interface</td>
      </tr>
      <tr>
          <td>Syntax</td>
          <td>Uses SQL-like syntax</td>
          <td>Uses function-based syntax</td>
      </tr>
      <tr>
          <td>Optimization</td>
          <td>Optimized with Catalyst</td>
          <td>Optimized with Catalyst</td>
      </tr>
  </tbody>
</table>
<h3 id="spark-streaming-vs-structured-streaming">Spark Streaming vs. Structured Streaming</h3>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>Spark Streaming</th>
          <th>Structured Streaming</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Processing</td>
          <td>Micro-batch processing</td>
          <td>Micro-batch and continuous processing</td>
      </tr>
      <tr>
          <td>API</td>
          <td>RDD-based API</td>
          <td>SQL-based API with DataFrame/Dataset support</td>
      </tr>
      <tr>
          <td>Complexity</td>
          <td>More complex and lower-level</td>
          <td>Simplified with high-level APIs</td>
      </tr>
      <tr>
          <td>Consistency</td>
          <td>Can be less consistent due to micro-batches</td>
          <td>Provides stronger consistency guarantees</td>
      </tr>
      <tr>
          <td>Performance</td>
          <td>Can be slower for complex queries</td>
          <td>Better performance with optimizations</td>
      </tr>
  </tbody>
</table>
<h3 id="shuffle-vs-mapreduce">Shuffle vs. MapReduce</h3>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>Shuffle</th>
          <th>MapReduce</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Operation</td>
          <td>Data reorganization across partitions</td>
          <td>Data processing model for distributed computing</td>
      </tr>
      <tr>
          <td>Efficiency</td>
          <td>Can be costly due to data movement</td>
          <td>Designed for batch processing with high I/O</td>
      </tr>
      <tr>
          <td>Performance</td>
          <td>Affects performance based on the amount of data movement</td>
          <td>Optimized for large-scale data processing but less efficient for iterative tasks</td>
      </tr>
      <tr>
          <td>Use Case</td>
          <td>Used in Spark for data redistribution</td>
          <td>Used in Hadoop for data processing tasks</td>
      </tr>
      <tr>
          <td>Implementation</td>
          <td>Integrated into Spark operations</td>
          <td>Core component of the Hadoop ecosystem</td>
      </tr>
  </tbody>
</table>
<h3 id="union-vs-join">Union vs. Join</h3>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>Union</th>
          <th>Join</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Operation</td>
          <td>Combines two DataFrames/RDDs into one</td>
          <td>Combines rows from two DataFrames/RDDs based on a key</td>
      </tr>
      <tr>
          <td>Data Requirements</td>
          <td>Requires same schema for both DataFrames/RDDs</td>
          <td>Requires a common key for joining</td>
      </tr>
      <tr>
          <td>Performance</td>
          <td>Generally faster as it does not require key matching</td>
          <td>Can be slower due to key matching and shuffling</td>
      </tr>
      <tr>
          <td>Output</td>
          <td>Stacks data vertically</td>
          <td>Merges data horizontally based on keys</td>
      </tr>
      <tr>
          <td>Use Case</td>
          <td>Appending data or combining datasets</td>
          <td>Merging related data based on keys</td>
      </tr>
  </tbody>
</table>
<h3 id="executor-vs-driver">Executor vs. Driver</h3>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>Executor</th>
          <th>Driver</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Role</td>
          <td>Executes tasks and processes data</td>
          <td>Coordinates and manages the Spark application</td>
      </tr>
      <tr>
          <td>Memory</td>
          <td>Memory allocated per executor for data processing</td>
          <td>Memory used for managing application execution</td>
      </tr>
      <tr>
          <td>Lifecycle</td>
          <td>Exists throughout the application execution</td>
          <td>Starts and stops the Spark application</td>
      </tr>
      <tr>
          <td>Tasks</td>
          <td>Runs the tasks assigned by the driver</td>
          <td>Schedules and coordinates tasks and jobs</td>
      </tr>
      <tr>
          <td>Parallelism</td>
          <td>Multiple executors run in parallel</td>
          <td>Single driver coordinates multiple executors</td>
      </tr>
  </tbody>
</table>
<h3 id="checkpointing-vs-caching">Checkpointing vs. Caching</h3>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>Checkpointing</th>
          <th>Caching</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Purpose</td>
          <td>Provides fault tolerance and reliability</td>
          <td>Improves performance by storing intermediate data</td>
      </tr>
      <tr>
          <td>Storage</td>
          <td>Writes data to stable storage (e.g., HDFS)</td>
          <td>Stores data in memory or on disk (depends on storage level)</td>
      </tr>
      <tr>
          <td>Use Case</td>
          <td>Used for recovery in case of failures</td>
          <td>Used for optimizing repeated operations</td>
      </tr>
      <tr>
          <td>Impact</td>
          <td>Can be more costly and slow</td>
          <td>Generally faster but not suitable for fault tolerance</td>
      </tr>
      <tr>
          <td>Data</td>
          <td>Data is written to external storage</td>
          <td>Data is kept in memory or disk storage for quick access</td>
      </tr>
  </tbody>
</table>
<h3 id="reducebykey-vs-aggregatebykey">ReduceByKey vs. AggregateByKey</h3>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>ReduceByKey</th>
          <th>AggregateByKey</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Operation</td>
          <td>Combines values with the same key using a function</td>
          <td>Performs custom aggregation and combinatory operations</td>
      </tr>
      <tr>
          <td>Efficiency</td>
          <td>More efficient for simple aggregations</td>
          <td>Flexible for complex aggregation scenarios</td>
      </tr>
      <tr>
          <td>Shuffling</td>
          <td>Involves shuffling but can be optimized</td>
          <td>Can be more complex due to custom aggregation</td>
      </tr>
      <tr>
          <td>Use Case</td>
          <td>Suitable for straightforward aggregations</td>
          <td>Ideal for advanced and custom aggregations</td>
      </tr>
      <tr>
          <td>Performance</td>
          <td>Generally faster for simple operations</td>
          <td>Performance varies with complexity</td>
      </tr>
  </tbody>
</table>
<h3 id="sqlcontext-vs-hivecontext-vs-sparksession">SQLContext vs. HiveContext vs. SparkSession</h3>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>SQLContext</th>
          <th>HiveContext</th>
          <th>SparkSession</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Purpose</td>
          <td>Provides SQL query capabilities</td>
          <td>Provides integration with Hive for SQL queries</td>
          <td>Unified entry point for Spark functionality</td>
      </tr>
      <tr>
          <td>Integration</td>
          <td>Basic SQL capabilities</td>
          <td>Integrates with Hive Metastore</td>
          <td>Combines SQL, DataFrame, and Streaming APIs</td>
      </tr>
      <tr>
          <td>Usage</td>
          <td>Legacy, less functionality</td>
          <td>Supports HiveQL and Hive UDFs</td>
          <td>Supports all Spark functionalities including Hive</td>
      </tr>
      <tr>
          <td>Configuration</td>
          <td>Less flexible and older</td>
          <td>Requires Hive setup and configuration</td>
          <td>Modern and flexible, manages configurations</td>
      </tr>
      <tr>
          <td>Capabilities</td>
          <td>Limited to SQL queries</td>
          <td>Extends SQL capabilities with Hive integration</td>
          <td>Comprehensive access to all Spark features</td>
      </tr>
  </tbody>
</table>
<h3 id="broadcast-join-vs-shuffle-join">Broadcast Join vs. Shuffle Join</h3>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>Broadcast Join</th>
          <th>Shuffle Join</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Operation</td>
          <td>Broadcasts a small dataset to all nodes</td>
          <td>Shuffles data across nodes for joining</td>
      </tr>
      <tr>
          <td>Data Size</td>
          <td>Suitable for small datasets</td>
          <td>Suitable for larger datasets</td>
      </tr>
      <tr>
          <td>Efficiency</td>
          <td>More efficient for small tables</td>
          <td>More suited for large datasets</td>
      </tr>
      <tr>
          <td>Performance</td>
          <td>Faster due to reduced shuffling</td>
          <td>Can be slower due to extensive shuffling</td>
      </tr>
      <tr>
          <td>Use Case</td>
          <td>Use when one dataset is small relative to others</td>
          <td>Use when both datasets are large</td>
      </tr>
  </tbody>
</table>
<h3 id="sparkcontext-vs-sparksession">SparkContext vs. SparkSession</h3>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>SparkContext</th>
          <th>SparkSession</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Purpose</td>
          <td>Entry point for Spark functionality</td>
          <td>Unified entry point for Spark functionalities</td>
      </tr>
      <tr>
          <td>Lifecycle</td>
          <td>Created before Spark jobs start</td>
          <td>Manages the Spark application lifecycle</td>
      </tr>
      <tr>
          <td>Functionality</td>
          <td>Provides access to RDD and basic Spark functionality</td>
          <td>Provides access to RDD, DataFrame, SQL, and Streaming APIs</td>
      </tr>
      <tr>
          <td>Configuration</td>
          <td>Configuration is less flexible</td>
          <td>More flexible and easier to configure</td>
      </tr>
      <tr>
          <td>Usage</td>
          <td>Older, used for legacy applications</td>
          <td>Modern and recommended for new applications</td>
      </tr>
  </tbody>
</table>
<h3 id="structured-streaming-vs-spark-streaming">Structured Streaming vs. Spark Streaming</h3>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>Structured Streaming</th>
          <th>Spark Streaming</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Processing</td>
          <td>Micro-batch and continuous processing</td>
          <td>Micro-batch processing</td>
      </tr>
      <tr>
          <td>API</td>
          <td>SQL-based API with DataFrame/Dataset support</td>
          <td>RDD-based API</td>
      </tr>
      <tr>
          <td>Complexity</td>
          <td>Simplified and high-level</td>
          <td>More complex and low-level</td>
      </tr>
      <tr>
          <td>Consistency</td>
          <td>Provides stronger consistency guarantees</td>
          <td>Can be less consistent due to micro-batches</td>
      </tr>
      <tr>
          <td>Performance</td>
          <td>Better performance with built-in optimizations</td>
          <td>Can be slower for complex queries</td>
      </tr>
  </tbody>
</table>
<h3 id="partitioning-vs-bucketing">Partitioning vs. Bucketing</h3>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>Partitioning</th>
          <th>Bucketing</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Purpose</td>
          <td>Divides data into multiple partitions based on a key</td>
          <td>Divides data into buckets based on a hash function</td>
      </tr>
      <tr>
          <td>Usage</td>
          <td>Used to optimize queries by reducing data scanned</td>
          <td>Used to improve join performance and maintain sorted data</td>
      </tr>
      <tr>
          <td>Shuffling</td>
          <td>Reduces shuffling by placing related data together</td>
          <td>Reduces shuffle during joins and aggregations</td>
      </tr>
      <tr>
          <td>Data Layout</td>
          <td>Data is physically separated based on partition key</td>
          <td>Data is organized into fixed-size buckets</td>
      </tr>
      <tr>
          <td>Performance</td>
          <td>Improves performance for queries involving partition keys</td>
          <td>Enhances performance for join operations</td>
      </tr>
  </tbody>
</table>

  <footer class="footline">
  </footer>
</article>
        </div>
      </main>
    </div>
    <aside id="R-sidebar" class="default-animation">
      <div id="R-header-topbar" class="default-animation"></div>
      <div id="R-header-wrapper" class="default-animation">
        <div id="R-header" class="default-animation">
<head>
  <style>
    #R-logo {
      display: flex;
      align-items: center;
      gap: 0.5rem;
      margin-right: auto;
    }

    #R-logo img {
      width: 48px;
      height: 48px;
      object-fit: contain;
      cursor: pointer;
      transition: transform 0.2s ease-in-out;
    }

    #R-logo img:hover {
      transform: scale(1.05);
    }

    #R-logo .logo-title {
  margin-left: 0.5rem;   
  font-size: 1.2rem;     
  line-height: 1;        
  display: flex;
  align-items: center;   
}
  </style>
</head>



<a id="R-logo" class="R-default" href="/">
  <img src="/images/logo.png" alt="brand logo">
  <span class="logo-title">Relearn</span>
</a>
        </div>
        <search><form action="/search/" method="get">
          <div class="searchbox default-animation">
            <button class="search-detail" type="submit" title="Search (CTRL+ALT+f)"><i class="fas fa-search"></i></button>
            <label class="a11y-only" for="R-search-by">Search</label>
            <input data-search-input id="R-search-by" name="search-by" class="search-by" type="search" placeholder="Search...">
            <button class="search-clear" type="button" data-search-clear="" title="Clear search"><i class="fas fa-times" title="Clear search"></i></button>
          </div>
        </form></search>
      </div>
      <div id="R-homelinks" class="default-animation">
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-homelinks">
          <ul class="space collapsible-menu">
          </ul>
        </div>
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-headercontrols">
          <ul class="">
          </ul>
        </div>
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
      </div>
      <div id="R-content-wrapper" class="highlightable">
        <div class="R-sidebarmenu R-shortcutmenu-main">
          <ul class="enlarge morespace collapsible-menu">
            <li class="" data-nav-id="/sql/"><input type="checkbox" id="R-section-6b91b02647756f9b5e29b003bccc4db0" aria-controls="R-subsections-6b91b02647756f9b5e29b003bccc4db0"><label for="R-section-6b91b02647756f9b5e29b003bccc4db0"><i class="fa-fw fas fa-chevron-right"></i><span class="a11y-only">Submenu SQL</span></label><a class="padding" href="/sql/">SQL<i class="fa-fw fas fa-check read-icon"></i></a><ul id="R-subsections-6b91b02647756f9b5e29b003bccc4db0" class="collapsible-menu">
            <li class="" data-nav-id="/sql/select/"><a class="padding" href="/sql/select/">SELECT Query<i class="fa-fw fas fa-check read-icon"></i></a></li>
            <li class="" data-nav-id="/sql/ddl/"><a class="padding" href="/sql/ddl/">DDL<i class="fa-fw fas fa-check read-icon"></i></a></li>
            <li class="" data-nav-id="/sql/dml/"><a class="padding" href="/sql/dml/">DML<i class="fa-fw fas fa-check read-icon"></i></a></li>
            <li class="" data-nav-id="/sql/filter/"><a class="padding" href="/sql/filter/">Filtering Data<i class="fa-fw fas fa-check read-icon"></i></a></li>
            <li class="" data-nav-id="/sql/joins/"><a class="padding" href="/sql/joins/">Joins<i class="fa-fw fas fa-check read-icon"></i></a></li>
            <li class="" data-nav-id="/sql/set-operations/"><a class="padding" href="/sql/set-operations/">Set Operations<i class="fa-fw fas fa-check read-icon"></i></a></li>
            <li class="" data-nav-id="/sql/string-functions/"><a class="padding" href="/sql/string-functions/">String Functions<i class="fa-fw fas fa-check read-icon"></i></a></li>
            <li class="" data-nav-id="/sql/number-functions/"><a class="padding" href="/sql/number-functions/">Number Functions<i class="fa-fw fas fa-check read-icon"></i></a></li>
            <li class="" data-nav-id="/sql/date-time1/"><a class="padding" href="/sql/date-time1/">Date &amp; Time Functions<i class="fa-fw fas fa-check read-icon"></i></a></li>
            <li class="" data-nav-id="/sql/date-time2/"><a class="padding" href="/sql/date-time2/">Date &amp; Time Formats<i class="fa-fw fas fa-check read-icon"></i></a></li>
            <li class="" data-nav-id="/sql/null-functions/"><a class="padding" href="/sql/null-functions/">NULL Functions<i class="fa-fw fas fa-check read-icon"></i></a></li>
            <li class="" data-nav-id="/sql/case-statement/"><a class="padding" href="/sql/case-statement/">CASE Statement<i class="fa-fw fas fa-check read-icon"></i></a></li>
            <li class="" data-nav-id="/sql/aggregate-functions/"><a class="padding" href="/sql/aggregate-functions/">Aggregate Functions<i class="fa-fw fas fa-check read-icon"></i></a></li>
            <li class="" data-nav-id="/sql/window-functions/"><a class="padding" href="/sql/window-functions/">Window Functions<i class="fa-fw fas fa-check read-icon"></i></a></li>
            <li class="" data-nav-id="/sql/window_aggregations/"><a class="padding" href="/sql/window_aggregations/">Window Aggregate Functions<i class="fa-fw fas fa-check read-icon"></i></a></li>
            <li class="" data-nav-id="/sql/window-ranking/"><a class="padding" href="/sql/window-ranking/">Window Ranking Functions<i class="fa-fw fas fa-check read-icon"></i></a></li>
            <li class="" data-nav-id="/sql/window-value-functions/"><a class="padding" href="/sql/window-value-functions/">Window Value Functions<i class="fa-fw fas fa-check read-icon"></i></a></li>
            <li class="" data-nav-id="/sql/subqueries/"><a class="padding" href="/sql/subqueries/">Subquery Functions<i class="fa-fw fas fa-check read-icon"></i></a></li>
            <li class="" data-nav-id="/sql/cte/"><a class="padding" href="/sql/cte/">Common Table Expressions (CTEs)<i class="fa-fw fas fa-check read-icon"></i></a></li>
            <li class="" data-nav-id="/sql/views/"><a class="padding" href="/sql/views/">Views<i class="fa-fw fas fa-check read-icon"></i></a></li>
            <li class="" data-nav-id="/sql/temporary-tables/"><a class="padding" href="/sql/temporary-tables/">Temporary Tables<i class="fa-fw fas fa-check read-icon"></i></a></li>
            <li class="" data-nav-id="/sql/stored-procedures/"><a class="padding" href="/sql/stored-procedures/">Stored Procedures<i class="fa-fw fas fa-check read-icon"></i></a></li>
            <li class="" data-nav-id="/sql/triggers/"><a class="padding" href="/sql/triggers/">Triggers<i class="fa-fw fas fa-check read-icon"></i></a></li>
            <li class="" data-nav-id="/sql/indexes/"><a class="padding" href="/sql/indexes/">Indexes<i class="fa-fw fas fa-check read-icon"></i></a></li>
            <li class="" data-nav-id="/sql/partitions/"><a class="padding" href="/sql/partitions/">Partitioning<i class="fa-fw fas fa-check read-icon"></i></a></li>
            <li class="" data-nav-id="/sql/performance-optimization/"><a class="padding" href="/sql/performance-optimization/">Performance Tips<i class="fa-fw fas fa-check read-icon"></i></a></li>
            <li class="" data-nav-id="/sql/ai-and-sql/"><a class="padding" href="/sql/ai-and-sql/">AI and SQL<i class="fa-fw fas fa-check read-icon"></i></a></li></ul></li>
            <li class="" data-nav-id="/azure_data_factory/"><a class="padding" href="/azure_data_factory/">ADF<i class="fa-fw fas fa-check read-icon"></i></a></li>
            <li class="parent " data-nav-id="/azure_data_bricks/"><input type="checkbox" id="R-section-be307a2f2d32c2a195add2e28563a24e" aria-controls="R-subsections-be307a2f2d32c2a195add2e28563a24e" checked><label for="R-section-be307a2f2d32c2a195add2e28563a24e"><i class="fa-fw fas fa-chevron-right"></i><span class="a11y-only">Submenu ADB</span></label><a class="padding" href="/azure_data_bricks/">ADB<i class="fa-fw fas fa-check read-icon"></i></a><ul id="R-subsections-be307a2f2d32c2a195add2e28563a24e" class="collapsible-menu">
            <li class="" data-nav-id="/azure_data_bricks/spark/"><a class="padding" href="/azure_data_bricks/spark/">Spark<i class="fa-fw fas fa-check read-icon"></i></a></li>
            <li class="" data-nav-id="/azure_data_bricks/df-creation/"><a class="padding" href="/azure_data_bricks/df-creation/">DF Basics<i class="fa-fw fas fa-check read-icon"></i></a></li>
            <li class="" data-nav-id="/azure_data_bricks/df-operations/"><a class="padding" href="/azure_data_bricks/df-operations/">DF Operations<i class="fa-fw fas fa-check read-icon"></i></a></li>
            <li class="" data-nav-id="/azure_data_bricks/functions/"><a class="padding" href="/azure_data_bricks/functions/">Functions<i class="fa-fw fas fa-check read-icon"></i></a></li>
            <li class="" data-nav-id="/azure_data_bricks/date/"><a class="padding" href="/azure_data_bricks/date/">Date Functions<i class="fa-fw fas fa-check read-icon"></i></a></li>
            <li class="" data-nav-id="/azure_data_bricks/nulls/"><a class="padding" href="/azure_data_bricks/nulls/">Handling Nulls<i class="fa-fw fas fa-check read-icon"></i></a></li>
            <li class="" data-nav-id="/azure_data_bricks/aggregations/"><a class="padding" href="/azure_data_bricks/aggregations/">Aggregate functions<i class="fa-fw fas fa-check read-icon"></i></a></li>
            <li class="" data-nav-id="/azure_data_bricks/joins/"><a class="padding" href="/azure_data_bricks/joins/">Joins<i class="fa-fw fas fa-check read-icon"></i></a></li>
            <li class="" data-nav-id="/azure_data_bricks/when/"><a class="padding" href="/azure_data_bricks/when/">When|Cast|Union<i class="fa-fw fas fa-check read-icon"></i></a></li>
            <li class="" data-nav-id="/azure_data_bricks/window-functions/"><a class="padding" href="/azure_data_bricks/window-functions/">Window Functions<i class="fa-fw fas fa-check read-icon"></i></a></li>
            <li class="" data-nav-id="/azure_data_bricks/explode/"><a class="padding" href="/azure_data_bricks/explode/">Explode<i class="fa-fw fas fa-check read-icon"></i></a></li>
            <li class="" data-nav-id="/azure_data_bricks/pivot/"><a class="padding" href="/azure_data_bricks/pivot/">Pivot<i class="fa-fw fas fa-check read-icon"></i></a></li>
            <li class="active " data-nav-id="/azure_data_bricks/comparisons/"><a class="padding" href="/azure_data_bricks/comparisons/">Comparisons<i class="fa-fw fas fa-check read-icon"></i></a></li></ul></li>
            <li class="" data-nav-id="/practice/"><input type="checkbox" id="R-section-21344c0a5719731e2ce7649deede330a" aria-controls="R-subsections-21344c0a5719731e2ce7649deede330a"><label for="R-section-21344c0a5719731e2ce7649deede330a"><i class="fa-fw fas fa-chevron-right"></i><span class="a11y-only">Submenu Practice Sets</span></label><a class="padding" href="/practice/">Practice Sets<i class="fa-fw fas fa-check read-icon"></i></a><ul id="R-subsections-21344c0a5719731e2ce7649deede330a" class="collapsible-menu">
            <li class="" data-nav-id="/practice/leetcode-sql/"><a class="padding" href="/practice/leetcode-sql/">SQL LeetCode<i class="fa-fw fas fa-check read-icon"></i></a></li>
            <li class="" data-nav-id="/practice/pyspark1/"><a class="padding" href="/practice/pyspark1/">Pyspark 1<i class="fa-fw fas fa-check read-icon"></i></a></li></ul></li>
            <li class="" data-nav-id="/interviewprep/"><input type="checkbox" id="R-section-4c46d08ac015ee7f150578d25f17ee2b" aria-controls="R-subsections-4c46d08ac015ee7f150578d25f17ee2b"><label for="R-section-4c46d08ac015ee7f150578d25f17ee2b"><i class="fa-fw fas fa-chevron-right"></i><span class="a11y-only">Submenu Interview Prep</span></label><a class="padding" href="/interviewprep/">Interview Prep<i class="fa-fw fas fa-check read-icon"></i></a><ul id="R-subsections-4c46d08ac015ee7f150578d25f17ee2b" class="collapsible-menu">
            <li class="" data-nav-id="/interviewprep/adf1/"><a class="padding" href="/interviewprep/adf1/">ADF 1<i class="fa-fw fas fa-check read-icon"></i></a></li>
            <li class="" data-nav-id="/interviewprep/pipeline1/"><a class="padding" href="/interviewprep/pipeline1/">Data Pipeline<i class="fa-fw fas fa-check read-icon"></i></a></li>
            <li class="" data-nav-id="/interviewprep/sql1/"><a class="padding" href="/interviewprep/sql1/">SQL Theory<i class="fa-fw fas fa-check read-icon"></i></a></li>
            <li class="" data-nav-id="/interviewprep/sql2/"><a class="padding" href="/interviewprep/sql2/">100 SQL Queries<i class="fa-fw fas fa-check read-icon"></i></a></li>
            <li class="" data-nav-id="/interviewprep/sql3/"><a class="padding" href="/interviewprep/sql3/">100 SQL Queries<i class="fa-fw fas fa-check read-icon"></i></a></li></ul></li>
          </ul>
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-shortcuts">
          <ul class="space collapsible-menu">
          </ul>
        </div>
        <div id="R-footer-margin"></div>
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-footercontrols">
          <ul class="">
            <li class="R-variantswitcher">
              <div class="padding menu-control">
                <i class="fa-fw fas fa-paint-brush"></i>
                <span>&nbsp;</span>
                <div class="control-style">
                  <label class="a11y-only" for="R-select-variant">Theme</label>
                  <select id="R-select-variant">
                    <option id="R-select-variant-auto" value="auto" selected>Auto</option>
                    <option id="R-select-variant-zen-light" value="zen-light">Zen Light</option>
                    <option id="R-select-variant-zen-dark" value="zen-dark">Zen Dark</option>
                  </select>
                </div>
                <div class="clear"></div>
              </div>
              <script>window.relearn.markVariant();</script>
            </li>
            <li class="R-historyclearer">
              <div class="padding menu-control">
                <i class="fa-fw fas fa-history"></i>
                <span>&nbsp;</span>
                <div class="control-style">
                  <button>Clear History</button>
                </div>
                <div class="clear"></div>
              </div>
            </li>
          </ul>
        </div>
<div id="R-footer"><p>Built with <a href="https://github.com/McShelby/hugo-theme-relearn" title="love"><i class="fas fa-heart"></i></a> by <a href="https://gohugo.io/">Hugo</a></p></div>
      </div>
    </aside>
    <script src="/js/clipboard/clipboard.min.js?1761498837" defer></script>
    <script src="/js/perfect-scrollbar/perfect-scrollbar.min.js?1761498837" defer></script>
    <script src="/js/theme.js?1761498837" defer></script>
  </body>
</html>
