var relearn_searchindex = [
  {
    "breadcrumb": "ADB",
    "content": "Creating DataFrame Creating DataFrame from Lists/Tuples # Sample Data data = [(1, \"Alice\"), (2, \"Bob\"), (3, \"Charlie\"), (4, \"David\"), (5, \"Eve\")] columns = [\"ID\", \"Name\"] # Create DataFrame df = spark.createDataFrame(data, columns) # Show DataFrame df.show() Creating DataFrame from Pandas import pandas as pd # Sample Pandas DataFrame pandas_df = pd.DataFrame(data, columns=columns) # Convert to PySpark DataFrame df_from_pandas = spark.createDataFrame(pandas_df) df_from_pandas.show() Create DataFrame from Dictionary data_dict = [{\"ID\": 1, \"Name\": \"Alice\"}, {\"ID\": 2, \"Name\": \"Bob\"}] df_from_dict = spark.createDataFrame(data_dict) df_from_dict.show() Create Empty DataFrame You can create an empty DataFrame with just schema definitions.\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType # Define Schema schema = StructType([ StructField(\"ID\", IntegerType(), True), StructField(\"Name\", StringType(), True) ]) # Create Empty DataFrame empty_df = spark.createDataFrame([], schema) empty_df.show() Creating DataFrame from Structured Data (CSV, JSON, Parquet) # Reading CSV file into DataFrame df_csv = spark.read.csv(\"/path/to/file.csv\", header=True, inferSchema=True) df_csv.show() # Reading JSON file into DataFrame df_json = spark.read.json(\"/path/to/file.json\") df_json.show() # Reading Parquet file into DataFrame df_parquet = spark.read.parquet(\"/path/to/file.parquet\") df_parquet.show() show() Function in PySpark DataFrames The show() function in PySpark displays the contents of a DataFrame in a tabular format. It has several useful parameters for customization:\nParameters: n: Number of rows to display (default is 20) truncate: If set to True, it truncates column values longer than 20 characters (default is True) vertical: If set to True, prints rows in a vertical format Usage Examples: # Show the first 3 rows, truncate columns to 25 characters, and display vertically: df.show(n=3, truncate=25, vertical=True) # Show entire DataFrame (default settings): df.show() # Show the first 10 rows: df.show(10) # Show DataFrame without truncating any columns: df.show(truncate=False) Loading Data from CSV File into a DataFrame Loading data into DataFrames is a fundamental step in any data processing workflow in PySpark. This document outlines how to load data from CSV files into a DataFrame, including using a custom schema and the implications of using the inferSchema option.\n1. Import Required Libraries Before loading the data, ensure you import the necessary modules:\nfrom pyspark.sql import SparkSession from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType 2. Define the Schema You can define a custom schema for your CSV file. This allows you to explicitly set the data types for each column.\n# Define the schema for the CSV file custom_schema = StructType([ StructField(\"id\", IntegerType(), True), StructField(\"name\", StringType(), True), StructField(\"age\", IntegerType(), True), StructField(\"salary\", DoubleType(), True) ]) 3. Read the CSV File Load the CSV file into a DataFrame using the read.csv() method. Here, header=True treats the first row as headers, and inferSchema=True allows Spark to automatically assign data types to columns.\n# Read the CSV file with the custom schema df = spark.read.csv(\"your_file.csv\", schema=custom_schema, header=True) 4. Load Multiple CSV Files To read multiple CSV files into a single DataFrame, you can pass a list of file paths. Ensure that the schema is consistent across all files.\n# List of file paths file_paths = [\"file1.csv\", \"file2.csv\", \"file3.csv\"] # Read multiple CSV files into a single DataFrame df = spark.read.csv(file_paths, header=True, inferSchema=True) 5. Load a CSV from FileStore Here is an example of loading a CSV file from Databricks FileStore:\ndf = spark.read.csv(\"/FileStore/tables/Order.csv\", header=True, inferSchema=True, sep=',') 6. Display the DataFrame Use the following commands to check the schema and display the DataFrame:\n# Print the schema of the DataFrame df.printSchema() # Show the first 20 rows of the DataFrame df.show() # Displays only the first 20 rows # Display the DataFrame in a tabular format display(df) # For Databricks notebooks Interview Question: How Does inferSchema Work? Behind the Scenes: When you use inferSchema, Spark runs a job that scans the CSV file from top to bottom to identify the best-suited data type for each column based on the values it encounters.\nDoes It Make Sense to Use inferSchema? Pros: Useful when the schema of the file keeps changing, as it allows Spark to automatically detect the data types. Cons: Performance Impact: Spark must scan the entire file, which can take extra time, especially for large files. Loss of Control: You lose the ability to explicitly define the schema, which may lead to incorrect data types if the data is inconsistent. Conclusion Loading data from CSV files into a DataFrame is straightforward in PySpark. Understanding how to define a schema and the implications of using inferSchema is crucial for optimizing your data processing workflows.\nThis document provides a comprehensive overview of how to load CSV data into DataFrames in PySpark, along with considerations for using schema inference. Let me know if you need any more details or adjustments!\nPySpark DataFrame Schema Definition 1. Defining Schema Programmatically with StructType from pyspark.sql.types import * # Define the schema using StructType employeeSchema = StructType([ StructField(\"ID\", IntegerType(), True), StructField(\"Name\", StringType(), True), StructField(\"Age\", IntegerType(), True), StructField(\"Salary\", DoubleType(), True), StructField(\"Joining_Date\", StringType(), True), # Keeping as String for date issues StructField(\"Department\", StringType(), True), StructField(\"Performance_Rating\", IntegerType(), True), StructField(\"Email\", StringType(), True), StructField(\"Address\", StringType(), True), StructField(\"Phone\", StringType(), True) ]) # Load the DataFrame with the defined schema df = spark.read.load(\"/FileStore/tables/employees.csv\", format=\"csv\", header=True, schema=employeeSchema) # Print the schema of the DataFrame df.printSchema() # Optionally display the DataFrame # display(df) 2. Defining Schema as a String # Define the schema as a string employeeSchemaString = ''' ID Integer, Name String, Age Integer, Salary Double, Joining_Date String, Department String, Performance_Rating Integer, Email String, Address String, Phone String ''' # Load the DataFrame with the defined schema df = spark.read.load(\"dbfs:/FileStore/shared_uploads/imsvk11@gmail.com/employee_data.csv\", format=\"csv\", header=True, schema=employeeSchemaString) # Print the schema of the DataFrame df.printSchema() # Optionally display the DataFrame # display(df) Explanation Schema Definition: Both methods define a schema for the DataFrame, accommodating the dataset’s requirements, including handling null values where applicable. Data Types: The Joining_Date column is defined as StringType to accommodate potential date format issues or missing values. Loading the DataFrame: The spark.read.load method is used to load the CSV file into a DataFrame using the specified schema. Printing the Schema: The df.printSchema() function allows you to verify that the DataFrame is structured as intended.",
    "description": "Creating DataFrame Creating DataFrame from Lists/Tuples # Sample Data data = [(1, \"Alice\"), (2, \"Bob\"), (3, \"Charlie\"), (4, \"David\"), (5, \"Eve\")] columns = [\"ID\", \"Name\"] # Create DataFrame df = spark.createDataFrame(data, columns) # Show DataFrame df.show() Creating DataFrame from Pandas import pandas as pd # Sample Pandas DataFrame pandas_df = pd.DataFrame(data, columns=columns) # Convert to PySpark DataFrame df_from_pandas = spark.createDataFrame(pandas_df) df_from_pandas.show() Create DataFrame from Dictionary data_dict = [{\"ID\": 1, \"Name\": \"Alice\"}, {\"ID\": 2, \"Name\": \"Bob\"}] df_from_dict = spark.createDataFrame(data_dict) df_from_dict.show() Create Empty DataFrame You can create an empty DataFrame with just schema definitions.\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType # Define Schema schema = StructType([ StructField(\"ID\", IntegerType(), True), StructField(\"Name\", StringType(), True) ]) # Create Empty DataFrame empty_df = spark.createDataFrame([], schema) empty_df.show() Creating DataFrame from Structured Data (CSV, JSON, Parquet) # Reading CSV file into DataFrame df_csv = spark.read.csv(\"/path/to/file.csv\", header=True, inferSchema=True) df_csv.show() # Reading JSON file into DataFrame df_json = spark.read.json(\"/path/to/file.json\") df_json.show() # Reading Parquet file into DataFrame df_parquet = spark.read.parquet(\"/path/to/file.parquet\") df_parquet.show() show() Function in PySpark DataFrames The show() function in PySpark displays the contents of a DataFrame in a tabular format. It has several useful parameters for customization:",
    "tags": [],
    "title": "DF Basics",
    "uri": "/azure_data_bricks/df-creation/"
  },
  {
    "breadcrumb": "",
    "content": "Course Content 1. Introduction What is SQL? Types of Databases (Relational vs Non-Relational) SQL vs NoSQL SQL Standards (ANSI SQL, T-SQL, PL/SQL, MySQL SQL, PostgreSQL SQL) 2. SQL Basics Database, Schema, Table Data Types (Numeric, String, Date/Time, Boolean, JSON, XML) CREATE, DROP, ALTER INSERT, UPDATE, DELETE SELECT Statement WHERE Clause ORDER BY LIMIT / TOP / FETCH 3. Filtering \u0026 Operators Comparison Operators Logical Operators (AND, OR, NOT) BETWEEN, IN, LIKE IS NULL / IS NOT NULL Pattern Matching (Wildcards, Regex) 4. Functions String Functions (CONCAT, SUBSTRING, TRIM, UPPER, LOWER, LENGTH) Numeric Functions (ROUND, CEIL, FLOOR, ABS, MOD) Date/Time Functions (NOW, DATEADD, DATEDIFF, FORMAT) Conversion Functions (CAST, CONVERT) Aggregate Functions (COUNT, SUM, AVG, MIN, MAX) 5. Joins INNER JOIN LEFT JOIN RIGHT JOIN FULL OUTER JOIN CROSS JOIN SELF JOIN USING vs ON 6. Subqueries \u0026 Nested Queries Scalar Subqueries Table Subqueries Correlated Subqueries EXISTS / NOT EXISTS IN / ANY / ALL 7. Set Operations UNION vs UNION ALL INTERSECT EXCEPT / MINUS 8. Grouping \u0026 Aggregation GROUP BY HAVING GROUPING SETS ROLLUP CUBE 9. Constraints \u0026 Keys Primary Key Foreign Key Unique Key Check Constraint Default Constraint Not Null 10. Indexing \u0026 Performance Clustered vs Non-Clustered Index Composite Index Covering Index Index Maintenance Query Execution Plans SQL Performance Tuning 11. Transactions BEGIN, COMMIT, ROLLBACK Savepoints ACID Properties Isolation Levels (Read Uncommitted, Read Committed, Repeatable Read, Serializable, Snapshot) Deadlocks \u0026 Locking 12. Views \u0026 Materialized Views Creating Views Updatable Views Indexed Views Materialized Views Refresh Strategies 13. Stored Procedures \u0026 Functions User-Defined Functions (Scalar, Table-Valued) Stored Procedures Parameters (IN, OUT, INOUT) Error Handling in Procedures Recursive Procedures Dynamic SQL 14. Triggers AFTER Trigger INSTEAD OF Trigger Row-level vs Statement-level Triggers Use Cases 15. Advanced SQL Window Functions (ROW_NUMBER, RANK, DENSE_RANK, NTILE) LAG \u0026 LEAD FIRST_VALUE \u0026 LAST_VALUE Common Table Expressions (CTEs) Recursive CTEs Pivoting and Unpivoting JSON \u0026 XML in SQL Hierarchical Queries Full-Text Search 16. Security Users \u0026 Roles GRANT, REVOKE, DENY Row-Level Security Column-Level Security Data Masking Encryption at Rest \u0026 In-Transit 17. Advanced Database Concepts Normalization \u0026 Denormalization Star \u0026 Snowflake Schema Partitioning (Horizontal, Vertical) Sharding Federation Replication High Availability \u0026 Failover 18. SQL with Big Data SQL on Hadoop / Hive Spark SQL SQL in Cloud Platforms (AWS Athena, BigQuery, Azure Synapse) 19. SQL in Practice Data Warehousing with SQL ETL using SQL Reporting Queries Performance Benchmarking Debugging Complex Queries 20. Capstone Project Design \u0026 Implement Normalized Database Build ETL Pipeline using SQL Create Analytical Queries \u0026 Reports Optimize Large-Scale SQL Workloads",
    "description": "Course Content 1. Introduction What is SQL? Types of Databases (Relational vs Non-Relational) SQL vs NoSQL SQL Standards (ANSI SQL, T-SQL, PL/SQL, MySQL SQL, PostgreSQL SQL) 2. SQL Basics Database, Schema, Table Data Types (Numeric, String, Date/Time, Boolean, JSON, XML) CREATE, DROP, ALTER INSERT, UPDATE, DELETE SELECT Statement WHERE Clause ORDER BY LIMIT / TOP / FETCH 3. Filtering \u0026 Operators Comparison Operators Logical Operators (AND, OR, NOT) BETWEEN, IN, LIKE IS NULL / IS NOT NULL Pattern Matching (Wildcards, Regex) 4. Functions String Functions (CONCAT, SUBSTRING, TRIM, UPPER, LOWER, LENGTH) Numeric Functions (ROUND, CEIL, FLOOR, ABS, MOD) Date/Time Functions (NOW, DATEADD, DATEDIFF, FORMAT) Conversion Functions (CAST, CONVERT) Aggregate Functions (COUNT, SUM, AVG, MIN, MAX) 5. Joins INNER JOIN LEFT JOIN RIGHT JOIN FULL OUTER JOIN CROSS JOIN SELF JOIN USING vs ON 6. Subqueries \u0026 Nested Queries Scalar Subqueries Table Subqueries Correlated Subqueries EXISTS / NOT EXISTS IN / ANY / ALL 7. Set Operations UNION vs UNION ALL INTERSECT EXCEPT / MINUS 8. Grouping \u0026 Aggregation GROUP BY HAVING GROUPING SETS ROLLUP CUBE 9. Constraints \u0026 Keys Primary Key Foreign Key Unique Key Check Constraint Default Constraint Not Null 10. Indexing \u0026 Performance Clustered vs Non-Clustered Index Composite Index Covering Index Index Maintenance Query Execution Plans SQL Performance Tuning 11. Transactions BEGIN, COMMIT, ROLLBACK Savepoints ACID Properties Isolation Levels (Read Uncommitted, Read Committed, Repeatable Read, Serializable, Snapshot) Deadlocks \u0026 Locking 12. Views \u0026 Materialized Views Creating Views Updatable Views Indexed Views Materialized Views Refresh Strategies 13. Stored Procedures \u0026 Functions User-Defined Functions (Scalar, Table-Valued) Stored Procedures Parameters (IN, OUT, INOUT) Error Handling in Procedures Recursive Procedures Dynamic SQL 14. Triggers AFTER Trigger INSTEAD OF Trigger Row-level vs Statement-level Triggers Use Cases 15. Advanced SQL Window Functions (ROW_NUMBER, RANK, DENSE_RANK, NTILE) LAG \u0026 LEAD FIRST_VALUE \u0026 LAST_VALUE Common Table Expressions (CTEs) Recursive CTEs Pivoting and Unpivoting JSON \u0026 XML in SQL Hierarchical Queries Full-Text Search 16. Security Users \u0026 Roles GRANT, REVOKE, DENY Row-Level Security Column-Level Security Data Masking Encryption at Rest \u0026 In-Transit 17. Advanced Database Concepts Normalization \u0026 Denormalization Star \u0026 Snowflake Schema Partitioning (Horizontal, Vertical) Sharding Federation Replication High Availability \u0026 Failover 18. SQL with Big Data SQL on Hadoop / Hive Spark SQL SQL in Cloud Platforms (AWS Athena, BigQuery, Azure Synapse) 19. SQL in Practice Data Warehousing with SQL ETL using SQL Reporting Queries Performance Benchmarking Debugging Complex Queries 20. Capstone Project Design \u0026 Implement Normalized Database Build ETL Pipeline using SQL Create Analytical Queries \u0026 Reports Optimize Large-Scale SQL Workloads",
    "tags": [],
    "title": "SQL",
    "uri": "/sql/"
  },
  {
    "breadcrumb": "",
    "content": "1. Introduction Overview of Azure Data Factory Data Integration in Cloud ADF Architecture ADF Components (Pipelines, Activities, Datasets, Linked Services, Triggers) ADF vs SSIS vs Synapse Pipelines 2. ADF Basics Creating an ADF Instance ADF Studio Overview Linked Services Datasets Pipelines Activities Triggers (Schedule, Tumbling Window, Event-based, Manual) 3. Data Movement Copy Activity Integration Runtime (IR) Types Azure IR Self-hosted IR Azure-SSIS IR Data Movement Performance Parallelism \u0026 Batch Copy 4. Data Transformation Mapping Data Flows Wrangling Data Flows Data Flow Debugging Joins, Aggregations, Filters, Derived Columns Surrogate Keys \u0026 Window Functions Data Flow Performance Tuning 5. Orchestration Control Activities (Execute Pipeline, ForEach, If Condition, Until, Switch, Wait) Parameterization Variables in Pipelines Expressions \u0026 Functions Dynamic Content Error Handling \u0026 Retry Policies 6. Data Sources \u0026 Destinations Azure Blob Storage Azure Data Lake Storage (ADLS) Azure SQL Database Azure Synapse Analytics On-Premises SQL Server Cosmos DB REST API SAP, Oracle, Teradata, Snowflake Amazon S3 Google Cloud Storage 7. Integration with Other Azure Services Azure Key Vault Integration Azure Monitor \u0026 Log Analytics Power BI Integration Event Grid \u0026 Event Hub Logic Apps \u0026 Functions Azure Machine Learning 8. CI/CD \u0026 DevOps with ADF Source Control with Git (GitHub, Azure Repos) Branching \u0026 Collaboration Publishing \u0026 ARM Templates Continuous Integration \u0026 Deployment Automated Testing Environment Promotion (Dev → Test → Prod) 9. Security Managed Identity in ADF Service Principals Access Control (RBAC) Data Encryption Network Security (VNET Integration, Private Endpoints) Credential Management with Key Vault 10. Monitoring \u0026 Troubleshooting Pipeline Monitoring Activity Run Details Debugging Failures Alerts \u0026 Notifications Logging with Log Analytics Performance Optimization 11. Advanced Features Incremental Data Loading (Watermarking, Change Data Capture) Slowly Changing Dimensions (SCD) Implementation Parameterized Pipelines \u0026 Templates Global Parameters Data Lineage \u0026 Impact Analysis ADF REST API Managed VNET Data Integration 12. Best Practices Designing Efficient Pipelines Naming Conventions Error Handling Framework Cost Optimization in ADF Scalability \u0026 Performance Tuning Version Control Strategies 13. Real-World Use Cases Building an ETL Pipeline with ADF Data Lake to Synapse ETL Real-Time Event Processing Hybrid Data Integration (On-Prem + Cloud) Machine Learning Model Deployment Orchestration 14. Capstone Project End-to-End Data Warehouse Load using ADF Incremental Data Pipeline with Delta Processing Enterprise Data Integration with ADF and Synapse",
    "description": "1. Introduction Overview of Azure Data Factory Data Integration in Cloud ADF Architecture ADF Components (Pipelines, Activities, Datasets, Linked Services, Triggers) ADF vs SSIS vs Synapse Pipelines 2. ADF Basics Creating an ADF Instance ADF Studio Overview Linked Services Datasets Pipelines Activities Triggers (Schedule, Tumbling Window, Event-based, Manual) 3. Data Movement Copy Activity Integration Runtime (IR) Types Azure IR Self-hosted IR Azure-SSIS IR Data Movement Performance Parallelism \u0026 Batch Copy 4. Data Transformation Mapping Data Flows Wrangling Data Flows Data Flow Debugging Joins, Aggregations, Filters, Derived Columns Surrogate Keys \u0026 Window Functions Data Flow Performance Tuning 5. Orchestration Control Activities (Execute Pipeline, ForEach, If Condition, Until, Switch, Wait) Parameterization Variables in Pipelines Expressions \u0026 Functions Dynamic Content Error Handling \u0026 Retry Policies 6. Data Sources \u0026 Destinations Azure Blob Storage Azure Data Lake Storage (ADLS) Azure SQL Database Azure Synapse Analytics On-Premises SQL Server Cosmos DB REST API SAP, Oracle, Teradata, Snowflake Amazon S3 Google Cloud Storage 7. Integration with Other Azure Services Azure Key Vault Integration Azure Monitor \u0026 Log Analytics Power BI Integration Event Grid \u0026 Event Hub Logic Apps \u0026 Functions Azure Machine Learning 8. CI/CD \u0026 DevOps with ADF Source Control with Git (GitHub, Azure Repos) Branching \u0026 Collaboration Publishing \u0026 ARM Templates Continuous Integration \u0026 Deployment Automated Testing Environment Promotion (Dev → Test → Prod) 9. Security Managed Identity in ADF Service Principals Access Control (RBAC) Data Encryption Network Security (VNET Integration, Private Endpoints) Credential Management with Key Vault 10. Monitoring \u0026 Troubleshooting Pipeline Monitoring Activity Run Details Debugging Failures Alerts \u0026 Notifications Logging with Log Analytics Performance Optimization 11. Advanced Features Incremental Data Loading (Watermarking, Change Data Capture) Slowly Changing Dimensions (SCD) Implementation Parameterized Pipelines \u0026 Templates Global Parameters Data Lineage \u0026 Impact Analysis ADF REST API Managed VNET Data Integration 12. Best Practices Designing Efficient Pipelines Naming Conventions Error Handling Framework Cost Optimization in ADF Scalability \u0026 Performance Tuning Version Control Strategies 13. Real-World Use Cases Building an ETL Pipeline with ADF Data Lake to Synapse ETL Real-Time Event Processing Hybrid Data Integration (On-Prem + Cloud) Machine Learning Model Deployment Orchestration 14. Capstone Project End-to-End Data Warehouse Load using ADF Incremental Data Pipeline with Delta Processing Enterprise Data Integration with ADF and Synapse",
    "tags": [],
    "title": "Azure Data Factory",
    "uri": "/azure_data_factory/"
  },
  {
    "breadcrumb": "ADB",
    "content": "Column Selection \u0026 Manipulation 1. Different Methods to Select Columns In PySpark, you can select specific columns in multiple ways:\n# Using col() function df.select(col(\"Name\")).show() # Using column() function df.select(column(\"Age\")).show() # Directly using string name df.select(\"Salary\").show() 2. Selecting Multiple Columns Together You can combine different methods to select multiple columns:\n# multiple column df2 = df.select(\"ID\", \"Name\", col(\"Salary\"), column(\"Department\"), df.Phone) df2.show() 3. Listing All Columns in a DataFrame To get a list of all the column names:\n# get all column name df.columns 4. Renaming Columns with alias() You can rename columns using the alias() method:\ndf.select( col(\"Name\").alias('EmployeeName'), # Rename \"Name\" to \"EmployeeName\" col(\"Salary\").alias('EmployeeSalary'), # Rename \"Salary\" to \"EmployeeSalary\" column(\"Department\"), # Select \"Department\" df.Joining_Date # Select \"Joining_Date\" ).show() 5. Using selectExpr() for Concise Column Selection selectExpr() allows you to use SQL expressions directly and rename columns concisely:\ndf.selectExpr(\"Name as EmployeeName\", \"Salary as EmployeeSalary\", \"Department\").show() Summary Use col(), column(), or string names to select columns. Use expr() and selectExpr() for SQL-like expressions and renaming. Use alias() to rename columns. Get the list of columns using df.columns. Adding, Renaming, and Dropping Columns 1. Adding New Columns with withColumn() In PySpark, the withColumn() function is widely used to add new columns to a DataFrame. You can either assign a constant value using lit() or perform transformations using existing columns.\nAdd a constant value column: newdf = df.withColumn(\"NewColumn\", lit(1)) Add a column based on an expression: newdf = df.withColumn(\"withinCountry\", expr(\"Country == 'India'\")) This function allows adding multiple columns, including calculated ones:\nExample:\nAssign a constant value with lit(). Perform calculations using existing columns like multiplying values. 2. Renaming Columns with withColumnRenamed() PySpark provides the withColumnRenamed() method to rename columns. This is especially useful when you want to change the names for clarity or to follow naming conventions:\nRenaming a column: python\nnew_df = df.withColumnRenamed(\"oldColumnName\", \"newColumnName\") Handling column names with special characters or spaces: If a column has special characters or spaces, you need to use backticks (`) to escape it:\npython\nnewdf.select(\"`New Column Name`\").show() 3. Dropping Columns with drop() To remove unwanted columns, you can use the drop() method:\nDrop a single column: df2 = df.drop(\"Country\") Drop multiple columns: df2 = df.drop(\"Country\", \"Region\") Dropping columns creates a new DataFrame, and the original DataFrame remains unchanged.\n4. Immutability of DataFrames In Spark, DataFrames are immutable by nature. This means that after creating a DataFrame, its contents cannot be changed. All transformations like adding, renaming, or dropping columns result in a new DataFrame, keeping the original one intact.\nFor instance, dropping columns creates a new DataFrame without altering the original: newdf = df.drop(\"ItemType\", \"SalesChannel\") This immutability ensures data consistency and supports Spark’s parallel processing, as transformations do not affect the source data.\nKey Points Use withColumn() for adding columns, with lit() for constant values and expressions for computed values. Use withColumnRenamed() to rename columns and backticks for special characters or spaces. Use drop() to remove one or more columns. DataFrames are immutable in Spark—transformations result in new DataFrames, leaving the original unchanged. Data Types, Filtering, and Unique Values Here’s a structured set of notes with code to cover changing data types, filtering data, and handling unique/distinct values in PySpark using the employee data:\n1. Changing Data Types (Schema Transformation) In PySpark, you can change the data type of a column using the cast() method. This is helpful when you need to convert data types for columns like Salary or Phone.\nfrom pyspark.sql.functions import col # Change the 'Salary' column from integer to double df = df.withColumn(\"Salary\", col(\"Salary\").cast(\"double\")) # Convert 'Phone' column to string df = df.withColumn(\"Phone\", col(\"Phone\").cast(\"string\")) df.printSchema() 2. Filtering Data You can filter rows based on specific conditions. For instance, to filter employees with a salary greater than 50,000:\n# Filter rows where Salary is greater than 50,000 filtered_df = df.filter(col(\"Salary\") \u003e 50000) filtered_df.show() # Filtering rows where Age is not null filtered_df = df.filter(df[\"Age\"].isNotNull()) filtered_df.show() 3. Multiple Filters (Chaining Conditions) You can also apply multiple conditions using \u0026 or | (AND/OR) to filter data. For example, finding employees over 30 years old and in the IT department:\n# Filter rows where Age \u003e 30 and Department is 'IT' filtered_df = df.filter((df[\"Age\"] \u003e 30) \u0026 (df[\"Department\"] == \"IT\")) filtered_df.show() 4. Filtering on Null or Non-Null Values Filtering based on whether a column has NULL values or not is crucial for data cleaning:\n# Filter rows where 'Address' is NULL filtered_df = df.filter(df[\"Address\"].isNull()) filtered_df.show() # Filter rows where 'Email' is NOT NULL filtered_df = df.filter(df[\"Email\"].isNotNull()) filtered_df.show() 5. Handling Unique or Distinct Data To get distinct rows or unique values from your dataset:\n# Get distinct rows from the entire DataFrame unique_df = df.distinct() unique_df.show() # Get distinct values from the 'Department' column unique_departments_df = df.select(\"Department\").distinct() unique_departments_df.show() To remove duplicates based on specific columns, such as Email or Phone, use dropDuplicates():\n# Remove duplicates based on 'Email' column unique_df = df.dropDuplicates([\"Email\"]) unique_df.show() # Remove duplicates based on both 'Phone' and 'Email' unique_df = df.dropDuplicates([\"Phone\", \"Email\"]) unique_df.show() 6. Counting Distinct Values You can count distinct values in a particular column, or combinations of columns:\n# Count distinct values in the 'Department' column distinct_count_department = df.select(\"Department\").distinct().count() print(\"Distinct Department Count:\", distinct_count_department) # Count distinct combinations of 'Department' and 'Performance_Rating' distinct_combinations_count = df.select(\"Department\", \"Performance_Rating\").distinct().count() print(\"Distinct Department and Performance Rating Combinations:\", distinct_combinations_count) This set of operations will help you efficiently manage and transform your data in PySpark, ensuring data integrity and accuracy for your analysis!\nMastering PySpark DataFrame Operations Changing Data Types: Easily modify column types using .cast(). E.g., change ‘Salary’ to double or ‘Phone’ to string for better data handling. Filtering Data: Use .filter() or .where() to extract specific rows. For example, filter employees with a salary over 50,000 or non-null Age. Multiple Conditions: Chain filters with \u0026 and | to apply complex conditions, such as finding employees over 30 in the IT department. Handling NULLs: Use .isNull() and .isNotNull() to filter rows with missing or available values, such as missing addresses or valid emails. Unique/Distinct Values: Use .distinct() to get unique rows or distinct values in a column. Remove duplicates based on specific fields like Email or Phone using .dropDuplicates(). Count Distinct Values: Count distinct values in one or multiple columns to analyze data diversity, such as counting unique departments or combinations of Department and Performance_Rating.",
    "description": "Column Selection \u0026 Manipulation 1. Different Methods to Select Columns In PySpark, you can select specific columns in multiple ways:\n# Using col() function df.select(col(\"Name\")).show() # Using column() function df.select(column(\"Age\")).show() # Directly using string name df.select(\"Salary\").show() 2. Selecting Multiple Columns Together You can combine different methods to select multiple columns:\n# multiple column df2 = df.select(\"ID\", \"Name\", col(\"Salary\"), column(\"Department\"), df.Phone) df2.show() 3. Listing All Columns in a DataFrame To get a list of all the column names:",
    "tags": [],
    "title": "DF Operations",
    "uri": "/azure_data_bricks/df-operations/"
  },
  {
    "breadcrumb": "",
    "content": "PySpark Basics 1. Introduction Big Data Overview Introduction to Apache Spark Introduction to Azure Databricks Databricks Architecture Databricks Workspace Overview 2. Spark Core Concepts RDDs (Resilient Distributed Datasets) Transformations and Actions Lazy Evaluation Spark Execution Model Caching and Persistence Partitioning 3. Spark SQL DataFrames and Datasets Spark SQL Engine Schema Inference Catalog and Metadata Temporary and Global Views SQL Queries in Spark Performance Optimization (Catalyst Optimizer, Tungsten) 4. Spark Data Sources Reading/Writing CSV, JSON, Parquet, ORC Working with Delta Lake JDBC Data Sources Streaming Data Sources Handling Nested Data (Struct, Array, Map) 5. Delta Lake Introduction to Delta Lake ACID Transactions Schema Enforcement \u0026 Evolution Time Travel Upserts \u0026 Deletes (MERGE) Delta Lake Optimization (Z-order, Optimize, Vacuum) 6. Spark Streaming \u0026 Structured Streaming Introduction to Streaming DStreams vs Structured Streaming Streaming Sources (Kafka, Event Hubs, Socket) Watermarking \u0026 Late Data State Management Streaming Sinks (Delta, Console, Kafka, Event Hubs) 7. Spark MLlib (Machine Learning) MLlib Basics Feature Engineering Pipelines \u0026 Transformers Classification Regression Clustering Model Persistence 8. Spark GraphX Introduction to GraphX Graph Processing Basics Graph Algorithms Use Cases 9. Performance Tuning Spark Configurations \u0026 Parameters Memory Management Shuffle Optimizations Caching Strategies Adaptive Query Execution (AQE) Skew Handling 10. Azure Databricks Essentials Databricks Clusters (Types, Autoscaling, Pools) Databricks Notebooks Databricks Repos \u0026 Git Integration Databricks Jobs \u0026 Scheduling Databricks Secrets Databricks Utilities (DBUtils) Workspace Security \u0026 Permissions 11. Integration with Azure Services Azure Data Lake Storage (ADLS) Integration Azure Blob Storage Integration Azure Synapse Analytics Azure Event Hubs Azure Data Factory (ADF) with Databricks Power BI with Databricks 12. Advanced Databricks Unity Catalog Databricks SQL Delta Live Tables (DLT) Databricks REST API Databricks CLI Monitoring \u0026 Logging Cost Optimization in Databricks 13. Real-World Use Cases ETL Pipeline with Databricks Streaming Data Pipeline Batch Data Processing Machine Learning Pipeline End-to-End Data Lakehouse Implementation 14. Best Practices Cluster Sizing \u0026 Configuration Code Versioning \u0026 Collaboration Data Governance Security \u0026 Compliance CI/CD with Databricks Productionizing Workloads 15. Capstone Project Build a Scalable Data Lakehouse with Databricks Real-Time Streaming Data Pipeline ML Model Training and Deployment",
    "description": "PySpark Basics 1. Introduction Big Data Overview Introduction to Apache Spark Introduction to Azure Databricks Databricks Architecture Databricks Workspace Overview 2. Spark Core Concepts RDDs (Resilient Distributed Datasets) Transformations and Actions Lazy Evaluation Spark Execution Model Caching and Persistence Partitioning 3. Spark SQL DataFrames and Datasets Spark SQL Engine Schema Inference Catalog and Metadata Temporary and Global Views SQL Queries in Spark Performance Optimization (Catalyst Optimizer, Tungsten) 4. Spark Data Sources Reading/Writing CSV, JSON, Parquet, ORC Working with Delta Lake JDBC Data Sources Streaming Data Sources Handling Nested Data (Struct, Array, Map) 5. Delta Lake Introduction to Delta Lake ACID Transactions Schema Enforcement \u0026 Evolution Time Travel Upserts \u0026 Deletes (MERGE) Delta Lake Optimization (Z-order, Optimize, Vacuum) 6. Spark Streaming \u0026 Structured Streaming Introduction to Streaming DStreams vs Structured Streaming Streaming Sources (Kafka, Event Hubs, Socket) Watermarking \u0026 Late Data State Management Streaming Sinks (Delta, Console, Kafka, Event Hubs) 7. Spark MLlib (Machine Learning) MLlib Basics Feature Engineering Pipelines \u0026 Transformers Classification Regression Clustering Model Persistence 8. Spark GraphX Introduction to GraphX Graph Processing Basics Graph Algorithms Use Cases 9. Performance Tuning Spark Configurations \u0026 Parameters Memory Management Shuffle Optimizations Caching Strategies Adaptive Query Execution (AQE) Skew Handling 10. Azure Databricks Essentials Databricks Clusters (Types, Autoscaling, Pools) Databricks Notebooks Databricks Repos \u0026 Git Integration Databricks Jobs \u0026 Scheduling Databricks Secrets Databricks Utilities (DBUtils) Workspace Security \u0026 Permissions 11. Integration with Azure Services Azure Data Lake Storage (ADLS) Integration Azure Blob Storage Integration Azure Synapse Analytics Azure Event Hubs Azure Data Factory (ADF) with Databricks Power BI with Databricks 12. Advanced Databricks Unity Catalog Databricks SQL Delta Live Tables (DLT) Databricks REST API Databricks CLI Monitoring \u0026 Logging Cost Optimization in Databricks 13. Real-World Use Cases ETL Pipeline with Databricks Streaming Data Pipeline Batch Data Processing Machine Learning Pipeline End-to-End Data Lakehouse Implementation 14. Best Practices Cluster Sizing \u0026 Configuration Code Versioning \u0026 Collaboration Data Governance Security \u0026 Compliance CI/CD with Databricks Productionizing Workloads 15. Capstone Project Build a Scalable Data Lakehouse with Databricks Real-Time Streaming Data Pipeline ML Model Training and Deployment",
    "tags": [],
    "title": "Azure Data Bricks",
    "uri": "/azure_data_bricks/"
  },
  {
    "breadcrumb": "ADB",
    "content": "Sorting and String Functions from pyspark.sql import SparkSession from pyspark.sql.functions import col, desc, asc, concat, concat_ws, initcap, lower, upper, instr, length, lit # Create a Spark session spark = SparkSession.builder.appName(\"SortingAndStringFunctions\").getOrCreate() # Sample data data = [ (\"USA\", \"North America\", 100, 50.5), (\"India\", \"Asia\", 300, 20.0), (\"Germany\", \"Europe\", 200, 30.5), (\"Australia\", \"Oceania\", 150, 60.0), (\"Japan\", \"Asia\", 120, 45.0), (\"Brazil\", \"South America\", 180, 25.0) ] # Define the schema columns = [\"Country\", \"Region\", \"UnitsSold\", \"UnitPrice\"] # Create DataFrame df = spark.createDataFrame(data, columns) # Display the original DataFrame df.show() Sorting the DataFrame 1. Sort by a single column (ascending order) df.orderBy(\"Country\").show(5)\nBy default, sorting is ascending. This shows the first 5 countries alphabetically.\n2. Sort by multiple columns df.orderBy(col(\"Country\").asc(), col(\"UnitsSold\").asc()).show()\nFirst sorts by Country, then within each country sorts by UnitsSold.\n3. Sort by column in descending order and limit df.orderBy(col(\"Country\").desc()).limit(3).show()\nSorts by Country in descending order and returns the top 3 rows.\n4. Sorting with null values last df.orderBy(col(\"Country\").asc_nulls_last()).show()\nEnsures null values appear at the end when sorting.\nKey Functions:\nUse .orderBy() or .sort() to sort DataFrames.\nControl order with asc() or desc().\nString Functions 1. Capitalize first letter of each word df.select(initcap(col(\"Country\"))).show()\nConverts \"united states\" → \"United States\".\n2. Convert all text to lowercase df.select(lower(col(\"Country\"))).show()\n3. Convert all text to uppercase df.select(upper(col(\"Country\"))).show()\nKey Functions:\ninitcap() → Capitalize first letter of each word.\nlower() → Convert to lowercase.\nupper() → Convert to uppercase.\nConcatenation Functions 1. Concatenate two columns df.select(concat(col(\"Region\"), col(\"Country\"))).show()\nJoins Region and Country without separator.\n2. Concatenate with a separator df.select(concat_ws(\" | \", col(\"Region\"), col(\"Country\"))).show()\nJoins with \" | \" as separator.\n3. Create a new concatenated column df.withColumn(\"RegionCountry\", concat_ws(\" \", col(\"Region\"), col(\"Country\"))).show()\nAdds a new column combining Region and Country.\nKey Functions:\nconcat() → Join columns directly.\nconcat_ws() → Join columns with a separator.\n📌 Summary Sorting: Use .orderBy() or .sort() with asc() / desc().\nString Manipulation: Use initcap(), lower(), upper().\nConcatenation: Use concat() or concat_ws() for flexible joins.\nSplit Function in DataFrame Let’s create a PySpark DataFrame for employee data with columns such as EmployeeID, Name, Department, and Skills. We’ll explore split, explode, and other useful array functions.\nSample Data Creation for Employee Data from pyspark.sql import SparkSession from pyspark.sql.functions import split, explode, size, array_contains, col # Sample employee data data = [ (1, \"Alice\", \"HR\", \"Communication Management\"), (2, \"Bob\", \"IT\", \"Programming Networking\"), (3, \"Charlie\", \"Finance\", \"Accounting Analysis\"), (4, \"David\", \"HR\", \"Recruiting Communication\"), (5, \"Eve\", \"IT\", \"Cloud DevOps\") ] # Define the schema columns = [\"EmployeeID\", \"Name\", \"Department\", \"Skills\"] # Create DataFrame df = spark.createDataFrame(data, columns) # Display the original DataFrame df.show(truncate=False) Examples 1. Split the Skills column df.select(\"*\", split(col(\"Skills\"), \" \").alias(\"Skills_Array\")).show(truncate=False)\nSplits the Skills column into an array of skills using space as a delimiter.\n2. Select the first skill from Skills_Array df.select(\"*\", split(col(\"Skills\"), \" \").alias(\"Skills_Array\")).selectExpr(\"*\", \"Skills_Array[0] as First_Skill\").show(truncate=False)\nUses index notation (Skills_Array[0]) to pick the first skill. Indexing starts from 0.\n3. Count the number of skills per employee df.select(\"*\", split(col(\"Skills\"), \" \").alias(\"Skills_Array\")).select(\"*\", size(col(\"Skills_Array\")).alias(\"Skills_Count\")).show(truncate=False)\nThe size() function returns the number of elements in the array.\n4. Check if the employee has “Cloud” skill df.select(\"*\", split(col(\"Skills\"), \" \").alias(\"Skills_Array\")).select(\"*\", array_contains(col(\"Skills_Array\"), \"Cloud\").alias(\"Has_Cloud_Skill\")).show(truncate=False)\narray_contains() returns True if \"Cloud\" exists in the skill set.\n5. Explode the Skills_Array into multiple rows df.select(\"*\", split(col(\"Skills\"), \" \").alias(\"Skills_Array\")).select(\"EmployeeID\", \"Name\", \"Department\", explode(col(\"Skills_Array\")).alias(\"Individual_Skill\")).show(truncate=False)\nexplode() flattens the array into rows, where each skill becomes a separate row for that employee.\n📌 Summary of Key Functions split() → Splits a string into an array.\nexplode() → Converts an array into multiple rows.\nsize() → Counts elements in an array.\narray_contains() → Checks if an array contains a value.\nselectExpr() → Lets you query arrays using SQL expressions like Skills_Array[0].\nTrim Function in DataFrame Let’s create a sample dataset for employees and demonstrate string trimming and padding functions in PySpark:\nltrim() rtrim() trim() lpad() rpad() Sample Data Creation for Employees from pyspark.sql import SparkSession from pyspark.sql.functions import lit, ltrim, rtrim, rpad, lpad, trim, col # Sample employee data with leading and trailing spaces in the 'Name' column data = [ (1, \" Alice \", \"HR\"), (2, \" Bob\", \"IT\"), (3, \"Charlie \", \"Finance\"), (4, \" David \", \"HR\"), (5, \"Eve \", \"IT\") ] # Define the schema for the DataFrame columns = [\"EmployeeID\", \"Name\", \"Department\"] # Create DataFrame df = spark.createDataFrame(data, columns) # Show the original DataFrame df.show(truncate=False) Applying Trimming and Padding Functions 1. Trimming Functions ltrim() → Removes leading spaces.\nrtrim() → Removes trailing spaces.\ntrim() → Removes both leading and trailing spaces.\n2. Padding Functions lpad() → Pads the left side of a string with a character up to a given length.\nrpad() → Pads the right side of a string with a character up to a given length.\nExample # Apply trimming and padding functions result_df = df.select( col(\"EmployeeID\"), col(\"Department\"), ltrim(col(\"Name\")).alias(\"ltrim_Name\"), # Remove leading spaces rtrim(col(\"Name\")).alias(\"rtrim_Name\"), # Remove trailing spaces trim(col(\"Name\")).alias(\"trim_Name\"), # Remove both leading \u0026 trailing spaces lpad(col(\"Name\"), 10, \"X\").alias(\"lpad_Name\"), # Left pad with \"X\" to length 10 rpad(col(\"Name\"), 10, \"Y\").alias(\"rpad_Name\") # Right pad with \"Y\" to length 10 ) # Show the resulting DataFrame result_df.show(truncate=False) Output Explanation ltrim_Name → Leading spaces removed.\nrtrim_Name → Trailing spaces removed.\ntrim_Name → Both leading \u0026 trailing spaces removed.\nlpad_Name → Padded left with \"X\" until length = 10.\nrpad_Name → Padded right with \"Y\" until length = 10.\n📌 Summary Use trim functions (ltrim, rtrim, trim) to clean up unwanted spaces.\nUse padding functions (lpad, rpad) to format strings with fixed lengths.",
    "description": "Sorting and String Functions from pyspark.sql import SparkSession from pyspark.sql.functions import col, desc, asc, concat, concat_ws, initcap, lower, upper, instr, length, lit # Create a Spark session spark = SparkSession.builder.appName(\"SortingAndStringFunctions\").getOrCreate() # Sample data data = [ (\"USA\", \"North America\", 100, 50.5), (\"India\", \"Asia\", 300, 20.0), (\"Germany\", \"Europe\", 200, 30.5), (\"Australia\", \"Oceania\", 150, 60.0), (\"Japan\", \"Asia\", 120, 45.0), (\"Brazil\", \"South America\", 180, 25.0) ] # Define the schema columns = [\"Country\", \"Region\", \"UnitsSold\", \"UnitPrice\"] # Create DataFrame df = spark.createDataFrame(data, columns) # Display the original DataFrame df.show() Sorting the DataFrame 1. Sort by a single column (ascending order) df.orderBy(\"Country\").show(5)\nBy default, sorting is ascending. This shows the first 5 countries alphabetically.",
    "tags": [],
    "title": "Functions",
    "uri": "/azure_data_bricks/functions/"
  },
  {
    "breadcrumb": "ADB",
    "content": "In PySpark, you can use various date functions to manipulate and analyze date and timestamp columns.\nWe’ll explore:\ncurrent_date current_timestamp date_add date_sub datediff months_between Sample Code from pyspark.sql import SparkSession from pyspark.sql.functions import current_date, current_timestamp, date_add, date_sub, col, datediff, months_between, to_date, lit # Generate a DataFrame with 10 rows, adding \"today\" and \"now\" columns dateDF = spark.range(10).withColumn(\"today\", current_date()).withColumn(\"now\", current_timestamp()) # Show the DataFrame with today and now columns dateDF.show(truncate=False) Key Functions current_date() → Returns current date. current_timestamp() → Returns current timestamp (date + time). date_sub(col(\"today\"), 5) → Subtracts 5 days. date_add(col(\"today\"), 5) → Adds 5 days. datediff(date1, date2) → Returns difference in days. months_between(date1, date2) → Returns difference in months. Working with dates and timestamps often requires converting formats and extracting components.\nWe’ll explore:\nto_date to_timestamp year, month, dayofmonth hour, minute, second Notes 1. to_date Converts a string to a date (default format: yyyy-MM-dd). If format doesn’t match, returns null. Example: to_date(lit(\"2017-12-11\"), \"yyyy-dd-MM\") 2. to_timestamp Converts a string with date \u0026 time into a timestamp. Allows extraction of time components. 3. Extracting Components year() → Extracts year. month() → Extracts month. dayofmonth() → Extracts day. hour() → Extracts hour. minute() → Extracts minute. second() → Extracts second. Example Output For input \"2017-12-11\" (format yyyy-dd-MM):\nYear: 2017 Month: 12 Day: 11 Hour: 0 Minute: 0 Second: 0 For invalid input (e.g., \"2017-20-12\"):\nResult: null",
    "description": "In PySpark, you can use various date functions to manipulate and analyze date and timestamp columns.\nWe’ll explore:\ncurrent_date current_timestamp date_add date_sub datediff months_between Sample Code from pyspark.sql import SparkSession from pyspark.sql.functions import current_date, current_timestamp, date_add, date_sub, col, datediff, months_between, to_date, lit # Generate a DataFrame with 10 rows, adding \"today\" and \"now\" columns dateDF = spark.range(10).withColumn(\"today\", current_date()).withColumn(\"now\", current_timestamp()) # Show the DataFrame with today and now columns dateDF.show(truncate=False) Key Functions current_date() → Returns current date. current_timestamp() → Returns current timestamp (date + time). date_sub(col(\"today\"), 5) → Subtracts 5 days. date_add(col(\"today\"), 5) → Adds 5 days. datediff(date1, date2) → Returns difference in days. months_between(date1, date2) → Returns difference in months. Working with dates and timestamps often requires converting formats and extracting components.\nWe’ll explore:",
    "tags": [],
    "title": "Date Functions",
    "uri": "/azure_data_bricks/date/"
  },
  {
    "breadcrumb": "ADB",
    "content": "Sample Sales Data with Null Values # Sample data: sales data with nulls data = [ (\"John\", \"North\", 100, None), (\"Doe\", \"East\", None, 50), (None, \"West\", 150, 30), (\"Alice\", None, 200, 40), (\"Bob\", \"South\", None, None), (None, None, None, None) ] columns = [\"Name\", \"Region\", \"UnitsSold\", \"Revenue\"] # Create DataFrame df = spark.createDataFrame(data, columns) df.show() 1. Detecting Null Values Use isNull() to identify rows where a column contains null values. The output is a boolean flag indicating whether the value is null. 2. Dropping Rows with Null Values dropna() removes rows with nulls in any column (default mode). Use how=\"all\" to remove rows only if all columns are null. Use subset=[\"col1\", \"col2\"] to target specific columns. 3. Filling Null Values fillna() replaces nulls with specified default values. Replace across all columns or selectively. Example: Replace Region nulls with \"Unknown\". Replace UnitsSold and Revenue nulls with 0. 4. Coalesce Function coalesce() returns the first non-null value among multiple columns. Useful when providing fallback values if some columns contain nulls. 5. Handling Nulls in Aggregations Nulls can distort aggregations like mean(). Use coalesce() to substitute nulls with defaults (e.g., 0.0). This prevents inaccurate results. 📌 Summary Detecting Nulls: Use isNull() to find null values. Dropping Nulls: Use dropna() to remove rows with nulls (all or specific columns). Filling Nulls: Use fillna() to replace nulls with defaults. Coalesce Function: Use coalesce() to return the first non-null value. Aggregations: Use coalesce() in aggregations to handle nulls safely.",
    "description": "Sample Sales Data with Null Values # Sample data: sales data with nulls data = [ (\"John\", \"North\", 100, None), (\"Doe\", \"East\", None, 50), (None, \"West\", 150, 30), (\"Alice\", None, 200, 40), (\"Bob\", \"South\", None, None), (None, None, None, None) ] columns = [\"Name\", \"Region\", \"UnitsSold\", \"Revenue\"] # Create DataFrame df = spark.createDataFrame(data, columns) df.show() 1. Detecting Null Values Use isNull() to identify rows where a column contains null values. The output is a boolean flag indicating whether the value is null. 2. Dropping Rows with Null Values dropna() removes rows with nulls in any column (default mode). Use how=\"all\" to remove rows only if all columns are null. Use subset=[\"col1\", \"col2\"] to target specific columns. 3. Filling Null Values fillna() replaces nulls with specified default values. Replace across all columns or selectively. Example: Replace Region nulls with \"Unknown\". Replace UnitsSold and Revenue nulls with 0. 4. Coalesce Function coalesce() returns the first non-null value among multiple columns. Useful when providing fallback values if some columns contain nulls. 5. Handling Nulls in Aggregations Nulls can distort aggregations like mean(). Use coalesce() to substitute nulls with defaults (e.g., 0.0). This prevents inaccurate results. 📌 Summary Detecting Nulls: Use isNull() to find null values. Dropping Nulls: Use dropna() to remove rows with nulls (all or specific columns). Filling Nulls: Use fillna() to replace nulls with defaults. Coalesce Function: Use coalesce() to return the first non-null value. Aggregations: Use coalesce() in aggregations to handle nulls safely.",
    "tags": [],
    "title": "Handling Nulls",
    "uri": "/azure_data_bricks/nulls/"
  },
  {
    "breadcrumb": "ADB",
    "content": "Basic Aggregate Functions Sample Data from pyspark.sql import Row # Create sample data data = [ Row(id=1, value=10), Row(id=2, value=20), Row(id=3, value=30), Row(id=4, value=None), Row(id=5, value=40), Row(id=6, value=20) ] # Create DataFrame df = spark.createDataFrame(data) # Show the DataFrame df.show() Aggregate Functions in PySpark Summation (sum) – Adds up the values in a column. Average (avg) – Computes the average of values in a column. Count (count) – Counts the number of non-null values in a column. Maximum (max) / Minimum (min) – Finds the highest and lowest values. Distinct Count (countDistinct) – Counts unique values in a column. Notes Handling Nulls:\ncount() counts only non-null values. sum(), avg(), max(), and min() ignore null values. Performance:\nAggregate functions can be expensive on large datasets; partitioning improves performance.\nUse Cases:\nSummation: Total sales, total revenue. Average: Average sales per day. Count: Number of transactions. Max/Min: Highest and lowest values (e.g., max sales in a day). Distinct Count: Unique customers, unique products. Part 2 – Advanced Aggregation Functions Sample Data from pyspark.sql import SparkSession from pyspark.sql import functions as F from pyspark.sql.types import StructType, StructField, StringType, IntegerType # Create Spark session spark = SparkSession.builder.appName(\"AggregationExamples\").getOrCreate() # Sample data data = [ (\"HR\", 10000, 500, \"John\"), (\"Finance\", 20000, 1500, \"Doe\"), (\"HR\", 15000, 1000, \"Alice\"), (\"Finance\", 25000, 2000, \"Eve\"), (\"HR\", 20000, 1500, \"Mark\") ] # Define schema schema = StructType([ StructField(\"department\", StringType(), True), StructField(\"salary\", IntegerType(), True), StructField(\"bonus\", IntegerType(), True), StructField(\"employee_name\", StringType(), True) ]) # Create DataFrame df = spark.createDataFrame(data, schema) df.show() 1. Grouped Aggregation Perform aggregation within groups based on a column.\nsum() → Adds values within the group. avg() → Computes group average. max() → Finds maximum value. min() → Finds minimum value. 2. Multiple Aggregations Perform several aggregations in one step.\ncount() → Number of rows in each group. avg() → Average of values. max() → Maximum value in group. 3. Concatenate Strings concat_ws() → Concatenates string values within a column, separated by a delimiter (,). 4. First and Last first() → Retrieves the first value of a column in a group. last() → Retrieves the last value of a column in a group. 5. Standard Deviation and Variance stddev() → Standard deviation of values. variance() → Variance of values. 6. Aggregation with Alias .alias() → Rename the result columns after aggregation. 7. Sum of Distinct Values sumDistinct() → Sums only unique values in a column (avoids double-counting duplicates). 📌 Summary Use basic aggregations (sum, avg, count, max, min, countDistinct) for general metrics.\nApply advanced aggregations (grouped, concat_ws, first, last, stddev, variance, sumDistinct) for deeper analysis.\nAlways consider null handling and performance optimizations when using aggregate functions in PySpark.",
    "description": "Basic Aggregate Functions Sample Data from pyspark.sql import Row # Create sample data data = [ Row(id=1, value=10), Row(id=2, value=20), Row(id=3, value=30), Row(id=4, value=None), Row(id=5, value=40), Row(id=6, value=20) ] # Create DataFrame df = spark.createDataFrame(data) # Show the DataFrame df.show() Aggregate Functions in PySpark Summation (sum) – Adds up the values in a column. Average (avg) – Computes the average of values in a column. Count (count) – Counts the number of non-null values in a column. Maximum (max) / Minimum (min) – Finds the highest and lowest values. Distinct Count (countDistinct) – Counts unique values in a column. Notes Handling Nulls:",
    "tags": [],
    "title": "Aggregate functions",
    "uri": "/azure_data_bricks/aggregations/"
  },
  {
    "breadcrumb": "ADB",
    "content": "Joins in PySpark In PySpark Joins are used to combine two DataFrames based on a common column or condition.\nTypes of Joins in PySpark Inner Join: Matches rows from both DataFrames. df1.join(df2, df1.common_column == df2.common_column, \"inner\") Left/Right Join: Keeps all rows from the left or right DataFrame and matches where possible. df1.join(df2, df1.common_column == df2.common_column, \"left\") df1.join(df2, df1.common_column == df2.common_column, \"right\") Full Join: Keeps all rows from both DataFrames. df1.join(df2, df1.common_column == df2.common_column, \"outer\") Left Semi: Filters df1 to rows that match df2 without including columns from df2. df1.join(df2, df1.common_column == df2.common_column, \"left_semi\") Left Anti: Filters df1 to rows that do not match df2. df1.join(df2, df1.common_column == df2.common_column, \"left_anti\") Cross Join: Returns the Cartesian product, combining all rows of both DataFrames. df1.crossJoin(df2) Explicit Condition Join: Allows complex join conditions, including columns with different names. df1.join(df2, df1.columnA == df2.columnB, \"inner\") df1.join(df2, df1.common_column == df2.common_column, \"inner\") df1.join(df2, df1.common_column == df2.common_column, \"left\") df1.join(df2, df1.common_column == df2.common_column, \"right\") df1.join(df2, df1.common_column == df2.common_column, \"outer\") df1.join(df2, df1.common_column == df2.common_column, \"left_semi\") df1.join(df2, df1.common_column == df2.common_column, \"left_anti\") df1.crossJoin(df2) df1.join(df2, df1.columnA == df2.columnB, \"inner\") Practice-1 from pyspark.sql import SparkSession from pyspark.sql import Row from pyspark.sql.functions import broadcast # Initialize Spark session spark = SparkSession.builder.appName(\"JoinsExample\").getOrCreate() # Sample DataFrames data1 = [Row(id=0), Row(id=1), Row(id=1), Row(id=None), Row(id=None)] data2 = [Row(id=1), Row(id=0), Row(id=None)] df1 = spark.createDataFrame(data1) df2 = spark.createDataFrame(data2) # Inner Join inner_join = df1.join(df2, on=\"id\", how=\"inner\") print(\"Inner Join:\") inner_join.show() # Right Join right_join = df1.join(df2, on=\"id\", how=\"right\") print(\"Right Join:\") right_join.show() # Full (Outer) Join full_join = df1.join(df2, on=\"id\", how=\"outer\") print(\"Full (Outer) Join:\") full_join.show() # Left Anti Join left_anti_join = df1.join(df2, on=\"id\", how=\"left_anti\") print(\"Left Anti Join:\") left_anti_join.show() # Right Anti Join (Equivalent to swapping DataFrames and performing Left Anti Join) right_anti_join = df2.join(df1, on=\"id\", how=\"left_anti\") print(\"Right Anti Join:\") right_anti_join.show() # Broadcast Join (Optimizing a join with a smaller DataFrame) broadcast_join = df1.join(broadcast(df2), on=\"id\", how=\"inner\") print(\"Broadcast Join:\") broadcast_join.show() Practice 2 PySpark Coding Questions Find employees whose location matches the location of their department\nDisplay: emp_id, emp_name, emp_location, dept_name, dept_location. Find departments that have no employees assigned to them\nDisplay: dept_id, dept_name, dept_head. Get the average salary of employees in each department\nDisplay: dept_name, average_salary. List the employees who earn more than the average salary of their department\nDisplay: emp_id, emp_name, emp_salary, dept_name, dept_location. # Sample DataFrames emp_data = [ Row(emp_id=1, emp_name=\"Alice\", emp_salary=50000, emp_dept_id=101, emp_location=\"New York\"), Row(emp_id=2, emp_name=\"Bob\", emp_salary=60000, emp_dept_id=102, emp_location=\"Los Angeles\"), Row(emp_id=3, emp_name=\"Charlie\", emp_salary=55000, emp_dept_id=101, emp_location=\"Chicago\"), Row(emp_id=4, emp_name=\"David\", emp_salary=70000, emp_dept_id=103, emp_location=\"San Francisco\"), Row(emp_id=5, emp_name=\"Eve\", emp_salary=48000, emp_dept_id=102, emp_location=\"Houston\") ] dept_data = [ Row(dept_id=101, dept_name=\"Engineering\", dept_head=\"John\", dept_location=\"New York\"), Row(dept_id=102, dept_name=\"Marketing\", dept_head=\"Mary\", dept_location=\"Los Angeles\"), Row(dept_id=103, dept_name=\"Finance\", dept_head=\"Frank\", dept_location=\"Chicago\") ] emp_columns = [\"emp_id\", \"emp_name\", \"emp_salary\", \"emp_dept_id\", \"emp_location\"] dept_columns = [\"dept_id\", \"dept_name\", \"dept_head\", \"dept_location\"] emp_df = spark.createDataFrame(emp_data, emp_columns) dept_df = spark.createDataFrame(dept_data, dept_columns) # Display emp data print(\"emp_data:\") emp_df.show() # Display dept data print(\"dept_data:\") dept_df.show() # Inner Join on emp_dept_id and dept_id inner_join = emp_df.join(dept_df, emp_df[\"emp_dept_id\"] == dept_df[\"dept_id\"], \"inner\") # Display the result print(\"Inner Join Result:\") inner_join.show() # Inner Join with Filtering Columns and WHERE Condition inner_join = emp_df.join(dept_df, emp_df[\"emp_dept_id\"] == dept_df[\"dept_id\"], \"inner\") \\ .select(\"emp_id\", \"emp_name\", \"emp_salary\", \"dept_name\", \"dept_location\") \\ .filter(\"emp_salary \u003e 55000\") # Add a WHERE condition # Display the result print(\"Inner Join with Filter and WHERE Condition:\") inner_join.show() # Left Join with Filtering Columns and WHERE Condition left_join_filtered = emp_df.join(dept_df, emp_df[\"emp_dept_id\"] == dept_df[\"dept_id\"], \"left\") \\ .select(\"emp_id\", \"emp_name\", \"dept_name\", \"dept_location\") \\ .filter(\"emp_salary \u003e 55000\") # Add a WHERE condition # Display the result print(\"Left Join with Filter and WHERE Condition:\") left_join_filtered.show() # Left Anti Join left_anti_join = emp_df.join( dept_df, emp_df[\"emp_dept_id\"] == dept_df[\"dept_id\"], \"left_anti\" ) # Display the result print(\"Left Anti Join Result:\") left_anti_join.show() Practice 3 PySpark Coding Questions List each employee along with their manager’s name\nDisplay: employee, manager. Find employees who do not have a manager (CEO-level employees)\nDisplay: employee, manager. Find all employees who directly report to “Manager A”\nDisplay: empid, ename, mrgid. Determine the hierarchy level of each employee\nCEO → Level 1, direct reports to CEO → Level 2, and so on. Display: empid, ename, mrgid, level. from pyspark.sql import SparkSession from pyspark.sql.functions import col, expr # Create a Spark session spark = SparkSession.builder.appName(\"EmployeeHierarchy\").getOrCreate() # Sample data data = [ (1, None, \"CEO\"), (2, 1, \"Manager A\"), (3, 1, \"Manager B\"), (4, 2, \"Employee X\"), (5, 3, \"Employee Y\"), ] columns = [\"empid\", \"mrgid\", \"ename\"] employee_df = spark.createDataFrame(data, columns) # Display the result print(\"emp_data:\") employee_df.show() # Self-join to find the manager and CEO manager_df = employee_df.alias(\"e\") \\ .join(employee_df.alias(\"m\"), col(\"e.mrgid\") == col(\"m.empid\"), \"left\") \\ .select( col(\"e.ename\").alias(\"employee\"), col(\"m.ename\").alias(\"manager\") ) # Display the result print(\"mgr:\") manager_df.show() # Filter for employees without a manager (CEO) manager_df2 = employee_df.alias(\"e1\") \\ .join(employee_df.alias(\"m1\"), col(\"e1.mrgid\") == col(\"m1.empid\"), \"left\") \\ .select( col(\"e1.ename\").alias(\"employee\"), col(\"m1.ename\").alias(\"manager\") ) \\ .filter(col(\"manager\").isNull()) # Display the result manager_df2.show()",
    "description": "Joins in PySpark In PySpark Joins are used to combine two DataFrames based on a common column or condition.\nTypes of Joins in PySpark Inner Join: Matches rows from both DataFrames. df1.join(df2, df1.common_column == df2.common_column, \"inner\") Left/Right Join: Keeps all rows from the left or right DataFrame and matches where possible. df1.join(df2, df1.common_column == df2.common_column, \"left\") df1.join(df2, df1.common_column == df2.common_column, \"right\") Full Join: Keeps all rows from both DataFrames. df1.join(df2, df1.common_column == df2.common_column, \"outer\") Left Semi: Filters df1 to rows that match df2 without including columns from df2. df1.join(df2, df1.common_column == df2.common_column, \"left_semi\") Left Anti: Filters df1 to rows that do not match df2. df1.join(df2, df1.common_column == df2.common_column, \"left_anti\") Cross Join: Returns the Cartesian product, combining all rows of both DataFrames. df1.crossJoin(df2) Explicit Condition Join: Allows complex join conditions, including columns with different names. df1.join(df2, df1.columnA == df2.columnB, \"inner\") df1.join(df2, df1.common_column == df2.common_column, \"inner\") df1.join(df2, df1.common_column == df2.common_column, \"left\") df1.join(df2, df1.common_column == df2.common_column, \"right\") df1.join(df2, df1.common_column == df2.common_column, \"outer\") df1.join(df2, df1.common_column == df2.common_column, \"left_semi\") df1.join(df2, df1.common_column == df2.common_column, \"left_anti\") df1.crossJoin(df2) df1.join(df2, df1.columnA == df2.columnB, \"inner\") Practice-1 from pyspark.sql import SparkSession from pyspark.sql import Row from pyspark.sql.functions import broadcast # Initialize Spark session spark = SparkSession.builder.appName(\"JoinsExample\").getOrCreate() # Sample DataFrames data1 = [Row(id=0), Row(id=1), Row(id=1), Row(id=None), Row(id=None)] data2 = [Row(id=1), Row(id=0), Row(id=None)] df1 = spark.createDataFrame(data1) df2 = spark.createDataFrame(data2) # Inner Join inner_join = df1.join(df2, on=\"id\", how=\"inner\") print(\"Inner Join:\") inner_join.show() # Right Join right_join = df1.join(df2, on=\"id\", how=\"right\") print(\"Right Join:\") right_join.show() # Full (Outer) Join full_join = df1.join(df2, on=\"id\", how=\"outer\") print(\"Full (Outer) Join:\") full_join.show() # Left Anti Join left_anti_join = df1.join(df2, on=\"id\", how=\"left_anti\") print(\"Left Anti Join:\") left_anti_join.show() # Right Anti Join (Equivalent to swapping DataFrames and performing Left Anti Join) right_anti_join = df2.join(df1, on=\"id\", how=\"left_anti\") print(\"Right Anti Join:\") right_anti_join.show() # Broadcast Join (Optimizing a join with a smaller DataFrame) broadcast_join = df1.join(broadcast(df2), on=\"id\", how=\"inner\") print(\"Broadcast Join:\") broadcast_join.show() Practice 2 PySpark Coding Questions Find employees whose location matches the location of their department",
    "tags": [],
    "title": "Joins",
    "uri": "/azure_data_bricks/joins/"
  },
  {
    "breadcrumb": "ADB",
    "content": "when and otherwise The when and otherwise functions in PySpark provide a way to create conditional expressions within a DataFrame, allowing you to specify different values for new or existing columns based on specific conditions.\nwhen:\nThe when function in PySpark is used to define a condition.\nIf the condition is met, it returns the specified value.\nYou can chain multiple when conditions to handle various cases.\notherwise:\nThe otherwise function specifies a default value to return if none of the conditions in the when statements are met.\nfrom pyspark.sql import SparkSession from pyspark.sql.functions import when from pyspark.sql.types import StructType, StructField, IntegerType, StringType # Initialize Spark session spark = SparkSession.builder.appName(\"WhenOtherwiseExample\").getOrCreate() # Define the schema for the dataset schema = StructType([ StructField(\"name\", StringType(), True), StructField(\"age\", IntegerType(), True), StructField(\"salary\", IntegerType(), True) ]) # Create a sample dataset data = [ (\"Alice\", 25, 3000), (\"Bob\", 35, 4000), (\"Charlie\", 40, 5000), (\"David\", 28, 4500), (\"Eve\", 32, 3500) ] # Create DataFrame df = spark.createDataFrame(data, schema) df.show() # Apply 'when' and 'otherwise' to add new columns based on conditions df = ( df.withColumn(\"status\", when(df.age \u003c 30, \"Young\").otherwise(\"Adult\")) .withColumn(\"income_bracket\", when(df.salary \u003c 4000, \"Low\") .when((df.salary \u003e= 4000) \u0026 (df.salary \u003c= 4500), \"Medium\") .otherwise(\"High\")) ) # Show the result df.show() Explanation status column\nAssigns \"Young\" if age \u003c 30. Otherwise assigns \"Adult\". income_bracket column\nAssigns \"Low\" if salary \u003c 4000. Assigns \"Medium\" if 4000 \u003c= salary \u003c= 4500. Assigns \"High\" for any other salary values. This approach allows for flexible handling of multiple conditions in PySpark DataFrames using when and otherwise.\ncast() and printSchema() In PySpark, the cast() function is used to change the data type of a column within a DataFrame.\nThis is helpful when you need to standardize column data types for data processing, schema consistency, or compatibility with other operations.\nPurpose:\nThe cast() function allows you to change the data type of a column, useful in situations like standardizing formats (e.g., converting strings to dates or integers).\nSyntax:\nThe cast() function is applied on individual columns and requires specifying the target data type in quotes.\nMultiple Columns:\nYou can cast multiple columns at once by using a list of cast expressions and passing them to select().\nSupported Data Types:\nPySpark supports various data types for casting, including:\nStringType IntegerType (or \"int\") DoubleType (or \"double\") DateType TimestampType BooleanType Others, based on the data types available in PySpark. from pyspark.sql.functions import col # Single column cast df = df.withColumn(\"column_name\", col(\"column_name\").cast(\"target_data_type\")) # Multiple columns cast with select cast_expr = [ col(\"column1_name\").cast(\"target_data_type1\"), col(\"column2_name\").cast(\"target_data_type2\"), # More columns and data types as needed ] df = df.select(*cast_expr) Example Let’s create a dataset and apply cast() to change the data types of multiple columns:\nfrom pyspark.sql import SparkSession from pyspark.sql.functions import col from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType # Initialize Spark session spark = SparkSession.builder.appName(\"CastExample\").getOrCreate() # Define the schema for the dataset schema = StructType([ StructField(\"name\", StringType(), True), StructField(\"age\", StringType(), True), # Stored as StringType initially StructField(\"height\", StringType(), True) # Stored as StringType initially ]) # Create a sample dataset data = [ (\"Alice\", \"25\", \"5.5\"), (\"Bob\", \"35\", \"6.1\"), (\"Charlie\", \"40\", \"5.8\"), ] # Create DataFrame df = spark.createDataFrame(data, schema) # Show schema and data before casting df.printSchema() df.show() # Define cast expressions for multiple columns cast_expr = [ col(\"name\").cast(\"string\"), col(\"age\").cast(\"int\"), # Casting age to IntegerType col(\"height\").cast(\"double\") # Casting height to DoubleType ] # Apply the cast expressions to the DataFrame df = df.select(*cast_expr) # Show the result df.printSchema() df.show() Explanation age column: Initially stored as StringType, it’s cast to IntegerType (or \"int\"). height column: Initially stored as StringType, it’s cast to DoubleType (or \"double\"). Advantages of Using cast() Schema Alignment: Ensures data types in different tables or DataFrames are compatible for joining or union operations. Data Consistency: Ensures all columns conform to expected data types for downstream data processing. Error Reduction: Minimizes issues arising from mismatched data types in computations or transformations. This approach using cast() provides a flexible and powerful way to manage data types in PySpark.\nprintSchema() Method in PySpark Purpose:\nTo display the schema of a DataFrame, which includes the column names, data types, and nullability of each column. Output Structure:\nThe schema is presented in a tree-like structure showing:\nColumn Name: The name of the column. Data Type: The data type of the column (e.g., string, integer, double, boolean, etc.). Nullability: Indicates whether the column can contain null values (e.g., nullable = true). Usage:\nCall df.printSchema() on a DataFrame df to see its structure. Useful for verifying the structure of the DataFrame after operations like select(), withColumn(), or cast(). union and unionAll Overview Purpose: Both union and unionAll are used to combine two DataFrames into a single DataFrame. DataFrame Compatibility: The two DataFrames must have the same schema (i.e., the same column names and data types) to perform the union operation. union() Functionality:\nCombines two DataFrames and retains all rows, including duplicates. Behavior:\nThe union() method does not remove duplicate rows, resulting in a DataFrame that may contain duplicates. unionAll() Functionality:\nCombines two DataFrames and retains all rows, including duplicates. Behavior:\nThe unionAll() method performs the union operation but does not eliminate duplicate rows (similar to union). Syntax # Using union to retain all rows including duplicates unioned_df = df1.union(df2) # Using unionAll to retain all rows including duplicates unionAll_df = df1.unionAll(df2) Example: Using union and unionAll in PySpark from pyspark.sql import SparkSession # Initialize Spark session spark = SparkSession.builder.appName(\"UnionExample\").getOrCreate() # Sample DataFrames data1 = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)] data2 = [(\"David\", 40), (\"Eve\", 45), (\"Alice\", 25)] columns = [\"name\", \"age\"] df1 = spark.createDataFrame(data1, columns) df2 = spark.createDataFrame(data2, columns) # Using union to retain all rows including duplicates unioned_df = df1.union(df2) # Using unionAll to retain all rows unionAll_df = df1.unionAll(df2) # Show the results print(\"unioned_df (No duplicates removed):\") unioned_df.show() print(\"unionAll_df (duplicates retained):\") unionAll_df.show() ## Removing Duplicate Rows in PySpark # Remove duplicate rows and create a new DataFrame unique_df = unioned_df.dropDuplicates() # or unique_df = unioned_df.distinct() print(\"unique_df (after removing duplicates):\") unique_df.show() Union and UnionByName In PySpark, both union and unionByName are operations that allow you to combine two or more DataFrames. However, they do this in slightly different ways, particularly regarding how they handle column names.\n1. union Definition:\nThe union() function is used to combine two DataFrames with the same schema (i.e., the same number of columns with the same data types).\nIt appends the rows of one DataFrame to the other.\nKey Characteristics:\nThe DataFrames must have the same number of columns. The columns must have compatible data types. It does not automatically handle column names that differ between DataFrames. Syntax DataFrame.union(otherDataFrame) from pyspark.sql import SparkSession # Create a Spark session spark = SparkSession.builder.appName(\"Union Example\").getOrCreate() # Create two DataFrames with the same schema data1 = [(\"Alice\", 1), (\"Bob\", 2)] data2 = [(\"Cathy\", 3), (\"David\", 4)] columns = [\"Name\", \"Id\"] df1 = spark.createDataFrame(data1, columns) df2 = spark.createDataFrame(data2, columns) # Perform union result_union = df1.union(df2) # Show the result result_union.show() 2. unionByName Definition:\nThe unionByName() function allows you to combine two DataFrames by matching column names.\nIf the DataFrames do not have the same schema, it will fill in missing columns with null.\nKey Characteristics:\nMatches DataFrames by column names rather than position. If the DataFrames have different columns, it will include all columns and fill in null for missing values in any DataFrame. You can specify allowMissingColumns=True to ignore missing columns. Syntax DataFrame.unionByName(otherDataFrame, allowMissingColumns=False) # Create two DataFrames with different schemas data3 = [(\"Eve\", 5), (\"Frank\", 6)] data4 = [(\"Grace\", \"New York\"), (\"Hannah\", \"Los Angeles\")] columns1 = [\"Name\", \"Id\"] columns2 = [\"Name\", \"City\"] df3 = spark.createDataFrame(data3, columns1) df4 = spark.createDataFrame(data4, columns2) # Perform unionByName (with allowMissingColumns=True to handle schema differences) result_union_by_name = df3.unionByName(df4, allowMissingColumns=True) # Show the result result_union_by_name.show() result_union_by_name.show() Summary of Differences union(): Requires DataFrames to have the same schema (same number of columns and compatible data types). It combines rows without checking column names. unionByName(): Matches DataFrames by column names. It can handle different schemas and fill missing columns with null (when allowMissingColumns=True). Feature Union UnionByName Column Matching Positional By Name Missing Columns Handling Does not allow Allows with null for missing Schema Requirement Must be identical Can differ Conclusion In PySpark:\nUse union() when you have DataFrames with the same schema and need a straightforward concatenation. Use unionByName() when your DataFrames have different schemas and you want to combine them by matching column names while handling missing columns.",
    "description": "when and otherwise The when and otherwise functions in PySpark provide a way to create conditional expressions within a DataFrame, allowing you to specify different values for new or existing columns based on specific conditions.\nwhen:\nThe when function in PySpark is used to define a condition.\nIf the condition is met, it returns the specified value.\nYou can chain multiple when conditions to handle various cases.\notherwise:\nThe otherwise function specifies a default value to return if none of the conditions in the when statements are met.",
    "tags": [],
    "title": "When|Cast|Union",
    "uri": "/azure_data_bricks/when/"
  },
  {
    "breadcrumb": "ADB",
    "content": "Windows Function in PySpark 1. Introduction to Window Functions Window functions allow you to perform calculations across a set of rows related to the current row within a specified partition.\nUnlike groupBy functions, window functions do not reduce the number of rows in the result; instead, they calculate a value for each row based on the specified window.\n2. Importing Required Libraries To use window functions, import the necessary modules from PySpark:\nfrom pyspark.sql import SparkSession from pyspark.sql import functions as F from pyspark.sql.window import Window 3. Creating a Window Specification A window specification defines how rows will be grouped (partitioned) and ordered within each group.\nExample – Basic Window Specification:\nwindow_spec = Window.partitionBy(\"category\").orderBy(\"timestamp\")\nExample – Advanced Window Specification:\nwindow_spec = Window.partitionBy(\"category\", \"sub_category\").orderBy(F.col(\"timestamp\"), F.col(\"score\"))\n4. Common Window Functions a. Row Number Function: row_number()\nDescription: Assigns a unique integer to each row within the partition (starting from 1).\ndf = df.withColumn(\"row_number\", F.row_number().over(window_spec))\nb. Rank\nFunction: rank()\nDescription: Assigns the same rank to rows with the same values in the order criteria. The next rank has a gap.\ndf = df.withColumn(\"rank\", F.rank().over(window_spec))\nc. Dense Rank\nFunction: dense_rank()\nDescription: Similar to rank, but does not leave gaps in ranking.\ndf = df.withColumn(\"dense_rank\", F.dense_rank().over(window_spec))\nd. Lead and Lag\nFunctions: lead(), lag()\nDescription:\nlead() → value of the next row within the window.\nlag() → value of the previous row within the window.\ndf = df.withColumn(\"next_value\", F.lead(\"value\").over(window_spec)) df = df.withColumn(\"previous_value\", F.lag(\"value\").over(window_spec))\ne. Aggregation Functions\nWindow functions can also compute aggregated values across the specified window.\ndf = df.withColumn(\"avg_value\", F.avg(\"value\").over(window_spec))\nOther common aggregations:\nSum: F.sum(\"column_name\").over(window_spec)\nMin: F.min(\"column_name\").over(window_spec)\nMax: F.max(\"column_name\").over(window_spec)\n5. Putting It All Together – Example from pyspark.sql import SparkSession from pyspark.sql import functions as F from pyspark.sql.window import Window # Initialize Spark session spark = SparkSession.builder.appName(\"WindowFunctionsExample\").getOrCreate() # Sample DataFrame data = [ (\"A\", \"X\", 1, \"2023-01-01\"), (\"A\", \"X\", 2, \"2023-01-02\"), (\"A\", \"Y\", 3, \"2023-01-01\"), (\"A\", \"Y\", 3, \"2023-01-02\"), (\"B\", \"X\", 5, \"2023-01-01\"), (\"B\", \"X\", 4, \"2023-01-02\"), ] columns = [\"category\", \"sub_category\", \"value\", \"timestamp\"] df = spark.createDataFrame(data, columns) # Define window specification window_spec = Window.partitionBy(\"category\", \"sub_category\") \\ .orderBy(F.col(\"timestamp\"), F.col(\"value\")) # Apply window functions df = df.withColumn(\"row_number\", F.row_number().over(window_spec)) df = df.withColumn(\"rank\", F.rank().over(window_spec)) df = df.withColumn(\"dense_rank\", F.dense_rank().over(window_spec)) df = df.withColumn(\"next_value\", F.lead(\"value\").over(window_spec)) df = df.withColumn(\"previous_value\", F.lag(\"value\").over(window_spec)) df = df.withColumn(\"avg_value\", F.avg(\"value\").over(window_spec)) df.show() Conclusion Window functions in PySpark are powerful tools for analyzing data within groups while retaining row-level details.\nBy defining window specifications and applying functions like rank, dense_rank, lead, lag, and aggregations, you can perform complex analytics efficiently.\n🔹 Windows Function in PySpark – Part 2\nfrom pyspark.sql import SparkSession from pyspark.sql.window import Window import pyspark.sql.functions as F # Sample data data = [ (\"Alice\", 100), (\"Bob\", 200), (\"Charlie\", 200), (\"David\", 300), (\"Eve\", 400), (\"Frank\", 500), (\"Grace\", 500), (\"Hank\", 600), (\"Ivy\", 700), (\"Jack\", 800) ] columns = [\"Name\", \"Score\"] df = spark.createDataFrame(data, columns) # Define window window_spec = Window.orderBy(\"Score\") # Ranking functions df1 = df.withColumn(\"Rank\", F.rank().over(window_spec)) df2 = df.withColumn(\"DenseRank\", F.dense_rank().over(window_spec)) df3 = df.withColumn(\"RowNumber\", F.row_number().over(window_spec)) # Lead \u0026 Lag df4 = df.withColumn(\"ScoreDifferenceWithNext\", F.lead(\"Score\").over(window_spec) - df[\"Score\"]) df5 = df.withColumn(\"ScoreDifferenceWithPrevious\", df[\"Score\"] - F.lag(\"Score\").over(window_spec)) 🔹 Windows Function in PySpark – Part 3 (Student Marks Analysis)\n# Updated sample data data = [ (\"Alice\", \"Math\", 90, 1), (\"Alice\", \"Science\", 85, 1), (\"Alice\", \"History\", 78, 1), (\"Bob\", \"Math\", 80, 1), (\"Bob\", \"Science\", 81, 1), (\"Bob\", \"History\", 77, 1), (\"Charlie\", \"Math\", 75, 1), (\"Charlie\", \"Science\", 82, 1), (\"Charlie\", \"History\", 79, 1), (\"Alice\", \"Physics\", 86, 2), (\"Alice\", \"Chemistry\", 92, 2), (\"Alice\", \"Biology\", 80, 2), (\"Bob\", \"Physics\", 94, 2), (\"Bob\", \"Chemistry\", 91, 2), (\"Bob\", \"Biology\", 96, 2), (\"Charlie\", \"Physics\", 89, 2), (\"Charlie\", \"Chemistry\", 88, 2), (\"Charlie\", \"Biology\", 85, 2), (\"Alice\", \"Computer Science\", 95, 3), (\"Alice\", \"Electronics\", 91, 3), (\"Alice\", \"Geography\", 97, 3), (\"Bob\", \"Computer Science\", 88, 3), (\"Bob\", \"Electronics\", 66, 3), (\"Bob\", \"Geography\", 92, 3), (\"Charlie\", \"Computer Science\", 92, 3), (\"Charlie\", \"Electronics\", 97, 3), (\"Charlie\", \"Geography\", 99, 3) ] columns = [\"First Name\", \"Subject\", \"Marks\", \"Semester\"] df = spark.createDataFrame(data, columns) # 1. Max marks in each semester window_spec_max_marks = Window.partitionBy(\"Semester\").orderBy(F.desc(\"Marks\")) max_marks_df = df.withColumn(\"Rank\", F.rank().over(window_spec_max_marks)) top_scorer = max_marks_df.filter(max_marks_df[\"Rank\"] == 1) # 2. Percentage of each student window_spec_total_marks = Window.partitionBy(\"First Name\", \"Semester\") df = df.withColumn(\"TotalMarks\", F.sum(\"Marks\").over(window_spec_total_marks)) df = df.withColumn(\"Percentage\", (F.col(\"TotalMarks\") / (3*100)).cast(\"decimal(5,2)\")*100) df2 = df.groupBy(\"First Name\",\"Semester\").agg(F.max(\"TotalMarks\").alias(\"TotalMarks\"), F.max(\"Percentage\").alias(\"Percentage\")) # 3. Top rank holder in each semester window_spec_rank = Window.partitionBy(\"Semester\").orderBy(F.desc(\"Percentage\")) rank_df = df.withColumn(\"Rank\", F.rank().over(window_spec_rank)) top_rank_holder = rank_df.filter(rank_df[\"Rank\"] == 1).select(\"First Name\",\"Semester\",\"Rank\",\"Percentage\").distinct() # 4. Max marks in each subject in each semester window_spec_max_subject_marks = Window.partitionBy(\"Semester\",\"Subject\").orderBy(F.desc(\"Marks\")) max_subject_marks_df = df.withColumn(\"Rank\", F.rank().over(window_spec_max_subject_marks)) max_subject_scorer = max_subject_marks_df.filter(max_subject_marks_df[\"Rank\"] == 1) Windows Function in PySpark – Part 4 (Highest Salary per Department) from pyspark.sql import functions as F from pyspark.sql.window import Window # Employee Data emp_data = [ (1, \"Alice\", 1, 6300), (2, \"Bob\", 1, 6200), (3, \"Charlie\", 2, 7000), (4, \"David\", 2, 7200), (5, \"Eve\", 1, 6300), (6, \"Frank\", 2, 7100) ] # Department Data dept_data = [ (1, \"HR\"), (2, \"Finance\") ] # Create DataFrames emp_df = spark.createDataFrame(emp_data, [\"EmpId\",\"EmpName\",\"DeptId\",\"Salary\"]) dept_df = spark.createDataFrame(dept_data, [\"DeptId\",\"DeptName\"]) # Window for salary ranking window_spec = Window.partitionBy(\"DeptId\").orderBy(F.desc(\"Salary\")) # Add rank \u0026 filter top salary ranked_salary_df = emp_df.withColumn(\"Rank\", F.rank().over(window_spec)) result_df = ranked_salary_df.filter(F.col(\"Rank\") == 1) # Join department names result_df = result_df.join(dept_df, [\"DeptId\"], \"left\") # Final Output result_df.select(\"EmpName\",\"DeptName\",\"Salary\").show()",
    "description": "Windows Function in PySpark 1. Introduction to Window Functions Window functions allow you to perform calculations across a set of rows related to the current row within a specified partition.\nUnlike groupBy functions, window functions do not reduce the number of rows in the result; instead, they calculate a value for each row based on the specified window.\n2. Importing Required Libraries To use window functions, import the necessary modules from PySpark:",
    "tags": [],
    "title": "Window Functions",
    "uri": "/azure_data_bricks/window-functions/"
  },
  {
    "breadcrumb": "ADB",
    "content": "Explode vs Explode_outer in PySpark In PySpark, explode and explode_outer are functions used to work with nested data structures, like arrays or maps, by “exploding” (flattening) each element of an array or key-value pair in a map into separate rows.\nThe key difference between explode and explode_outer is in handling null or empty arrays, which makes them useful in different scenarios.\n1. explode() The explode() function takes a column with array or map data and creates a new row for each element in the array (or each key-value pair in the map).\nIf the array is empty or null, explode() will drop the row entirely.\nKey Characteristics Converts each element in an array or each entry in a map into its own row. Drops rows with null or empty arrays. Example: Using explode() in PySpark from pyspark.sql import SparkSession from pyspark.sql.functions import explode # Initialize Spark session spark = SparkSession.builder.appName(\"ExplodeExample\").getOrCreate() # Sample DataFrame with arrays data = [ (\"Alice\", [\"Math\", \"Science\"]), (\"Bob\", [\"History\"]), (\"Cathy\", []), # Empty array (\"David\", None) # Null array ] df = spark.createDataFrame(data, [\"Name\", \"Subjects\"]) df.show() # Use explode to flatten the array exploded_df = df.select(\"Name\", explode(\"Subjects\").alias(\"Subject\")) # Show the result exploded_df.show() 1. explode() The explode() function expands the Subjects array into individual rows.\nRows with empty ([]) or null (None) arrays are removed, which is why Cathy and David do not appear in the output.\nKey Characteristics Converts each element in an array or each entry in a map into its own row. Drops rows with null or empty arrays. 2. explode_outer() The explode_outer() function works similarly to explode(), but it keeps rows with null or empty arrays.\nWhen explode_outer() encounters a null or empty array, it still generates a row for that entry, with null as the value in the resulting column.\nKey Characteristics Converts each element in an array or each entry in a map into its own row. Retains rows with null or empty arrays, using null values in the exploded column. # Use explode_outer to flatten the array while keeping null or empty rows exploded_outer_df = df.select( \"Name\", F.explode_outer(\"Subjects\").alias(\"Subject\") ) # Show the result exploded_outer_df.show() Explanation: explode_outer() explode_outer() expands the Subjects array into individual rows. Unlike explode(), rows with empty ([]) or null arrays (None) are kept in the result, with null values in the Subject column for these cases. Summary Table of Differences Function Description Null/Empty Arrays Behavior explode() Expands each element of an array or map into rows Drops rows with null or empty arrays explode_outer() Similar to explode(), but retains null/empty Keeps rows with null/empty arrays, fills with null These functions are very useful when working with complex, nested data structures, especially when dealing with JSON or other hierarchical data.",
    "description": "Explode vs Explode_outer in PySpark In PySpark, explode and explode_outer are functions used to work with nested data structures, like arrays or maps, by “exploding” (flattening) each element of an array or key-value pair in a map into separate rows.\nThe key difference between explode and explode_outer is in handling null or empty arrays, which makes them useful in different scenarios.\n1. explode() The explode() function takes a column with array or map data and creates a new row for each element in the array (or each key-value pair in the map).\nIf the array is empty or null, explode() will drop the row entirely.",
    "tags": [],
    "title": "Explode",
    "uri": "/azure_data_bricks/explode/"
  },
  {
    "breadcrumb": "ADB",
    "content": "The pivot operation in PySpark is used to transpose rows into columns based on a specified column’s unique values.\nIt’s particularly useful for creating wide-format data, where values in one column become new column headers, and corresponding values from another column fill those headers.\nKey Concepts groupBy and pivot\nThe pivot method is typically used in combination with groupBy. You group by certain columns and pivot one column to create new columns. Aggregation Function\nYou need to specify an aggregation function (like sum, avg, count, etc.) to fill the values in the pivoted columns. Performance Consideration\nPivoting can be computationally expensive, especially with a high number of unique values in the pivot column. For better performance, explicitly specify the values to pivot if possible. Syntax\ndataframe.groupBy(\"group_column\").pivot(\"pivot_column\").agg(aggregation_function) Example Code: Pivot in PySpark Sample Data Imagine we have a DataFrame of sales data with the following schema:\nProduct Region Sales A North 100 B North 150 A South 200 B South 300 We want to pivot the data so that regions (North, South) become columns and the sales values are aggregated.\nCode Implementation Example: Pivot in PySpark from pyspark.sql import SparkSession from pyspark.sql.functions import sum # Create a Spark session spark = SparkSession.builder.appName(\"PivotExample\").getOrCreate() # Create a sample DataFrame data = [ (\"A\", \"North\", 100), (\"B\", \"North\", 150), (\"A\", \"South\", 200), (\"B\", \"South\", 300) ] columns = [\"Product\", \"Region\", \"Sales\"] df = spark.createDataFrame(data, columns) # Pivot the DataFrame pivoted_df = df.groupBy(\"Product\").pivot(\"Region\").agg(sum(\"Sales\")) # Show the results pivoted_df.show() Output Product North South A 100 200 B 150 300 Explanation of Code groupBy(“Product”)\nGroups the data by the Product column. pivot(“Region”)\nTransforms unique values in the Region column (North, South) into new columns. agg(sum(“Sales”))\nComputes the sum of Sales for each combination of Product and the new columns created by the pivot. Notes Explicit Pivot Values: To improve performance, you can specify the pivot values explicitly. df.groupBy(\"Product\").pivot(\"Region\", [\"North\", \"South\"]).agg(sum(\"Sales\"))\nHandling Null Values: If some combinations of groupBy and pivot values have no corresponding rows, the resulting cells will contain null. Alternative Aggregations: You can use other aggregation functions like avg, max, min, etc. This approach is commonly used in creating summary reports or preparing data for machine learning models where wide-format data is required.\nUnpivot in PySpark The unpivot operation (also called melting) is used to transform a wide-format table into a long-format table. This means columns are turned into rows, effectively reversing the pivot operation. PySpark doesn’t have a direct unpivot function like Pandas’ melt, but you can achieve it using the selectExpr method or a combination of stack and other DataFrame transformations.\nKey Concepts 1. Purpose of Unpivot: Simplifies data analysis by converting column headers into a single column (e.g., categorical variables).\nIdeal for scenarios where you need to aggregate data further or visualize it in a long format.\n2. Syntax Overview: Use the stack function inside a selectExpr to unpivot.\nStack reshapes the DataFrame by creating multiple rows for specified columns.\n3. Performance: Unpivoting can generate many rows, especially if the original DataFrame is wide with numerous columns. Ensure your environment can handle the resulting data volume. Example: Unpivot in PySpark Sample Data Suppose we have the following DataFrame:\nProduct North South East West A 100 200 150 130 B 150 300 200 180 from pyspark.sql import SparkSession # Create a Spark session spark = SparkSession.builder.appName(\"UnpivotExample\").getOrCreate() # Sample data data = [ (\"A\", 100, 200, 150, 130), (\"B\", 150, 300, 200, 180) ] columns = [\"Product\", \"North\", \"South\", \"East\", \"West\"] # Create the DataFrame df = spark.createDataFrame(data, columns) # Unpivot the DataFrame using stack unpivoted_df = df.selectExpr( \"Product\", \"stack(4, 'North', North, 'South', South, 'East', East, 'West', West) as (Region, Sales)\" ) # Show the results unpivoted_df.show() Explanation of Code Input DataFrame:\nEach column (North, South, East, West) represents a region’s sales for each product. selectExpr with stack:\nThe stack function takes two arguments: The number of columns being unpivoted (4 in this case). A sequence of column-value pairs: ‘ColumnName1’, ColumnValue1, ‘ColumnName2’, ColumnValue2, …. The result is two new columns: the first contains the column names (now rows, Region), and the second contains the corresponding values (Sales). Aliasing Columns:\nThe stack result is aliased as (Region, Sales) to give meaningful names to the new columns. Alternative Methods Using withColumn and union:\nIf stack isn’t flexible enough, you can manually combine rows for each column:\nfrom pyspark.sql import functions as F # Create a DataFrame with union operations for unpivoting north = df.select(\"Product\", F.lit(\"North\").alias(\"Region\"), F.col(\"North\").alias(\"Sales\")) south = df.select(\"Product\", F.lit(\"South\").alias(\"Region\"), F.col(\"South\").alias(\"Sales\")) east = df.select(\"Product\", F.lit(\"East\").alias(\"Region\"), F.col(\"East\").alias(\"Sales\")) west = df.select(\"Product\", F.lit(\"West\").alias(\"Region\"), F.col(\"West\").alias(\"Sales\")) # Combine all rows using union unpivoted_df = north.union(south).union(east).union(west) # Show results unpivoted_df.show() Notes Performance Considerations\nstack is efficient for unpivoting a large number of columns. The union method may become unwieldy for many columns, but it offers more control over the transformation process. Dynamic Column Unpivoting\nIf the column names are not fixed (dynamic), you can: Collect the column names dynamically using df.columns. Construct the selectExpr or union queries programmatically. Resulting Format\nAfter unpivoting, the data will have more rows but fewer columns. Ensure downstream processes are optimized to handle the increased row count. Unpivoting is a powerful operation for restructuring data and is frequently used in data preprocessing, reporting, and machine learning pipelines.",
    "description": "The pivot operation in PySpark is used to transpose rows into columns based on a specified column’s unique values.\nIt’s particularly useful for creating wide-format data, where values in one column become new column headers, and corresponding values from another column fill those headers.\nKey Concepts groupBy and pivot\nThe pivot method is typically used in combination with groupBy. You group by certain columns and pivot one column to create new columns. Aggregation Function",
    "tags": [],
    "title": "Pivot",
    "uri": "/azure_data_bricks/pivot/"
  },
  {
    "breadcrumb": "ADB",
    "content": "Hadoop vs. Spark Architecture Aspect Hadoop Spark Storage Uses HDFS for storage Uses in-memory processing for speed Processing MapReduce is disk-based In-memory processing improves performance Integration Runs independently or with Hadoop ecosystem Can run on top of Hadoop; more flexible Complexity More complex setup and deployment Simpler to deploy and configure Performance Slower for iterative tasks due to disk I/O Better performance for iterative tasks RDD vs. DataFrame vs. Dataset Aspect RDD DataFrame Dataset API Level Low-level, more control High-level, optimized with Catalyst High-level, type-safe Schema No schema, unstructured Uses schema for structured data Strongly typed, compile-time type safety Optimization No built-in optimization Optimized using Catalyst Optimized using Catalyst, with type safety Type Safety No type safety No compile-time type safety Provides compile-time type safety Performance Less optimized for performance Better performance due to optimizations Combines type safety with optimization Action vs. Transformation in Spark Aspect Action Transformation Execution Triggers execution of the Spark job Builds up a logical plan of data operations Return Type Returns results or output Returns a new RDD/DataFrame Evaluation Eager evaluation; executes immediately Lazy evaluation; executed when an action is triggered Computation Involves actual computation (e.g., collect()) Defines data transformations (e.g., map()) Performance Can cause data processing; affects performance Does not affect performance until an action is called Map vs. FlatMap Aspect Map FlatMap Output Returns one output element per input element Can return zero or more output elements per input Flattening Does not flatten output Flattens the output into a single level Use Case Suitable for one-to-one transformations Suitable for one-to-many transformations Complexity Simpler, straightforward More complex due to variable number of outputs Examples map(x =\u003e x * 2) flatMap(x =\u003e x.split(\" \")) GroupByKey vs. ReduceByKey Aspect GroupByKey ReduceByKey Operation Groups all values by key Aggregates values with the same key Efficiency Can lead to high shuffling More efficient due to partial aggregation Data Movement Requires shuffling of all values Minimizes data movement through local aggregation Use Case Useful for simple grouping Preferred for aggregations and reductions Performance Less efficient with large datasets Better performance for large datasets Repartition vs. Coalesce Aspect Repartition Coalesce Partitioning Can increase or decrease the number of partitions Only decreases the number of partitions Shuffling Involves full shuffle Avoids full shuffle, more efficient Efficiency More expensive due to shuffling More efficient for reducing partitions Use Case Used for increasing partitions or balancing load Used for reducing partitions, typically after filtering Performance Can be costly for large datasets More cost-effective for reducing partitions Cache vs. Persist Aspect Cache Persist Storage Level Defaults to MEMORY_ONLY Can use various storage levels (e.g., MEMORY_AND_DISK) Flexibility Simplified, with default storage level Offers more options for storage levels Use Case Suitable for simple caching scenarios Suitable for complex caching scenarios requiring different storage levels Implementation Easier to use, shorthand for MEMORY_ONLY More flexible, allows custom storage options Performance Suitable when memory suffices More efficient when dealing with larger datasets and limited memory Narrow vs. Wide Transformation Aspect Narrow Transformation Wide Transformation Partitioning Each parent partition is used by one child partition Requires data from multiple partitions Shuffling No shuffling required Involves shuffling of data Performance More efficient and less costly Less efficient due to data movement Examples map(), filter() groupByKey(), join() Complexity Simpler and faster More complex and slower due to data movement Collect vs. Take Aspect Collect Take Output Retrieves all data from the RDD/DataFrame Retrieves a specified number of elements Memory Usage Can be expensive and use a lot of memory More memory-efficient Use Case Used when you need the entire dataset Useful for sampling or debugging Performance Can cause performance issues with large data Faster and more controlled Action Type Triggers full data retrieval Triggers partial data retrieval Broadcast Variable vs. Accumulator Aspect Broadcast Variable Accumulator Purpose Efficiently shares read-only data across tasks Tracks metrics and aggregates values Data Type Data that is shared and read-only Counters and sums, often numerical Use Case Useful for large lookup tables or configurations Useful for aggregating metrics like counts Efficiency Reduces data transfer by broadcasting data once Efficient for aggregating values across tasks Mutability Immutable, read-only Mutable, can be updated during computation Spark SQL vs. DataFrame API Aspect Spark SQL DataFrame API Interface Executes SQL queries Provides a programmatic interface Syntax Uses SQL-like syntax Uses function-based syntax Optimization Optimized with Catalyst Optimized with Catalyst Spark Streaming vs. Structured Streaming Aspect Spark Streaming Structured Streaming Processing Micro-batch processing Micro-batch and continuous processing API RDD-based API SQL-based API with DataFrame/Dataset support Complexity More complex and lower-level Simplified with high-level APIs Consistency Can be less consistent due to micro-batches Provides stronger consistency guarantees Performance Can be slower for complex queries Better performance with optimizations Shuffle vs. MapReduce Aspect Shuffle MapReduce Operation Data reorganization across partitions Data processing model for distributed computing Efficiency Can be costly due to data movement Designed for batch processing with high I/O Performance Affects performance based on the amount of data movement Optimized for large-scale data processing but less efficient for iterative tasks Use Case Used in Spark for data redistribution Used in Hadoop for data processing tasks Implementation Integrated into Spark operations Core component of the Hadoop ecosystem Union vs. Join Aspect Union Join Operation Combines two DataFrames/RDDs into one Combines rows from two DataFrames/RDDs based on a key Data Requirements Requires same schema for both DataFrames/RDDs Requires a common key for joining Performance Generally faster as it does not require key matching Can be slower due to key matching and shuffling Output Stacks data vertically Merges data horizontally based on keys Use Case Appending data or combining datasets Merging related data based on keys Executor vs. Driver Aspect Executor Driver Role Executes tasks and processes data Coordinates and manages the Spark application Memory Memory allocated per executor for data processing Memory used for managing application execution Lifecycle Exists throughout the application execution Starts and stops the Spark application Tasks Runs the tasks assigned by the driver Schedules and coordinates tasks and jobs Parallelism Multiple executors run in parallel Single driver coordinates multiple executors Checkpointing vs. Caching Aspect Checkpointing Caching Purpose Provides fault tolerance and reliability Improves performance by storing intermediate data Storage Writes data to stable storage (e.g., HDFS) Stores data in memory or on disk (depends on storage level) Use Case Used for recovery in case of failures Used for optimizing repeated operations Impact Can be more costly and slow Generally faster but not suitable for fault tolerance Data Data is written to external storage Data is kept in memory or disk storage for quick access ReduceByKey vs. AggregateByKey Aspect ReduceByKey AggregateByKey Operation Combines values with the same key using a function Performs custom aggregation and combinatory operations Efficiency More efficient for simple aggregations Flexible for complex aggregation scenarios Shuffling Involves shuffling but can be optimized Can be more complex due to custom aggregation Use Case Suitable for straightforward aggregations Ideal for advanced and custom aggregations Performance Generally faster for simple operations Performance varies with complexity SQLContext vs. HiveContext vs. SparkSession Aspect SQLContext HiveContext SparkSession Purpose Provides SQL query capabilities Provides integration with Hive for SQL queries Unified entry point for Spark functionality Integration Basic SQL capabilities Integrates with Hive Metastore Combines SQL, DataFrame, and Streaming APIs Usage Legacy, less functionality Supports HiveQL and Hive UDFs Supports all Spark functionalities including Hive Configuration Less flexible and older Requires Hive setup and configuration Modern and flexible, manages configurations Capabilities Limited to SQL queries Extends SQL capabilities with Hive integration Comprehensive access to all Spark features Broadcast Join vs. Shuffle Join Aspect Broadcast Join Shuffle Join Operation Broadcasts a small dataset to all nodes Shuffles data across nodes for joining Data Size Suitable for small datasets Suitable for larger datasets Efficiency More efficient for small tables More suited for large datasets Performance Faster due to reduced shuffling Can be slower due to extensive shuffling Use Case Use when one dataset is small relative to others Use when both datasets are large SparkContext vs. SparkSession Aspect SparkContext SparkSession Purpose Entry point for Spark functionality Unified entry point for Spark functionalities Lifecycle Created before Spark jobs start Manages the Spark application lifecycle Functionality Provides access to RDD and basic Spark functionality Provides access to RDD, DataFrame, SQL, and Streaming APIs Configuration Configuration is less flexible More flexible and easier to configure Usage Older, used for legacy applications Modern and recommended for new applications Structured Streaming vs. Spark Streaming Aspect Structured Streaming Spark Streaming Processing Micro-batch and continuous processing Micro-batch processing API SQL-based API with DataFrame/Dataset support RDD-based API Complexity Simplified and high-level More complex and low-level Consistency Provides stronger consistency guarantees Can be less consistent due to micro-batches Performance Better performance with built-in optimizations Can be slower for complex queries Partitioning vs. Bucketing Aspect Partitioning Bucketing Purpose Divides data into multiple partitions based on a key Divides data into buckets based on a hash function Usage Used to optimize queries by reducing data scanned Used to improve join performance and maintain sorted data Shuffling Reduces shuffling by placing related data together Reduces shuffle during joins and aggregations Data Layout Data is physically separated based on partition key Data is organized into fixed-size buckets Performance Improves performance for queries involving partition keys Enhances performance for join operations",
    "description": "Hadoop vs. Spark Architecture Aspect Hadoop Spark Storage Uses HDFS for storage Uses in-memory processing for speed Processing MapReduce is disk-based In-memory processing improves performance Integration Runs independently or with Hadoop ecosystem Can run on top of Hadoop; more flexible Complexity More complex setup and deployment Simpler to deploy and configure Performance Slower for iterative tasks due to disk I/O Better performance for iterative tasks RDD vs. DataFrame vs. Dataset Aspect RDD DataFrame Dataset API Level Low-level, more control High-level, optimized with Catalyst High-level, type-safe Schema No schema, unstructured Uses schema for structured data Strongly typed, compile-time type safety Optimization No built-in optimization Optimized using Catalyst Optimized using Catalyst, with type safety Type Safety No type safety No compile-time type safety Provides compile-time type safety Performance Less optimized for performance Better performance due to optimizations Combines type safety with optimization Action vs. Transformation in Spark Aspect Action Transformation Execution Triggers execution of the Spark job Builds up a logical plan of data operations Return Type Returns results or output Returns a new RDD/DataFrame Evaluation Eager evaluation; executes immediately Lazy evaluation; executed when an action is triggered Computation Involves actual computation (e.g., collect()) Defines data transformations (e.g., map()) Performance Can cause data processing; affects performance Does not affect performance until an action is called Map vs. FlatMap Aspect Map FlatMap Output Returns one output element per input element Can return zero or more output elements per input Flattening Does not flatten output Flattens the output into a single level Use Case Suitable for one-to-one transformations Suitable for one-to-many transformations Complexity Simpler, straightforward More complex due to variable number of outputs Examples map(x =\u003e x * 2) flatMap(x =\u003e x.split(\" \")) GroupByKey vs. ReduceByKey Aspect GroupByKey ReduceByKey Operation Groups all values by key Aggregates values with the same key Efficiency Can lead to high shuffling More efficient due to partial aggregation Data Movement Requires shuffling of all values Minimizes data movement through local aggregation Use Case Useful for simple grouping Preferred for aggregations and reductions Performance Less efficient with large datasets Better performance for large datasets Repartition vs. Coalesce Aspect Repartition Coalesce Partitioning Can increase or decrease the number of partitions Only decreases the number of partitions Shuffling Involves full shuffle Avoids full shuffle, more efficient Efficiency More expensive due to shuffling More efficient for reducing partitions Use Case Used for increasing partitions or balancing load Used for reducing partitions, typically after filtering Performance Can be costly for large datasets More cost-effective for reducing partitions Cache vs. Persist Aspect Cache Persist Storage Level Defaults to MEMORY_ONLY Can use various storage levels (e.g., MEMORY_AND_DISK) Flexibility Simplified, with default storage level Offers more options for storage levels Use Case Suitable for simple caching scenarios Suitable for complex caching scenarios requiring different storage levels Implementation Easier to use, shorthand for MEMORY_ONLY More flexible, allows custom storage options Performance Suitable when memory suffices More efficient when dealing with larger datasets and limited memory Narrow vs. Wide Transformation Aspect Narrow Transformation Wide Transformation Partitioning Each parent partition is used by one child partition Requires data from multiple partitions Shuffling No shuffling required Involves shuffling of data Performance More efficient and less costly Less efficient due to data movement Examples map(), filter() groupByKey(), join() Complexity Simpler and faster More complex and slower due to data movement Collect vs. Take Aspect Collect Take Output Retrieves all data from the RDD/DataFrame Retrieves a specified number of elements Memory Usage Can be expensive and use a lot of memory More memory-efficient Use Case Used when you need the entire dataset Useful for sampling or debugging Performance Can cause performance issues with large data Faster and more controlled Action Type Triggers full data retrieval Triggers partial data retrieval Broadcast Variable vs. Accumulator Aspect Broadcast Variable Accumulator Purpose Efficiently shares read-only data across tasks Tracks metrics and aggregates values Data Type Data that is shared and read-only Counters and sums, often numerical Use Case Useful for large lookup tables or configurations Useful for aggregating metrics like counts Efficiency Reduces data transfer by broadcasting data once Efficient for aggregating values across tasks Mutability Immutable, read-only Mutable, can be updated during computation Spark SQL vs. DataFrame API Aspect Spark SQL DataFrame API Interface Executes SQL queries Provides a programmatic interface Syntax Uses SQL-like syntax Uses function-based syntax Optimization Optimized with Catalyst Optimized with Catalyst Spark Streaming vs. Structured Streaming Aspect Spark Streaming Structured Streaming Processing Micro-batch processing Micro-batch and continuous processing API RDD-based API SQL-based API with DataFrame/Dataset support Complexity More complex and lower-level Simplified with high-level APIs Consistency Can be less consistent due to micro-batches Provides stronger consistency guarantees Performance Can be slower for complex queries Better performance with optimizations Shuffle vs. MapReduce Aspect Shuffle MapReduce Operation Data reorganization across partitions Data processing model for distributed computing Efficiency Can be costly due to data movement Designed for batch processing with high I/O Performance Affects performance based on the amount of data movement Optimized for large-scale data processing but less efficient for iterative tasks Use Case Used in Spark for data redistribution Used in Hadoop for data processing tasks Implementation Integrated into Spark operations Core component of the Hadoop ecosystem Union vs. Join Aspect Union Join Operation Combines two DataFrames/RDDs into one Combines rows from two DataFrames/RDDs based on a key Data Requirements Requires same schema for both DataFrames/RDDs Requires a common key for joining Performance Generally faster as it does not require key matching Can be slower due to key matching and shuffling Output Stacks data vertically Merges data horizontally based on keys Use Case Appending data or combining datasets Merging related data based on keys Executor vs. Driver Aspect Executor Driver Role Executes tasks and processes data Coordinates and manages the Spark application Memory Memory allocated per executor for data processing Memory used for managing application execution Lifecycle Exists throughout the application execution Starts and stops the Spark application Tasks Runs the tasks assigned by the driver Schedules and coordinates tasks and jobs Parallelism Multiple executors run in parallel Single driver coordinates multiple executors Checkpointing vs. Caching Aspect Checkpointing Caching Purpose Provides fault tolerance and reliability Improves performance by storing intermediate data Storage Writes data to stable storage (e.g., HDFS) Stores data in memory or on disk (depends on storage level) Use Case Used for recovery in case of failures Used for optimizing repeated operations Impact Can be more costly and slow Generally faster but not suitable for fault tolerance Data Data is written to external storage Data is kept in memory or disk storage for quick access ReduceByKey vs. AggregateByKey Aspect ReduceByKey AggregateByKey Operation Combines values with the same key using a function Performs custom aggregation and combinatory operations Efficiency More efficient for simple aggregations Flexible for complex aggregation scenarios Shuffling Involves shuffling but can be optimized Can be more complex due to custom aggregation Use Case Suitable for straightforward aggregations Ideal for advanced and custom aggregations Performance Generally faster for simple operations Performance varies with complexity SQLContext vs. HiveContext vs. SparkSession Aspect SQLContext HiveContext SparkSession Purpose Provides SQL query capabilities Provides integration with Hive for SQL queries Unified entry point for Spark functionality Integration Basic SQL capabilities Integrates with Hive Metastore Combines SQL, DataFrame, and Streaming APIs Usage Legacy, less functionality Supports HiveQL and Hive UDFs Supports all Spark functionalities including Hive Configuration Less flexible and older Requires Hive setup and configuration Modern and flexible, manages configurations Capabilities Limited to SQL queries Extends SQL capabilities with Hive integration Comprehensive access to all Spark features Broadcast Join vs. Shuffle Join Aspect Broadcast Join Shuffle Join Operation Broadcasts a small dataset to all nodes Shuffles data across nodes for joining Data Size Suitable for small datasets Suitable for larger datasets Efficiency More efficient for small tables More suited for large datasets Performance Faster due to reduced shuffling Can be slower due to extensive shuffling Use Case Use when one dataset is small relative to others Use when both datasets are large SparkContext vs. SparkSession Aspect SparkContext SparkSession Purpose Entry point for Spark functionality Unified entry point for Spark functionalities Lifecycle Created before Spark jobs start Manages the Spark application lifecycle Functionality Provides access to RDD and basic Spark functionality Provides access to RDD, DataFrame, SQL, and Streaming APIs Configuration Configuration is less flexible More flexible and easier to configure Usage Older, used for legacy applications Modern and recommended for new applications Structured Streaming vs. Spark Streaming Aspect Structured Streaming Spark Streaming Processing Micro-batch and continuous processing Micro-batch processing API SQL-based API with DataFrame/Dataset support RDD-based API Complexity Simplified and high-level More complex and low-level Consistency Provides stronger consistency guarantees Can be less consistent due to micro-batches Performance Better performance with built-in optimizations Can be slower for complex queries Partitioning vs. Bucketing Aspect Partitioning Bucketing Purpose Divides data into multiple partitions based on a key Divides data into buckets based on a hash function Usage Used to optimize queries by reducing data scanned Used to improve join performance and maintain sorted data Shuffling Reduces shuffling by placing related data together Reduces shuffle during joins and aggregations Data Layout Data is physically separated based on partition key Data is organized into fixed-size buckets Performance Improves performance for queries involving partition keys Enhances performance for join operations",
    "tags": [],
    "title": "Comparisons",
    "uri": "/azure_data_bricks/differences/"
  },
  {
    "breadcrumb": "",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/categories/"
  },
  {
    "breadcrumb": "",
    "content": "Contents Azure DataBricks SQL Azure Data Factory",
    "description": "Contents Azure DataBricks SQL Azure Data Factory",
    "tags": [],
    "title": "Data Engineering Notes",
    "uri": "/"
  },
  {
    "breadcrumb": "",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/tags/"
  }
]
