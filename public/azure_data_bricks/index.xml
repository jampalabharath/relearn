<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Azure Data Bricks :: Data Engineering Notes</title>
    <link>https://example.org/azure_data_bricks/</link>
    <description>1. Introduction Big Data Overview Introduction to Apache Spark Introduction to Azure Databricks Databricks Architecture Databricks Workspace Overview 2. Spark Core Concepts RDDs (Resilient Distributed Datasets) Transformations and Actions Lazy Evaluation Spark Execution Model Caching and Persistence Partitioning 3. Spark SQL DataFrames and Datasets Spark SQL Engine Schema Inference Catalog and Metadata Temporary and Global Views SQL Queries in Spark Performance Optimization (Catalyst Optimizer, Tungsten) 4. Spark Data Sources Reading/Writing CSV, JSON, Parquet, ORC Working with Delta Lake JDBC Data Sources Streaming Data Sources Handling Nested Data (Struct, Array, Map) 5. Delta Lake Introduction to Delta Lake ACID Transactions Schema Enforcement &amp; Evolution Time Travel Upserts &amp; Deletes (MERGE) Delta Lake Optimization (Z-order, Optimize, Vacuum) 6. Spark Streaming &amp; Structured Streaming Introduction to Streaming DStreams vs Structured Streaming Streaming Sources (Kafka, Event Hubs, Socket) Watermarking &amp; Late Data State Management Streaming Sinks (Delta, Console, Kafka, Event Hubs) 7. Spark MLlib (Machine Learning) MLlib Basics Feature Engineering Pipelines &amp; Transformers Classification Regression Clustering Model Persistence 8. Spark GraphX Introduction to GraphX Graph Processing Basics Graph Algorithms Use Cases 9. Performance Tuning Spark Configurations &amp; Parameters Memory Management Shuffle Optimizations Caching Strategies Adaptive Query Execution (AQE) Skew Handling 10. Azure Databricks Essentials Databricks Clusters (Types, Autoscaling, Pools) Databricks Notebooks Databricks Repos &amp; Git Integration Databricks Jobs &amp; Scheduling Databricks Secrets Databricks Utilities (DBUtils) Workspace Security &amp; Permissions 11. Integration with Azure Services Azure Data Lake Storage (ADLS) Integration Azure Blob Storage Integration Azure Synapse Analytics Azure Event Hubs Azure Data Factory (ADF) with Databricks Power BI with Databricks 12. Advanced Databricks Unity Catalog Databricks SQL Delta Live Tables (DLT) Databricks REST API Databricks CLI Monitoring &amp; Logging Cost Optimization in Databricks 13. Real-World Use Cases ETL Pipeline with Databricks Streaming Data Pipeline Batch Data Processing Machine Learning Pipeline End-to-End Data Lakehouse Implementation 14. Best Practices Cluster Sizing &amp; Configuration Code Versioning &amp; Collaboration Data Governance Security &amp; Compliance CI/CD with Databricks Productionizing Workloads 15. Capstone Project Build a Scalable Data Lakehouse with Databricks Real-Time Streaming Data Pipeline ML Model Training and Deployment</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="https://example.org/azure_data_bricks/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Joins</title>
      <link>https://example.org/azure_data_bricks/joins/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.org/azure_data_bricks/joins/</guid>
      <description>Joins in PySpark In PySpark Joins are used to combine two DataFrames based on a common column or condition.&#xA;Types of Joins in PySpark Inner Join: Matches rows from both DataFrames. df1.join(df2, df1.common_column == df2.common_column, &#34;inner&#34;) Left/Right Join: Keeps all rows from the left or right DataFrame and matches where possible. df1.join(df2, df1.common_column == df2.common_column, &#34;left&#34;) df1.join(df2, df1.common_column == df2.common_column, &#34;right&#34;) Full Join: Keeps all rows from both DataFrames. df1.join(df2, df1.common_column == df2.common_column, &#34;outer&#34;) Left Semi: Filters df1 to rows that match df2 without including columns from df2. df1.join(df2, df1.common_column == df2.common_column, &#34;left_semi&#34;) Left Anti: Filters df1 to rows that do not match df2. df1.join(df2, df1.common_column == df2.common_column, &#34;left_anti&#34;) Cross Join: Returns the Cartesian product, combining all rows of both DataFrames. df1.crossJoin(df2) Explicit Condition Join: Allows complex join conditions, including columns with different names. df1.join(df2, df1.columnA == df2.columnB, &#34;inner&#34;) df1.join(df2, df1.common_column == df2.common_column, &#34;inner&#34;) df1.join(df2, df1.common_column == df2.common_column, &#34;left&#34;) df1.join(df2, df1.common_column == df2.common_column, &#34;right&#34;) df1.join(df2, df1.common_column == df2.common_column, &#34;outer&#34;) df1.join(df2, df1.common_column == df2.common_column, &#34;left_semi&#34;) df1.join(df2, df1.common_column == df2.common_column, &#34;left_anti&#34;) df1.crossJoin(df2) df1.join(df2, df1.columnA == df2.columnB, &#34;inner&#34;) Practice-1 from pyspark.sql import SparkSession from pyspark.sql import Row from pyspark.sql.functions import broadcast # Initialize Spark session spark = SparkSession.builder.appName(&#34;JoinsExample&#34;).getOrCreate() # Sample DataFrames data1 = [Row(id=0), Row(id=1), Row(id=1), Row(id=None), Row(id=None)] data2 = [Row(id=1), Row(id=0), Row(id=None)] df1 = spark.createDataFrame(data1) df2 = spark.createDataFrame(data2) # Inner Join inner_join = df1.join(df2, on=&#34;id&#34;, how=&#34;inner&#34;) print(&#34;Inner Join:&#34;) inner_join.show() # Right Join right_join = df1.join(df2, on=&#34;id&#34;, how=&#34;right&#34;) print(&#34;Right Join:&#34;) right_join.show() # Full (Outer) Join full_join = df1.join(df2, on=&#34;id&#34;, how=&#34;outer&#34;) print(&#34;Full (Outer) Join:&#34;) full_join.show() # Left Anti Join left_anti_join = df1.join(df2, on=&#34;id&#34;, how=&#34;left_anti&#34;) print(&#34;Left Anti Join:&#34;) left_anti_join.show() # Right Anti Join (Equivalent to swapping DataFrames and performing Left Anti Join) right_anti_join = df2.join(df1, on=&#34;id&#34;, how=&#34;left_anti&#34;) print(&#34;Right Anti Join:&#34;) right_anti_join.show() # Broadcast Join (Optimizing a join with a smaller DataFrame) broadcast_join = df1.join(broadcast(df2), on=&#34;id&#34;, how=&#34;inner&#34;) print(&#34;Broadcast Join:&#34;) broadcast_join.show() Practice 2 PySpark Coding Questions Find employees whose location matches the location of their department</description>
    </item>
    <item>
      <title>When|Cast|Union</title>
      <link>https://example.org/azure_data_bricks/casewhen/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.org/azure_data_bricks/casewhen/</guid>
      <description>when and otherwise The when and otherwise functions in PySpark provide a way to create conditional expressions within a DataFrame, allowing you to specify different values for new or existing columns based on specific conditions.&#xA;when:&#xA;The when function in PySpark is used to define a condition.&#xA;If the condition is met, it returns the specified value.&#xA;You can chain multiple when conditions to handle various cases.&#xA;otherwise:&#xA;The otherwise function specifies a default value to return if none of the conditions in the when statements are met.</description>
    </item>
  </channel>
</rss>