# Copy Activity

## Copy Activity in Azure Data Factory

### What is Copy Activity?

Copy Activity is the core data movement activity in **Azure Data Factory (ADF)**. It allows you to securely and efficiently copy data from a source data store to a destination (sink) data store. This is widely used for **data ingestion**, **ETL pipelines**, and **data migration** across on-premises, cloud, and hybrid environments.

---

### Key Features

- **Supports 100+ connectors** for structured, semi-structured, and unstructured data.
    
- **Scalable data movement** â€“ can handle gigabytes to terabytes of data.
    
- **Secure integration** with managed identities, Key Vault, and encrypted channels.
    
- **Rich transformation support** like column mapping, schema drift handling, and format conversions.
    
- **Parallelism & partitioning** for high-performance copying.
    

---

### Components of Copy Activity

1. **Source (From)** â€“ The dataset or linked service where the data resides (e.g., SQL DB, Blob storage, REST API).
    
2. **Sink (To)** â€“ The destination dataset where the data will be written (e.g., ADLS, Synapse, Cosmos DB).
    
3. **Mapping** â€“ Defines how columns/fields from the source map to the sink.
    
4. **Settings** â€“ Performance tuning options such as degree of parallelism, staging, and fault tolerance.
    

---

### Workflow of Copy Activity

1. A pipeline triggers the Copy Activity.
    
2. ADF reads data from the **source** using a linked service.
    
3. Data is optionally transformed (format, schema mapping, partitioning).
    
4. Data is written to the **sink**.
    
5. The process is logged and monitored in the ADF monitoring dashboard.
    

---

### Supported Data Stores

- **Azure Sources/Sinks:** Blob Storage, Data Lake (ADLS Gen1/Gen2), Azure SQL, Synapse, Cosmos DB, Table Storage.
    
- **On-Premises:** SQL Server, Oracle, SAP, File system (via Self-hosted IR).
    
- **SaaS Applications:** Salesforce, Dynamics 365, ServiceNow, Google Analytics, etc.
    
- **Big Data & NoSQL:** Amazon S3, MongoDB, Cassandra, Hadoop.
    

---

### Performance Tuning Options

- **Parallel Copy:** Enable parallelism to copy partitions of data simultaneously.
    
- **Staging:** Use Azure Blob/ADLS as a staging area for complex transformations.
    
- **Batch Size & Block Size:** Adjust for optimal throughput.
    
- **Self-hosted IR Scaling:** Scale out compute for large hybrid data transfers.
    

---

### Monitoring & Troubleshooting

- **Activity Runs View:** Shows status (Success, Failed, In Progress).
    
- **Detailed Metrics:** Throughput, rows read/written, time taken.
    
- **Retry Policies:** Configure automatic retries on transient failures.
    
- **Integration with Log Analytics:** Centralized monitoring and alerting.
    

---

### Example Use Cases

- Migrating on-prem SQL Server data to Azure SQL Database.
    
- Ingesting CSV/JSON/XML files from Blob Storage to Synapse for reporting.
    
- Copying SaaS application data (e.g., Salesforce) into Data Lake for analytics.
    
- Hybrid ETL: Moving data from on-prem Oracle DB to ADLS for big data processing.
    

---
### Best Practices

- Use **Integration Runtime** close to data sources for better performance.
    
- Leverage **incremental loads** instead of full loads to optimize costs.
    
- Enable **fault tolerance** for large jobs.
    
- Store sensitive connection strings in **Azure Key Vault**.
    
- Use **parameterized datasets** for reusability.




==========================================


## ğŸ“‚ Source Options for ADLS in Copy Activity

### 1. **File Path Settings**

- **File path type** â†’ Choose how to select files:
    
    - _File path_ â†’ Specific file (container/folder/file).
        
    - _Wildcard file path_ â†’ Use `*` or `?` to match multiple files. Example: `input/*.csv`.
        
    - _Folder path_ â†’ Copy all files in a folder.
        
- **Recursive** â†’ If enabled, copies files from subfolders as well.
    

---

### 2. **File Filter Options**

- **File name wildcard** â†’ Filter by pattern (e.g., `*.json`, `2025-09-*.parquet`).
    
- **Modified date filter** â†’ Pick files based on _last modified time_. Useful for **incremental loads**.
    
- **List of files** â†’ Provide explicit list of files (parameterized).
    

---

### 3. **Binary Copy**

- **Binary Copy Mode** â†’ If enabled, files are copied as-is (no parsing). Used for images, videos, executables.
    

---

### 4. **Compression**

- If files are compressed, choose:
    
    - **Gzip, Bzip2, Deflate, Zip** â†’ Auto-decompress on read.
        
    - _None_ â†’ If files are plain text or structured formats.
        

---

### 5. **File Format Settings** (if not binary copy)

- **Delimited Text (CSV/TSV):**
    
    - Row delimiter, column delimiter
        
    - Escape character, quote character
        
    - First row as header (yes/no)
        
- **JSON:**
    
    - Single object or Array of objects
        
- **Parquet / ORC / Avro:**
    
    - Schema auto-detection
        
    - Support for column projection
        

---

### 6. **Partition Options** (for performance)

- **Max concurrent connections** â†’ Number of parallel threads to read files.
    
- **Partition by folder/file** â†’ ADF can split work by file or by folder for faster ingestion.
    

---

### 7. **Additional Options**

- **Preserve hierarchy** â†’ Decide how to keep folder structure when moving data:
    
    - _PreserveHierarchy_ â†’ Keeps folder structure.
        
    - _FlattenHierarchy_ â†’ Drops folder paths, copies only files.
        
    - _MergeFiles_ â†’ Combines multiple files into a single output file.
        
- **Ignore empty files** â†’ Skip empty files instead of failing.
    
- **Max concurrent connections** â†’ Speed up large dataset ingestion.
    

---

âœ… Example Scenarios:

- **Incremental ingestion:** Use _last modified filter_ and wildcards (`2025-09-16*.csv`).
    
- **Big Data migration:** Enable _recursive_ + _parallel reads_ + _PreserveHierarchy_.
    
- **Data Lake to Synapse pipeline:** Use _Parquet/CSV reader_ with schema mapping.


## ğŸ—„ï¸ Source Options for SQL Server in Copy Activity

### 1. **Data Retrieval Options**

- **Table** â†’ Pick a specific table from the connected database.
    
- **Query** â†’ Provide a custom SQL query (parameterized if needed).
    
- **Stored Procedure** (only in some cases) â†’ Call a stored procedure and use its result set as source.
    

---

### 2. **Query Behavior**

- **Use Query from Dataset or Inline** â†’ You can store SQL queries in the dataset definition or write inline queries in the activity.
    
- **Parameterization** â†’ Pass pipeline parameters into the query (e.g., `SELECT * FROM Sales WHERE OrderDate = '@{pipeline().parameters.Date}'`).
    

---

### 3. **Batching & Fetch Options**

- **Batch Size** â†’ Number of rows fetched in one go (default varies, typically ~10,000). Helps tune performance for large datasets.
    
- **Isolation Level** â†’ Read Uncommitted, Read Committed, Snapshot, etc. (controls concurrency & locking).
    

---

### 4. **Partition Options (for parallel reads)**

- **Partition Option** â†’ Split source data into multiple partitions for faster reads. Options:
    
    - _None_ â†’ Single-threaded copy.
        
    - _Physical Partitions_ â†’ Use table partitions (if available).
        
    - _Dynamic Range_ â†’ Partition data based on numeric/date column (e.g., `OrderID BETWEEN 1â€“10000`, etc.).
        
- **Partition Column** â†’ Column used for partitioning (usually numeric or date).
    
- **Number of Partitions** â†’ Controls parallelism (e.g., 4 partitions = 4 parallel queries).
    

---

### 5. **Additional Options**

- **SQL Command Timeout** â†’ Maximum time allowed for query execution.
    
- **Detect Changes (Incremental Load)** â†’ Use watermark columns (like `LastModifiedDate`) for incremental copies.
    
- **Enable CDC (Change Data Capture)** â†’ If SQL Server has CDC enabled, ADF can use it to capture only changed rows.
    

---

### 6. **Data Consistency & Validation**

- **Table/Query Validation** â†’ Validate schema before copying.
    
- **Null Handling** â†’ How NULL values are handled when mapping.
    
- **String Handling** â†’ Options for trimming, padding, or encoding.
    

---

âœ… **Example Use Cases**

- Full table load â†’ Choose _Table_ option.
    
- Incremental load â†’ Use _Query_ with a `WHERE LastModified > @{pipeline().parameters.LastRunTime}` filter.
    
- High-volume migration â†’ Use _Dynamic Range partitioning_ on an indexed column for parallel ingestion


# 1ï¸âƒ£ Sink: **CSV (Delimited Text in Blob/ADLS)**

When you choose **CSV** as the **sink format**, youâ€™ll see file-related settings.

### **File Path Options**

- **File name option**
    
    - _Output to single file_ â†’ All data is written into one file.
        
    - _Per partition/file_ â†’ Each partition creates a separate file (e.g., `part-0001.csv`).
        
    - _Preserve source hierarchy_ â†’ Keeps folder/file structure from source.
        
- **File naming pattern** â†’ Custom names like `data_{date}.csv`.
    
- **Folder path** â†’ Target container/folder in Blob/ADLS.
    

### **File Format Settings**

- **Row delimiter** â†’ `\n` (default), `\r\n`, or custom.
    
- **Column delimiter** â†’ `,` (default), `;`, `|`, `\t`, etc.
    
- **Quote character** â†’ Usually `"`.
    
- **Escape character** â†’ Usually `\`.
    
- **Null value representation** â†’ For example, empty string, `\N`, or `"NULL"`.
    
- **Write headers** â†’ Yes/No (include column names as first row).
    

### **Compression**

- None, Gzip, Bzip2, Deflate, Zip.
    

### **Copy Behavior**

- _Overwrite_ (replace existing files).
    
- _Append_ (add new file without deleting old ones).
    
- _Merge_ (combine into existing file â€“ limited scenarios).
    

---

# 2ï¸âƒ£ Sink: **SQL Server**

When **SQL Server** is the **sink**, the options are more database-related.

### **Table & Write Options**

- **Table name** â†’ Select or provide the target table.
    
- **Auto-create table** â†’ ADF can auto-generate the table if it doesnâ€™t exist (based on source schema).
    
- **Pre-copy script** â†’ SQL script that runs before insert (e.g., `TRUNCATE TABLE TargetTable`).
    
- **Stored procedure sink** â†’ Instead of direct inserts, call a stored procedure with input parameters.
    

### **Write Behavior**

- **Insert** â†’ Default (append new rows).
    
- **Upsert** â†’ Insert new + update existing (requires key column).
    
- **Update** â†’ Update only existing rows.
    
- **Stored procedure** â†’ Custom merge logic via proc.
    

### **Batching & Performance**

- **Batch size** â†’ Number of rows per batch insert.
    
- **Bulk insert (PolyBase/BCP)** â†’ Faster loading for large volumes.
    
- **Table lock** â†’ Lock the table during write (improves speed but reduces concurrency).
    

### **Additional Options**

- **Identity insert** â†’ Control whether identity columns are written or auto-generated.
    
- **Null handling** â†’ Define how nulls map to SQL columns.
    
- **Transaction settings** â†’ Commit mode (per batch or per entire load).
    
- **SQL command timeout** â†’ Max allowed time for inserts.
    

---

âœ… **When to use what?**

- **CSV Sink** â†’ Good for **data lake staging**, archival, or feeding BI/ML pipelines.
    
- **SQL Server Sink** â†’ Good for **operational databases, reporting, or direct analytics**.
  
  
  