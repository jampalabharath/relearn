<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Interview Prep :: Data Engineering Notes</title>
    <link>http://localhost:1313/interviewprep/</link>
    <description></description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="http://localhost:1313/interviewprep/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ADF 1</title>
      <link>http://localhost:1313/interviewprep/adf1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/interviewprep/adf1/</guid>
      <description>1. What are the different types of Integration runtimes(IR) in ADF, and when should you use each? Integration runtime (IR) is a compute engine that provides computational resources to perform data movement, transformation, and orchestration in ADF.&#xA;Types of Integration Runtimes Azure IR&#xA;Use when performing data movement, transformation, and orchestration entirely within the cloud. Self-Hosted IR&#xA;Use when any source or destination is an on-premises system. Required to pull data from on-prem systems. Azure SSIS IR</description>
    </item>
    <item>
      <title>Data Pipeline</title>
      <link>http://localhost:1313/interviewprep/pipeline1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/interviewprep/pipeline1/</guid>
      <description>Databricks Medallion Playbook (ADLS CSV ‚Üí Delta) End‚Äëto‚Äëend guide for incremental loading, data validations/quality, and SCD Type 2 using Databricks + ADLS with Medallion layers: landing ‚Üí raw (bronze) ‚Üí rawint (silver‚Äëstaging) ‚Üí curated (silver) ‚Üí enriched (gold). Uses Delta Lake everywhere.&#xA;0) Principles &amp; Conventions General&#xA;Use Delta for all managed tables; avoid plain CSV/Parquet beyond landing. Prefer Auto Loader for incremental files; fall back to COPY INTO for one‚Äëoff backfills. Treat raw as immutable; never update, only append/reprocess. Store ingestion metadata on every row: ingest_id, ingest_ts, source_file, source_mod_ts, batch_id. Naming</description>
    </item>
    <item>
      <title>SQL Theory</title>
      <link>http://localhost:1313/interviewprep/sql1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/interviewprep/sql1/</guid>
      <description>1. What is SQL and what is it used for? SQL (Structured Query Language) is a domain-specific, declarative programming language designed for managing relational databases. It is the primary language for tasks like data retrieval, data manipulation, and database administration.&#xA;Core Components DDL (Data Definition Language): Used for defining and modifying the structure of the database. DML (Data Manipulation Language): Deals with adding, modifying, and removing data in the database. DCL (Data Control Language): Manages the permissions and access rights of the database. TCL (Transaction Control Language): Governs the transactional management of the database, such as commits or rollbacks. Common Database Management Tasks Data Retrieval and Reporting: Retrieve and analyze data, generate reports, and build dashboards.</description>
    </item>
    <item>
      <title>100 SQL Queries</title>
      <link>http://localhost:1313/interviewprep/sql2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/interviewprep/sql2/</guid>
      <description>This cheat sheet provides a quick reference to common SQL queries and concepts used in data analysis, from fundamental commands to advanced techniques.&#xA;Section 1: Basic Queries (1‚Äì10) Use these to select and filter data from a table.&#xA;-- 1. Select all columns from a table SELECT * FROM employees; -- 2. Select specific columns SELECT first_name, last_name, hire_date FROM employees; -- 3. Count all rows SELECT COUNT(*) FROM employees; -- 4. Count non-null values in a column SELECT COUNT(salary) FROM employees; -- 5. Find unique values in a column SELECT DISTINCT department FROM employees; -- 6. Filter data using WHERE SELECT * FROM employees WHERE department = &#39;Sales&#39;; -- 7. Filter with multiple conditions using AND SELECT * FROM employees WHERE department = &#39;Sales&#39; AND salary &gt; 60000; -- 8. Filter with multiple conditions using OR SELECT * FROM employees WHERE department = &#39;Sales&#39; OR department = &#39;Marketing&#39;; -- 9. Combine AND and OR with parentheses SELECT * FROM employees WHERE (department = &#39;Sales&#39; OR department = &#39;Marketing&#39;) AND salary &gt; 70000; -- 10. Exclude a value using NOT SELECT * FROM employees WHERE NOT department = &#39;IT&#39;; Section 2: Filtering &amp; Pattern Matching (11‚Äì20) Advanced filtering using IN, BETWEEN, and LIKE.</description>
    </item>
    <item>
      <title>SQL cheat sheet</title>
      <link>http://localhost:1313/interviewprep/sql3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/interviewprep/sql3/</guid>
      <description>üß© Data Manipulation Language (DML) Command Description Syntax Example SELECT Retrieves data from a database. SELECT column1, column2 FROM table_name; SELECT first_name, last_name FROM customers; INSERT Adds new records to a table. INSERT INTO table_name (column1, column2) VALUES (value1, value2); INSERT INTO customers (first_name, last_name) VALUES (&#39;Mary&#39;, &#39;Doe&#39;); UPDATE Modifies existing records in a table. UPDATE table_name SET column1 = value1, column2 = value2 WHERE condition; UPDATE employees SET employee_name = &#39;John Doe&#39;, department = &#39;Marketing&#39;; DELETE Removes records from a table. DELETE FROM table_name WHERE condition; DELETE FROM employees WHERE employee_name = &#39;John Doe&#39;; üèóÔ∏è Data Definition Language (DDL) Command Description Syntax Example CREATE Creates a new database object (table, view, etc.). CREATE TABLE table_name (column1 datatype1, column2 datatype2, ...); CREATE TABLE employees (employee_id INT PRIMARY KEY, first_name VARCHAR(50), last_name VARCHAR(50), age INT); ALTER Adds, deletes, or modifies columns in an existing table. ALTER TABLE table_name ADD column_name datatype; ALTER TABLE customers ADD email VARCHAR(100); DROP Deletes an existing table or database object. DROP TABLE table_name; DROP TABLE customers; TRUNCATE Removes all rows from a table but keeps the structure. TRUNCATE TABLE table_name; TRUNCATE TABLE customers; üîê Data Control Language (DCL) Command Description Syntax Example GRANT Gives privileges to users or roles. GRANT SELECT, INSERT ON table_name TO user_name; GRANT SELECT, INSERT ON employees TO &#39;John Doe&#39;; REVOKE Removes privileges previously granted. REVOKE SELECT, INSERT ON table_name FROM user_name; REVOKE SELECT, INSERT ON employees FROM &#39;John Doe&#39;; üîç Querying Data Command Description Syntax Example SELECT Statement Retrieves data from one or more tables. SELECT column1, column2 FROM table_name; SELECT first_name, last_name FROM customers; WHERE Clause Filters rows based on specific conditions. SELECT * FROM table_name WHERE condition; SELECT * FROM customers WHERE age &gt; 30; ORDER BY Clause Sorts the result set by one or more columns. SELECT * FROM table_name ORDER BY column_name ASC/DESC; SELECT * FROM products ORDER BY price DESC; GROUP BY Clause Groups rows sharing a property and is used with aggregates. SELECT column_name, COUNT(*) FROM table_name GROUP BY column_name; SELECT category, COUNT(*) FROM products GROUP BY category; HAVING Clause Filters grouped results (used with GROUP BY). SELECT column_name, COUNT(*) FROM table_name GROUP BY column_name HAVING condition; SELECT category, COUNT(*) FROM products GROUP BY category HAVING COUNT(*) &gt; 5; üîó Joining Commands Command Description Syntax Example INNER JOIN Returns rows with matching values in both tables. SELECT * FROM table1 INNER JOIN table2 ON table1.column = table2.column; SELECT * FROM employees INNER JOIN departments ON employees.department_id = departments.id; LEFT JOIN / LEFT OUTER JOIN Returns all rows from the left table and matching rows from the right table. SELECT * FROM table1 LEFT JOIN table2 ON table1.column = table2.column; SELECT * FROM employees LEFT JOIN departments ON employees.department_id = departments.id; RIGHT JOIN / RIGHT OUTER JOIN Returns all rows from the right table and matching rows from the left table. SELECT * FROM table1 RIGHT JOIN table2 ON table1.column = table2.column; SELECT * FROM employees RIGHT JOIN departments ON employees.department_id = departments.department_id; FULL JOIN / FULL OUTER JOIN Returns all rows when there‚Äôs a match in either table. SELECT * FROM table1 FULL JOIN table2 ON table1.column = table2.column; sql SELECT * FROM employees LEFT JOIN departments ON employees.employee_id = departments.employee_id UNION SELECT * FROM employees RIGHT JOIN departments ON employees.employee_id = departments.employee_id; CROSS JOIN Combines every row from the first table with every row from the second (Cartesian product). SELECT * FROM table1 CROSS JOIN table2; SELECT * FROM employees CROSS JOIN departments; SELF JOIN Joins a table with itself. SELECT * FROM table1 t1, table1 t2 WHERE t1.column = t2.column; SELECT * FROM employees t1, employees t2 WHERE t1.employee_id = t2.employee_id; NATURAL JOIN Matches columns with the same name in both tables. SELECT * FROM table1 NATURAL JOIN table2; SELECT * FROM employees NATURAL JOIN departments; üßÆ Subqueries in SQL Command Description Syntax Example IN Checks if a value matches any in a subquery result. SELECT column(s) FROM table WHERE value IN (subquery); SELECT * FROM customers WHERE city IN (SELECT city FROM suppliers); ANY Compares a value to any returned by a subquery (used with =, &gt;, &lt;, etc.). SELECT column(s) FROM table WHERE value &lt; ANY (subquery); SELECT * FROM products WHERE price &lt; ANY (SELECT unit_price FROM supplier_products); ALL Compares a value to all values returned by a subquery. SELECT column(s) FROM table WHERE value &gt; ALL (subquery); SELECT * FROM orders WHERE order_amount &gt; ALL (SELECT total_amount FROM previous_orders); üìä Aggregate Functions Command Description Syntax Example COUNT() Counts rows or non-null values in a column. SELECT COUNT(column_name) FROM table_name; SELECT COUNT(age) FROM employees; SUM() Calculates the sum of values in a column. SELECT SUM(column_name) FROM table_name; SELECT SUM(revenue) FROM sales; AVG() Calculates the average of values in a column. SELECT AVG(column_name) FROM table_name; SELECT AVG(price) FROM products; MIN() Returns the minimum value in a column. SELECT MIN(column_name) FROM table_name; SELECT MIN(price) FROM products; MAX() Returns the maximum value in a column. SELECT MAX(column_name) FROM table_name; SELECT MAX(price) FROM products; üî§ String Functions Command Description Syntax Example CONCAT() Concatenates two or more strings. SELECT CONCAT(string1, string2, ...) AS concatenated_string FROM table_name; SELECT CONCAT(first_name, &#39; &#39;, last_name) AS full_name FROM employees; SUBSTRING() / SUBSTR() Extracts a substring from a string. SELECT SUBSTRING(string FROM start_position [FOR length]) AS substring FROM table_name; SELECT SUBSTRING(product_name FROM 1 FOR 5) AS substring FROM products; CHAR_LENGTH() / LENGTH() Returns the number of characters in a string. SELECT CHAR_LENGTH(string) AS length FROM table_name; SELECT CHAR_LENGTH(product_name) AS length FROM products; UPPER() Converts all characters to uppercase. SELECT UPPER(string) AS uppercase_string FROM table_name; SELECT UPPER(first_name) AS uppercase_first_name FROM employees; LOWER() Converts all characters to lowercase. SELECT LOWER(string) AS lowercase_string FROM table_name; SELECT LOWER(last_name) AS lowercase_last_name FROM employees; TRIM() Removes specified prefixes, suffixes, or whitespace. SELECT TRIM([LEADING / TRAILING / BOTH] characters FROM string) AS trimmed_string FROM table_name; SELECT TRIM(TRAILING &#39; &#39; FROM full_name) AS trimmed_full_name FROM customers; LEFT() Returns characters from the left of a string. SELECT LEFT(string, num_characters) AS left_string FROM table_name; SELECT LEFT(product_name, 5) AS left_product_name FROM products; RIGHT() Returns characters from the right of a string. SELECT RIGHT(string, num_characters) AS right_string FROM table_name; SELECT RIGHT(order_number, 4) AS right_order_number FROM orders; REPLACE() Replaces occurrences of a substring within a string. SELECT REPLACE(string, old_substring, new_substring) AS replaced_string FROM table_name; SELECT REPLACE(description, &#39;old_string&#39;, &#39;new_string&#39;) AS replaced_description FROM product_descriptions; üïí Date and Time SQL Commands Command Description Syntax Example CURRENT_DATE() Returns the current date. SELECT CURRENT_DATE() AS current_date; ‚Äî CURRENT_TIME() Returns the current time. SELECT CURRENT_TIME() AS current_time; ‚Äî CURRENT_TIMESTAMP() Returns the current date and time. SELECT CURRENT_TIMESTAMP() AS current_timestamp; ‚Äî DATE_PART() Extracts a specific part (year, month, day, etc.) from a date or time. SELECT DATE_PART(&#39;part&#39;, date_expression) AS extracted_part; ‚Äî DATE_ADD() / DATE_SUB() Adds or subtracts a specified interval (days, months, years) to/from a date. SELECT DATE_ADD(date_expression, INTERVAL value unit) AS new_date; ‚Äî EXTRACT() Extracts a specific component from a date or time. SELECT EXTRACT(part FROM date_expression) AS extracted_part; ‚Äî TO_CHAR() Converts a date or time to a specific string format. SELECT TO_CHAR(date_expression, &#39;format&#39;) AS formatted_date; ‚Äî TIMESTAMPDIFF() Calculates the difference between two timestamps in a specified unit. SELECT TIMESTAMPDIFF(unit, timestamp1, timestamp2) AS difference; ‚Äî DATEDIFF() Returns the number of days between two dates. SELECT DATEDIFF(date1, date2) AS difference_in_days; ‚Äî ‚öôÔ∏è Conditional Expressions Command Description Syntax Example CASE Statement Performs conditional logic within a query. sql SELECT column1, column2, CASE WHEN condition1 THEN result1 WHEN condition2 THEN result2 ELSE default_result END AS alias FROM table_name; sql SELECT order_id, total_amount, CASE WHEN total_amount &gt; 1000 THEN &#39;High Value Order&#39; WHEN total_amount &gt; 500 THEN &#39;Medium Value Order&#39; ELSE &#39;Low Value Order&#39; END AS order_status FROM orders; IF() Function Evaluates a condition and returns a value based on the evaluation. SELECT IF(condition, true_value, false_value) AS alias FROM table_name; SELECT name, age, IF(age &gt; 50, &#39;Senior&#39;, &#39;Junior&#39;) AS employee_category FROM employees; COALESCE() Function Returns the first non-null value from a list. SELECT COALESCE(value1, value2, ...) AS alias FROM table_name; SELECT COALESCE(first_name, middle_name) AS preferred_name FROM employees; NULLIF() Function Returns NULL if two expressions are equal. SELECT NULLIF(expression1, expression2) AS alias FROM table_name; SELECT NULLIF(total_amount, discounted_amount) AS diff_amount FROM orders; üîÅ Set Operations Command Description Syntax Example UNION Combines result sets of two or more SELECT statements (removes duplicates). sql SELECT column1, column2 FROM table1 UNION SELECT column1, column2 FROM table2; sql SELECT first_name, last_name FROM customers UNION SELECT first_name, last_name FROM employees; INTERSECT Returns common rows that appear in both result sets. sql SELECT column1, column2 FROM table1 INTERSECT SELECT column1, column2 FROM table2; sql SELECT first_name, last_name FROM customers INTERSECT SELECT first_name, last_name FROM employees; EXCEPT Returns rows from the first query that are not present in the second. sql SELECT column1, column2 FROM table1 EXCEPT SELECT column1, column2 FROM table2; sql SELECT first_name, last_name FROM customers EXCEPT SELECT first_name, last_name FROM employees; üíæ Transaction Control Commands (TCL) Command Description Syntax Example COMMIT Saves all the changes made during the current transaction and makes them permanent. COMMIT; sql BEGIN TRANSACTION; -- SQL statements and changes within the transaction INSERT INTO employees (name, age) VALUES (&#39;Alice&#39;, 30); UPDATE products SET price = 25.00 WHERE category = &#39;Electronics&#39;; COMMIT; ROLLBACK Undoes all the changes made during the current transaction and discards them. ROLLBACK; sql BEGIN TRANSACTION; -- SQL statements and changes within the transaction INSERT INTO employees (name, age) VALUES (&#39;Bob&#39;, 35); UPDATE products SET price = 30.00 WHERE category = &#39;Electronics&#39;; ROLLBACK; SAVEPOINT Sets a point within a transaction to which you can later roll back. SAVEPOINT savepoint_name; sql BEGIN TRANSACTION; INSERT INTO employees (name, age) VALUES (&#39;Carol&#39;, 28); SAVEPOINT before_update; UPDATE products SET price = 40.00 WHERE category = &#39;Electronics&#39;; SAVEPOINT after_update; DELETE FROM customers WHERE age &gt; 60; ROLLBACK TO before_update; -- DELETE is rolled back, but UPDATE remains COMMIT; ROLLBACK TO SAVEPOINT Rolls back the transaction to a specific savepoint without affecting prior operations. ROLLBACK TO SAVEPOINT savepoint_name; sql BEGIN TRANSACTION; INSERT INTO employees (name, age) VALUES (&#39;David&#39;, 42); SAVEPOINT before_update; UPDATE products SET price = 50.00 WHERE category = &#39;Electronics&#39;; SAVEPOINT after_update; DELETE FROM customers WHERE age &gt; 60; ROLLBACK TO SAVEPOINT before_update; -- UPDATE is rolled back, but INSERT remains COMMIT; SET TRANSACTION Configures properties for the current transaction, such as isolation level or mode. SET TRANSACTION [ISOLATION LEVEL { READ COMMITTED / SERIALIZABLE }]; sql BEGIN TRANSACTION; -- Set isolation level to READ COMMITTED SET TRANSACTION ISOLATION LEVEL READ COMMITTED; INSERT INTO employees (name, age) VALUES (&#39;Emily&#39;, 35); UPDATE products SET price = 60.00 WHERE category = &#39;Electronics&#39;; COMMIT;</description>
    </item>
    <item>
      <title>SQL &amp; PySpark Queries</title>
      <link>http://localhost:1313/interviewprep/quick-ref/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/interviewprep/quick-ref/</guid>
      <description>This guide pairs each SQL example with a PySpark DataFrame API equivalent.&#xA;Assumptions: df is employees, and other tables are similarly named DataFrames (departments, orders, etc.).&#xA;Imports used in PySpark examples&#xA;from pyspark.sql import functions as F from pyspark.sql.functions import col, lit, when, lower, upper, count, sum, avg, max, min, row_number, rank, dense_rank, ntile, lead, lag, coalesce from pyspark.sql.window import Window üß© Section 1: Basic Queries (1‚Äì10) SQL -- 1 SELECT * FROM employees; -- 2 SELECT first_name, last_name, hire_date FROM employees; -- 3 SELECT COUNT(*) FROM employees; -- 4 SELECT COUNT(salary) FROM employees; -- 5 SELECT DISTINCT department FROM employees; -- 6 SELECT * FROM employees WHERE department = &#39;Sales&#39;; -- 7 SELECT * FROM employees WHERE department = &#39;Sales&#39; AND salary &gt; 60000; -- 8 SELECT * FROM employees WHERE department = &#39;Sales&#39; OR department = &#39;Marketing&#39;; -- 9 SELECT * FROM employees WHERE (department = &#39;Sales&#39; OR department = &#39;Marketing&#39;) AND salary &gt; 70000; -- 10 SELECT * FROM employees WHERE NOT department = &#39;IT&#39;; PySpark # 1 df.show() # 2 df.select(&#34;first_name&#34;, &#34;last_name&#34;, &#34;hire_date&#34;).show() # 3 df.count() # 4 df.select(F.count(&#34;salary&#34;)).show() # 5 df.select(&#34;department&#34;).distinct().show() # 6 df.filter(col(&#34;department&#34;) == &#34;Sales&#34;).show() # 7 df.filter((col(&#34;department&#34;) == &#34;Sales&#34;) &amp; (col(&#34;salary&#34;) &gt; 60000)).show() # 8 df.filter((col(&#34;department&#34;) == &#34;Sales&#34;) | (col(&#34;department&#34;) == &#34;Marketing&#34;)).show() # 9 df.filter(((col(&#34;department&#34;) == &#34;Sales&#34;) | (col(&#34;department&#34;) == &#34;Marketing&#34;)) &amp; (col(&#34;salary&#34;) &gt; 70000)).show() # 10 df.filter(col(&#34;department&#34;) != &#34;IT&#34;).show() üîé Section 2: Filtering &amp; Pattern Matching (11‚Äì20) SQL -- 11 SELECT * FROM employees WHERE department IN (&#39;Sales&#39;, &#39;Marketing&#39;, &#39;IT&#39;); -- 12 SELECT * FROM employees WHERE department NOT IN (&#39;Sales&#39;, &#39;Marketing&#39;); -- 13 SELECT * FROM employees WHERE salary BETWEEN 50000 AND 75000; -- 14 SELECT * FROM employees WHERE first_name ILIKE &#39;%jo%&#39;; -- 15 SELECT * FROM employees WHERE last_name LIKE &#39;Smi%&#39;; -- 16 SELECT * FROM employees WHERE email LIKE &#39;%@gmail.com&#39;; -- 17 SELECT * FROM employees WHERE first_name LIKE &#39;_a%&#39;; -- 18 SELECT * FROM employees WHERE manager_id IS NULL; -- 19 SELECT * FROM employees WHERE manager_id IS NOT NULL; -- 20 SELECT * FROM employees WHERE hire_date BETWEEN &#39;2023-01-01&#39; AND &#39;2023-12-31&#39;; PySpark # 11 df.filter(col(&#34;department&#34;).isin(&#34;Sales&#34;, &#34;Marketing&#34;, &#34;IT&#34;)).show() # 12 df.filter(~col(&#34;department&#34;).isin(&#34;Sales&#34;, &#34;Marketing&#34;)).show() # 13 df.filter(col(&#34;salary&#34;).between(50000, 75000)).show() # 14 (case-insensitive search) df.filter(lower(col(&#34;first_name&#34;)).like(&#34;%jo%&#34;)).show() # 15 df.filter(col(&#34;last_name&#34;).startswith(&#34;Smi&#34;)).show() # 16 df.filter(col(&#34;email&#34;).endswith(&#34;@gmail.com&#34;)).show() # 17 (single char then &#39;a&#39;) df.filter(col(&#34;first_name&#34;).rlike(&#34;^.a.*&#34;)).show() # 18 df.filter(col(&#34;manager_id&#34;).isNull()).show() # 19 df.filter(col(&#34;manager_id&#34;).isNotNull()).show() # 20 df.filter(col(&#34;hire_date&#34;).between(&#34;2023-01-01&#34;, &#34;2023-12-31&#34;)).show() ‚ÜïÔ∏è Section 3: Sorting &amp; Limiting (21‚Äì30) SQL -- 21 SELECT * FROM employees ORDER BY last_name ASC; -- 22 SELECT * FROM employees ORDER BY salary DESC; -- 23 SELECT * FROM employees ORDER BY department ASC, salary DESC; -- 24 SELECT * FROM employees ORDER BY salary DESC LIMIT 10; -- 25 SELECT * FROM employees ORDER BY hire_date DESC LIMIT 5; -- 26 SELECT DISTINCT department FROM employees ORDER BY department; -- 27 SELECT * FROM employees ORDER BY employee_id OFFSET 10 LIMIT 10; -- 28 SELECT * FROM employees ORDER BY salary DESC LIMIT 1; -- 29 SELECT DISTINCT salary FROM employees ORDER BY salary DESC LIMIT 1 OFFSET 1; -- 30 SELECT first_name, last_name, (salary * 0.1) AS bonus FROM employees ORDER BY bonus DESC; PySpark # 21 df.orderBy(col(&#34;last_name&#34;).asc()).show() # 22 df.orderBy(col(&#34;salary&#34;).desc()).show() # 23 df.orderBy(col(&#34;department&#34;).asc(), col(&#34;salary&#34;).desc()).show() # 24 df.orderBy(col(&#34;salary&#34;).desc()).limit(10).show() # 25 df.orderBy(col(&#34;hire_date&#34;).desc()).limit(5).show() # 26 df.select(&#34;department&#34;).distinct().orderBy(&#34;department&#34;).show() # 27 (rows 11‚Äì20): use ROW_NUMBER then filter w = Window.orderBy(&#34;employee_id&#34;) df.withColumn(&#34;rn&#34;, row_number().over(w)).filter((col(&#34;rn&#34;) &gt;= 11) &amp; (col(&#34;rn&#34;) &lt;= 20)).drop(&#34;rn&#34;).show() # 28 df.orderBy(col(&#34;salary&#34;).desc()).limit(1).show() # 29 (second-highest salary) ‚Äì two ways df.select(&#34;salary&#34;).distinct().orderBy(col(&#34;salary&#34;).desc()).limit(2).orderBy(col(&#34;salary&#34;).asc()).limit(1).show() # or using dense_rank w2 = Window.orderBy(col(&#34;salary&#34;).desc()) df.select(&#34;salary&#34;, dense_rank().over(w2).alias(&#34;dr&#34;)).filter(col(&#34;dr&#34;) == 2).select(&#34;salary&#34;).distinct().show() # 30 df.select(&#34;first_name&#34;, &#34;last_name&#34;, (col(&#34;salary&#34;) * 0.1).alias(&#34;bonus&#34;)).orderBy(col(&#34;bonus&#34;).desc()).show() üìä Section 4: Aggregation &amp; Grouping (31‚Äì40) SQL -- 31 SELECT department, COUNT(*) FROM employees GROUP BY department; -- 32 SELECT department, AVG(salary) FROM employees GROUP BY department; -- 33 SELECT department, SUM(salary) FROM employees GROUP BY department; -- 34 SELECT department, MAX(salary) FROM employees GROUP BY department; -- 35 SELECT department, MIN(salary) FROM employees GROUP BY department; -- 36 SELECT department, AVG(salary) FROM employees GROUP BY department HAVING AVG(salary) &gt; 65000; -- 37 SELECT department, COUNT(*) FROM employees GROUP BY department HAVING COUNT(*) &gt; 5; -- 38 SELECT department, AVG(salary) FROM employees WHERE hire_date &gt; &#39;2022-01-01&#39; GROUP BY department; -- 39 SELECT department, job_title, AVG(salary) FROM employees GROUP BY department, job_title; -- 40 SELECT department, MAX(salary) FROM employees GROUP BY department; PySpark # 31 df.groupBy(&#34;department&#34;).count().show() # 32 df.groupBy(&#34;department&#34;).agg(avg(&#34;salary&#34;).alias(&#34;avg_salary&#34;)).show() # 33 df.groupBy(&#34;department&#34;).agg(sum(&#34;salary&#34;).alias(&#34;total_salary&#34;)).show() # 34 df.groupBy(&#34;department&#34;).agg(max(&#34;salary&#34;).alias(&#34;max_salary&#34;)).show() # 35 df.groupBy(&#34;department&#34;).agg(min(&#34;salary&#34;).alias(&#34;min_salary&#34;)).show() # 36 df.groupBy(&#34;department&#34;).agg(avg(&#34;salary&#34;).alias(&#34;avg_salary&#34;)).filter(col(&#34;avg_salary&#34;) &gt; 65000).show() # 37 df.groupBy(&#34;department&#34;).count().filter(col(&#34;count&#34;) &gt; 5).show() # 38 df.filter(col(&#34;hire_date&#34;) &gt; &#34;2022-01-01&#34;).groupBy(&#34;department&#34;).agg(avg(&#34;salary&#34;)).show() # 39 df.groupBy(&#34;department&#34;, &#34;job_title&#34;).agg(avg(&#34;salary&#34;).alias(&#34;avg_salary&#34;)).show() # 40 df.groupBy(&#34;department&#34;).agg(max(&#34;salary&#34;).alias(&#34;max_salary&#34;)).show() üîó Section 5: Joins (41‚Äì60) SQL -- 41 SELECT e.first_name, d.department_name FROM employees e INNER JOIN departments d ON e.department_id = d.department_id; -- 42 SELECT e.first_name, d.department_name FROM employees e LEFT JOIN departments d ON e.department_id = d.department_id; -- 43 SELECT e.first_name FROM employees e LEFT JOIN departments d ON e.department_id = d.department_id WHERE d.department_id IS NULL; -- 44 SELECT e.first_name, d.department_name FROM employees e RIGHT JOIN departments d ON e.department_id = d.department_id; -- 45 SELECT e.first_name, d.department_name FROM employees e FULL OUTER JOIN departments d ON e.department_id = d.department_id; -- 46 SELECT e.first_name, p.project_name, d.department_name FROM employees e JOIN projects p ON e.employee_id = p.employee_id JOIN departments d ON e.department_id = d.department_id; -- 47 SELECT e.first_name AS employee, m.first_name AS manager FROM employees e JOIN employees m ON e.manager_id = m.employee_id; -- 48 SELECT o.order_id, c.customer_name FROM orders o JOIN customers c ON o.customer_zip_code = c.customer_zip_code; -- 49 SELECT * FROM employees CROSS JOIN departments; -- 50 SELECT p.* FROM products p LEFT JOIN sales s ON p.product_id = s.product_id WHERE s.sale_id IS NULL; -- 51 SELECT e.first_name, e.last_name FROM employees e LEFT JOIN employee_projects ep ON e.employee_id = ep.employee_id WHERE ep.project_id IS NULL; -- 52 SELECT p.project_name FROM projects p LEFT JOIN employee_projects ep ON p.project_id = ep.project_id WHERE ep.employee_id IS NULL; -- 53 SELECT d.department_name, SUM(e.salary) AS total_salary FROM departments d LEFT JOIN employees e ON d.department_id = e.department_id GROUP BY d.department_name; -- 54 SELECT d.department_name, COUNT(e.employee_id) AS num_employees FROM departments d LEFT JOIN employees e ON d.department_id = e.department_id GROUP BY d.department_name; -- 55 SELECT o.*, c.customer_name FROM orders o JOIN customers c ON o.customer_id = c.customer_id AND o.order_date = c.last_purchase_date; -- 56 SELECT e.first_name, m.first_name AS manager_name, d.department_name FROM employees e JOIN employees m ON e.manager_id = m.employee_id JOIN departments d ON m.department_id = d.department_id; -- 57 SELECT e.employee_id, h.salary_grade FROM employees e JOIN salary_grades h ON e.salary BETWEEN h.min_salary AND h.max_salary; -- 58 SELECT e.first_name, d.department_name FROM employees e JOIN departments d ON e.department_id = d.department_id WHERE d.location_city = &#39;New York&#39;; -- 59 SELECT d.department_name, AVG(e.salary) AS avg_salary FROM employees e JOIN departments d ON e.department_id = d.department_id GROUP BY d.department_name; -- 60 SELECT a.order_id, b.item_name FROM table_a a JOIN table_b b ON a.product_id = b.item_id; PySpark # 41 employees.join(departments, &#34;department_id&#34;, &#34;inner&#34;).select(&#34;first_name&#34;, &#34;department_name&#34;).show() # 42 employees.join(departments, &#34;department_id&#34;, &#34;left&#34;).select(&#34;first_name&#34;, &#34;department_name&#34;).show() # 43 employees.join(departments, &#34;department_id&#34;, &#34;left&#34;)\ .filter(departments.department_id.isNull())\ .select(employees.first_name).show() # 44 employees.join(departments, &#34;department_id&#34;, &#34;right&#34;).select(&#34;first_name&#34;, &#34;department_name&#34;).show() # 45 employees.join(departments, &#34;department_id&#34;, &#34;outer&#34;).select(&#34;first_name&#34;, &#34;department_name&#34;).show() # 46 employees.join(projects, &#34;employee_id&#34;)\ .join(departments, &#34;department_id&#34;)\ .select(&#34;first_name&#34;, &#34;project_name&#34;, &#34;department_name&#34;).show() # 47 e = employees.alias(&#34;e&#34;); m = employees.alias(&#34;m&#34;) e.join(m, col(&#34;e.manager_id&#34;) == col(&#34;m.employee_id&#34;))\ .select(col(&#34;e.first_name&#34;).alias(&#34;employee&#34;), col(&#34;m.first_name&#34;).alias(&#34;manager&#34;)).show() # 48 orders.join(customers, orders.customer_zip_code == customers.customer_zip_code)\ .select(&#34;order_id&#34;, &#34;customer_name&#34;).show() # 49 employees.crossJoin(departments).show() # 50 products.join(sales, &#34;product_id&#34;, &#34;left&#34;)\ .filter(col(&#34;sale_id&#34;).isNull())\ .select(&#34;product_id&#34;, *[c for c in products.columns if c != &#34;product_id&#34;]).show() # 51 employees.join(employee_projects, &#34;employee_id&#34;, &#34;left&#34;)\ .filter(col(&#34;project_id&#34;).isNull())\ .select(&#34;first_name&#34;, &#34;last_name&#34;).show() # 52 projects.join(employee_projects, &#34;project_id&#34;, &#34;left&#34;)\ .filter(col(&#34;employee_id&#34;).isNull())\ .select(&#34;project_name&#34;).show() # 53 departments.join(employees, &#34;department_id&#34;, &#34;left&#34;)\ .groupBy(&#34;department_name&#34;).agg(sum(&#34;salary&#34;).alias(&#34;total_salary&#34;)).show() # 54 departments.join(employees, &#34;department_id&#34;, &#34;left&#34;)\ .groupBy(&#34;department_name&#34;).agg(count(&#34;employee_id&#34;).alias(&#34;num_employees&#34;)).show() # 55 orders.join(customers, (orders.customer_id == customers.customer_id) &amp; (orders.order_date == customers.last_purchase_date))\ .select(orders[&#34;*&#34;], customers.customer_name).show() # 56 e = employees.alias(&#34;e&#34;); m = employees.alias(&#34;m&#34;); d = departments.alias(&#34;d&#34;) e.join(m, col(&#34;e.manager_id&#34;) == col(&#34;m.employee_id&#34;))\ .join(d, col(&#34;m.department_id&#34;) == col(&#34;d.department_id&#34;))\ .select(col(&#34;e.first_name&#34;), col(&#34;m.first_name&#34;).alias(&#34;manager_name&#34;), col(&#34;d.department_name&#34;)).show() # 57 employees.join(salary_grades, (employees.salary &gt;= salary_grades.min_salary) &amp; (employees.salary &lt;= salary_grades.max_salary))\ .select(&#34;employee_id&#34;, &#34;salary_grade&#34;).show() # 58 employees.join(departments, &#34;department_id&#34;)\ .filter(col(&#34;location_city&#34;) == &#34;New York&#34;)\ .select(&#34;first_name&#34;, &#34;department_name&#34;).show() # 59 employees.join(departments, &#34;department_id&#34;)\ .groupBy(&#34;department_name&#34;).agg(avg(&#34;salary&#34;).alias(&#34;avg_salary&#34;)).show() # 60 table_a.join(table_b, table_a.product_id == table_b.item_id)\ .select(table_a.order_id, table_b.item_name).show() üß† Section 6: Subqueries (61‚Äì75) SQL -- 61 SELECT * FROM employees WHERE salary &gt; (SELECT AVG(salary) FROM employees); -- 62 SELECT * FROM employees WHERE department_id IN ( SELECT department_id FROM departments WHERE location_city = &#39;San Francisco&#39; ); -- 63 SELECT d.department_name, s.avg_salary FROM ( SELECT department_id, AVG(salary) AS avg_salary FROM employees GROUP BY department_id ) AS s JOIN departments d ON s.department_id = d.department_id; -- 64 SELECT first_name, salary, (SELECT AVG(salary) FROM employees) AS company_avg_salary FROM employees; -- 65 SELECT * FROM employees e WHERE salary &gt; (SELECT AVG(salary) FROM employees WHERE department_id = e.department_id); -- 66 SELECT d.department_name FROM departments d WHERE EXISTS (SELECT 1 FROM employees e WHERE e.department_id = d.department_id AND e.salary &gt; 100000); -- 67 SELECT d.department_name FROM departments d WHERE NOT EXISTS (SELECT 1 FROM employees e WHERE e.department_id = d.department_id); -- 68 SELECT * FROM departments WHERE department_id IN (SELECT DISTINCT department_id FROM employees); -- 69 SELECT * FROM employees e WHERE EXISTS (SELECT 1 FROM orders o WHERE o.employee_id = e.employee_id); -- 70 SELECT d.department_name, (SELECT SUM(salary) FROM employees e WHERE e.department_id = d.department_id) AS total_department_salary FROM departments d; -- 71 SELECT department_name FROM departments WHERE department_id = ( SELECT department_id FROM employees GROUP BY department_id ORDER BY AVG(salary) DESC LIMIT 1 ); -- 72 SELECT customer_name FROM customers WHERE customer_id IN (SELECT customer_id FROM orders WHERE product_id = 123); -- 73 SELECT COUNT(*) FROM employees WHERE salary &gt; (SELECT AVG(salary) FROM employees); -- 74 SELECT customer_name FROM customers WHERE customer_id NOT IN (SELECT DISTINCT customer_id FROM orders); -- 75 SELECT e.first_name, e.salary, e.job_title FROM employees e WHERE e.salary &lt; (SELECT AVG(salary) FROM employees WHERE job_title = e.job_title); PySpark # 61 avg_sal = df.select(avg(&#34;salary&#34;).alias(&#34;avg&#34;)).first()[&#34;avg&#34;] df.filter(col(&#34;salary&#34;) &gt; avg_sal).show() # 62 sf_depts = departments.filter(col(&#34;location_city&#34;) == &#34;San Francisco&#34;).select(&#34;department_id&#34;).distinct() df.join(sf_depts, &#34;department_id&#34;).show() # 63 avg_df = df.groupBy(&#34;department_id&#34;).agg(avg(&#34;salary&#34;).alias(&#34;avg_salary&#34;)) avg_df.join(departments, &#34;department_id&#34;).select(&#34;department_name&#34;, &#34;avg_salary&#34;).show() # 64 avg_val = df.select(avg(&#34;salary&#34;).alias(&#34;avg&#34;)).first()[&#34;avg&#34;] df.select(&#34;first_name&#34;, &#34;salary&#34;, lit(avg_val).alias(&#34;company_avg_salary&#34;)).show() # 65 dept_avg = df.groupBy(&#34;department_id&#34;).agg(avg(&#34;salary&#34;).alias(&#34;dept_avg&#34;)) df.join(dept_avg, &#34;department_id&#34;).filter(col(&#34;salary&#34;) &gt; col(&#34;dept_avg&#34;)).show() # 66 high_paid = df.filter(col(&#34;salary&#34;) &gt; 100000).select(&#34;department_id&#34;).distinct() departments.join(high_paid, &#34;department_id&#34;).select(&#34;department_name&#34;).distinct().show() # 67 departments.join(df.select(&#34;department_id&#34;).distinct(), &#34;department_id&#34;, &#34;left_anti&#34;).select(&#34;department_name&#34;).show() # 68 departments.join(df.select(&#34;department_id&#34;).distinct(), &#34;department_id&#34;).show() # 69 df.join(orders.select(&#34;employee_id&#34;).distinct(), &#34;employee_id&#34;).select(df[&#34;*&#34;]).distinct().show() # 70 dept_totals = df.groupBy(&#34;department_id&#34;).agg(sum(&#34;salary&#34;).alias(&#34;total_department_salary&#34;)) departments.join(dept_totals, &#34;department_id&#34;).select(&#34;department_name&#34;, &#34;total_department_salary&#34;).show() # 71 dept_avg = df.groupBy(&#34;department_id&#34;).agg(avg(&#34;salary&#34;).alias(&#34;avg_salary&#34;)) top_dept = dept_avg.orderBy(col(&#34;avg_salary&#34;).desc()).limit(1) departments.join(top_dept, &#34;department_id&#34;).select(&#34;department_name&#34;).show() # 72 buyers = orders.filter(col(&#34;product_id&#34;) == 123).select(&#34;customer_id&#34;).distinct() customers.join(buyers, &#34;customer_id&#34;).select(&#34;customer_name&#34;).show() # 73 df.filter(col(&#34;salary&#34;) &gt; avg_sal).count() # 74 customers.join(orders.select(&#34;customer_id&#34;).distinct(), &#34;customer_id&#34;, &#34;left_anti&#34;).select(&#34;customer_name&#34;).show() # 75 job_avg = df.groupBy(&#34;job_title&#34;).agg(avg(&#34;salary&#34;).alias(&#34;job_avg&#34;)) df.join(job_avg, &#34;job_title&#34;).filter(col(&#34;salary&#34;) &lt; col(&#34;job_avg&#34;)).select(&#34;first_name&#34;, &#34;salary&#34;, &#34;job_title&#34;).show() ü™ü Section 7: Window Functions (76‚Äì85) SQL -- 76 SELECT first_name, department, salary, ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) AS rn FROM employees; -- 77 SELECT first_name, department, salary, RANK() OVER (PARTITION BY department ORDER BY salary DESC) AS rank FROM employees; -- 78 SELECT first_name, department, salary, DENSE_RANK() OVER (PARTITION BY department ORDER BY salary DESC) AS dense_rank FROM employees; -- 79 SELECT first_name, salary, NTILE(4) OVER (ORDER BY salary DESC) AS quartile FROM employees; -- 80 SELECT order_date, total_amount, LEAD(total_amount, 1) OVER (ORDER BY order_date) AS next_order_amount FROM orders; -- 81 SELECT order_date, total_amount, LAG(total_amount, 1, 0) OVER (ORDER BY order_date) AS previous_order_amount FROM orders; -- 82 SELECT order_date, total_amount, SUM(total_amount) OVER (ORDER BY order_date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS running_total FROM orders; -- 83 SELECT order_date, total_amount, AVG(total_amount) OVER (ORDER BY order_date ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) AS moving_avg FROM orders; -- 84 SELECT * FROM ( SELECT first_name, department, salary, RANK() OVER (PARTITION BY department ORDER BY salary DESC) AS rn FROM employees ) AS ranked_employees WHERE rn &lt;= 3; -- 85 SELECT product_id, SUM(sales) AS product_sales, SUM(SUM(sales)) OVER () AS total_sales, (SUM(sales) / SUM(SUM(sales)) OVER ()) * 100 AS percentage_of_total FROM sales GROUP BY product_id; PySpark w_dept = Window.partitionBy(&#34;department&#34;).orderBy(col(&#34;salary&#34;).desc()) # 76 df.select(&#34;first_name&#34;, &#34;department&#34;, &#34;salary&#34;, row_number().over(w_dept).alias(&#34;rn&#34;)).show() # 77 df.select(&#34;first_name&#34;, &#34;department&#34;, &#34;salary&#34;, rank().over(w_dept).alias(&#34;rank&#34;)).show() # 78 df.select(&#34;first_name&#34;, &#34;department&#34;, &#34;salary&#34;, dense_rank().over(w_dept).alias(&#34;dense_rank&#34;)).show() # 79 df.select(&#34;first_name&#34;, &#34;salary&#34;, ntile(4).over(Window.orderBy(col(&#34;salary&#34;).desc())).alias(&#34;quartile&#34;)).show() # 80 orders.select(&#34;order_date&#34;, &#34;total_amount&#34;, lead(&#34;total_amount&#34;, 1).over(Window.orderBy(&#34;order_date&#34;)).alias(&#34;next_order_amount&#34;)).show() # 81 orders.select(&#34;order_date&#34;, &#34;total_amount&#34;, lag(&#34;total_amount&#34;, 1, 0).over(Window.orderBy(&#34;order_date&#34;)).alias(&#34;previous_order_amount&#34;)).show() # 82 orders.select(&#34;order_date&#34;, &#34;total_amount&#34;, F.sum(&#34;total_amount&#34;).over(Window.orderBy(&#34;order_date&#34;) .rowsBetween(Window.unboundedPreceding, Window.currentRow)).alias(&#34;running_total&#34;)).show() # 83 orders.select(&#34;order_date&#34;, &#34;total_amount&#34;, avg(&#34;total_amount&#34;).over(Window.orderBy(&#34;order_date&#34;).rowsBetween(-2, Window.currentRow)).alias(&#34;moving_avg&#34;)).show() # 84 df.withColumn(&#34;rn&#34;, rank().over(w_dept)).filter(col(&#34;rn&#34;) &lt;= 3).drop(&#34;rn&#34;).show() # 85 prod_sales = sales.groupBy(&#34;product_id&#34;).agg(sum(&#34;sales&#34;).alias(&#34;product_sales&#34;)) prod_sales.withColumn(&#34;total_sales&#34;, sum(&#34;product_sales&#34;).over(Window.partitionBy()))\ .withColumn(&#34;percentage_of_total&#34;, (col(&#34;product_sales&#34;) / col(&#34;total_sales&#34;)) * 100)\ .select(&#34;product_id&#34;, &#34;product_sales&#34;, &#34;total_sales&#34;, &#34;percentage_of_total&#34;).show() üßÆ Section 8: CTEs &amp; Data Manipulation (86‚Äì100) SQL -- 86 WITH department_avg AS ( SELECT department_id, AVG(salary) AS avg_dept_salary FROM employees GROUP BY department_id ) SELECT e.first_name, e.salary, da.avg_dept_salary FROM employees e JOIN department_avg da ON e.department_id = da.department_id WHERE e.salary &gt; da.avg_dept_salary; -- 87 INSERT INTO employees (first_name, last_name, salary) VALUES (&#39;John&#39;, &#39;Doe&#39;, 60000); -- 88 UPDATE employees SET salary = salary * 1.05 WHERE department = &#39;IT&#39;; -- 89 DELETE FROM employees WHERE employee_id = 101; -- 90 TRUNCATE TABLE old_data; -- 91 SELECT first_name FROM employees UNION SELECT first_name FROM customers; -- 92 SELECT first_name FROM employees UNION ALL SELECT first_name FROM customers; -- 93 SELECT first_name, salary, CASE WHEN salary &gt; 100000 THEN &#39;High Earner&#39; WHEN salary BETWEEN 50000 AND 100000 THEN &#39;Mid-Range&#39; ELSE &#39;Junior&#39; END AS salary_level FROM employees; -- 94 SELECT department, COUNT(CASE WHEN salary &gt; 70000 THEN 1 END) AS high_salary_count, COUNT(CASE WHEN salary &lt;= 70000 THEN 1 END) AS low_salary_count FROM employees GROUP BY department; -- 95 SELECT CAST(order_date AS DATE) FROM orders; -- 96 SELECT COALESCE(email, &#39;No Email Provided&#39;) FROM employees; -- 97 SELECT NULLIF(salary, 0) FROM employees; -- 98 (Recursive CTE) WITH RECURSIVE subordinates AS ( SELECT employee_id, manager_id FROM employees WHERE employee_id = 1 UNION ALL SELECT e.employee_id, e.manager_id FROM employees e JOIN subordinates s ON e.manager_id = s.employee_id ) SELECT * FROM subordinates; -- 99 SELECT department, job_title, SUM(salary) FROM employees GROUP BY GROUPING SETS ((department), (job_title), ()); -- 100 SELECT department, job_title, SUM(salary) FROM employees GROUP BY ROLLUP(department, job_title); PySpark # 86 (Use SQL CTE via temp view) df.createOrReplaceTempView(&#34;employees&#34;) spark.sql(&#34;&#34;&#34; WITH department_avg AS ( SELECT department_id, AVG(salary) AS avg_dept_salary FROM employees GROUP BY department_id ) SELECT e.first_name, e.salary, da.avg_dept_salary FROM employees e JOIN department_avg da ON e.department_id = da.department_id WHERE e.salary &gt; da.avg_dept_salary &#34;&#34;&#34;).show() # 87 (Insert-like: union new rows) df_new = spark.createDataFrame([(&#34;John&#34;, &#34;Doe&#34;, 60000)], [&#34;first_name&#34;, &#34;last_name&#34;, &#34;salary&#34;]) df = df.unionByName(df_new) # 88 (Update-like) df = df.withColumn(&#34;salary&#34;, when(col(&#34;department&#34;) == &#34;IT&#34;, col(&#34;salary&#34;) * 1.05).otherwise(col(&#34;salary&#34;))) # 89 (Delete-like) df = df.filter(col(&#34;employee_id&#34;) != 101) # 90 (Truncate-like in-memory) df = df.limit(0) # for Delta table: spark.sql(&#34;TRUNCATE TABLE old_data&#34;) # 91 UNION (distinct) df_union_distinct = df1.union(df2).distinct() # 92 UNION ALL df_union_all = df1.union(df2) # 93 CASE/WHEN df.select(&#34;first_name&#34;, &#34;salary&#34;, when(col(&#34;salary&#34;) &gt; 100000, &#34;High Earner&#34;) .when((col(&#34;salary&#34;) &gt;= 50000) &amp; (col(&#34;salary&#34;) &lt;= 100000), &#34;Mid-Range&#34;) .otherwise(&#34;Junior&#34;).alias(&#34;salary_level&#34;)).show() # 94 Pivot without true pivot df.groupBy(&#34;department&#34;).agg( count(when(col(&#34;salary&#34;) &gt; 70000, 1)).alias(&#34;high_salary_count&#34;), count(when(col(&#34;salary&#34;) &lt;= 70000, 1)).alias(&#34;low_salary_count&#34;) ).show() # 95 CAST orders.select(col(&#34;order_date&#34;).cast(&#34;date&#34;).alias(&#34;order_date&#34;)).show() # 96 COALESCE df.select(coalesce(col(&#34;email&#34;), lit(&#34;No Email Provided&#34;)).alias(&#34;email&#34;)).show() # 97 NULLIF(salary,0) =&gt; NULL when equal df.select(when(col(&#34;salary&#34;) == 0, lit(None)).otherwise(col(&#34;salary&#34;)).alias(&#34;salary_or_null&#34;)).show() # 98 Recursive CTE not native; iterative approach example (hierarchy traversal) # Start with seed seed = df.filter(col(&#34;employee_id&#34;) == 1).select(&#34;employee_id&#34;, &#34;manager_id&#34;) result = seed frontier = seed while True: step = df.select(&#34;employee_id&#34;, &#34;manager_id&#34;)\ .join(frontier, df.manager_id == frontier.employee_id, &#34;inner&#34;)\ .select(df.employee_id, df.manager_id).dropDuplicates() new_nodes = step.join(result, [&#34;employee_id&#34;, &#34;manager_id&#34;], &#34;left_anti&#34;) if new_nodes.rdd.isEmpty(): break result = result.unionByName(new_nodes).dropDuplicates() frontier = new_nodes # result contains recursive closure # 99 GROUPING SETS -&gt; cube with filters cube_df = df.cube(&#34;department&#34;, &#34;job_title&#34;).agg(sum(&#34;salary&#34;).alias(&#34;sum_salary&#34;)) # Keep only grouping sets: (department), (job_title), () from pyspark.sql.functions import grouping_id cube_df.filter( (grouping_id(&#34;department&#34;, &#34;job_title&#34;) == 1) | # (department,NULL) (grouping_id(&#34;department&#34;, &#34;job_title&#34;) == 2) | # (NULL,job_title) (grouping_id(&#34;department&#34;, &#34;job_title&#34;) == 3) # (NULL,NULL) ).show() # 100 ROLLUP df.rollup(&#34;department&#34;, &#34;job_title&#34;).agg(sum(&#34;salary&#34;).alias(&#34;sum_salary&#34;)).show() ‚úÖ Notes Some SQL constructs (OFFSET, Recursive CTE) don‚Äôt map 1:1 to PySpark; patterns above show practical equivalents. Replace .show() with .write.format(&#34;delta&#34;)... to persist as needed.</description>
    </item>
  </channel>
</rss>