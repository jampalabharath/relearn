var relearn_searchindex = [
  {
    "breadcrumb": "ADB",
    "content": "What is Spark? Spark is an open source unified computing engine with a set of libraries for parallel data processing on a computer cluster.\nIt supports widely used programming languages such as:\nScala Python Java R It processes data in memory (RAM), which makes it 100 times faster than traditional Hadoop MapReduce.\nflowchart TD\rsubgraph DriverProgram[Driver Program]\rSS[SparkSession]\rSC[SparkContext]\rSS --\u003e SC\rend\rCM[Cluster Manager]\rsubgraph Worker1[Worker Node]\rsubgraph Executor1[Executor]\rT1[Task]\rT2[Task]\rC1[Cache]\rend\rend\rsubgraph Worker2[Worker Node]\rsubgraph Executor2[Executor]\rT3[Task]\rT4[Task]\rC2[Cache]\rend\rend\rSC --\u003e CM\rCM --\u003e Executor1\rCM --\u003e Executor2\rSC --\u003e Executor1\rSC --\u003e Executor2\rExecutor1 --\u003e T1\rExecutor1 --\u003e T2\rExecutor1 --\u003e C1\rExecutor2 --\u003e T3\rExecutor2 --\u003e T4\rExecutor2 --\u003e C2 Spark Components Following represents Spark components at a high level:\nLow Level API – RDD \u0026 Distributed Variables Structured API – DataFrames, Datasets, and SQL Libraries and Ecosystem – Structured Streaming and Advanced Analytics LAYER STRUCTURE Libraries \u0026 Ecosystem Structured API Low Level API How Spark Works 1. Drivers and Executors Driver: The brain of a Spark application. It translates your code into a logical execution plan and coordinates work. Executors: The workers. They run on cluster nodes, do the actual computation, and store data in memory/disk. Think of the Driver as a manager and Executors as employees doing the tasks.\n2. Jobs, Stages, and Tasks Job: Triggered when you call an action (like .collect() or .save()). A job = big unit of work. Stage: Spark splits the job into smaller parts based on shuffle boundaries (data movement points). Task: The smallest unit. Each stage is broken into many tasks, one per partition of data. 👉 Flow: Job → Stages → Tasks → Results\nflowchart LR\rJOB[JOB]\rSTAGE1[STAGE]\rSTAGE2[STAGE]\rTASK1[TASK]\rTASK2[TASK]\rTASK3[TASK]\rJOB --\u003e STAGE1\rJOB --\u003e STAGE2\rSTAGE1 --\u003e TASK1\rSTAGE1 --\u003e TASK2\rSTAGE2 --\u003e TASK3 What is Partition? To allow every executor to work in parallel, Spark breaks down the data into chunks called partitions.\nWhat is Transformation? The instruction or code to modify and transform data is known as Transformation.\nExamples: select,where,groupBy etc.\nTransformation helps in building the logical plan.\nTwo Types:\nNarrow Transformation Wide Transformation What are Actions? To trigger the execution we need to call an Action.\nThis basically executes the plan created by Transformation.\nActions are of three types:\nView data in console Collect data to native language Write data to output data sources Spark prefers Lazy Evaluation Transformations are lazy → Spark doesn’t execute them immediately; it just builds a logical plan (DAG). Execution happens only on actions → When an action (collect, count, save) is called, Spark optimizes the DAG and runs it.\nShuffle in Spark When: Happens during wide transformations (groupByKey, reduceByKey, join, etc.). What: Data is redistributed across the cluster so records with the same key end up in the same partition. Impact: A new stage is created in the DAG because shuffle requires data movement across executors. Shuffling occurs because wide transformations require related data (e.g., same keys) to be co-located in the same partition, which necessitates repartitioning and redistributing data across nodes.\nNote In Spark, actions (like count, collect, saveAsTextFile) create a job and trigger its execution, wide transformations (such as groupByKey, reduceByKey, join) introduce shuffle boundaries by redistributing data across partitions, and these boundaries split the job into stages, where tasks are scheduled and executed in parallel on cluster nodes.\nWhat is Spark Session? The Driver Process is known as Spark Session. It is the entry point for a Spark execution. The Spark Session instance executes the code in the cluster. The relation is one-to-one, i.e., for one Spark Application, there will be one Spark Session instance. Structured API - DataFrames DataFrame is the most common Structured API, represented like a table. The table is represented in form of Rows and Columns. DataFrame has schema, which is the metadata for the columns. Data in DataFrames are in partitions. DataFrames are immutable. Note DataFrames are immutable, meaning every transformation creates a new DataFrame without altering the original one.\nStructured API Execution Plan 1. Logial Planning The Spark Driver first converts your code (Transformations/Actions) into a logical plan. Represents what needs to be done without worrying about how it will be executed.\nflowchart LR\rA[Unresolved Logical Plan] --\u003e B[Resolved Logical Plan]\rB --\u003e C[Catalyst Optimizer]\rC --\u003e D[Optimized Logical Plan] 2. Physical Planning Spark converts the logical plan into a physical plan, deciding how to execute it (which operations run where, partitioning, joins, etc.). Optimizes for performance (e.g., choosing sort merge join vs broadcast join).\nflowchart LR\rA[Optimized Logical Plan] --\u003e B[Multiple Physical Plans]\rB --\u003e C[Cost Optimizer]\rC --\u003e D[Best Physical Plan]\rD --\u003e E[Sent to Cluster for Execution] 3. DAG (Directed Acyclic Graph) Spark breaks the physical plan into a DAG of stages. Shows dependencies between stages and ensures tasks are executed in the correct order.\nFlow: DAG → Stages → Tasks → Execution\nSummary Flow flowchart LR\rA[User Code] --\u003e B[Logical Plan] --\u003e C[Physical Plan] --\u003e D[DAG of Stages] --\u003e E[Tasks executed by Executors] Creating a Spark Session in PySpark A SparkSession is the entry point to using PySpark. It allows interaction with Spark’s functionalities (DataFrame, SQL, etc.).\nExample from pyspark.sql import SparkSession spark = ( SparkSession .builder .appName(\"Spark Introduction\") .master(\"local[*]\") .getOrCreate() ) SparkSession.builder: used to configure the session. .appName(\"...\"): sets the application name. .master(\"local[*]\"): runs Spark locally using all CPU cores. .getOrCreate(): returns an existing session or creates a new one. Note:In Databricks, you don’t need to create a SparkSession manually. A spark session is already available by default, so you can directly use spark.read, spark.sql, etc.\nCreating DataFrames in PySpark 1. From In-Memory Data You can create a DataFrame directly from Python objects (lists, tuples, dicts).\ndata = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)] columns = [\"Name\", \"Age\"] df = spark.createDataFrame(data, columns) df.show() 2. From File You can load data from CSV, JSON, Parquet, etc.\ndf = spark.read.csv(\"path/to/file.csv\", header=True, inferSchema=True) df.show() df = spark.read.json(\"path/to/file.json\") df.show() 01 Spark Session # Spark Session from pyspark.sql import SparkSession spark = ( SparkSession .builder .appName(\"Spark Introduction\") .master(\"local[*]\") .getOrCreate() ) spark # Emp Data \u0026 Schema emp_data = [ [\"001\",\"101\",\"John Doe\",\"30\",\"Male\",\"50000\",\"2015-01-01\"], [\"002\",\"101\",\"Jane Smith\",\"25\",\"Female\",\"45000\",\"2016-02-15\"], [\"003\",\"102\",\"Bob Brown\",\"35\",\"Male\",\"55000\",\"2014-05-01\"], [\"004\",\"102\",\"Alice Lee\",\"28\",\"Female\",\"48000\",\"2017-09-30\"], [\"005\",\"103\",\"Jack Chan\",\"40\",\"Male\",\"60000\",\"2013-04-01\"], [\"006\",\"103\",\"Jill Wong\",\"32\",\"Female\",\"52000\",\"2018-07-01\"], [\"007\",\"101\",\"James Johnson\",\"42\",\"Male\",\"70000\",\"2012-03-15\"], [\"008\",\"102\",\"Kate Kim\",\"29\",\"Female\",\"51000\",\"2019-10-01\"], [\"009\",\"103\",\"Tom Tan\",\"33\",\"Male\",\"58000\",\"2016-06-01\"], [\"010\",\"104\",\"Lisa Lee\",\"27\",\"Female\",\"47000\",\"2018-08-01\"], [\"011\",\"104\",\"David Park\",\"38\",\"Male\",\"65000\",\"2015-11-01\"], [\"012\",\"105\",\"Susan Chen\",\"31\",\"Female\",\"54000\",\"2017-02-15\"], [\"013\",\"106\",\"Brian Kim\",\"45\",\"Male\",\"75000\",\"2011-07-01\"], [\"014\",\"107\",\"Emily Lee\",\"26\",\"Female\",\"46000\",\"2019-01-01\"], [\"015\",\"106\",\"Michael Lee\",\"37\",\"Male\",\"63000\",\"2014-09-30\"], [\"016\",\"107\",\"Kelly Zhang\",\"30\",\"Female\",\"49000\",\"2018-04-01\"], [\"017\",\"105\",\"George Wang\",\"34\",\"Male\",\"57000\",\"2016-03-15\"], [\"018\",\"104\",\"Nancy Liu\",\"29\",\"Female\",\"50000\",\"2017-06-01\"], [\"019\",\"103\",\"Steven Chen\",\"36\",\"Male\",\"62000\",\"2015-08-01\"], [\"020\",\"102\",\"Grace Kim\",\"32\",\"Female\",\"53000\",\"2018-11-01\"] ] emp_schema = \"employee_id string, department_id string, name string, age string, gender string, salary string, hire_date string\" # Create emp DataFrame emp = spark.createDataFrame(data=emp_data, schema=emp_schema) # Check number of partitions emp.rdd.getNumPartitions() # Show data (ACTION) emp.show() # Write our first Transformation (EMP salary \u003e 50000) emp_final = emp.where(\"salary \u003e 50000\") # Validate number of Partitions emp_final.rdd.getNumPartitions() # Write data as CSV output (ACTION) emp_final.write.format(\"csv\").save(\"data/output/1/emp.csv\") 02 Basic Transformations 1 # Spark Session from pyspark.sql import SparkSession spark = ( SparkSession .builder .appName(\"Basic Transformation - I\") .master(\"local[*]\") .getOrCreate() ) spark # Emp Data \u0026 Schema emp_data = [ [\"001\",\"101\",\"John Doe\",\"30\",\"Male\",\"50000\",\"2015-01-01\"], [\"002\",\"101\",\"Jane Smith\",\"25\",\"Female\",\"45000\",\"2016-02-15\"], [\"003\",\"102\",\"Bob Brown\",\"35\",\"Male\",\"55000\",\"2014-05-01\"], [\"004\",\"102\",\"Alice Lee\",\"28\",\"Female\",\"48000\",\"2017-09-30\"], [\"005\",\"103\",\"Jack Chan\",\"40\",\"Male\",\"60000\",\"2013-04-01\"], [\"006\",\"103\",\"Jill Wong\",\"32\",\"Female\",\"52000\",\"2018-07-01\"], [\"007\",\"101\",\"James Johnson\",\"42\",\"Male\",\"70000\",\"2012-03-15\"], [\"008\",\"102\",\"Kate Kim\",\"29\",\"Female\",\"51000\",\"2019-10-01\"], [\"009\",\"103\",\"Tom Tan\",\"33\",\"Male\",\"58000\",\"2016-06-01\"], [\"010\",\"104\",\"Lisa Lee\",\"27\",\"Female\",\"47000\",\"2018-08-01\"], [\"011\",\"104\",\"David Park\",\"38\",\"Male\",\"65000\",\"2015-11-01\"], [\"012\",\"105\",\"Susan Chen\",\"31\",\"Female\",\"54000\",\"2017-02-15\"], [\"013\",\"106\",\"Brian Kim\",\"45\",\"Male\",\"75000\",\"2011-07-01\"], [\"014\",\"107\",\"Emily Lee\",\"26\",\"Female\",\"46000\",\"2019-01-01\"], [\"015\",\"106\",\"Michael Lee\",\"37\",\"Male\",\"63000\",\"2014-09-30\"], [\"016\",\"107\",\"Kelly Zhang\",\"30\",\"Female\",\"49000\",\"2018-04-01\"], [\"017\",\"105\",\"George Wang\",\"34\",\"Male\",\"57000\",\"2016-03-15\"], [\"018\",\"104\",\"Nancy Liu\",\"29\",\"Female\",\"50000\",\"2017-06-01\"], [\"019\",\"103\",\"Steven Chen\",\"36\",\"Male\",\"62000\",\"2015-08-01\"], [\"020\",\"102\",\"Grace Kim\",\"32\",\"Female\",\"53000\",\"2018-11-01\"] ] emp_schema = \"employee_id string, department_id string, name string, age string, gender string, salary string, hire_date string\" # Create emp DataFrame emp = spark.createDataFrame(data=emp_data, schema=emp_schema) # Show emp dataframe (ACTION) emp.show() # Schema for emp emp.schema # Small Example for Schema from pyspark.sql.types import StructType, StructField, StringType, IntegerType schema_string = \"name string, age int\" schema_spark = StructType([ StructField(\"name\", StringType(), True), StructField(\"age\", IntegerType(), True) ]) # Columns and expression from pyspark.sql.functions import col, expr emp[\"salary\"] # SELECT columns # select employee_id, name, age, salary from emp emp_filtered = emp.select(col(\"employee_id\"), expr(\"name\"), emp.age, emp.salary) # SHOW Dataframe (ACTION) emp_filtered.show() # Using expr for select # select employee_id as emp_id, name, cast(age as int) as age, salary from emp_filtered emp_casted = emp_filtered.select(expr(\"employee_id as emp_id\"), emp.name, expr(\"cast(age as int) as age\"), emp.salary) # SHOW Dataframe (ACTION) emp_casted.show() emp_casted_1 = emp_filtered.selectExpr(\"employee_id as emp_id\", \"name\", \"cast(age as int) as age\", \"salary\") emp_casted_1.show() emp_casted.printSchema() # Filter emp based on Age \u003e 30 # select emp_id, name, age, salary from emp_casted where age \u003e 30 emp_final = emp_casted.select(\"emp_id\", \"name\", \"age\", \"salary\").where(\"age \u003e 30\") # SHOW Dataframe (ACTION) emp_final.show() # Write the data back as CSV (ACTION) emp_final.write.format(\"csv\").save(\"data/output/2/emp.csv\") # Bonus TIP schema_str = \"name string, age int\" from pyspark.sql.types import _parse_datatype_string schema_spark = _parse_datatype_string(schema_str) schema_spark 03 Basic Transformations 2 # Spark Session from pyspark.sql import SparkSession spark = ( SparkSession .builder .appName(\"Basic Transformation - II\") .master(\"local[*]\") .getOrCreate() ) spark # Emp Data \u0026 Schema emp_data = [ [\"001\",\"101\",\"John Doe\",\"30\",\"Male\",\"50000\",\"2015-01-01\"], [\"002\",\"101\",\"Jane Smith\",\"25\",\"Female\",\"45000\",\"2016-02-15\"], [\"003\",\"102\",\"Bob Brown\",\"35\",\"Male\",\"55000\",\"2014-05-01\"], [\"004\",\"102\",\"Alice Lee\",\"28\",\"Female\",\"48000\",\"2017-09-30\"], [\"005\",\"103\",\"Jack Chan\",\"40\",\"Male\",\"60000\",\"2013-04-01\"], [\"006\",\"103\",\"Jill Wong\",\"32\",\"Female\",\"52000\",\"2018-07-01\"], [\"007\",\"101\",\"James Johnson\",\"42\",\"Male\",\"70000\",\"2012-03-15\"], [\"008\",\"102\",\"Kate Kim\",\"29\",\"Female\",\"51000\",\"2019-10-01\"], [\"009\",\"103\",\"Tom Tan\",\"33\",\"Male\",\"58000\",\"2016-06-01\"], [\"010\",\"104\",\"Lisa Lee\",\"27\",\"Female\",\"47000\",\"2018-08-01\"], [\"011\",\"104\",\"David Park\",\"38\",\"Male\",\"65000\",\"2015-11-01\"], [\"012\",\"105\",\"Susan Chen\",\"31\",\"Female\",\"54000\",\"2017-02-15\"], [\"013\",\"106\",\"Brian Kim\",\"45\",\"Male\",\"75000\",\"2011-07-01\"], [\"014\",\"107\",\"Emily Lee\",\"26\",\"Female\",\"46000\",\"2019-01-01\"], [\"015\",\"106\",\"Michael Lee\",\"37\",\"Male\",\"63000\",\"2014-09-30\"], [\"016\",\"107\",\"Kelly Zhang\",\"30\",\"Female\",\"49000\",\"2018-04-01\"], [\"017\",\"105\",\"George Wang\",\"34\",\"Male\",\"57000\",\"2016-03-15\"], [\"018\",\"104\",\"Nancy Liu\",\"29\",\"Female\",\"50000\",\"2017-06-01\"], [\"019\",\"103\",\"Steven Chen\",\"36\",\"Male\",\"62000\",\"2015-08-01\"], [\"020\",\"102\",\"Grace Kim\",\"32\",\"Female\",\"53000\",\"2018-11-01\"] ] emp_schema = \"employee_id string, department_id string, name string, age string, gender string, salary string, hire_date string\" # Create emp DataFrame emp = spark.createDataFrame(data=emp_data, schema=emp_schema) # Show emp dataframe (ACTION) emp.show() # Print Schema emp.printSchema() # Casting Column # select employee_id, name, age, cast(salary as double) as salary from emp from pyspark.sql.functions import col, cast emp_casted = emp.select(\"employee_id\", \"name\", \"age\", col(\"salary\").cast(\"double\")) emp_casted.printSchema() # Adding Columns # select employee_id, name, age, salary, (salary * 0.2) as tax from emp_casted emp_taxed = emp_casted.withColumn(\"tax\", col(\"salary\") * 0.2) emp_taxed.show() # Literals # select employee_id, name, age, salary, tax, 1 as columnOne, 'two' as columnTwo from emp_taxed from pyspark.sql.functions import lit emp_new_cols = emp_taxed.withColumn(\"columnOne\", lit(1)).withColumn(\"columnTwo\", lit('two')) emp_new_cols.show() # Renaming Columns # select employee_id as emp_id, name, age, salary, tax, columnOne, columnTwo from emp_new_cols emp_1 = emp_new_cols.withColumnRenamed(\"employee_id\", \"emp_id\") emp_1.show() # Column names with Spaces # select employee_id as emp_id, name, age, salary, tax, columnOne, columnTwo as `Column Two` from emp_new_cols emp_2 = emp_new_cols.withColumnRenamed(\"columnTwo\", \"Column Two\") emp_2.show() # Remove Column emp_dropped = emp_new_cols.drop(\"columnTwo\", \"columnOne\") emp_dropped.show() # Filter data # select employee_id as emp_id, name, age, salary, tax, columnOne from emp_col_dropped where tax \u003e 1000 emp_filtered = emp_dropped.where(\"tax \u003e 10000\") emp_filtered.show() # LIMIT data # select employee_id as emp_id, name, age, salary, tax, columnOne from emp_filtered limit 5 emp_limit = emp_filtered.limit(5) # Show data emp_limit.show(2) # Bonus TIP # Add multiple columns columns = { \"tax\" : col(\"salary\") * 0.2 , \"oneNumber\" : lit(1), \"columnTwo\" : lit(\"two\") } emp_final = emp.withColumns(columns) emp_final.show() 04 String And Dates # Spark Session from pyspark.sql import SparkSession spark = ( SparkSession .builder .appName(\"Working with Strings \u0026 Dates\") .master(\"local[*]\") .getOrCreate() ) spark # Emp Data \u0026 Schema emp_data = [ [\"001\",\"101\",\"John Doe\",\"30\",\"Male\",\"50000\",\"2015-01-01\"], [\"002\",\"101\",\"Jane Smith\",\"25\",\"Female\",\"45000\",\"2016-02-15\"], [\"003\",\"102\",\"Bob Brown\",\"35\",\"Male\",\"55000\",\"2014-05-01\"], [\"004\",\"102\",\"Alice Lee\",\"28\",\"Female\",\"48000\",\"2017-09-30\"], [\"005\",\"103\",\"Jack Chan\",\"40\",\"Male\",\"60000\",\"2013-04-01\"], [\"006\",\"103\",\"Jill Wong\",\"32\",\"Female\",\"52000\",\"2018-07-01\"], [\"007\",\"101\",\"James Johnson\",\"42\",\"Male\",\"70000\",\"2012-03-15\"], [\"008\",\"102\",\"Kate Kim\",\"29\",\"Female\",\"51000\",\"2019-10-01\"], [\"009\",\"103\",\"Tom Tan\",\"33\",\"Male\",\"58000\",\"2016-06-01\"], [\"010\",\"104\",\"Lisa Lee\",\"27\",\"Female\",\"47000\",\"2018-08-01\"], [\"011\",\"104\",\"David Park\",\"38\",\"Male\",\"65000\",\"2015-11-01\"], [\"012\",\"105\",\"Susan Chen\",\"31\",\"Female\",\"54000\",\"2017-02-15\"], [\"013\",\"106\",\"Brian Kim\",\"45\",\"Male\",\"75000\",\"2011-07-01\"], [\"014\",\"107\",\"Emily Lee\",\"26\",\"Female\",\"46000\",\"2019-01-01\"], [\"015\",\"106\",\"Michael Lee\",\"37\",\"Male\",\"63000\",\"2014-09-30\"], [\"016\",\"107\",\"Kelly Zhang\",\"30\",\"Female\",\"49000\",\"2018-04-01\"], [\"017\",\"105\",\"George Wang\",\"34\",\"Male\",\"57000\",\"2016-03-15\"], [\"018\",\"104\",\"Nancy Liu\",\"29\",\"\",\"50000\",\"2017-06-01\"], [\"019\",\"103\",\"Steven Chen\",\"36\",\"Male\",\"62000\",\"2015-08-01\"], [\"020\",\"102\",\"Grace Kim\",\"32\",\"Female\",\"53000\",\"2018-11-01\"] ] emp_schema = \"employee_id string, department_id string, name string, age string, gender string, salary string, hire_date string\" # Create emp DataFrame emp = spark.createDataFrame(data=emp_data, schema=emp_schema) # Show emp dataframe (ACTION) emp.show() # Print Schema emp.printSchema() # Case When # select employee_id, name, age, salary, gender, # case when gender = 'Male' then 'M' when gender = 'Female' then 'F' else null end as new_gender, hire_date from emp from pyspark.sql.functions import when, col, expr emp_gender_fixed = emp.withColumn(\"new_gender\", when(col(\"gender\") == 'Male', 'M') .when(col(\"gender\") == 'Female', 'F') .otherwise(None) ) emp_gender_fixed_1 = emp.withColumn(\"new_gender\", expr(\"case when gender = 'Male' then 'M' when gender = 'Female' then 'F' else null end\")) emp_gender_fixed_1.show() # Replace in Strings # select employee_id, name, replace(name, 'J', 'Z') as new_name, age, salary, gender, new_gender, hire_date from emp_gender_fixed from pyspark.sql.functions import regexp_replace emp_name_fixed = emp_gender_fixed.withColumn(\"new_name\", regexp_replace(col(\"name\"), \"J\", \"Z\")) emp_name_fixed.show() # Convert Date # select *, to_date(hire_date, 'YYYY-MM-DD') as hire_date from emp_name_fixed from pyspark.sql.functions import to_date emp_date_fix = emp_name_fixed.withColumn(\"hire_date\", to_date(col(\"hire_date\"), 'yyyy-MM-dd')) emp_date_fix.printSchema() # Add Date Columns # Add current_date, current_timestamp, extract year from hire_date from pyspark.sql.functions import current_date, current_timestamp emp_dated = emp_date_fix.withColumn(\"date_now\", current_date()).withColumn(\"timestamp_now\", current_timestamp()) emp_dated.show(truncate=False) # Drop Null gender records emp_1 = emp_dated.na.drop() emp_1.show() # Fix Null values # select *, nvl('new_gender', 'O') as new_gender from emp_dated from pyspark.sql.functions import coalesce, lit emp_null_df = emp_dated.withColumn(\"new_gender\", coalesce(col(\"new_gender\"), lit(\"O\"))) emp_null_df.show() # Drop old columns and Fix new column names emp_final = emp_null_df.drop(\"name\", \"gender\").withColumnRenamed(\"new_name\", \"name\").withColumnRenamed(\"new_gender\", \"gender\") emp_final.show() # Write data as CSV emp_final.write.format(\"csv\").save(\"data/output/4/emp.csv\") # Bonus TIP # Convert date into String and extract date information from pyspark.sql.functions import date_format emp_fixed = emp_final.withColumn(\"date_year\", date_format(col(\"timestamp_now\"), \"z\")) emp_fixed.show() 05 Sort Union Aggregation # Spark Session from pyspark.sql import SparkSession spark = ( SparkSession .builder .appName(\"Sort Union \u0026 Aggregation\") .master(\"local[*]\") .getOrCreate() ) spark # Emp Data \u0026 Schema emp_data_1 = [ [\"001\",\"101\",\"John Doe\",\"30\",\"Male\",\"50000\",\"2015-01-01\"], [\"002\",\"101\",\"Jane Smith\",\"25\",\"Female\",\"45000\",\"2016-02-15\"], [\"003\",\"102\",\"Bob Brown\",\"35\",\"Male\",\"55000\",\"2014-05-01\"], [\"004\",\"102\",\"Alice Lee\",\"28\",\"Female\",\"48000\",\"2017-09-30\"], [\"005\",\"103\",\"Jack Chan\",\"40\",\"Male\",\"60000\",\"2013-04-01\"], [\"006\",\"103\",\"Jill Wong\",\"32\",\"Female\",\"52000\",\"2018-07-01\"], [\"007\",\"101\",\"James Johnson\",\"42\",\"Male\",\"70000\",\"2012-03-15\"], [\"008\",\"102\",\"Kate Kim\",\"29\",\"Female\",\"51000\",\"2019-10-01\"], [\"009\",\"103\",\"Tom Tan\",\"33\",\"Male\",\"58000\",\"2016-06-01\"], [\"010\",\"104\",\"Lisa Lee\",\"27\",\"Female\",\"47000\",\"2018-08-01\"] ] emp_data_2 = [ [\"011\",\"104\",\"David Park\",\"38\",\"Male\",\"65000\",\"2015-11-01\"], [\"012\",\"105\",\"Susan Chen\",\"31\",\"Female\",\"54000\",\"2017-02-15\"], [\"013\",\"106\",\"Brian Kim\",\"45\",\"Male\",\"75000\",\"2011-07-01\"], [\"014\",\"107\",\"Emily Lee\",\"26\",\"Female\",\"46000\",\"2019-01-01\"], [\"015\",\"106\",\"Michael Lee\",\"37\",\"Male\",\"63000\",\"2014-09-30\"], [\"016\",\"107\",\"Kelly Zhang\",\"30\",\"Female\",\"49000\",\"2018-04-01\"], [\"017\",\"105\",\"George Wang\",\"34\",\"Male\",\"57000\",\"2016-03-15\"], [\"018\",\"104\",\"Nancy Liu\",\"29\",\"\",\"50000\",\"2017-06-01\"], [\"019\",\"103\",\"Steven Chen\",\"36\",\"Male\",\"62000\",\"2015-08-01\"], [\"020\",\"102\",\"Grace Kim\",\"32\",\"Female\",\"53000\",\"2018-11-01\"] ] emp_schema = \"employee_id string, department_id string, name string, age string, gender string, salary string, hire_date string\" # Create emp DataFrame emp_data_1 = spark.createDataFrame(data=emp_data_1, schema=emp_schema) emp_data_2 = spark.createDataFrame(data=emp_data_2, schema=emp_schema) # Show emp dataframe (ACTION) emp_data_1.show() emp_data_2.show() # Print Schema emp_data_1.printSchema() emp_data_2.printSchema() # UNION and UNION ALL # select * from emp_data_1 UNION select * from emp_data_2 emp = emp_data_1.unionAll(emp_data_2) emp.show() # Sort the emp data based on desc Salary # select * from emp order by salary desc from pyspark.sql.functions import desc, asc, col emp_sorted = emp.orderBy(col(\"salary\").asc()) emp_sorted.show() # Aggregation # select dept_id, count(employee_id) as total_dept_count from emp_sorted group by dept_id from pyspark.sql.functions import count emp_count = emp_sorted.groupBy(\"department_id\").agg(count(\"employee_id\").alias(\"total_dept_count\")) emp_count.show() # Aggregation # select dept_id, sum(salary) as total_dept_salary from emp_sorted group by dept_id from pyspark.sql.functions import sum emp_sum = emp_sorted.groupBy(\"department_id\").agg(sum(\"salary\").alias(\"total_dept_salary\")) emp_sum.show() # Aggregation with having clause # select dept_id, avg(salary) as avg_dept_salary from emp_sorted group by dept_id having avg(salary) \u003e 50000 from pyspark.sql.functions import avg emp_avg = emp_sorted.groupBy(\"department_id\").agg(avg(\"salary\").alias(\"avg_dept_salary\")).where(\"avg_dept_salary \u003e 50000\") emp_avg.show() # Bonus TIP - unionByName # In case the column sequence is different emp_data_2_other = emp_data_2.select(\"employee_id\", \"salary\", \"department_id\", \"name\", \"hire_date\", \"gender\", \"age\") emp_data_1.printSchema() emp_data_2_other.printSchema() emp_fixed = emp_data_1.unionByName(emp_data_2_other) emp_fixed.show() emp.count() 06 Unique Data And Window # Spark Session from pyspark.sql import SparkSession spark = ( SparkSession .builder .appName(\"Unique data \u0026 Window Functions\") .master(\"local[*]\") .getOrCreate() ) spark # Emp Data \u0026 Schema emp_data = [ [\"001\",\"101\",\"John Doe\",\"30\",\"Male\",\"50000\",\"2015-01-01\"], [\"002\",\"101\",\"Jane Smith\",\"25\",\"Female\",\"45000\",\"2016-02-15\"], [\"003\",\"102\",\"Bob Brown\",\"35\",\"Male\",\"55000\",\"2014-05-01\"], [\"004\",\"102\",\"Alice Lee\",\"28\",\"Female\",\"48000\",\"2017-09-30\"], [\"005\",\"103\",\"Jack Chan\",\"40\",\"Male\",\"60000\",\"2013-04-01\"], [\"006\",\"103\",\"Jill Wong\",\"32\",\"Female\",\"52000\",\"2018-07-01\"], [\"007\",\"101\",\"James Johnson\",\"42\",\"Male\",\"70000\",\"2012-03-15\"], [\"008\",\"102\",\"Kate Kim\",\"29\",\"Female\",\"51000\",\"2019-10-01\"], [\"009\",\"103\",\"Tom Tan\",\"33\",\"Male\",\"58000\",\"2016-06-01\"], [\"010\",\"104\",\"Lisa Lee\",\"27\",\"Female\",\"47000\",\"2018-08-01\"], [\"011\",\"104\",\"David Park\",\"38\",\"Male\",\"65000\",\"2015-11-01\"], [\"012\",\"105\",\"Susan Chen\",\"31\",\"Female\",\"54000\",\"2017-02-15\"], [\"013\",\"106\",\"Brian Kim\",\"45\",\"Male\",\"75000\",\"2011-07-01\"], [\"014\",\"107\",\"Emily Lee\",\"26\",\"Female\",\"46000\",\"2019-01-01\"], [\"015\",\"106\",\"Michael Lee\",\"37\",\"Male\",\"63000\",\"2014-09-30\"], [\"016\",\"107\",\"Kelly Zhang\",\"30\",\"Female\",\"49000\",\"2018-04-01\"], [\"017\",\"105\",\"George Wang\",\"34\",\"Male\",\"57000\",\"2016-03-15\"], [\"018\",\"104\",\"Nancy Liu\",\"29\",\"\",\"50000\",\"2017-06-01\"], [\"019\",\"103\",\"Steven Chen\",\"36\",\"Male\",\"62000\",\"2015-08-01\"], [\"020\",\"102\",\"Grace Kim\",\"32\",\"Female\",\"53000\",\"2018-11-01\"] ] emp_schema = \"employee_id string, department_id string, name string, age string, gender string, salary string, hire_date string\" # Create emp DataFrame emp = spark.createDataFrame(data=emp_data, schema=emp_schema) # Show emp dataframe (ACTION) emp.show() # Print Schema emp.printSchema() # Get unique data # select distinct emp.* from emp emp_unique = emp.distinct() emp_unique.show() # Unique of department_ids # select distinct department_id from emp emp_dept_id = emp.select(\"department_id\").distinct() emp_dept_id.show() # Window Functions # select *, max(salary) over(partition by department_id order by salary desc) as max_salary from emp_unique from pyspark.sql.window import Window from pyspark.sql.functions import max, col, desc window_spec = Window.partitionBy(col(\"department_id\")).orderBy(col(\"salary\").desc()) max_func = max(col(\"salary\")).over(window_spec) emp_1 = emp.withColumn(\"max_salary\", max_func) emp_1.show() # Window Functions - 2nd highest salary of each department # select *, row_number() over(partition by department_id order by salary desc) as rn from emp_unique where rn = 2 from pyspark.sql.window import Window from pyspark.sql.functions import row_number, desc, col window_spec = Window.partitionBy(col(\"department_id\")).orderBy(col(\"salary\").desc()) rn = row_number().over(window_spec) emp_2 = emp.withColumn(\"rn\", rn).where(\"rn = 2\") emp_2.show() # Window function using expr # select *, row_number() over(partition by department_id order by salary desc) as rn from emp_unique where rn = 2 from pyspark.sql.functions import expr emp_3 = emp.withColumn(\"rn\", expr(\"row_number() over(partition by department_id order by salary desc)\")).where(\"rn = 2\") emp_3.show() # Bonus TIP # Databricks community cloud 07 Joins And Data Partitions # Spark Session from pyspark.sql import SparkSession spark = ( SparkSession .builder .appName(\"Joins and Data Partitions\") .master(\"local[*]\") .getOrCreate() ) spark # Emp Data \u0026 Schema emp_data = [ [\"001\",\"101\",\"John Doe\",\"30\",\"Male\",\"50000\",\"2015-01-01\"], [\"002\",\"101\",\"Jane Smith\",\"25\",\"Female\",\"45000\",\"2016-02-15\"], [\"003\",\"102\",\"Bob Brown\",\"35\",\"Male\",\"55000\",\"2014-05-01\"], [\"004\",\"102\",\"Alice Lee\",\"28\",\"Female\",\"48000\",\"2017-09-30\"], [\"005\",\"103\",\"Jack Chan\",\"40\",\"Male\",\"60000\",\"2013-04-01\"], [\"006\",\"103\",\"Jill Wong\",\"32\",\"Female\",\"52000\",\"2018-07-01\"], [\"007\",\"101\",\"James Johnson\",\"42\",\"Male\",\"70000\",\"2012-03-15\"], [\"008\",\"102\",\"Kate Kim\",\"29\",\"Female\",\"51000\",\"2019-10-01\"], [\"009\",\"103\",\"Tom Tan\",\"33\",\"Male\",\"58000\",\"2016-06-01\"], [\"010\",\"104\",\"Lisa Lee\",\"27\",\"Female\",\"47000\",\"2018-08-01\"], [\"011\",\"104\",\"David Park\",\"38\",\"Male\",\"65000\",\"2015-11-01\"], [\"012\",\"105\",\"Susan Chen\",\"31\",\"Female\",\"54000\",\"2017-02-15\"], [\"013\",\"106\",\"Brian Kim\",\"45\",\"Male\",\"75000\",\"2011-07-01\"], [\"014\",\"107\",\"Emily Lee\",\"26\",\"Female\",\"46000\",\"2019-01-01\"], [\"015\",\"106\",\"Michael Lee\",\"37\",\"Male\",\"63000\",\"2014-09-30\"], [\"016\",\"107\",\"Kelly Zhang\",\"30\",\"Female\",\"49000\",\"2018-04-01\"], [\"017\",\"105\",\"George Wang\",\"34\",\"Male\",\"57000\",\"2016-03-15\"], [\"018\",\"104\",\"Nancy Liu\",\"29\",\"\",\"50000\",\"2017-06-01\"], [\"019\",\"103\",\"Steven Chen\",\"36\",\"Male\",\"62000\",\"2015-08-01\"], [\"020\",\"102\",\"Grace Kim\",\"32\",\"Female\",\"53000\",\"2018-11-01\"] ] emp_schema = \"employee_id string, department_id string, name string, age string, gender string, salary string, hire_date string\" dept_data = [ [\"101\", \"Sales\", \"NYC\", \"US\", \"1000000\"], [\"102\", \"Marketing\", \"LA\", \"US\", \"900000\"], [\"103\", \"Finance\", \"London\", \"UK\", \"1200000\"], [\"104\", \"Engineering\", \"Beijing\", \"China\", \"1500000\"], [\"105\", \"Human Resources\", \"Tokyo\", \"Japan\", \"800000\"], [\"106\", \"Research and Development\", \"Perth\", \"Australia\", \"1100000\"], [\"107\", \"Customer Service\", \"Sydney\", \"Australia\", \"950000\"] ] dept_schema = \"department_id string, department_name string, city string, country string, budget string\" # Create emp \u0026 dept DataFrame emp = spark.createDataFrame(data=emp_data, schema=emp_schema) dept = spark.createDataFrame(data=dept_data, schema=dept_schema) # Show emp dataframe (ACTION) emp.show() dept.show() # Print Schema emp.printSchema() dept.printSchema() # Get number of partitions for emp emp.rdd.getNumPartitions() # Get number of partitions for dept dept.rdd.getNumPartitions() # Repartition of data using repartition \u0026 coalesce emp_partitioned = emp.repartition(4, \"department_id\") emp_partitioned.rdd.getNumPartitions() # Find the partition info for partitions and reparition from pyspark.sql.functions import spark_partition_id emp_1 = emp.repartition(4, \"department_id\").withColumn(\"partition_num\", spark_partition_id()) emp_1.show() # INNER JOIN datasets # select e.emp_name, d.department_name, d.department_id, e.salary # from emp e inner join dept d on emp.department_id = dept.department_id df_joined = emp.alias(\"e\").join(dept.alias(\"d\"), how=\"inner\", on=emp.department_id==dept.department_id) df_joined.select(\"e.name\", \"d.department_id\", \"d.department_name\", \"e.salary\").show() # LEFT OUTER JOIN datasets # select e.emp_name, d.department_name, d.department_id, e.salary # from emp e left outer join dept d on emp.department_id = dept.department_id df_joined = emp.alias(\"e\").join(dept.alias(\"d\"), how=\"left_outer\", on=emp.department_id==dept.department_id) df_joined.select(\"e.name\", \"d.department_name\", \"d.department_id\", \"e.salary\").show() # Write the final dataset df_joined.select(\"e.name\", \"d.department_name\", \"d.department_id\",\"e.salary\").write.format(\"csv\").save(\"data/output/7/emp_joined.csv\") # Bonus TIP # Joins with cascading conditions # Join with Department_id and only for departments 101 or 102 # Join with not null/null conditions df_final = emp.join(dept, how=\"left_outer\", on=(emp.department_id==dept.department_id) \u0026 ((emp.department_id == \"101\") | (emp.department_id == \"102\")) \u0026 (emp.salary.isNull()) ) df_final.show() 08 Reading From Csv Files # Spark Session from pyspark.sql import SparkSession spark = ( SparkSession .builder .appName(\"Reading from CSV Files\") .master(\"local[*]\") .getOrCreate() ) spark # Read a csv file into dataframe df = spark.read.format(\"csv\").option(\"header\", True).option(\"inferSchema\", True).load(\"data/input/emp.csv\") df.printSchema() df.show() # Reading with Schema _schema = \"employee_id int, department_id int, name string, age int, gender string, salary double, hire_date date\" df_schema = spark.read.format(\"csv\").option(\"header\",True).schema(_schema).load(\"data/input/emp.csv\") df_schema.show() # Handle BAD records - PERMISSIVE (Default mode) _schema = \"employee_id int, department_id int, name string, age int, gender string, salary double, hire_date date, bad_record string\" df_p = spark.read.format(\"csv\").schema(_schema).option(\"columnNameOfCorruptRecord\", \"bad_record\").option(\"header\", True).load(\"data/input/emp_new.csv\") df_p.printSchema() df_p.show() # Handle BAD records - DROPMALFORMED _schema = \"employee_id int, department_id int, name string, age int, gender string, salary double, hire_date date\" df_m = spark.read.format(\"csv\").option(\"header\", True).option(\"mode\", \"DROPMALFORMED\").schema(_schema).load(\"data/input/emp_new.csv\") df_m.printSchema() df_m.show() # Handle BAD records - FAILFAST _schema = \"employee_id int, department_id int, name string, age int, gender string, salary double, hire_date date\" df_m = spark.read.format(\"csv\").option(\"header\", True).option(\"mode\", \"FAILFAST\").schema(_schema).load(\"data/input/emp_new.csv\") df_m.printSchema() df_m.show() # BONUS TIP # Multiple options _options = { \"header\" : \"true\", \"inferSchema\" : \"true\", \"mode\" : \"PERMISSIVE\" } df = (spark.read.format(\"csv\").options(**_options).load(\"data/input/emp.csv\")) df.show() spark.stop() 09 Reading Complex Data Formats # Spark Session from pyspark.sql import SparkSession spark = ( SparkSession .builder .appName(\"Reading Complex Data Formats\") .master(\"local[*]\") .getOrCreate() ) spark # Read Parquet Sales data df_parquet = spark.read.format(\"parquet\").load(\"data/input/sales_total_parquet/*.parquet\") df_parquet.printSchema() df_parquet.show() # Read ORC Sales data df_orc = spark.read.format(\"orc\").load(\"data/input/sales_total_orc/*.orc\") df_orc.printSchema() df_orc.show() # Benefits of Columnar Storage # Lets create a simple Python decorator - {get_time} to get the execution timings # If you dont know about Python decorators - check out : https://www.geeksforgeeks.org/decorators-in-python/ import time def get_time(func): def inner_get_time() -\u003e str: start_time = time.time() func() end_time = time.time() return (f\"Execution time: {(end_time - start_time)*1000} ms\") print(inner_get_time()) @get_time def x(): df = spark.read.format(\"parquet\").load(\"data/input/sales_data.parquet\") df.count() @get_time def x(): df = spark.read.format(\"parquet\").load(\"data/input/sales_data.parquet\") df.select(\"trx_id\").count() # BONUS TIP # RECURSIVE READ sales_recursive |__ sales_1\\1.parquet |__ sales_1\\sales_2\\2.parquet df_1 = spark.read.format(\"parquet\").load(\"data/input/sales_recursive/sales_1/1.parquet\") df_1.show() df_1 = spark.read.format(\"parquet\").load(\"data/input/sales_recursive/sales_1/sales_2/2.parquet\") df_1.show() df_1 = spark.read.format(\"parquet\").option(\"recursiveFileLookup\", True).load(\"data/input/sales_recursive/\") df_1.show() 10 Read Json Files # Spark Session from pyspark.sql import SparkSession spark = ( SparkSession .builder .appName(\"Reading and Parsing JSON Files/Data\") .master(\"local[*]\") .getOrCreate() ) spark # Read Single line JSON file df_single = spark.read.format(\"json\").load(\"data/input/order_singleline.json\") df_single.printSchema() df_single.show() # Read Multiline JSON file df_multi = spark.read.format(\"json\").option(\"multiLine\", True).load(\"data/input/order_multiline.json\") df_multi.printSchema() df_multi.show() df = spark.read.format(\"text\").load(\"data/input/order_singleline.json\") df.printSchema() df.show(truncate=False) # With Schema _schema = \"customer_id string, order_id string, contact array\u003clong\u003e\" df_schema = spark.read.format(\"json\").schema(_schema).load(\"data/input/order_singleline.json\") df_schema.show() root |-- contact: array (nullable = true) | |-- element: long (containsNull = true) |-- customer_id: string (nullable = true) |-- order_id: string (nullable = true) |-- order_line_items: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- amount: double (nullable = true) | | |-- item_id: string (nullable = true) | | |-- qty: long (nullable = true) _schema = \"contact array\u003cstring\u003e, customer_id string, order_id string, order_line_items array\u003cstruct\u003camount double, item_id string, qty long\u003e\u003e\" df_schema_new = spark.read.format(\"json\").schema(_schema).load(\"data/input/order_singleline.json\") df_schema_new.printSchema() df_schema_new.show() # Function from_json to read from a column _schema = \"contact array\u003cstring\u003e, customer_id string, order_id string, order_line_items array\u003cstruct\u003camount double, item_id string, qty long\u003e\u003e\" from pyspark.sql.functions import from_json df_expanded = df.withColumn(\"parsed\", from_json(df.value, _schema)) df_expanded.printSchema() df_expanded.show() # Function to_json to parse a JSON string from pyspark.sql.functions import to_json df_unparsed = df_expanded.withColumn(\"unparsed\", to_json(df_expanded.parsed)) df_unparsed.printSchema() df_unparsed.select(\"unparsed\").show(truncate=False) # Get values from Parsed JSON df_1 = df_expanded.select(\"parsed.*\") from pyspark.sql.functions import explode df_2 = df_1.withColumn(\"expanded_line_items\", explode(\"order_line_items\")) df_2.show() df_3 = df_2.select(\"contact\", \"customer_id\", \"order_id\", \"expanded_line_items.*\") df_3.show() # Explode Array fields df_final = df_3.withColumn(\"contact_expanded\", explode(\"contact\")) df_final.printSchema() df_final.drop(\"contact\").show() 11 Writing Data # Spark Session from pyspark.sql import SparkSession spark = ( SparkSession .builder .appName(\"Writing data\") .master(\"local[*]\") .getOrCreate() ) spark # Spark available cores with defaultParallism in Spark UI spark.sparkContext.defaultParallelism # Emp Data \u0026 Schema emp_data = [ [\"001\",\"101\",\"John Doe\",\"30\",\"Male\",\"50000\",\"2015-01-01\"], [\"002\",\"101\",\"Jane Smith\",\"25\",\"Female\",\"45000\",\"2016-02-15\"], [\"003\",\"102\",\"Bob Brown\",\"35\",\"Male\",\"55000\",\"2014-05-01\"], [\"004\",\"102\",\"Alice Lee\",\"28\",\"Female\",\"48000\",\"2017-09-30\"], [\"005\",\"103\",\"Jack Chan\",\"40\",\"Male\",\"60000\",\"2013-04-01\"], [\"006\",\"103\",\"Jill Wong\",\"32\",\"Female\",\"52000\",\"2018-07-01\"], [\"007\",\"101\",\"James Johnson\",\"42\",\"Male\",\"70000\",\"2012-03-15\"], [\"008\",\"102\",\"Kate Kim\",\"29\",\"Female\",\"51000\",\"2019-10-01\"], [\"009\",\"103\",\"Tom Tan\",\"33\",\"Male\",\"58000\",\"2016-06-01\"], [\"010\",\"104\",\"Lisa Lee\",\"27\",\"Female\",\"47000\",\"2018-08-01\"], [\"011\",\"104\",\"David Park\",\"38\",\"Male\",\"65000\",\"2015-11-01\"], [\"012\",\"105\",\"Susan Chen\",\"31\",\"Female\",\"54000\",\"2017-02-15\"], [\"013\",\"106\",\"Brian Kim\",\"45\",\"Male\",\"75000\",\"2011-07-01\"], [\"014\",\"107\",\"Emily Lee\",\"26\",\"Female\",\"46000\",\"2019-01-01\"], [\"015\",\"106\",\"Michael Lee\",\"37\",\"Male\",\"63000\",\"2014-09-30\"], [\"016\",\"107\",\"Kelly Zhang\",\"30\",\"Female\",\"49000\",\"2018-04-01\"], [\"017\",\"105\",\"George Wang\",\"34\",\"Male\",\"57000\",\"2016-03-15\"], [\"018\",\"104\",\"Nancy Liu\",\"29\",\"Female\",\"50000\",\"2017-06-01\"], [\"019\",\"103\",\"Steven Chen\",\"36\",\"Male\",\"62000\",\"2015-08-01\"], [\"020\",\"102\",\"Grace Kim\",\"32\",\"Female\",\"53000\",\"2018-11-01\"] ] emp_schema = \"employee_id string, department_id string, name string, age string, gender string, salary string, hire_date string\" # Create emp DataFrame emp = spark.createDataFrame(data=emp_data, schema=emp_schema) # Get number of partitions and show data emp.rdd.getNumPartitions() emp.show() # Write the data in parquet format emp.write.format(\"parquet\").save(\"data/output/11/2/emp.parquet\") # View data partition information from pyspark.sql.functions import spark_partition_id emp.withColumn(\"partition_id\", spark_partition_id()).show() emp.write.format(\"csv\").option(\"header\", True).save(\"data/output/11/3/emp.csv\") # Write the data with Partition to output location emp.write.format(\"csv\").partitionBy(\"department_id\").option(\"header\", True).save(\"data/output/11/4/emp.csv\") # Write Modes - append, overwrite, ignore and error emp.write.format(\"csv\").mode(\"error\").option(\"header\", True).save(\"data/output/11/3/emp.csv\") # Bonus TIP # What if we need to write only 1 output file to share with DownStream? emp.repartition(1).write.format(\"csv\").option(\"header\", True).save(\"data/output/11/5/emp.csv\") 12 Understand Cluster To setup PySpark Cluster with 2 worker, 1 master and 1 history server Follow the below instructions carefully 👇🏻\nClone or Download the docker images repository from https://github.com/subhamkharwal/docker-images Open CMD prompt (on Windows) or Terminal (on Mac) and move the cloned/downloaded folder Change to folder pyspark-cluster-with-jupyter and run command docker compose up The above command would setup a group of containers with 1 Jupyter Lab, 1 Master node, 2 worker nodes and 1 history server Run all the containers, go into the logs of Jupyter Lab container in Docker Desktop and copy the token from the URL which looks like http://127.0.0.1:8888/lab?token=c23436751add815d6fce10071c3958aac7b4f8ebbcf05255 Open Jupyter Lab on url https://localhost:8888, paste the token and setup a new password. [IMPORTANT] Make sure to place your file to read or write your files to location /data/\u003cyour path\u003e in order to work with cluster. (/data is important, this path is mounted across the cluster to access files or data) To see all your data files after execution, run the below command %%sh ls -ltr /data/ # Spark Session from pyspark.sql import SparkSession spark = ( SparkSession .builder .appName(\"Cluster Execution\") .master(\"spark://17e348267994:7077\") .config(\"spark.executor.instances\", 4) .config(\"spark.executor.cores\", 4) .config(\"spark.executor.memory\", \"512M\") .getOrCreate() ) spark # Create a sample data frame df = spark.range(10) # Write the data of the data frame df.write.format(\"csv\").option(\"header\", True).save(\"/data/output/15/3/range.csv\") # Stop Spark Settion spark.stop() Understand Cluster (Python) ''' ##### To setup PySpark Cluster with 2 worker, 1 master and 1 history server Follow the below instructions carefully 👇🏻 - Clone or Download the docker images repository from `https://github.com/subhamkharwal/docker-images` - Open CMD prompt (on Windows) or Terminal (on Mac) and move the cloned/downloaded folder - Change to folder `pyspark-cluster-with-jupyter` and run command `docker compose up` - The above command would setup a group of containers with 1 Jupyter Lab, 1 Master node, 2 worker nodes and 1 history server - Run all the containers, go into the logs of Jupyter Lab container in Docker Desktop and copy the token from the URL which looks like `http://127.0.0.1:8888/lab?token=c23436751add815d6fce10071c3958aac7b4f8ebbcf05255` - Open Jupyter Lab on url `https://localhost:8888`, paste the token and setup a new password. - [IMPORTANT] Make sure to place your file to read or write your files to location `/data/\u003cyour path\u003e` in order to work with cluster. (/data is important, this path is mounted across the cluster to access files or data) - To see all your data files after execution, run the below command ```shell %%sh ls -ltr /data/ ’''\nSpark Session from pyspark.sql import SparkSession\nspark = ( SparkSession .builder .appName(“Cluster Execution”) .getOrCreate() )\ndf = spark.range(10)\ndf.write.format(“csv”).option(“header”, True).save(\"/data/output/15/6/range.csv\")\n## 13 User Defined Functions\r```python\r# Spark Session\rfrom pyspark.sql import SparkSession\rspark = (\rSparkSession\r.builder\r.appName(\"User Defined Functions\")\r.master(\"spark://17e348267994:7077\")\r.config(\"spark.executor.cores\", 2)\r.config(\"spark.cores.max\", 6)\r.config(\"spark.executor.memory\", \"512M\")\r.getOrCreate()\r)\rspark # Read employee data emp_schema = \"employee_id string, department_id string, name string, age string, gender string, salary string, hire_date string\" emp = spark.read.format(\"csv\").option(\"header\", True).schema(emp_schema).load(\"/data/output/3/emp.csv\") emp.rdd.getNumPartitions() # Create a function to generate 10% of Salary as Bonus def bonus(salary): return int(salary) * 0.1 # Register as UDF from pyspark.sql.functions import udf bonus_udf = udf(bonus) spark.udf.register(\"bonus_sql_udf\", bonus, \"double\") # Create new column as bonus using UDF from pyspark.sql.functions import expr emp.withColumn(\"bonus\", expr(\"bonus_sql_udf(salary)\")).show() # Create new column as bonus without UDF emp.withColumn(\"bonus\", expr(\"salary * 0.1\")).show() # Stop Spark Session spark.stop() 14 Understand Dag Plan # Spark Session from pyspark.sql import SparkSession spark = ( SparkSession .builder .appName(\"Understand Plans and DAG\") .master(\"local[*]\") .getOrCreate() ) spark # Disable AQE and Broadcast join spark.conf.set(\"spark.sql.adaptive.enabled\", False) spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False) spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1) # Check default Parallism spark.sparkContext.defaultParallelism # Create dataframes df_1 = spark.range(4, 200, 2) df_2 = spark.range(2, 200, 4) df_2.rdd.getNumPartitions() # Re-partition data df_3 = df_1.repartition(5) df_4 = df_2.repartition(7) df_4.rdd.getNumPartitions() # Join the dataframes df_joined = df_3.join(df_4, on=\"id\") # Get the sum of ids df_sum = df_joined.selectExpr(\"sum(id) as total_sum\") # View data df_sum.show() # Explain plan df_sum.explain() # Union the data again to see the skipped stages df_union = df_sum.union(df_4) df_union.show() # Explain plan df_union.explain() # DataFrame to RDD df_1.rdd 15 Optimizing Shuffles # Spark Session from pyspark.sql import SparkSession spark = ( SparkSession .builder .appName(\"Optimizing Shuffles\") .master(\"spark://17e348267994:7077\") .config(\"spark.cores.max\", 16) .config(\"spark.executor.cores\", 4) .config(\"spark.executor.memory\", \"512M\") .getOrCreate() ) spark # Check Spark defaultParallelism spark.sparkContext.defaultParallelism # Disable AQE and Broadcast join spark.conf.set(\"spark.sql.adaptive.enabled\", False) spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False) spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1) # Read EMP CSV file with 10M records _schema = \"first_name string, last_name string, job_title string, dob string, email string, phone string, salary double, department_id int\" emp = spark.read.format(\"csv\").schema(_schema).option(\"header\", True).load(\"/data/input/datasets/employee_records.csv\") # Find out avg salary as per dept from pyspark.sql.functions import avg emp_avg = emp.groupBy(\"department_id\").agg(avg(\"salary\").alias(\"avg_sal\")) # Write data for performance Benchmarking emp_avg.write.format(\"noop\").mode(\"overwrite\").save() # Check Spark Shuffle Partition setting spark.conf.get(\"spark.sql.shuffle.partitions\") spark.conf.set(\"spark.sql.shuffle.partitions\", 16) from pyspark.sql.functions import spark_partition_id emp.withColumn(\"partition_id\", spark_partition_id()).where(\"partition_id = 0\").show() # Read the partitioned data emp_part = spark.read.format(\"csv\").schema(_schema).option(\"header\", True).load(\"/data/input/emp_partitioned.csv/\") emp_avg = emp_part.groupBy(\"department_id\").agg(avg(\"salary\").alias(\"avg_sal\")) emp_avg.write.format(\"noop\").mode(\"overwrite\").save() 16 Spark Caching Techiniques # Spark Session from pyspark.sql import SparkSession spark = ( SparkSession .builder .appName(\"Understand Caching\") .master(\"local[*]\") .config(\"spark.executor.memory\", \"512M\") .getOrCreate() ) spark # Read Sales CSV Data - 752MB Size ~ 7.2M Records _schema = \"transacted_at string, trx_id string, retailer_id string, description string, amount double, city_id string\" df = spark.read.format(\"csv\").schema(_schema).option(\"header\", True).load(\"data/input/new_sales.csv\") df.where(\"amount \u003e 300\").show() # Cache DataFrame (cache or persist) df_cache = df.where(\"amount \u003e 100\").cache() df_cache.count() df.where(\"amount \u003e 50\").show() # MEMORY_ONLY, MEMORY_AND_DISK, MEMORY_ONLY_SER, MEMORY_AND_DISK_SER, DISK_ONLY, MEMORY_ONLY_2, MEMORY_AND_DISK_2 import pyspark df_persist = df.persist(pyspark.StorageLevel.MEMORY_ONLY_2) df_persist.write.format(\"noop\").mode(\"overwrite\").save() # Remove Cache spark.catalog.clearCache() 17 Distributed Shared Variables # Spark Session from pyspark.sql import SparkSession spark = ( SparkSession .builder .appName(\"Distributed Shared Variables\") .master(\"spark://17e348267994:7077\") .config(\"spark.cores.max\", 16) .config(\"spark.executor.cores\", 4) .config(\"spark.executor.memory\", \"512M\") .getOrCreate() ) spark # Read EMP CSV data _schema = \"first_name string, last_name string, job_title string, dob string, email string, phone string, salary double, department_id int\" emp = spark.read.format(\"csv\").schema(_schema).option(\"header\", True).load(\"/data/input/datasets/employee_records.csv\") # Variable (Lookup) dept_names = {1 : 'Department 1', 2 : 'Department 2', 3 : 'Department 3', 4 : 'Department 4', 5 : 'Department 5', 6 : 'Department 6', 7 : 'Department 7', 8 : 'Department 8', 9 : 'Department 9', 10 : 'Department 10'} # Broadcast the variable broadcast_dept_names = spark.sparkContext.broadcast(dept_names) # Check the value of the variable broadcast_dept_names.value # Create UDF to return Department name from pyspark.sql.functions import udf, col @udf def get_dept_names(dept_id): return broadcast_dept_names.value.get(dept_id) emp_final = emp.withColumn(\"dept_name\", get_dept_names(col(\"department_id\"))) emp_final.show() # Calculate total salary of Department 6 from pyspark.sql.functions import sum emp.where(\"department_id = 6\").groupBy(\"department_id\").agg(sum(\"salary\").cast(\"long\")).show() # Accumulators dept_sal = spark.sparkContext.accumulator(0) # Use foreach def calculate_salary(department_id, salary): if department_id == 6: dept_sal.add(salary) emp.foreach(lambda row : calculate_salary(row.department_id, row.salary)) # View total value dept_sal.value # Stop Spark Session spark.stop() 18 Optimizing Joins # Spark Session from pyspark.sql import SparkSession spark = ( SparkSession .builder .appName(\"Optimizing Joins\") .master(\"spark://17e348267994:7077\") .config(\"spark.cores.max\", 16) .config(\"spark.executor.cores\", 4) .config(\"spark.executor.memory\", \"512M\") .getOrCreate() ) spark # Disable AQE and Broadcast join spark.conf.set(\"spark.sql.adaptive.enabled\", False) spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False) spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1) Join Big and Small table - SortMerge vs BroadCast Join # Read EMP CSV data _schema = \"first_name string, last_name string, job_title string, dob string, email string, phone string, salary double, department_id int\" emp = spark.read.format(\"csv\").schema(_schema).option(\"header\", True).load(\"/data/input/datasets/employee_records.csv\") # Read DEPT CSV data _dept_schema = \"department_id int, department_name string, description string, city string, state string, country string\" dept = spark.read.format(\"csv\").schema(_dept_schema).option(\"header\", True).load(\"/data/input/datasets/department_data.csv\") # Join Datasets from pyspark.sql.functions import broadcast df_joined = emp.join(broadcast(dept), on=emp.department_id==dept.department_id, how=\"left_outer\") df_joined.write.format(\"noop\").mode(\"overwrite\").save() df_joined.explain() Join Big and Big table - SortMerge without Buckets # Read Sales data sales_schema = \"transacted_at string, trx_id string, retailer_id string, description string, amount double, city_id string\" sales = spark.read.format(\"csv\").schema(sales_schema).option(\"header\", True).load(\"/data/input/datasets/new_sales.csv\") # Read City data city_schema = \"city_id string, city string, state string, state_abv string, country string\" city = spark.read.format(\"csv\").schema(city_schema).option(\"header\", True).load(\"/data/input/datasets/cities.csv\") # Join Data df_sales_joined = sales.join(city, on=sales.city_id==city.city_id, how=\"left_outer\") df_sales_joined.write.format(\"noop\").mode(\"overwrite\").save() # Explain Plan Write Sales and City data in Buckets # Write Sales data in Buckets sales.write.format(\"csv\").mode(\"overwrite\").bucketBy(4, \"city_id\").option(\"header\", True).option(\"path\", \"/data/input/datasets/sales_bucket.csv\").saveAsTable(\"sales_bucket\") # Write City data in Buckets city.write.format(\"csv\").mode(\"overwrite\").bucketBy(4, \"city_id\").option(\"header\", True).option(\"path\", \"/data/input/datasets/city_bucket.csv\").saveAsTable(\"city_bucket\") # Check tables spark.sql(\"show tables in default\").show() Join Sales and City data - SortMerge with Bucket # Read Sales table sales_bucket = spark.read.table(\"sales_bucket\") # Read City table city_bucket = spark.read.table(\"city_bucket\") # Join datasets df_joined_bucket = sales_bucket.join(city_bucket, on=sales_bucket.city_id==city_bucket.city_id, how=\"left_outer\") # Write dataset df_joined_bucket.write.format(\"noop\").mode(\"overwrite\").save() df_joined_bucket.explain() # View how tasks are reading Bucket data Points to note 19 Dynamic Allocation # Spark Session from pyspark.sql import SparkSession spark = ( SparkSession .builder .appName(\"Dynamic Allocation\") .master(\"spark://197e20b418a6:7077\") .config(\"spark.executor.cores\", 2) .config(\"spark.executor.memory\", \"512M\") .config(\"spark.dynamicAllocation.enabled\", True) .config(\"spark.dynamicAllocation.minExecutors\", 0) .config(\"spark.dynamicAllocation.maxExecutors\", 5) .config(\"spark.dynamicAllocation.initialExecutors\", 1) .config(\"spark.dynamicAllocation.shuffleTracking.enabled\", True) .config(\"spark.dynamicAllocation.executorIdleTimeout\", \"60s\") .config(\"spark.dynamicAllocation.cachedExecutorIdleTimeout\", \"60s\") .getOrCreate() ) spark # Read Sales data sales_schema = \"transacted_at string, trx_id string, retailer_id string, description string, amount double, city_id string\" sales = spark.read.format(\"csv\").schema(sales_schema).option(\"header\", True).load(\"/data/input/new_sales.csv\") # Read City data city_schema = \"city_id string, city string, state string, state_abv string, country string\" city = spark.read.format(\"csv\").schema(city_schema).option(\"header\", True).load(\"/data/input/cities.csv\") # Join Data df_sales_joined = sales.join(city, on=sales.city_id==city.city_id, how=\"left_outer\") df_sales_joined.write.format(\"noop\").mode(\"overwrite\").save() # Difference between Scale UP in Databricks and Dynamic Allocation 20 Skewness And Spillage # Spark Session from pyspark.sql import SparkSession spark = ( SparkSession .builder .appName(\"Optimizing Skewness and Spillage\") .master(\"spark://197e20b418a6:7077\") .config(\"spark.cores.max\", 8) .config(\"spark.executor.cores\", 4) .config(\"spark.executor.memory\", \"512M\") .getOrCreate() ) spark # Disable AQE and Broadcast join spark.conf.set(\"spark.sql.adaptive.enabled\", False) spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False) spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1) # Read Employee data _schema = \"first_name string, last_name string, job_title string, dob string, email string, phone string, salary double, department_id int\" emp = spark.read.format(\"csv\").schema(_schema).option(\"header\", True).load(\"/data/input/employee_records_skewed.csv\") # Read DEPT CSV data _dept_schema = \"department_id int, department_name string, description string, city string, state string, country string\" dept = spark.read.format(\"csv\").schema(_dept_schema).option(\"header\", True).load(\"/data/input/department_data.csv\") # Join Datasets df_joined = emp.join(dept, on=emp.department_id==dept.department_id, how=\"left_outer\") df_joined.write.format(\"noop\").mode(\"overwrite\").save() #Explain Plan df_joined.explain() # Check the partition details to understand distribution from pyspark.sql.functions import spark_partition_id, count, lit part_df = df_joined.withColumn(\"partition_num\", spark_partition_id()).groupBy(\"partition_num\").agg(count(lit(1)).alias(\"count\")) part_df.show() # Verify Employee data based on department_id from pyspark.sql.functions import count, lit, desc, col emp.groupBy(\"department_id\").agg(count(lit(1))).show() # Set shuffle partitions to a lesser number - 16 spark.conf.set(\"spark.sql.shuffle.partitions\", 32) # Let prepare the salt import random from pyspark.sql.functions import udf # UDF to return a random number every time and add to Employee as salt @udf def salt_udf(): return random.randint(0, 32) # Salt Data Frame to add to department salt_df = spark.range(0, 32) salt_df.show() # Salted Employee from pyspark.sql.functions import lit, concat salted_emp = emp.withColumn(\"salted_dept_id\", concat(\"department_id\", lit(\"_\"), salt_udf())) salted_emp.show() # Salted Department salted_dept = dept.join(salt_df, how=\"cross\").withColumn(\"salted_dept_id\", concat(\"department_id\", lit(\"_\"), \"id\")) salted_dept.where(\"department_id = 9\").show() # Lets make the salted join now salted_joined_df = salted_emp.join(salted_dept, on=salted_emp.salted_dept_id==salted_dept.salted_dept_id, how=\"left_outer\") salted_joined_df.write.format(\"noop\").mode(\"overwrite\").save() # Check the partition details to understand distribution from pyspark.sql.functions import spark_partition_id, count part_df = salted_joined_df.withColumn(\"partition_num\", spark_partition_id()).groupBy(\"partition_num\").agg(count(lit(1)).alias(\"count\")) part_df.show() 21 Aqe Spark # Spark Session from pyspark.sql import SparkSession spark = ( SparkSession .builder .appName(\"AQE in Spark\") .master(\"spark://197e20b418a6:7077\") .config(\"spark.cores.max\", 8) .config(\"spark.executor.cores\", 4) .config(\"spark.executor.memory\", \"512M\") .getOrCreate() ) spark # Disable AQE and Broadcast join spark.conf.set(\"spark.sql.adaptive.enabled\", False) spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False) spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1) # Read Employee data _schema = \"first_name string, last_name string, job_title string, dob string, email string, phone string, salary double, department_id int\" emp = spark.read.format(\"csv\").schema(_schema).option(\"header\", True).load(\"/data/input/employee_records_skewed.csv\") # Read DEPT CSV data _dept_schema = \"department_id int, department_name string, description string, city string, state string, country string\" dept = spark.read.format(\"csv\").schema(_dept_schema).option(\"header\", True).load(\"/data/input/department_data.csv\") # Join Datasets df_joined = emp.join(dept, on=emp.department_id==dept.department_id, how=\"left_outer\") df_joined.write.format(\"noop\").mode(\"overwrite\").save() #Explain Plan df_joined.explain() # Coalescing post-shuffle partitions - remove un-necessary shuffle partitions # Skewed join optimization (balance partitions size) - join smaller partitions and split bigger partition spark.conf.set(\"spark.sql.adaptive.enabled\", True) spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", True) # Fix partition sizes to avoid Skew spark.conf.set(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"8MB\") #Default value: 64MB spark.conf.set(\"spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\", \"10MB\") #Default value: 256MB # Converting sort-merge join to broadcast join spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"10MB\") # Join Datasets - without specifying specific broadcast table df_joined = emp.join(dept, on=emp.department_id==dept.department_id, how=\"left_outer\") df_joined.write.format(\"noop\").mode(\"overwrite\").save() 22 Spark Sql # Spark Session from pyspark.sql import SparkSession spark = ( SparkSession .builder .appName(\"Spark SQL\") .master(\"local[*]\") .enableHiveSupport() .config(\"spark.sql.warehouse.dir\", \"/data/output/spark-warehouse\") .getOrCreate() ) spark # Read Employee data _schema = \"first_name string, last_name string, job_title string, dob string, email string, phone string, salary double, department_id int\" emp = spark.read.format(\"csv\").schema(_schema).option(\"header\", True).load(\"/data/input/employee_records_skewed.csv\") # Read DEPT CSV data _dept_schema = \"department_id int, department_name string, description string, city string, state string, country string\" dept = spark.read.format(\"csv\").schema(_dept_schema).option(\"header\", True).load(\"/data/input/department_data.csv\") # Spark Catalog (Metadata) - in-memory/hive spark.conf.get(\"spark.sql.catalogImplementation\") # Show databases db = spark.sql(\"show databases\") db.show() spark.sql(\"show tables in default\").show() # Register dataframes are temp views emp.createOrReplaceTempView(\"emp_view\") dept.createOrReplaceTempView(\"dept_view\") # Show tables/view in catalog # View data from table emp_filtered = spark.sql(\"\"\" select * from emp_view where department_id = 1 \"\"\") emp_filtered.show() # Create a new column dob_year and register as temp view emp_temp = spark.sql(\"\"\" select e.*, date_format(dob, 'yyyy') as dob_year from emp_view e \"\"\") emp_temp.createOrReplaceTempView(\"emp_temp_view\") spark.sql(\"select * from emp_temp_view\").show() # Join emp and dept - HINTs emp_final = spark.sql(\"\"\" select /*+ BROADCAST(d) */ e.* , d.department_name from emp_view e left outer join dept_view d on e.department_id = d.department_id \"\"\") # Show emp data emp_final.show() # Write the data as Table emp_final.write.format(\"parquet\").saveAsTable(\"emp_final\") # Read the data from Table emp_new = spark.sql(\"select * from emp_final\") emp_new.show() # Persist metadata # Show details of metadata spark.sql(\"describe extended emp_final\").show() 23 Delta Lake # Spark Session spark # Default Catalog for Databricks spark.conf.get(\"spark.sql.catalogImplementation\") %sql show databases # Read Sales parquet data df_sales = spark.read.parquet(\"/data/input/sales_data.parquet\") # View data display(df_sales) # Write data as hive table df_sales.write.format(\"parquet\").mode(\"overwrite\").option(\"path\", \"/data/output/sales_parquet_1/\").saveAsTable(\"sales_parquet\") # View files display(dbutils.fs.ls(\"/data/output/sales_parquet_1/\")) %sql show tables in default %sql describe extended sales_parquet %sql update default.sales_parquet set amount = 0 where trx_id = '1734117021' # Write Sales data as Delta Table df_sales.write.format(\"delta\").mode(\"overwrite\").option(\"path\", \"/data/output/sales_delta_1/\").saveAsTable(\"sales_delta\") %sql show tables in default %sql describe extended sales_delta # View files for delta display(dbutils.fs.ls(\"/data/output/sales_delta_1/\")) dbutils.fs.head(\"/data/output/sales_delta_1/_delta_log/00000000000000000001.json\") %sql describe history sales_delta %sql update default.sales_delta set amount = 0 where trx_id = '1734117021' %sql select * from default.sales_delta # Read a particular version - pyspark api df_sales_delta = spark.read.table(\"sales_delta@v1\") display(df_sales_delta.where(\"trx_id = '1734117021'\")) %sql select * from sales_delta@v0 where trx_id = '1734117021' df_new = spark.sql(\"select *, current_timestamp() as time_now from sales_delta@v0 where trx_id = '1734117021'\") display(df_new) # Append data to existing delta table df_new.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", True).option(\"path\", \"/data/output/sales_delta_1/\").saveAsTable(\"sales_delta\") # View data # Reading Delta Table using Delta libraries from delta import DeltaTable dt = DeltaTable.forName(spark, \"sales_delta\") display(dt.history()) # Converting a Parquet to Delta - Check and Convert DeltaTable.isDeltaTable(spark, \"/data/output/sales_delta_1\") DeltaTable.convertToDelta(spark, \"parquet.`/data/output/sales_parquet_1`\") # Validate display(dbutils.fs.ls(\"/data/output/sales_parquet_1\")) %sql describe extended sales_parquet %sql CONVERT TO DELTA sales_parquet %sql RESTORE TABLE sales_delta TO VERSION AS OF 1 %sql select * from sales_delta where trx_id = '1734117021' spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\",\"false\") dt = DeltaTable.forName(spark, \"sales_delta\") dt.vacuum(0) display(dbutils.fs.ls(\"/data/output/sales_delta_1\")) %sql select * from sales_delta@v0 where trx_id = '1734117021' # Converting back to Parquet 24 Data Scanning And Partitioning # Demo dataset from Databricks display(dbutils.fs.ls(\"/databricks-datasets/nyctaxi/tables/nyctaxi_yellow/\")) display(dbutils.fs.ls(\"/data/input/nyctaxi/parquet/\")) # Check the count and verify data scanning # data copied at location - \"/data/input/nyctaxi/parquet/\" df_parquet = spark.read.parquet(\"/data/input/nyctaxi/parquet/\") display(df_parquet) # Check filter data # select count(1) from nyctaxi where vendor_id = 'VTS' and trip_distance \u003e 1.8 df_parquet.where(\"vendor_id = 'VTS' and trip_distance \u003e 1.8\").count() # Write the data in partitioned format df_parquet.write.format(\"parquet\").mode(\"overwrite\").partitionBy(\"vendor_id\").option(\"path\", \"/data/input/nyctaxi/partitioned/\").saveAsTable(\"nyctaxi_partitioned\") display(dbutils.fs.ls(\"/data/input/nyctaxi/partitioned/\")) df_partitioned = spark.read.parquet(\"/data/input/nyctaxi/partitioned/\") df_partitioned.where(\"vendor_id = 'VTS' and trip_distance \u003e 1.8\").count() %sql select count(1) from nyctaxi_partitioned where vendor_id = 'VTS' and trip_distance \u003e 1.8 25 Delta Lake Optimization And Z Ordering # Dataset display(dbutils.fs.ls(\"/databricks-datasets/definitive-guide/data/retail-data/all/\")) %fs head dbfs:/data/input/sales/sales.csv # Write the data in form of delta table df = spark.read.csv(path=\"/data/input/sales/sales.csv\", inferSchema=True, header=True) df.repartition(16).write.format(\"delta\").mode(\"overwrite\").partitionBy(\"country\").option(\"path\", \"/data/output/sales_delta_partitioned/\").saveAsTable(\"sales_delta_partitioned\") %sql select * from sales_delta # Data at delta location display(dbutils.fs.ls(\"/data/output/sales_delta/\")) display(dbutils.fs.ls(\"/data/output/sales_delta_partitioned/Country=Australia/\")) %sql select * from sales_delta_partitioned where InvoiceNo = '576394' and country = 'Australia' %sql select min(invoiceno), max(invoiceno), _metadata.file_name from sales_delta group by _metadata.file_name order by min(invoiceno) spark.conf.set(\"spark.databricks.delta.optimize.maxFileSize\", 64*1024*8) %sql OPTIMIZE sales_delta_partitioned where country = 'Australia' ZORDER BY (InvoiceNo) %sql select min(invoiceno), max(invoiceno), _metadata.file_name from sales_delta group by _metadata.file_name order by min(invoiceno) %sql select country, min(invoiceno), max(invoiceno), _metadata.file_name from sales_delta group by country, _metadata.file_name order by country, min(invoiceno) 26 Run Concurrent Tasks # Spark Session from pyspark.sql import SparkSession spark = ( SparkSession .builder .appName(\"Run Concurrent/Parallel task in Spark\") .master(\"spark://197e20b418a6:7077\") .config(\"spark.cores.max\", 8) .config(\"spark.executor.cores\", 4) .config(\"spark.executor.memory\", \"512M\") .getOrCreate() ) spark def extract_country_data(_country: str): try: # Read Cities data df_cities = ( spark .read .format(\"csv\") .option(\"header\", True) .load(\"/data/input/cities.csv\") ) # Fiter data df_final = df_cities.where(f\"lower(country) = lower('{_country}')\") # Write data ( df_final .coalesce(1) .write .format(\"csv\") .mode(\"overwrite\") .option(\"header\", True) .save(f\"/data/output/countries/{_country.lower()}/\") ) return f\"Data Extracted for {_country} at: [/data/output/countries/{_country.lower()}/]\" except Exception as e: raise Exception(e) # Use For loops to execute the jobs import time # Set start time start_time = time.time() # Run all extracts through for-loop _countries = ['India', 'Iran', 'Ghana', 'Australia'] for _country in _countries: print(extract_country_data(_country)) # End time end_time = time.time() # Total time taken print(f\"total time = {end_time - start_time} seconds\") # Use threads to run the queries in concurrently/parallely import time import concurrent.futures # Set start time start_time = time.time() _countries = ['India', 'Iran', 'Ghana', 'Australia'] with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor: results = {executor.submit(extract_country_data, _country) for _country in _countries} for result in results: print(result.result()) # End time end_time = time.time() # Total time taken print(f\"total time = {end_time - start_time} seconds\") 27 Spark Memory And Oom # Spark Session from pyspark.sql import SparkSession spark = ( SparkSession .builder .appName(\"Spark Memory Management\") .master(\"spark://b7db50cd9f83:7077\") .config(\"spark.cores.max\", 8) .config(\"spark.executor.cores\", 4) .config(\"spark.executor.memory\", \"512MB\") .getOrCreate() ) spark Spark Memory Calculation Per Executor\n# JVM On-Heap Usable memory (89% of executor memory) # Subtracting Reserve Memory (300MB) # Total Spark Memory (Unified Memory - Storage + Execution Memory) (60% default) spark.memory.fraction = 0.6 # User / Undefined Memory (Not controlled by Spark) (remaining 40% default) # Storage Memory (spark.memory.storageFraction = 0.5) # Execution Memory # Execution Memory per core Out Of Memory Error Demo on Executors\n# Disable AQE and Broadcast join spark.conf.set(\"spark.sql.adaptive.enabled\", False) spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False) spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1) %%sh ls -ltrh /data/datasets/oom_example/ # Read file # Cache data # Explode data to count words # Write with noop format for simulation",
    "description": "What is Spark? Spark is an open source unified computing engine with a set of libraries for parallel data processing on a computer cluster.\nIt supports widely used programming languages such as:\nScala Python Java R It processes data in memory (RAM), which makes it 100 times faster than traditional Hadoop MapReduce.\nflowchart TD\rsubgraph DriverProgram[Driver Program]\rSS[SparkSession]\rSC[SparkContext]\rSS --\u003e SC\rend\rCM[Cluster Manager]\rsubgraph Worker1[Worker Node]\rsubgraph Executor1[Executor]\rT1[Task]\rT2[Task]\rC1[Cache]\rend\rend\rsubgraph Worker2[Worker Node]\rsubgraph Executor2[Executor]\rT3[Task]\rT4[Task]\rC2[Cache]\rend\rend\rSC --\u003e CM\rCM --\u003e Executor1\rCM --\u003e Executor2\rSC --\u003e Executor1\rSC --\u003e Executor2\rExecutor1 --\u003e T1\rExecutor1 --\u003e T2\rExecutor1 --\u003e C1\rExecutor2 --\u003e T3\rExecutor2 --\u003e T4\rExecutor2 --\u003e C2 Spark Components Following represents Spark components at a high level:",
    "tags": [],
    "title": "Spark",
    "uri": "/azure_data_bricks/spark/"
  },
  {
    "breadcrumb": "Interview Prep",
    "content": "1. What are the different types of Integration runtimes(IR) in ADF, and when should you use each? Integration runtime (IR) is a compute engine that provides computational resources to perform data movement, transformation, and orchestration in ADF.\nTypes of Integration Runtimes Azure IR\nUse when performing data movement, transformation, and orchestration entirely within the cloud. Self-Hosted IR\nUse when any source or destination is an on-premises system. Required to pull data from on-prem systems. Azure SSIS IR\nSpecifically built to lift and shift Microsoft SSIS packages. Managing Integration Runtimes in ADF Navigate to ADF -\u003e Manage tab -\u003e Integration Runtimes. AutoResolveIntegrationRuntime is present by default. Creating a New IR Click on Add. Choose the type of IR: Self-Hosted, SSIS, or Airflow. 2. Platform for Creating Azure Self-Hosted IR Self-Hosted IR requires compute outside of Azure, so you can use: A local machine A hosted virtual machine 3. Handling Schema Drift in ADF When dealing with evolving datasets, e.g., a new column appears in the source not present in the destination:\nUse Mapping Dataflows to apply transformations. Options in Mapping Dataflows Schema Drift\nAvailable when creating the source dataset in Mapping Dataflows. Allows handling evolving schemas without errors. Example: If a 5th column appears in the source, it will be written to the destination, with NULL for existing records. Auto Mapping\nAutomatically maps columns between source and destination, eliminating manual schema mapping. Recommended Approach Combine Schema Drift and Auto Mapping to handle evolving datasets seamlessly. 4. How would you load multiple data files from REST APIs using SINGLE copy activity? Steps Create a new pipeline\nGo to Author tab → Create a new pipeline (name it pipAPI). Add a ForEach activity\nDrag a ForEach activity into the pipeline. Configure ForEach parameters\nIn the Parameters tab of the ForEach activity, create a parameter: Name: loop_value Type: Array Default Value: [ {\"fname\": \"x.csv\"}, {\"fname\": \"y.csv\"}, {\"fname\": \"z.csv\"} ] Set the ForEach items\nGo to the Settings tab → under Items, add dynamic content: @pipeline().parameters.loop_value Add a Copy Data activity inside ForEach\nDrag a Copy Data activity inside the ForEach loop. Source Configuration Create Source Dataset\nCreate a dataset ds_api with: Datastore: HTTP Data format: CSV Choose or create a linked service ls_api: Integration Runtime: AutoResolveIR Base URL: https://raw.githubuserdata.com/ Authentication type: Anonymous Test and create the connection. Parameterize the Source Dataset\nGo to the Parameters tab: Add parameter: Name: p_filenm Type: String Default value: @item().fname Go to the Connection tab → under Relative URL, add dynamic content: anshikagit/ADF/refs/heads/main/Data/@{dataset().p_filenm} Sink Configuration Create Sink Dataset\nCreate a dataset ds_adls with: Datastore: ADLS Gen2 Data format: CSV Choose or create a linked service ls_adls: Integration Runtime: AutoResolveIR Storage account name: ansadfstrg11 Test and create the connection. Parameterize the Sink Dataset\nGo to the Parameters tab: Add parameters: p_folder: string p_file: string Default values: @item().fname In the Connection tab, under File path, add dynamic content: Directory: @dataset().p_folder Filename: @dataset().p_file.csv Run the Pipeline Click Debug to execute the pipeline and verify multiple file copies from REST API to ADLS Gen2. 5. How would you design a fault tolerant ADF pipeline to process billions of records daily? To handle large-scale data processing (billions of records per day) in Azure Data Factory (ADF) efficiently and reliably, the following mechanisms can be leveraged:\na. Retry Policy ADF provides built-in Retry Policy for all activities. You can specify: Number of retries — how many times an activity should retry upon failure. Retry interval — time gap between retries. Example:\nIf a Copy Activity fails due to transient network issues, setting:\nRetry count: 3 Retry interval: 30 seconds\nensures ADF automatically retries before marking the activity as failed. b. Parallelism in Mapping Data Flows i. Parallelism within a Single Mapping Data Flow Enables multiple sink operations to run simultaneously within the same data flow. Example:\nProcessing customer data and writing to both: A SQL database (for analytics) A Data Lake (for archival) By enabling “Run in parallel” in Sink Properties, both sinks execute concurrently instead of sequentially — improving overall performance.\nii. Parallelism Across Multiple Data Flow Activities in a Pipeline 1. Independent Data Sources\nIf two datasets (e.g., Sales and Inventory) are independent: Create two Data Flow activities — one for each dataset. Place them in parallel branches within the pipeline. ADF executes both simultaneously, reducing total runtime. 2. Parallel ForEach Processing\nWhen processing multiple files (e.g., CSVs in a folder) with identical transformation logic: Use a ForEach activity to iterate through the files. Enable parallel execution inside ForEach. ADF will process multiple files concurrently, greatly accelerating throughput. c. Failure Notification and Alerts To ensure quick response to failures:\nGo to the Monitor tab in ADF. Click New Alert Rule. Provide details: Name: failAlert1 Severity: Choose appropriate level. In Target criteria: Add metrics such as: Activity type(s) Name(s) Pipeline name(s) Failure type(s) Define condition, threshold, period, and frequency for triggering alerts. Configure Notifications: Add Email, SMS, or both. Provide recipient details. Save to activate the alert. Example:\nFor the Copy Activity inside the ForEach (as in Question 4), include the ForEach activity itself in alert criteria since a child failure causes the parent to fail as well.\nBy combining Retry Policies, Parallelism, and Automated Failure Alerts, you can design a highly fault-tolerant, scalable, and resilient ADF pipeline capable of processing billions of records daily.\n6. Your ADF Pipeline is Running Slow — How to Identify and Fix Performance Bottlenecks a. Initial Step: Monitor and Identify As a Data Engineer, the first step is to monitor the pipeline and identify which activity is causing delays. Use the Monitor tab in ADF to review activity runtimes and pinpoint the bottleneck. b. Common Root Causes and Fixes Slow Data Flows\nIssue: Data flow execution takes longer than expected. Fix: Prune the data in the early stages (apply filters or reduce columns before transformation). Slow Copy Activity\nIssue: Copying data from on-premises to Azure takes too long due to Self-Hosted Integration Runtime (IR) throttling. Fix: Scale up the IR VM, allocate more CPU/memory, or distribute load across multiple IR nodes. 7. Implementing Incremental Data Load in ADF for Large Transactional Databases a. Watermark Approach — Step-by-Step Identify a Watermark Column\nChoose a column that updates with each new or modified record, e.g., lastModifiedDate or an increasing ID.\nStore the Watermark Value\nMaintain the last processed watermark value in a control table or pipeline variable.\nFilter Data\nUse a Lookup Activity to fetch the last watermark from the control table. Use a Copy Activity with a dynamic query: SELECT * FROM SourceTable WHERE lastModifiedDate \u003e @{activity('Lookup').output.firstRow.LastWatermark} Update the Watermark\nAfter the successful copy, use a Stored Procedure or Script Activity to update the watermark table with the new maximum watermark value.\nb. CDC (Change Data Capture) Approach Enable CDC on Source\nActivate CDC on the source database and required tables — this tracks inserts, updates, and deletes in system tables.\nUse ADF’s Built-in CDC Feature\nADF can directly read from CDC system tables to identify changed data automatically.\nExtract and Apply Changes\nUse the __$operation field to determine change type: 1: Delete 2: Insert 3/4: Update Apply logic accordingly in your sink. Manage Checkpoints Automatically\nADF maintains checkpoints (like LSN) to ensure only new changes since the last run are processed.\n8. Processing a JSON File with Unknown Structure in ADF Enable Schema Drift to handle schema evolution dynamically. In a Mapping Data Flow: Turn on Debug Mode to preview the schema. Apply Flatten Transformation to denormalize nested JSON structures for processing. 9. Implementing Real-Time Streaming Pipeline in ADF Scenario A new file arrives in an ADLS container, and you need to copy it automatically to another container.\nSolution: Event-Based Trigger Use an Event-Based Trigger to automatically start the pipeline whenever a new file arrives. Configuration Steps Register Event Grid Services under Subscriptions. Create a new Event-Based Trigger in ADF: Trigger type: Blob Created Linked service: Connect to the ADLS container. Action: Launch the desired pipeline (e.g., Copy Activity). Note: Registration of Event Grid Services is mandatory before using Event-Based Triggers.\n10. How Would You Load Data to Different Locations Based on File Name in ADF a. Get File List (Get Metadata Activity) Start the pipeline with a Get Metadata activity. Configure it to retrieve Child Items from the source folder in your storage account. This activity outputs a list of all files present in the specified source location. b. Iterate Over Files (For Each Activity) Pass the output of the Get Metadata activity to a For Each activity. This allows the pipeline to loop through each file found in the source folder and process them one by one. c. Conditional Routing (If Condition Activity) Inside the For Each loop, add an If Condition activity. Use expressions to check the file name for specific patterns or keywords to determine where each file should go. Example expressions: @contains(item().name, 'sales') or @startsWith(item().name, 'marketing')\nBased on the Condition\nIf the file name contains “sales”, route it to a Sales destination folder. If it contains “marketing”, route it to a Marketing destination folder. d. Load Data (Copy Data Activity) Each branch of the If Condition activity will contain a Copy Data activity. Configure as follows: Source Dataset: The current file being processed (@item().name) Sink Dataset: The target destination based on the file name (e.g., Sales or Marketing folder) The Copy Data activity will copy files to their respective destinations according to the defined condition logic. Example Pipeline Flow Get Metadata Activity → Retrieve list of files from the source folder. For Each Activity → Iterate through each file. If Condition Activity → Check the file name for keywords like “sales” or “marketing.” Copy Data Activity → Load the file into the corresponding destination folder based on the condition. 11. Storing All File Names in a Variable \u0026 Counting Files in a Folder a. Get File List Use a Get Metadata activity configured with Child Items. This returns a single array containing the names of all files and subfolders in the specified location. b. Iterate Over Files Use a For Each activity to loop through the array returned by Get Metadata. Inside the loop, you can process each file individually. c. Store File Names Use Set Variable and Append Variable activities to store and manipulate the list of file names: Set Variable: Assign an entire list of file names to a pipeline variable (e.g., fileNamesArray). Append Variable: When building the list dynamically within a loop, add one filename at a time. Useful if looping through multiple folders and combining all filenames into a single list. d. Count the Files Use a second Set Variable activity to store the count of files. Create a variable of type Integer (e.g., fileCount). Use the following expression in the Value field: @length(variables('fileNamesArray'))\n12. Pipeline Design in ADF to Copy Only Files Not Present in Destination a. Get Source File List Use a Get Metadata activity to retrieve a list of all files from the source folder. b. Get Destination File List Use a second Get Metadata activity to retrieve all files from the destination folder. Store this list in a pipeline variable (e.g., destinationFiles). c. Iterate and Compare Use a For Each activity to loop through the array of source files obtained from the first Get Metadata activity. d. Implement the Condition Inside the loop, add an If Condition activity to check if the current file from the source exists in the destination variable. Expression example: @not(contains(string(variables('destinationFiles')), item().name))\ne. Copy the File Place a Copy Data activity inside the True block of the If Condition. This ensures that only files not present in the destination are copied from the source. 13. Calling Secrets Stored in Key Vault in ADF Creation of Secret in Key Vault Create Key Vault\nIn the Azure Portal, search for “Key Vaults” and click Create. Fill in the basic details: Subscription, Resource Group, Name (globally unique), and Region. Assign Key Vault Administrator Role\nNavigate to IAM and assign the Key Vault Administrator role to the ID authorized to create secrets. Create a Secret\nGo to Secrets inside the Key Vault and create a new secret. (Optional) Provide 3rd Party Access\nTo allow access to external services like Databricks: Key Vault → Access Configuration → Choose Vault Access Policy to grant data plane access. By default, Azure RBAC is used. This step can also be configured during Key Vault creation. Set Access Policies for ADF\nNavigate to Access Policies or Access Control (IAM) in the Key Vault. Grant your Azure Data Factory Get and List permissions for secrets via its Managed Identity. Calling the Secret in ADF 1. Standard Method (Recommended) Create a Key Vault Linked Service\nGo to Manage → Linked Services → New in your ADF instance. Choose Azure Key Vault and point it to your Key Vault. Reference the Secret in Another Linked Service\nWhen creating a linked service for a data store (e.g., SQL Database), select the option to Reference a Secret for sensitive fields like passwords. Choose the Key Vault Linked Service created above and provide the secret name (e.g., MyDatabasePassword). 2. Advanced Method (Using Web Activity) Use Web Activity to Retrieve Secret Dynamically\nAdd a Web Activity to your pipeline. Configure REST API call to Key Vault: https://\u003cyour_key_vault_name\u003e.vault.azure.net/secrets/\u003cyour_secret_name\u003e?api-version=7.4 Authentication: Managed Identity\nResource: https://vault.azure.net Method: GET Use the Secret in Subsequent Activities\nThe secret value will be in the Web Activity output. Reference it using an expression: @activity('Web1').output.value 14. Avoiding Out-of-Memory (OOM) Errors in ADF Pipelines Processing Large Datasets a. Optimize Your Integration Runtime (IR) The IR is the compute engine for your pipeline. Proper configuration ensures enough memory for large workloads. Mapping Data Flows Scale up the Azure IR used by your data flow. Increase Core Count and choose Memory Optimized compute type. This provides more resources to the underlying Spark cluster for large transformations. Self-Hosted IR For on-premises data processing, add more nodes to your Self-Hosted IR. Distributes workload across machines to prevent a single node from running out of memory. b. Partition Large Datasets Breaking data into smaller chunks prevents OOM issues. Files Use Get Metadata to retrieve all files in a folder. Use a For Each loop to process files individually via Copy Data or Mapping Data Flow. In Data Flow source options, set partitioning (single, multiple, or none) to enable parallel processing. Database Tables Implement iterative range copy for large tables: Use a Lookup activity to find min and max values of a key column (e.g., ID or date). Use a For Each loop to copy smaller chunks (e.g., 100,000 rows at a time) with a parameterized query. c. Tune Activity-Specific Settings Mapping Data Flow Be cautious with Broadcast joins or lookups: Broadcasting improves performance but can cause OOM if the dataset is too large. Turn off broadcasting for large streams if necessary. Prune or filter data early in the flow to reduce memory usage. Copy Data Activity For extremely large single files (XML, JSON, Excel), OOM can occur. Use the Binary Copy feature to move files without loading content into memory. 15. Ensuring Idempotency in ADF Pipelines Idempotency ensures that running the same process multiple times with the same input produces the same result as running it once, preventing duplicate processing of records. This can be achieved using UPSERT on file data, often with inline datasets (e.g., Delta Lake) in Mapping Data Flows.\na. Source Transformations Configure at least two source transformations: Incoming Data Source – your new data. Destination Data Source – existing data in the target. Use inline datasets to define connection details and schema directly in the data flow. b. Exists Transformation Use an Exists transformation to implement idempotency. It acts like a WHERE EXISTS clause in SQL: Checks if rows from the incoming data stream (left stream) exist in the destination stream (right stream) based on a key (e.g., primary key). c. Conditional Split Chain a Conditional Split transformation after the Exists transformation. Split the data into two streams: New Records – rows where the key does not exist in the destination; these will be inserted. Existing Records – rows where the key does exist; these will be updated. d. Alter Row \u0026 Sink Connect an Alter Row transformation to each stream from the Conditional Split: New Records → set Insert policy. Existing Records → set Upsert or Update policy. Connect both Alter Row outputs to a single Sink transformation. Configure the sink to allow Insert and Upsert/Update policies. Define the key columns for correct record matching. This approach ensures that duplicate processing of the same records does not occur, maintaining idempotency in your ADF pipeline.\n16. Enforcing File Processing Order in ADF To process files in a specific order (e.g., sales.csv → product.csv → cust.csv), use the Sequential option in the For Each activity. This ensures that files are processed one after another rather than in parallel. 17. Processing Two Files with Different Schemas Using a Single Copy Data Activity a. Source Dataset Create a source dataset that takes the file name as a parameter. In the file path settings, use a dynamic expression: @item().name This passes the current file name from the For Each loop to the Copy Data activity.\nb. Dynamic Mapping Delete Static Mapping In the Mapping tab of the Copy Data activity, remove existing column mappings. You can either delete the entire mapping table or remove columns manually. Enable Schema Drift In the source settings of the Copy Data activity, enable Schema Drift. This allows the activity to dynamically handle different schemas at runtime. Map Dynamically Option 1: Auto-Mapping\nLeave the Mapping tab blank. ADF will automatically map columns from source to sink based on matching names. Option 2: Explicit Dynamic Mapping\nIf column transformation or renaming is required, define dynamic mappings using expressions. For simple copy scenarios, leaving the mapping blank is the most efficient approach.",
    "description": "1. What are the different types of Integration runtimes(IR) in ADF, and when should you use each? Integration runtime (IR) is a compute engine that provides computational resources to perform data movement, transformation, and orchestration in ADF.\nTypes of Integration Runtimes Azure IR\nUse when performing data movement, transformation, and orchestration entirely within the cloud. Self-Hosted IR\nUse when any source or destination is an on-premises system. Required to pull data from on-prem systems. Azure SSIS IR",
    "tags": [],
    "title": "ADF 1",
    "uri": "/interviewprep/adf1/"
  },
  {
    "breadcrumb": "ADB",
    "content": "Creating DataFrame Creating DataFrame from Lists/Tuples # Sample Data data = [(1, \"Alice\"), (2, \"Bob\"), (3, \"Charlie\"), (4, \"David\"), (5, \"Eve\")] columns = [\"ID\", \"Name\"] # Create DataFrame df = spark.createDataFrame(data, columns) # Show DataFrame df.show() Creating DataFrame from Pandas import pandas as pd # Sample Pandas DataFrame pandas_df = pd.DataFrame(data, columns=columns) # Convert to PySpark DataFrame df_from_pandas = spark.createDataFrame(pandas_df) df_from_pandas.show() Create DataFrame from Dictionary data_dict = [{\"ID\": 1, \"Name\": \"Alice\"}, {\"ID\": 2, \"Name\": \"Bob\"}] df_from_dict = spark.createDataFrame(data_dict) df_from_dict.show() Create Empty DataFrame You can create an empty DataFrame with just schema definitions.\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType # Define Schema schema = StructType([ StructField(\"ID\", IntegerType(), True), StructField(\"Name\", StringType(), True) ]) # Create Empty DataFrame empty_df = spark.createDataFrame([], schema) empty_df.show() Creating DataFrame from Structured Data (CSV, JSON, Parquet) # Reading CSV file into DataFrame df_csv = spark.read.csv(\"/path/to/file.csv\", header=True, inferSchema=True) df_csv.show() # Reading JSON file into DataFrame df_json = spark.read.json(\"/path/to/file.json\") df_json.show() # Reading Parquet file into DataFrame df_parquet = spark.read.parquet(\"/path/to/file.parquet\") df_parquet.show() show() Function in PySpark DataFrames The show() function in PySpark displays the contents of a DataFrame in a tabular format. It has several useful parameters for customization:\nParameters: n: Number of rows to display (default is 20) truncate: If set to True, it truncates column values longer than 20 characters (default is True) vertical: If set to True, prints rows in a vertical format Usage Examples: # Show the first 3 rows, truncate columns to 25 characters, and display vertically: df.show(n=3, truncate=25, vertical=True) # Show entire DataFrame (default settings): df.show() # Show the first 10 rows: df.show(10) # Show DataFrame without truncating any columns: df.show(truncate=False) Loading Data from CSV File into a DataFrame Loading data into DataFrames is a fundamental step in any data processing workflow in PySpark. This document outlines how to load data from CSV files into a DataFrame, including using a custom schema and the implications of using the inferSchema option.\n1. Import Required Libraries Before loading the data, ensure you import the necessary modules:\nfrom pyspark.sql import SparkSession from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType 2. Define the Schema You can define a custom schema for your CSV file. This allows you to explicitly set the data types for each column.\n# Define the schema for the CSV file custom_schema = StructType([ StructField(\"id\", IntegerType(), True), StructField(\"name\", StringType(), True), StructField(\"age\", IntegerType(), True), StructField(\"salary\", DoubleType(), True) ]) 3. Read the CSV File Load the CSV file into a DataFrame using the read.csv() method. Here, header=True treats the first row as headers, and inferSchema=True allows Spark to automatically assign data types to columns.\n# Read the CSV file with the custom schema df = spark.read.csv(\"your_file.csv\", schema=custom_schema, header=True) 4. Load Multiple CSV Files To read multiple CSV files into a single DataFrame, you can pass a list of file paths. Ensure that the schema is consistent across all files.\n# List of file paths file_paths = [\"file1.csv\", \"file2.csv\", \"file3.csv\"] # Read multiple CSV files into a single DataFrame df = spark.read.csv(file_paths, header=True, inferSchema=True) 5. Load a CSV from FileStore Here is an example of loading a CSV file from Databricks FileStore:\ndf = spark.read.csv(\"/FileStore/tables/Order.csv\", header=True, inferSchema=True, sep=',') 6. Display the DataFrame Use the following commands to check the schema and display the DataFrame:\n# Print the schema of the DataFrame df.printSchema() # Show the first 20 rows of the DataFrame df.show() # Displays only the first 20 rows # Display the DataFrame in a tabular format display(df) # For Databricks notebooks Interview Question: How Does inferSchema Work? Behind the Scenes: When you use inferSchema, Spark runs a job that scans the CSV file from top to bottom to identify the best-suited data type for each column based on the values it encounters.\nDoes It Make Sense to Use inferSchema? Pros:\nUseful when the schema of the file keeps changing, as it allows Spark to automatically detect the data types. Cons:\nPerformance Impact: Spark must scan the entire file, which can take extra time, especially for large files. Loss of Control: You lose the ability to explicitly define the schema, which may lead to incorrect data types if the data is inconsistent. Conclusion Loading data from CSV files into a DataFrame is straightforward in PySpark. Understanding how to define a schema and the implications of using inferSchema is crucial for optimizing your data processing workflows.\nThis document provides a comprehensive overview of how to load CSV data into DataFrames in PySpark, along with considerations for using schema inference. Let me know if you need any more details or adjustments!\nPySpark DataFrame Schema Definition 1. Defining Schema Programmatically with StructType from pyspark.sql.types import * # Define the schema using StructType employeeSchema = StructType([ StructField(\"ID\", IntegerType(), True), StructField(\"Name\", StringType(), True), StructField(\"Age\", IntegerType(), True), StructField(\"Salary\", DoubleType(), True), StructField(\"Joining_Date\", StringType(), True), # Keeping as String for date issues StructField(\"Department\", StringType(), True), StructField(\"Performance_Rating\", IntegerType(), True), StructField(\"Email\", StringType(), True), StructField(\"Address\", StringType(), True), StructField(\"Phone\", StringType(), True) ]) # Load the DataFrame with the defined schema df = spark.read.load(\"/FileStore/tables/employees.csv\",format=\"csv\", header=True, schema=employeeSchema) # Print the schema of the DataFrame df.printSchema() # Optionally display the DataFrame # display(df) 2. Defining Schema as a String # Define the schema as a string employeeSchemaString = ''' ID Integer, Name String, Age Integer, Salary Double, Joining_Date String, Department String, Performance_Rating Integer, Email String, Address String, Phone String ''' # Load the DataFrame with the defined schema df = spark.read.load(\"dbfs:/FileStore/shared_uploads/imsvk11@gmail.com/employee_data.csv\", format=\"csv\", header=True, schema=employeeSchemaString) # Print the schema of the DataFrame df.printSchema() # Optionally display the DataFrame # display(df) Explanation Schema Definition: Both methods define a schema for the DataFrame, accommodating the dataset’s requirements, including handling null values where applicable. Data Types: The Joining_Date column is defined as StringType to accommodate potential date format issues or missing values. Loading the DataFrame: The spark.read.load method is used to load the CSV file into a DataFrame using the specified schema. Printing the Schema: The df.printSchema() function allows you to verify that the DataFrame is structured as intended.",
    "description": "Creating DataFrame Creating DataFrame from Lists/Tuples # Sample Data data = [(1, \"Alice\"), (2, \"Bob\"), (3, \"Charlie\"), (4, \"David\"), (5, \"Eve\")] columns = [\"ID\", \"Name\"] # Create DataFrame df = spark.createDataFrame(data, columns) # Show DataFrame df.show() Creating DataFrame from Pandas import pandas as pd # Sample Pandas DataFrame pandas_df = pd.DataFrame(data, columns=columns) # Convert to PySpark DataFrame df_from_pandas = spark.createDataFrame(pandas_df) df_from_pandas.show() Create DataFrame from Dictionary data_dict = [{\"ID\": 1, \"Name\": \"Alice\"}, {\"ID\": 2, \"Name\": \"Bob\"}] df_from_dict = spark.createDataFrame(data_dict) df_from_dict.show() Create Empty DataFrame You can create an empty DataFrame with just schema definitions.\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType # Define Schema schema = StructType([ StructField(\"ID\", IntegerType(), True), StructField(\"Name\", StringType(), True) ]) # Create Empty DataFrame empty_df = spark.createDataFrame([], schema) empty_df.show() Creating DataFrame from Structured Data (CSV, JSON, Parquet) # Reading CSV file into DataFrame df_csv = spark.read.csv(\"/path/to/file.csv\", header=True, inferSchema=True) df_csv.show() # Reading JSON file into DataFrame df_json = spark.read.json(\"/path/to/file.json\") df_json.show() # Reading Parquet file into DataFrame df_parquet = spark.read.parquet(\"/path/to/file.parquet\") df_parquet.show() show() Function in PySpark DataFrames The show() function in PySpark displays the contents of a DataFrame in a tabular format. It has several useful parameters for customization:",
    "tags": [],
    "title": "DF Basics",
    "uri": "/azure_data_bricks/df-creation/"
  },
  {
    "breadcrumb": "SQL",
    "content": "This guide covers various SELECT query techniques used for retrieving, filtering, sorting, and aggregating data efficiently.\nComments -- This is a single-line comment. /* This is a multiple-line comment */ 1. SELECT ALL COLUMNS -- Retrieve All Customer Data SELECT * FROM customers; -- Retrieve All Order Data SELECT * FROM orders; 2. SELECT SPECIFIC COLUMNS -- Retrieve each customer's name, country, and score SELECT first_name, country, score FROM customers; 3. WHERE CLAUSE -- Retrieve customers with a score not equal to 0 SELECT * FROM customers WHERE score != 0; -- Retrieve customers from Germany SELECT * FROM customers WHERE country = 'Germany'; -- Retrieve the name and country of customers from Germany SELECT first_name, country FROM customers WHERE country = 'Germany'; 4. ORDER BY -- Sort by highest score first SELECT * FROM customers ORDER BY score DESC; -- Sort by lowest score first SELECT * FROM customers ORDER BY score ASC; -- Sort by country SELECT * FROM customers ORDER BY country ASC; -- Sort by country, then highest score SELECT * FROM customers ORDER BY country ASC, score DESC; -- Customers with score != 0, sorted by highest score SELECT first_name, country, score FROM customers WHERE score != 0 ORDER BY score DESC; 5. GROUP BY -- Find total score for each country SELECT country, SUM(score) AS total_score FROM customers GROUP BY country; -- ❌ Invalid: first_name not grouped or aggregated SELECT country, first_name, SUM(score) AS total_score FROM customers GROUP BY country; -- Find total score \u0026 number of customers per country SELECT country, SUM(score) AS total_score, COUNT(id) AS total_customers FROM customers GROUP BY country; 6. HAVING -- Average score \u003e 430 for each country SELECT country, AVG(score) AS avg_score FROM customers GROUP BY country HAVING AVG(score) \u003e 430; -- Only consider customers with score != 0 SELECT country, AVG(score) AS avg_score FROM customers WHERE score != 0 GROUP BY country HAVING AVG(score) \u003e 430; 7. DISTINCT -- Unique list of countries SELECT DISTINCT country FROM customers; 8. TOP -- Retrieve only 3 customers SELECT TOP 3 * FROM customers; -- Top 3 customers with highest scores SELECT TOP 3 * FROM customers ORDER BY score DESC; -- Lowest 2 customers by score SELECT TOP 2 * FROM customers ORDER BY score ASC; -- Two most recent orders SELECT TOP 2 * FROM orders ORDER BY order_date DESC; 9. All Together -- Average score per country, excluding score=0, -- only \u003e 430, ordered by highest avg_score SELECT country, AVG(score) AS avg_score FROM customers WHERE score != 0 GROUP BY country HAVING AVG(score) \u003e 430 ORDER BY AVG(score) DESC; 10. COOL STUFF – Additional SQL Features -- Execute multiple queries at once SELECT * FROM customers; SELECT * FROM orders; -- Select static values SELECT 123 AS static_number; SELECT 'Hello' AS static_string; -- Assign a constant value in query results SELECT id, first_name, 'New Customer' AS customer_type FROM customers;",
    "description": "This guide covers various SELECT query techniques used for retrieving, filtering, sorting, and aggregating data efficiently.\nComments -- This is a single-line comment. /* This is a multiple-line comment */ 1. SELECT ALL COLUMNS -- Retrieve All Customer Data SELECT * FROM customers; -- Retrieve All Order Data SELECT * FROM orders; 2. SELECT SPECIFIC COLUMNS -- Retrieve each customer's name, country, and score SELECT first_name, country, score FROM customers; 3. WHERE CLAUSE -- Retrieve customers with a score not equal to 0 SELECT * FROM customers WHERE score != 0; -- Retrieve customers from Germany SELECT * FROM customers WHERE country = 'Germany'; -- Retrieve the name and country of customers from Germany SELECT first_name, country FROM customers WHERE country = 'Germany'; 4. ORDER BY -- Sort by highest score first SELECT * FROM customers ORDER BY score DESC; -- Sort by lowest score first SELECT * FROM customers ORDER BY score ASC; -- Sort by country SELECT * FROM customers ORDER BY country ASC; -- Sort by country, then highest score SELECT * FROM customers ORDER BY country ASC, score DESC; -- Customers with score != 0, sorted by highest score SELECT first_name, country, score FROM customers WHERE score != 0 ORDER BY score DESC; 5. GROUP BY -- Find total score for each country SELECT country, SUM(score) AS total_score FROM customers GROUP BY country; -- ❌ Invalid: first_name not grouped or aggregated SELECT country, first_name, SUM(score) AS total_score FROM customers GROUP BY country; -- Find total score \u0026 number of customers per country SELECT country, SUM(score) AS total_score, COUNT(id) AS total_customers FROM customers GROUP BY country; 6. HAVING -- Average score \u003e 430 for each country SELECT country, AVG(score) AS avg_score FROM customers GROUP BY country HAVING AVG(score) \u003e 430; -- Only consider customers with score != 0 SELECT country, AVG(score) AS avg_score FROM customers WHERE score != 0 GROUP BY country HAVING AVG(score) \u003e 430; 7. DISTINCT -- Unique list of countries SELECT DISTINCT country FROM customers; 8. TOP -- Retrieve only 3 customers SELECT TOP 3 * FROM customers; -- Top 3 customers with highest scores SELECT TOP 3 * FROM customers ORDER BY score DESC; -- Lowest 2 customers by score SELECT TOP 2 * FROM customers ORDER BY score ASC; -- Two most recent orders SELECT TOP 2 * FROM orders ORDER BY order_date DESC; 9. All Together -- Average score per country, excluding score=0, -- only \u003e 430, ordered by highest avg_score SELECT country, AVG(score) AS avg_score FROM customers WHERE score != 0 GROUP BY country HAVING AVG(score) \u003e 430 ORDER BY AVG(score) DESC; 10. COOL STUFF – Additional SQL Features -- Execute multiple queries at once SELECT * FROM customers; SELECT * FROM orders; -- Select static values SELECT 123 AS static_number; SELECT 'Hello' AS static_string; -- Assign a constant value in query results SELECT id, first_name, 'New Customer' AS customer_type FROM customers;",
    "tags": [],
    "title": "SELECT Query",
    "uri": "/sql/select/"
  },
  {
    "breadcrumb": "",
    "content": "Course Content 1. Introduction What is SQL? Types of Databases (Relational vs Non-Relational) SQL vs NoSQL SQL Standards (ANSI SQL, T-SQL, PL/SQL, MySQL SQL, PostgreSQL SQL) 2. SQL Basics Database, Schema, Table Data Types (Numeric, String, Date/Time, Boolean, JSON, XML) CREATE, DROP, ALTER INSERT, UPDATE, DELETE SELECT Statement WHERE Clause ORDER BY LIMIT / TOP / FETCH 3. Filtering \u0026 Operators Comparison Operators Logical Operators (AND, OR, NOT) BETWEEN, IN, LIKE IS NULL / IS NOT NULL Pattern Matching (Wildcards, Regex) 4. Functions String Functions (CONCAT, SUBSTRING, TRIM, UPPER, LOWER, LENGTH) Numeric Functions (ROUND, CEIL, FLOOR, ABS, MOD) Date/Time Functions (NOW, DATEADD, DATEDIFF, FORMAT) Conversion Functions (CAST, CONVERT) Aggregate Functions (COUNT, SUM, AVG, MIN, MAX) 5. Joins INNER JOIN LEFT JOIN RIGHT JOIN FULL OUTER JOIN CROSS JOIN SELF JOIN USING vs ON 6. Subqueries \u0026 Nested Queries Scalar Subqueries Table Subqueries Correlated Subqueries EXISTS / NOT EXISTS IN / ANY / ALL 7. Set Operations UNION vs UNION ALL INTERSECT EXCEPT / MINUS 8. Grouping \u0026 Aggregation GROUP BY HAVING GROUPING SETS ROLLUP CUBE 9. Constraints \u0026 Keys Primary Key Foreign Key Unique Key Check Constraint Default Constraint Not Null 10. Indexing \u0026 Performance Clustered vs Non-Clustered Index Composite Index Covering Index Index Maintenance Query Execution Plans SQL Performance Tuning 11. Transactions BEGIN, COMMIT, ROLLBACK Savepoints ACID Properties Isolation Levels (Read Uncommitted, Read Committed, Repeatable Read, Serializable, Snapshot) Deadlocks \u0026 Locking 12. Views \u0026 Materialized Views Creating Views Updatable Views Indexed Views Materialized Views Refresh Strategies 13. Stored Procedures \u0026 Functions User-Defined Functions (Scalar, Table-Valued) Stored Procedures Parameters (IN, OUT, INOUT) Error Handling in Procedures Recursive Procedures Dynamic SQL 14. Triggers AFTER Trigger INSTEAD OF Trigger Row-level vs Statement-level Triggers Use Cases 15. Advanced SQL Window Functions (ROW_NUMBER, RANK, DENSE_RANK, NTILE) LAG \u0026 LEAD FIRST_VALUE \u0026 LAST_VALUE Common Table Expressions (CTEs) Recursive CTEs Pivoting and Unpivoting JSON \u0026 XML in SQL Hierarchical Queries Full-Text Search 16. Security Users \u0026 Roles GRANT, REVOKE, DENY Row-Level Security Column-Level Security Data Masking Encryption at Rest \u0026 In-Transit 17. Advanced Database Concepts Normalization \u0026 Denormalization Star \u0026 Snowflake Schema Partitioning (Horizontal, Vertical) Sharding Federation Replication High Availability \u0026 Failover 18. SQL with Big Data SQL on Hadoop / Hive Spark SQL SQL in Cloud Platforms (AWS Athena, BigQuery, Azure Synapse) 19. SQL in Practice Data Warehousing with SQL ETL using SQL Reporting Queries Performance Benchmarking Debugging Complex Queries 20. Capstone Project Design \u0026 Implement Normalized Database Build ETL Pipeline using SQL Create Analytical Queries \u0026 Reports Optimize Large-Scale SQL Workloads",
    "description": "Course Content 1. Introduction What is SQL? Types of Databases (Relational vs Non-Relational) SQL vs NoSQL SQL Standards (ANSI SQL, T-SQL, PL/SQL, MySQL SQL, PostgreSQL SQL) 2. SQL Basics Database, Schema, Table Data Types (Numeric, String, Date/Time, Boolean, JSON, XML) CREATE, DROP, ALTER INSERT, UPDATE, DELETE SELECT Statement WHERE Clause ORDER BY LIMIT / TOP / FETCH 3. Filtering \u0026 Operators Comparison Operators Logical Operators (AND, OR, NOT) BETWEEN, IN, LIKE IS NULL / IS NOT NULL Pattern Matching (Wildcards, Regex) 4. Functions String Functions (CONCAT, SUBSTRING, TRIM, UPPER, LOWER, LENGTH) Numeric Functions (ROUND, CEIL, FLOOR, ABS, MOD) Date/Time Functions (NOW, DATEADD, DATEDIFF, FORMAT) Conversion Functions (CAST, CONVERT) Aggregate Functions (COUNT, SUM, AVG, MIN, MAX) 5. Joins INNER JOIN LEFT JOIN RIGHT JOIN FULL OUTER JOIN CROSS JOIN SELF JOIN USING vs ON 6. Subqueries \u0026 Nested Queries Scalar Subqueries Table Subqueries Correlated Subqueries EXISTS / NOT EXISTS IN / ANY / ALL 7. Set Operations UNION vs UNION ALL INTERSECT EXCEPT / MINUS 8. Grouping \u0026 Aggregation GROUP BY HAVING GROUPING SETS ROLLUP CUBE 9. Constraints \u0026 Keys Primary Key Foreign Key Unique Key Check Constraint Default Constraint Not Null 10. Indexing \u0026 Performance Clustered vs Non-Clustered Index Composite Index Covering Index Index Maintenance Query Execution Plans SQL Performance Tuning 11. Transactions BEGIN, COMMIT, ROLLBACK Savepoints ACID Properties Isolation Levels (Read Uncommitted, Read Committed, Repeatable Read, Serializable, Snapshot) Deadlocks \u0026 Locking 12. Views \u0026 Materialized Views Creating Views Updatable Views Indexed Views Materialized Views Refresh Strategies 13. Stored Procedures \u0026 Functions User-Defined Functions (Scalar, Table-Valued) Stored Procedures Parameters (IN, OUT, INOUT) Error Handling in Procedures Recursive Procedures Dynamic SQL 14. Triggers AFTER Trigger INSTEAD OF Trigger Row-level vs Statement-level Triggers Use Cases 15. Advanced SQL Window Functions (ROW_NUMBER, RANK, DENSE_RANK, NTILE) LAG \u0026 LEAD FIRST_VALUE \u0026 LAST_VALUE Common Table Expressions (CTEs) Recursive CTEs Pivoting and Unpivoting JSON \u0026 XML in SQL Hierarchical Queries Full-Text Search 16. Security Users \u0026 Roles GRANT, REVOKE, DENY Row-Level Security Column-Level Security Data Masking Encryption at Rest \u0026 In-Transit 17. Advanced Database Concepts Normalization \u0026 Denormalization Star \u0026 Snowflake Schema Partitioning (Horizontal, Vertical) Sharding Federation Replication High Availability \u0026 Failover 18. SQL with Big Data SQL on Hadoop / Hive Spark SQL SQL in Cloud Platforms (AWS Athena, BigQuery, Azure Synapse) 19. SQL in Practice Data Warehousing with SQL ETL using SQL Reporting Queries Performance Benchmarking Debugging Complex Queries 20. Capstone Project Design \u0026 Implement Normalized Database Build ETL Pipeline using SQL Create Analytical Queries \u0026 Reports Optimize Large-Scale SQL Workloads",
    "tags": [],
    "title": "SQL",
    "uri": "/sql/"
  },
  {
    "breadcrumb": "Practice Sets",
    "content": "175. Combine Two Tables Description Table: Person\nColumn Name Type PersonId int FirstName varchar LastName varchar PersonId is the primary key column for this table. Table: Address\nColumn Name Type AddressId int PersonId int City varchar State varchar AddressId is the primary key column for this table. Write a SQL query for a report that provides the following information for each person in the Person table, regardless if there is an address for them:\nFirstName LastName City State Solution SELECT FirstName, LastName, City, State FROM Person AS p LEFT JOIN Address AS a ON p.PersonId = a.PersonId; 176. Second Highest Salary Description Write a SQL query to get the second highest salary from the Employee table.\nTable: Employee\nColumn Name Type Id int Salary int For example, given the above Employee table:\nId Salary 1 100 2 200 3 300 The query should return 200 as the second highest salary.\nIf there is no second highest salary, then the query should return null.\nResult Table:\nSecondHighestSalary 200 Solution SELECT IFNULL(( SELECT DISTINCT Salary FROM Employee ORDER BY Salary DESC LIMIT 1 OFFSET 1 ), NULL) AS SecondHighestSalary; 177. Nth Highest Salary Description Write a SQL query to get the nth highest salary from the Employee table.\nTable: Employee\nColumn Name Type Id int Salary int For example, given the above Employee table:\nId Salary 1 100 2 200 3 300 The nth highest salary where n = 2 is 200.\nIf there is no nth highest salary, then the query should return null.\nResult Table:\ngetNthHighestSalary(2) 200 Solution CREATE FUNCTION getNthHighestSalary(N INT) RETURNS INT BEGIN DECLARE M INT; SET M = N - 1; RETURN ( SELECT DISTINCT Salary FROM Employee ORDER BY Salary DESC LIMIT 1 OFFSET M ); END 178. Rank Scores Description Write a SQL query to rank scores. If there is a tie between two scores, both should have the same ranking.\nAfter a tie, the next ranking number should be the next consecutive integer value (no “holes” between ranks).\nTable: Scores\nColumn Name Type Id int Score float For example, given the above Scores table:\nId Score 1 3.50 2 3.65 3 4.00 4 3.85 5 4.00 6 3.65 The query should generate the following report (ordered by highest score):\nScore Rank 4.00 1 4.00 1 3.85 2 3.65 3 3.65 3 3.50 4 Solution (MS SQL Server) SELECT Score, DENSE_RANK() OVER(ORDER BY Score DESC) AS Rank FROM Scores; Solution (MySQL using Variables) SELECT Score, @rank := @rank + (@prev \u003c\u003e (@prev := Score)) AS Rank FROM Scores, (SELECT @rank := 0, @prev := -1) AS a ORDER BY Score DESC; Solution (MySQL using COUNT) SELECT Score, (SELECT COUNT(DISTINCT Score) FROM Scores WHERE Score \u003e= s.Score) AS Rank FROM Scores AS s ORDER BY Score DESC; 180. Consecutive Numbers Description Write a SQL query to find all numbers that appear at least three times consecutively.\nTable: Logs\nColumn Name Type Id int Num int For example, given the above Logs table:\nId Num 1 1 2 1 3 1 4 2 5 1 6 2 7 2 The number 1 is the only number that appears consecutively at least three times.\nResult Table:\nConsecutiveNums 1 Solution (MySQL, user-defined variables) SELECT DISTINCT Num AS ConsecutiveNums FROM ( SELECT Num, @cnt := IF(@prev = (@prev := Num), @cnt + 1, 1) AS freq FROM Logs, (SELECT @cnt := 0, @prev := (SELECT Num FROM Logs LIMIT 1)) AS c ) AS n WHERE freq \u003e 2; 181. Employees Earning More Than Their Managers Description The Employee table holds all employees including their managers.\nEvery employee has an Id, and there is also a column for the ManagerId.\nTable: Employee\nColumn Name Type Id int Name varchar Salary int ManagerId int Example data:\nId Name Salary ManagerId 1 Joe 70000 3 2 Henry 80000 4 3 Sam 60000 NULL 4 Max 90000 NULL Write a SQL query to find employees who earn more than their managers.\nFor the above table, Joe is the only employee who earns more than his manager.\nResult Table:\nEmployee Joe Solution SELECT e.Name AS Employee FROM Employee AS e INNER JOIN Employee AS m ON e.ManagerId = m.Id WHERE e.Salary \u003e m.Salary; 182. Duplicate Emails Description Write a SQL query to find all duplicate emails in a table named Person.\nTable: Person\nColumn Name Type Id int Email varchar Example data:\nId Email 1 a@b.com 2 c@d.com 3 a@b.com The query should return:\nEmail a@b.com Note: All emails are in lowercase.\nSolution SELECT Email FROM Person GROUP BY Email HAVING COUNT(Email) \u003e 1; 183. Customers Who Never Order Description Suppose that a website contains two tables: Customers and Orders.\nWrite a SQL query to find all customers who never order anything.\nTable: Customers\nColumn Name Type Id int Name varchar Example data:\nId Name 1 Joe 2 Henry 3 Sam 4 Max Table: Orders\nColumn Name Type Id int CustomerId int Example data:\nId CustomerId 1 3 2 1 Using the above tables, the query should return:\nCustomers Henry Max Solution SELECT Name AS Customers FROM Customers WHERE Id NOT IN ( SELECT CustomerId FROM Orders ); 196. Delete Duplicate Emails Description Write a SQL query to delete all duplicate email entries in a table named Person,\nkeeping only unique emails based on the smallest Id.\nTable: Person\nColumn Name Type Id int Email varchar Example data:\nId Email 1 john@example.com 2 bob@example.com 3 john@example.com After running the query, the table should look like:\nId Email 1 john@example.com 2 bob@example.com Note: Your output is the whole Person table after executing your SQL. Use DELETE statement.\nSolution (MySQL, using subquery) DELETE p.* FROM Person AS p JOIN ( SELECT Email, MIN(Id) AS minId FROM Person GROUP BY Email HAVING COUNT(*) \u003e 1 ) AS q ON p.Email = q.Email WHERE p.Id \u003e q.minId; Solution (MySQL, self join) DELETE p1 FROM Person AS p1 JOIN Person AS p2 ON p1.Email = p2.Email WHERE p1.Id \u003e p2.Id; 197. Rising Temperature Description Given a Weather table, write a SQL query to find all dates’ Ids with higher temperature compared to the previous (yesterday’s) date.\nTable: Weather\nColumn Name Type Id int RecordDate date Temperature int Example data:\nId RecordDate Temperature 1 2015-01-01 10 2 2015-01-02 25 3 2015-01-03 20 4 2015-01-04 30 The query should return the following Ids:\nId 2 4 Solution (MySQL) SELECT w1.Id FROM Weather AS w1, Weather AS w2 WHERE DATEDIFF(w1.RecordDate, w2.RecordDate) = 1 AND w1.Temperature \u003e w2.Temperature; 511. Game Play Analysis I Description Table: Activity\nColumn Name Type player_id int device_id int event_date date games_played int (player_id, event_date) is the primary key of this table. This table shows the activity of players of a game. Each row is a record of a player who logged in and played a number of games (possibly 0) before logging out on some day using some device. Write an SQL query that reports the first login date for each player.\nExample: Activity table\nplayer_id device_id event_date games_played 1 2 2016-03-01 5 1 2 2016-05-02 6 2 3 2017-06-25 1 3 1 2016-03-02 0 3 4 2018-07-03 5 Result table\nplayer_id first_login 1 2016-03-01 2 2017-06-25 3 2016-03-02 Solution (MySQL) SELECT player_id, MIN(event_date) AS first_login FROM Activity GROUP BY player_id ORDER BY player_id; 512. Game Play Analysis II Description Table: Activity\nColumn Name Type player_id int device_id int event_date date games_played int (player_id, event_date) is the primary key of this table. This table shows the activity of players of a game. Each row is a record of a player who logged in and played a number of games (possibly 0) before logging out on some day using some device. Write an SQL query that reports the device that is first logged in for each player.\nExample: Activity table\nplayer_id device_id event_date games_played 1 2 2016-03-01 5 1 2 2016-05-02 6 2 3 2017-06-25 1 3 1 2016-03-02 0 3 4 2018-07-03 5 Result table\nplayer_id device_id 1 2 2 3 3 1 Solution (MySQL) SELECT player_id, device_id FROM Activity WHERE (player_id, event_date) IN ( SELECT player_id, MIN(event_date) FROM Activity GROUP BY player_id ); 534. Game Play Analysis III Description Table: Activity\nColumn Name Type player_id int device_id int event_date date games_played int (player_id, event_date) is the primary key of this table. This table shows the activity of players of a game. Each row is a record of a player who logged in and played a number of games (possibly 0) before logging out on some day using some device. Write an SQL query that reports for each player and date, how many games played so far by the player.\nThat is, the total number of games played by the player until that date. Only consider the days the player logged in.\nExample: Activity table\nplayer_id device_id event_date games_played 1 2 2016-03-01 5 1 2 2016-05-02 6 1 3 2017-06-25 1 3 1 2016-03-02 0 3 4 2018-07-03 5 Result table\nplayer_id event_date games_played_so_far 1 2016-03-01 5 1 2016-05-02 11 1 2017-06-25 12 3 2016-03-02 0 3 2018-07-03 5 Explanation:\nFor player 1: 5 + 6 = 11 games by 2016-05-02, 5 + 6 + 1 = 12 games by 2017-06-25. For player 3: 0 + 5 = 5 games by 2018-07-03. Solution (MySQL, using user-defined variables) SELECT player_id, event_date, games_played_so_far FROM ( SELECT player_id, event_date, @games := IF(player_id = @player, @games + games_played, games_played) AS games_played_so_far, @player := player_id FROM (SELECT * FROM Activity ORDER BY player_id, event_date) AS a, (SELECT @player := -1, @games := 0) AS tmp ) AS t; 570. Managers with at Least 5 Direct Reports Description The Employee table holds all employees including their managers.\nEvery employee has an Id, and there is also a column for the ManagerId.\nTable: Employee\nId Name Department ManagerId 101 John A null 102 Dan A 101 103 James A 101 104 Amy A 101 105 Anne A 101 106 Ron B 101 Write a SQL query to find managers with at least 5 direct reports.\nNo one would report to himself.\nResult table\nName John Solution (MySQL) SELECT Name FROM Employee WHERE Id IN ( SELECT ManagerId FROM Employee GROUP BY ManagerId HAVING COUNT(*) \u003e= 5 ); 577. Employee Bonus Description Select all employees’ name and bonus whose bonus is less than 1000.\nTable: Employee\nColumn Name Type empId int name varchar supervisor int salary int Example data:\nempId name supervisor salary 1 John 3 1000 2 Dan 3 2000 3 Brad null 4000 4 Thomas 3 4000 Table: Bonus\nColumn Name Type empId int bonus int Example data:\nempId bonus 2 500 4 2000 Result table\nname bonus John null Dan 500 Brad null Solution (MySQL) SELECT name, bonus FROM Employee AS e LEFT JOIN Bonus AS b ON e.empId = b.empId WHERE bonus \u003c 1000 OR bonus IS NULL; 584. Find Customer Referee Description Given a table customer holding customer information and the referee.\nTable: customer\nColumn Name Type id int name varchar referee_id int Example data:\nid name referee_id 1 Will NULL 2 Jane NULL 3 Alex 2 4 Bill NULL 5 Zack 1 6 Mark 2 Write a query to return the list of customers NOT referred by the person with id 2.\nResult table\nname Will Jane Bill Zack Solution (MySQL) SELECT name FROM customer WHERE referee_id IS NULL OR referee_id \u003c\u003e 2; 586. Customer Placing the Largest Number of Orders Description Query the customer_number from the orders table for the customer who has placed the largest number of orders.\nIt is guaranteed that exactly one customer will have placed more orders than any other customer.\nTable: orders\nColumn Type order_number (PK) int customer_number int order_date date required_date date shipped_date date status char(15) comment char(200) Example data\norder_number customer_number order_date required_date shipped_date status comment 1 1 2017-04-09 2017-04-13 2017-04-12 Closed 2 2 2017-04-15 2017-04-20 2017-04-18 Closed 3 3 2017-04-16 2017-04-25 2017-04-20 Closed 4 3 2017-04-18 2017-04-28 2017-04-25 Closed Result table\ncustomer_number 3 Explanation:\nCustomer ‘3’ has 2 orders, which is greater than customer ‘1’ or ‘2’, who each have only 1 order.\nFollow-up:\nIf more than one customer has the largest number of orders, the query can return all such customer_number.\nSolution (MySQL) SELECT customer_number FROM ( SELECT customer_number, COUNT(*) AS cnt FROM orders GROUP BY customer_number ) AS e ORDER BY e.cnt DESC LIMIT 1; 595. Big Countries Description There is a table World:\nColumn Name Type name varchar continent varchar area int population int gdp int A country is big if it has an area bigger than 3 million sq km or a population of more than 25 million.\nWrite a SQL query to output big countries’ name, population, and area.\nExample data\nname continent area population gdp Afghanistan Asia 652230 25500100 20343000 Albania Europe 28748 2831741 12960000 Algeria Africa 2381741 37100000 188681000 Andorra Europe 468 78115 3712000 Angola Africa 1246700 20609294 100990000 Result table\nname population area Afghanistan 25500100 652230 Algeria 37100000 2381741 Solution (MySQL) SELECT name, population, area FROM World WHERE area \u003e= 3000000 OR population \u003e= 25000000; 596. Classes More Than 5 Students Description There is a table courses with columns student and class.\nList out all classes which have 5 or more students.\nDuplicate students in a class should not be counted.\nExample data\nstudent class A Math B English C Math D Biology E Math F Computer G Math H Math I Math Result table\nclass Math Solution (MySQL) SELECT class FROM courses GROUP BY class HAVING COUNT(DISTINCT student) \u003e= 5; 597. Friend Requests I: Overall Acceptance Rate Description In a social network, people send friend requests and accept others’ requests.\nGiven two tables:\nTable: friend_request\nsender_id send_to_id request_date 1 2 2016-06-01 1 3 2016-06-01 1 4 2016-06-01 2 3 2016-06-02 3 4 2016-06-09 Table: request_accepted\nrequester_id accepter_id accept_date 1 2 2016-06-03 1 3 2016-06-08 2 3 2016-06-08 3 4 2016-06-09 3 4 2016-06-10 Write a query to find the overall acceptance rate of requests rounded to 2 decimals:\naccept_rate = number of unique accepted requests ÷ number of unique requests.\nResult table\naccept_rate 0.80 Note:\nOnly distinct requests and acceptances are counted. If there are no requests, return 0.00. Accepted requests may not appear in the friend_request table. Solution (MySQL) SELECT ROUND(IF(requests = 0, 0, accepts / requests), 2) AS accept_rate FROM ( SELECT COUNT(DISTINCT sender_id, send_to_id) AS requests FROM friend_request ) AS r, ( SELECT COUNT(DISTINCT requester_id, accepter_id) AS accepts FROM request_accepted ) AS a; 603. Consecutive Available Seats Description Several friends at a cinema want to reserve consecutive available seats.\nGiven a table cinema:\nseat_id free 1 1 2 0 3 1 4 1 5 1 seat_id is an auto-increment integer. free is a boolean (1 = free, 0 = occupied). Consecutive available seats are 2 or more consecutive free seats. Return all consecutive available seats ordered by seat_id.\nResult table\nseat_id 3 4 5 Solution (MySQL) SELECT DISTINCT c1.seat_id FROM cinema AS c1 JOIN cinema AS c2 JOIN cinema AS c3 ON c1.seat_id = c2.seat_id + 1 OR c1.seat_id = c3.seat_id - 1 WHERE c1.free = 1 AND c2.free = 1 AND c3.free = 1; -- Alternative solution SELECT DISTINCT c2.seat_id FROM cinema AS c1, cinema AS c2 WHERE c1.free = 1 AND c2.free = 1 AND c1.seat_id = c2.seat_id + 1 UNION SELECT DISTINCT c1.seat_id FROM cinema AS c1, cinema AS c2 WHERE c1.free = 1 AND c2.free = 1 AND c1.seat_id = c2.seat_id + 1 ORDER BY seat_id; 607. Sales Person Description Given three tables: salesperson, company, and orders,\noutput all salesperson names who did not have sales to company 'RED'.\nTable: salesperson\nsales_id name salary commission_rate hire_date 1 John 100000 6 4/1/2006 2 Amy 120000 5 5/1/2010 3 Mark 65000 12 12/25/2008 4 Pam 25000 25 1/1/2005 5 Alex 50000 10 2/3/2007 Table: company\ncom_id name city 1 RED Boston 2 ORANGE New York 3 YELLOW Boston 4 GREEN Austin Table: orders\norder_id order_date com_id sales_id amount 1 1/1/2014 3 4 100000 2 2/1/2014 4 5 5000 3 3/1/2014 1 1 50000 4 4/1/2014 1 4 25000 Result table\nname Amy Mark Alex Explanation:\nOnly ‘John’ and ‘Pam’ have sales to 'RED'. Output all other salesperson names.\nSolution (MySQL) SELECT s.name FROM salesperson AS s WHERE s.sales_id NOT IN ( SELECT o.sales_id FROM orders AS o LEFT JOIN company AS c ON o.com_id = c.com_id WHERE c.name = 'RED' ); 608. Tree Node Description Given a table tree where id is the identifier of a tree node and p_id is its parent node’s id:\nid p_id 1 null 2 1 3 1 4 2 5 2 Each node can be one of three types:\nLeaf: Node with no children. Root: Node with no parent. Inner: Node with parent and at least one child. Write a query to print the node id and its Type. Sort output by id.\nResult table\nid Type 1 Root 2 Inner 3 Leaf 4 Leaf 5 Leaf Explanation:\nNode 1 is Root because its parent is NULL. Node 2 is Inner because it has a parent and children. Nodes 3, 4, and 5 are Leaf because they have a parent and no children. Tree visualization:\ngraph TD\rn1[1]\rn2[2]\rn3[3]\rn4[4]\rn5[5]\rn1 --\u003e n2\rn1 --\u003e n3\rn2 --\u003e n4\rn2 --\u003e n5 Solution (MySQL) SELECT id, CASE WHEN p_id IS NULL THEN 'Root' WHEN p_id IS NOT NULL AND id IN (SELECT DISTINCT p_id FROM tree) THEN 'Inner' ELSE 'Leaf' END AS Type FROM tree; 610. Triangle Judgement Description Given a table triangle with columns x, y, and z representing the lengths of three sides,\nwrite a query to determine whether the sides can form a triangle.\nx y z 13 15 30 10 20 15 Result table\nx y z triangle 13 15 30 No 10 20 15 Yes Explanation:\nA triangle is valid if the sum of any two sides is greater than the third side.\nSolution (MySQL) SELECT x, y, z, CASE WHEN x + y \u003e z AND x + z \u003e y AND y + z \u003e x THEN 'Yes' ELSE 'No' END AS triangle FROM triangle; 612. Shortest Distance in a Plane Description Given a table point_2d with coordinates (x, y) of unique points in a plane,\nwrite a query to find the shortest distance between any two points, rounded to 2 decimals.\nx y -1 -1 0 0 -1 -2 Result table\nshortest 1.00 Explanation:\nThe shortest distance is 1.00, between points (-1,-1) and (-1,-2).\nSolution (MySQL) SELECT ROUND(MIN(dist), 2) AS shortest FROM ( SELECT IF(a.x = b.x AND a.y = b.y, 10000, SQRT(POWER(a.x - b.x, 2) + POWER(a.y - b.y, 2))) AS dist FROM point_2d AS a, point_2d AS b ) AS d; 613. Shortest Distance in a Line Description Given a table point with the x-coordinates of unique points on the x-axis,\nwrite a query to find the shortest distance between any two points.\nx -1 0 2 Result table\nshortest 1 Explanation:\nThe shortest distance is 1, between points -1 and 0.\nSolution (MySQL) SELECT MIN(ABS(a.x - b.x)) AS shortest FROM point AS a, point AS b WHERE a.x != b.x; 619. Biggest Single Number Description Given a table my_numbers containing numbers in column num (may include duplicates),\nwrite a query to find the largest number that appears only once.\nnum 8 8 3 3 1 4 5 6 Result table\nnum 6 Explanation:\nThe number 6 is the largest number that appears exactly once.\nSolution (MySQL) SELECT MAX(num) AS num FROM ( SELECT num FROM my_numbers GROUP BY num HAVING COUNT(num) = 1 ) AS n; 620. Not Boring Movies Description Given a table cinema with columns id, movie, description, and rating,\nwrite a query to output movies with odd-numbered IDs and a description that is not ‘boring’, ordered by rating descending.\nTable: cinema\nid movie description rating 1 War great 3D 8.9 2 Science fiction 8.5 3 irish boring 6.2 4 Ice song Fantacy 8.6 5 House card Interesting 9.1 Expected output\nid movie description rating 5 House card Interesting 9.1 1 War great 3D 8.9 Solution (MySQL) SELECT id, movie, description, rating FROM cinema WHERE id % 2 = 1 AND description \u003c\u003e 'boring' ORDER BY rating DESC; 626. Exchange Seats Description Given a table seat with columns id and student, Mary wants to swap seats for adjacent students.\nIf the number of students is odd, the last student keeps their seat.\nTable: seat\nid student 1 Abbot 2 Doris 3 Emerson 4 Green 5 Jeames Expected output\nid student 1 Doris 2 Abbot 3 Green 4 Emerson 5 Jeames Solution (MySQL) SELECT IF(MOD(id, 2) = 0, id - 1, IF(id \u003c (SELECT MAX(id) FROM seat), id + 1, id)) AS id, student FROM seat ORDER BY id; 627. Swap Salary Description Given a table salary with a sex column containing 'm' or 'f', swap all values: 'm' → 'f' and 'f' → 'm' using a single update statement.\nTable: salary\nid name sex salary 1 A m 2500 2 B f 1500 3 C m 5500 4 D f 500 Expected output after update\nid name sex salary 1 A f 2500 2 B m 1500 3 C f 5500 4 D m 500 Solution (MySQL) UPDATE salary SET sex = IF(sex = 'm', 'f', 'm'); 1045. Customers Who Bought All Products Description Given two tables Customer and Product, write an SQL query to find customers who bought all the products available in the Product table.\nTable: Customer\ncustomer_id product_key 1 5 2 6 3 5 3 6 1 6 Table: Product\nproduct_key 5 6 Expected Output\ncustomer_id 1 3 The customers who bought all the products (5 and 6) are customers with id 1 and 3.\nSolution (MySQL) SELECT customer_id FROM Customer GROUP BY customer_id HAVING SUM(DISTINCT product_key) = ( SELECT SUM(product_key) FROM Product ); 1050. Actors and Directors Who Cooperated At Least Three Times Description Given a table ActorDirector, write an SQL query to find the pairs (actor_id, director_id) where the actor has cooperated with the director at least 3 times.\nTable: ActorDirector\nactor_id director_id timestamp 1 1 0 1 1 1 1 1 2 1 2 3 1 2 4 2 1 5 2 1 6 Expected Output\nactor_id director_id 1 1 The only pair is (1, 1) where they cooperated exactly 3 times.\nSolution (MySQL) SELECT actor_id, director_id FROM ActorDirector GROUP BY actor_id, director_id HAVING COUNT(*) \u003e= 3; 1068. Product Sales Analysis I Description Given two tables Sales and Product, write an SQL query to report all product names of the products in the Sales table along with their selling year and price.\nTable: Sales\nsale_id product_id year quantity price 1 100 2008 10 5000 2 100 2009 12 5000 7 200 2011 15 9000 Table: Product\nproduct_id product_name 100 Nokia 200 Apple 300 Samsung Expected Output\nproduct_name year price Nokia 2008 5000 Nokia 2009 5000 Apple 2011 9000 Solution (MySQL) SELECT DISTINCT P.product_name, S.year, S.price FROM (SELECT DISTINCT product_id, year, price FROM Sales) S INNER JOIN Product AS P USING (product_id); 1069. Product Sales Analysis II Description Given two tables Sales and Product, write an SQL query to report the total quantity sold for every product id.\nTable: Sales\nsale_id product_id year quantity price 1 100 2008 10 5000 2 100 2009 12 5000 7 200 2011 15 9000 Table: Product\nproduct_id product_name 100 Nokia 200 Apple 300 Samsung Expected Output\nproduct_id total_quantity 100 22 200 15 Solution (MySQL) SELECT product_id, SUM(quantity) AS total_quantity FROM Sales GROUP BY product_id; 1070. Product Sales Analysis III Description Given two tables Sales and Product, write an SQL query to select the product id, year, quantity, and price for the first year of every product sold.\nTable: Sales\nsale_id product_id year quantity price 1 100 2008 10 5000 2 100 2009 12 5000 7 200 2011 15 9000 Table: Product\nproduct_id product_name 100 Nokia 200 Apple 300 Samsung Expected Output\nproduct_id first_year quantity price 100 2008 10 5000 200 2011 15 9000 Solution (MySQL) SELECT product_id, year AS first_year, quantity, price FROM Sales WHERE (product_id, year) IN ( SELECT product_id, MIN(year) AS year FROM Sales GROUP BY product_id ); 1075. Project Employees I Description Given two tables Project and Employee, write an SQL query to report the average experience years of all the employees for each project, rounded to 2 digits.\nTable: Project\nproject_id employee_id 1 1 1 2 1 3 2 1 2 4 Table: Employee\nemployee_id name experience_years 1 Khaled 3 2 Ali 2 3 John 1 4 Doe 2 Expected Output\nproject_id average_years 1 2.00 2 2.50 The average experience years for the first project is (3 + 2 + 1) / 3 = 2.00 and for the second project is (3 + 2) / 2 = 2.50.\nSolution (MySQL) SELECT project_id, ROUND(AVG(experience_years), 2) AS average_years FROM Project AS p LEFT JOIN Employee AS e ON p.employee_id = e.employee_id GROUP BY project_id; 1076. Project Employees II Description Given two tables Project and Employee, write an SQL query to report all the projects that have the most employees.\nTable: Project\nproject_id employee_id 1 1 1 2 1 3 2 1 2 4 Table: Employee\nemployee_id name experience_years 1 Khaled 3 2 Ali 2 3 John 1 4 Doe 2 Expected Output\nproject_id 1 The first project has 3 employees while the second one has 2.\nSolution (MySQL) SELECT project_id FROM Project GROUP BY project_id HAVING COUNT(employee_id) \u003e= ( SELECT COUNT(employee_id) AS cnt FROM Project GROUP BY project_id ORDER BY cnt DESC LIMIT 1 ); 1077. Project Employees III Description Given two tables Project and Employee, write an SQL query to report the most experienced employees in each project. In case of a tie, report all employees with the maximum number of experience years.\nTable: Project\nproject_id employee_id 1 1 1 2 1 3 2 1 2 4 Table: Employee\nemployee_id name experience_years 1 Khaled 3 2 Ali 2 3 John 3 4 Doe 2 Expected Output\nproject_id employee_id 1 1 1 3 2 1 Both employees with id 1 and 3 have the most experience among the employees of the first project. For the second project, the employee with id 1 has the most experience.\nSolution (MySQL) SELECT p.project_id, e.employee_id FROM Project AS p JOIN Employee AS e ON p.employee_id = e.employee_id WHERE (p.project_id, e.experience_years) IN ( SELECT p.project_id, MAX(e.experience_years) AS experience_years FROM Project AS p JOIN Employee AS e ON p.employee_id = e.employee_id GROUP BY p.project_id ); 1082. Sales Analysis I Description Given two tables Product and Sales, write an SQL query to report the best seller by total sales price. If there is a tie, report all of them.\nTable: Product\nproduct_id product_name unit_price 1 S8 1000 2 G4 800 3 iPhone 1400 Table: Sales\nseller_id product_id buyer_id sale_date quantity price 1 1 1 2019-01-21 2 2000 1 2 2 2019-02-17 1 800 2 2 3 2019-06-02 1 800 3 3 4 2019-05-13 2 2800 Expected Output\nseller_id 1 3 Both sellers with id 1 and 3 sold products with the most total price of 2800.\nSolution (MySQL) SELECT seller_id FROM Sales GROUP BY seller_id HAVING SUM(price) \u003e= ( SELECT SUM(price) AS total_price FROM Sales GROUP BY seller_id ORDER BY total_price DESC LIMIT 1 ); 1083. Sales Analysis II Description Given two tables Product and Sales, write an SQL query to report the buyers who have bought S8 but not iPhone.\nTable: Product\nproduct_id product_name unit_price 1 S8 1000 2 G4 800 3 iPhone 1400 Table: Sales\nseller_id product_id buyer_id sale_date quantity price 1 1 1 2019-01-21 2 2000 1 2 2 2019-02-17 1 800 2 1 3 2019-06-02 1 800 3 3 3 2019-05-13 2 2800 Expected Output\nbuyer_id 1 The buyer with id 1 bought an S8 but didn’t buy an iPhone. The buyer with id 3 bought both.\nSolution (MySQL) SELECT DISTINCT s.buyer_id FROM Sales AS s JOIN Product AS p ON s.product_id = p.product_id WHERE product_name = 'S8' AND s.buyer_id NOT IN ( SELECT buyer_id FROM Sales AS s JOIN Product AS p ON s.product_id = p.product_id WHERE product_name = 'iPhone' ); 1084. Sales Analysis III Description Given two tables Product and Sales, write an SQL query to report the products that were only sold in spring 2019 (between 2019-01-01 and 2019-03-31 inclusive).\nTable: Product\nproduct_id product_name unit_price 1 S8 1000 2 G4 800 3 iPhone 1400 Table: Sales\nseller_id product_id buyer_id sale_date quantity price 1 1 1 2019-01-21 2 2000 1 2 2 2019-02-17 1 800 2 2 3 2019-06-02 1 800 3 3 4 2019-05-13 2 2800 Expected Output\nproduct_id product_name 1 S8 The product with id 1 was only sold in spring 2019, while the other products were sold after.\nSolution (MySQL) SELECT product_id, product_name FROM Sales INNER JOIN Product USING(product_id) GROUP BY product_id HAVING SUM(IF(sale_date BETWEEN '2019-01-01' AND '2019-03-31', 1, 0)) = SUM(IF(sale_date IS NOT NULL, 1, 0)); 1112. Highest Grade For Each Student Description Given a table Enrollments, write an SQL query to find the highest grade with its corresponding course for each student. In case of a tie, select the course with the smallest course_id. The output must be sorted by increasing student_id.\nTable: Enrollments\nstudent_id course_id grade 2 2 95 2 3 95 1 1 90 1 2 99 3 1 80 3 2 75 3 3 82 Expected Output\nstudent_id course_id grade 1 2 99 2 2 95 3 3 82 Solution (MySQL) SELECT student_id, MIN(course_id) AS course_id, grade FROM Enrollments WHERE (student_id, grade) IN ( SELECT student_id, MAX(grade) FROM Enrollments GROUP BY student_id ) GROUP BY student_id ORDER BY student_id ASC; 1113. Reported Posts Description Given a table Actions, write an SQL query that reports the number of posts reported yesterday for each report reason. Assume today is 2019-07-05.\nTable: Actions\nuser_id post_id action_date action extra 1 1 2019-07-01 view null 1 1 2019-07-01 like null 1 1 2019-07-01 share null 2 4 2019-07-04 view null 2 4 2019-07-04 report spam 3 4 2019-07-04 view null 3 4 2019-07-04 report spam 4 3 2019-07-02 view null 4 3 2019-07-02 report spam 5 2 2019-07-04 view null 5 2 2019-07-04 report racism 5 5 2019-07-04 view null 5 5 2019-07-04 report racism Expected Output\nreport_reason report_count spam 1 racism 2 We only count reports from yesterday (2019-07-04).\nSolution (MySQL) SELECT extra AS report_reason, COUNT(DISTINCT post_id) AS report_count FROM Actions WHERE action_date = '2019-07-04' AND action = 'report' GROUP BY extra; 1126. Active Businesses Description Given a table Events, write an SQL query to find all active businesses.\nAn active business is a business that has more than one event type with occurences greater than the average occurences of that event type among all businesses.\nTable: Events\nbusiness_id event_type occurences 1 reviews 7 3 reviews 3 1 ads 11 2 ads 7 3 ads 6 1 page views 3 2 page views 12 Expected Output\nbusiness_id 1 Average for ‘reviews’, ‘ads’ and ‘page views’ are (7+3)/2=5, (11+7+6)/3=8, (3+12)/2=7.5 respectively.\nBusiness with id 1 has 7 ‘reviews’ events (\u003e5) and 11 ‘ads’ events (\u003e8), so it is an active business.\nSolution (MySQL) SELECT business_id FROM Events e JOIN ( SELECT event_type, AVG(occurences) AS avg_occurences FROM Events GROUP BY event_type ) AS a ON e.event_type = a.event_type WHERE e.occurences \u003e a.avg_occurences GROUP BY e.business_id HAVING COUNT(*) \u003e 1; 1141. User Activity for the Past 30 Days I Description Given a table Activity, write an SQL query to find the daily active user count for a period of 30 days ending 2019-07-27 inclusively.\nA user is considered active on a day if they performed at least one activity on that day.\nTable: Activity\nuser_id session_id activity_date activity_type 1 1 2019-07-20 open_session 1 1 2019-07-20 scroll_down 1 1 2019-07-20 end_session 2 4 2019-07-20 open_session 2 4 2019-07-21 send_message 2 4 2019-07-21 end_session 3 2 2019-07-21 open_session 3 2 2019-07-21 send_message 3 2 2019-07-21 end_session 4 3 2019-06-25 open_session 4 3 2019-06-25 end_session Expected Output\nday active_users 2019-07-20 2 2019-07-21 2 Days with zero active users are not included.\nSolution (MySQL) SELECT activity_date AS day, COUNT(DISTINCT user_id) AS active_users FROM Activity WHERE activity_date BETWEEN '2019-06-28' AND '2019-07-27' GROUP BY day; 1142. User Activity for the Past 30 Days II Description Given a table Activity, write an SQL query to find the average number of sessions per user for a period of 30 days ending 2019-07-27 inclusively, rounded to 2 decimal places.\nOnly count sessions with at least one activity in that time period.\nTable: Activity\nuser_id session_id activity_date activity_type 1 1 2019-07-20 open_session 1 1 2019-07-20 scroll_down 1 1 2019-07-20 end_session 2 4 2019-07-20 open_session 2 4 2019-07-21 send_message 2 4 2019-07-21 end_session 3 2 2019-07-21 open_session 3 2 2019-07-21 send_message 3 2 2019-07-21 end_session 3 5 2019-07-21 open_session 3 5 2019-07-21 scroll_down 3 5 2019-07-21 end_session 4 3 2019-06-25 open_session 4 3 2019-06-25 end_session Expected Output\naverage_sessions_per_user 1.33 Explanation: Users 1 and 2 each had 1 session, user 3 had 2 sessions. Average = (1+1+2)/3 = 1.33.\nSolution (MySQL) SELECT ROUND(IFNULL(SUM(sessions) / COUNT(user_id), 0), 2) AS average_sessions_per_user FROM ( SELECT user_id, COUNT(DISTINCT session_id) AS sessions FROM Activity WHERE activity_date BETWEEN '2019-06-28' AND '2019-07-27' GROUP BY user_id HAVING COUNT(*) \u003e= 1 ) AS u; 1148. Article Views I Description Given a table Views, write an SQL query to find all authors who viewed at least one of their own articles, sorted in ascending order by their id.\nTable: Views\narticle_id author_id viewer_id view_date 1 3 5 2019-08-01 1 3 6 2019-08-02 2 7 7 2019-08-01 2 7 6 2019-08-02 4 7 1 2019-07-22 3 4 4 2019-07-21 3 4 4 2019-07-21 Expected Output\nid 4 7 Solution (MySQL) SELECT DISTINCT author_id AS id FROM Views WHERE author_id = viewer_id ORDER BY author_id; 1164. Product Price at a Given Date Description Given a table Products, write an SQL query to find the prices of all products on 2019-08-16. Assume the price of all products before any change is 10.\nTable: Products\nproduct_id new_price change_date 1 20 2019-08-14 2 50 2019-08-14 1 30 2019-08-15 1 35 2019-08-16 2 65 2019-08-17 3 20 2019-08-18 Expected Output\nproduct_id price 2 50 1 35 3 10 Solution (MySQL) SELECT i.product_id, MAX( IF( i.product_id NOT IN ( SELECT product_id FROM Products WHERE change_date \u003c= DATE '2019-08-16' GROUP BY product_id ), 10, ( SELECT new_price FROM Products WHERE product_id = i.product_id AND change_date = q.max_change_date ) ) ) AS price FROM (SELECT DISTINCT product_id FROM Products) AS i, ( SELECT product_id, MAX(change_date) AS max_change_date FROM Products WHERE change_date \u003c= DATE '2019-08-16' GROUP BY product_id ) AS q GROUP BY i.product_id; 1173. Immediate Food Delivery I Description Given a table Delivery, calculate the percentage of immediate orders. An order is immediate if order_date = customer_pref_delivery_date. Round the result to 2 decimal places.\nTable: Delivery\ndelivery_id customer_id order_date customer_pref_delivery_date 1 1 2019-08-01 2019-08-02 2 5 2019-08-02 2019-08-02 3 1 2019-08-11 2019-08-11 4 3 2019-08-24 2019-08-26 5 4 2019-08-21 2019-08-22 6 2 2019-08-11 2019-08-13 Expected Output\nimmediate_percentage 33.33 Solution (MySQL) SELECT ROUND( SUM(CASE WHEN order_date = customer_pref_delivery_date THEN 1 ELSE 0 END) / COUNT(delivery_id) * 100, 2) AS immediate_percentage FROM Delivery; 1174. Immediate Food Delivery II Description Given a table Delivery, calculate the percentage of immediate orders among the first orders of all customers. An order is immediate if order_date = customer_pref_delivery_date. Round the result to 2 decimal places. Each customer has exactly one first order (earliest order_date).\nTable: Delivery\ndelivery_id customer_id order_date customer_pref_delivery_date 1 1 2019-08-01 2019-08-02 2 2 2019-08-02 2019-08-02 3 1 2019-08-11 2019-08-12 4 3 2019-08-24 2019-08-24 5 3 2019-08-21 2019-08-22 6 2 2019-08-11 2019-08-13 7 4 2019-08-09 2019-08-09 Expected Output\nimmediate_percentage 50.00 Solution (MySQL) SELECT ROUND( SUM(IF(order_date = customer_pref_delivery_date, 1, 0)) / COUNT(*) * 100, 2 ) AS immediate_percentage FROM Delivery WHERE (customer_id, order_date) IN ( SELECT customer_id, MIN(order_date) FROM Delivery GROUP BY customer_id ); 1179. Reformat Department Table Description Given a table Department with id, revenue, and month, reformat it so that each department has one row, and there is a revenue column for each month. The month values are [\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"].\nTable: Department\nid revenue month 1 8000 Jan 2 9000 Jan 3 10000 Feb 1 7000 Feb 1 6000 Mar Expected Output\nid Jan_Revenue Feb_Revenue Mar_Revenue … Dec_Revenue 1 8000 7000 6000 … null 2 9000 null null … null 3 null 10000 null … null Solution (MySQL) SELECT id, SUM(IF(month = 'Jan', revenue, NULL)) AS Jan_Revenue, SUM(IF(month = 'Feb', revenue, NULL)) AS Feb_Revenue, SUM(IF(month = 'Mar', revenue, NULL)) AS Mar_Revenue, SUM(IF(month = 'Apr', revenue, NULL)) AS Apr_Revenue, SUM(IF(month = 'May', revenue, NULL)) AS May_Revenue, SUM(IF(month = 'Jun', revenue, NULL)) AS Jun_Revenue, SUM(IF(month = 'Jul', revenue, NULL)) AS Jul_Revenue, SUM(IF(month = 'Aug', revenue, NULL)) AS Aug_Revenue, SUM(IF(month = 'Sep', revenue, NULL)) AS Sep_Revenue, SUM(IF(month = 'Oct', revenue, NULL)) AS Oct_Revenue, SUM(IF(month = 'Nov', revenue, NULL)) AS Nov_Revenue, SUM(IF(month = 'Dec', revenue, NULL)) AS Dec_Revenue FROM Department GROUP BY id; 1193. Monthly Transactions I Description Given a table Transactions with id, country, state, amount, and trans_date, calculate for each month and country:\nTotal number of transactions Total amount of transactions Number of approved transactions Total amount of approved transactions Table: Transactions\nid country state amount trans_date 121 US approved 1000 2018-12-18 122 US declined 2000 2018-12-19 123 US approved 2000 2019-01-01 124 DE approved 2000 2019-01-07 Expected Output\nmonth country trans_count approved_count trans_total_amount approved_total_amount 2018-12 US 2 1 3000 1000 2019-01 US 1 1 2000 2000 2019-01 DE 1 1 2000 2000 Solution (MySQL) SELECT DATE_FORMAT(trans_date, '%Y-%m') AS month, country, COUNT(*) AS trans_count, SUM(IF(state = 'approved', 1, 0)) AS approved_count, SUM(amount) AS trans_total_amount, SUM(IF(state = 'approved', amount, 0)) AS approved_total_amount FROM Transactions GROUP BY DATE_FORMAT(trans_date, '%Y-%m'), country; 1204. Last Person to Fit in the Elevator Description Given a table Queue with person_id, person_name, weight, and turn, find the last person who can fit in an elevator without exceeding a weight limit of 1000. People enter the elevator in the order of turn.\nTable: Queue\nperson_id person_name weight turn 5 George Washington 250 1 3 John Adams 350 2 6 Thomas Jefferson 400 3 2 Will Johnliams 200 4 4 Thomas Jefferson 175 5 1 James Elephant 500 6 Result Table\nperson_name Thomas Jefferson Explanation: The first three people (250 + 350 + 400 = 1000) fit exactly. The last one in this group is Thomas Jefferson (turn 3).\nSolution (MySQL) SELECT person_name FROM ( SELECT person_name, @total_weight := @total_weight + weight AS total_weight FROM Queue, (SELECT @total_weight := 0) AS tmp ORDER BY turn ) AS t WHERE total_weight \u003c= 1000 ORDER BY total_weight DESC LIMIT 1; 1211. Queries Quality and Percentage Description Given a Queries table with query_name, result, position, and rating, compute for each query_name:\nQuality: Average of the ratio rating / position. Poor query percentage: Percentage of queries with rating \u003c 3. Both metrics should be rounded to 2 decimal places.\nExample Table: Queries\nquery_name result position rating Dog Golden Retriever 1 5 Dog German Shepherd 2 5 Dog Mule 200 1 Cat Shirazi 5 2 Cat Siamese 3 3 Cat Sphynx 7 4 Result Table\nquery_name quality poor_query_percentage Dog 2.50 33.33 Cat 0.66 33.33 Solution (MySQL) SELECT query_name, ROUND(AVG(rating / position), 2) AS quality, ROUND(AVG(IF(rating \u003c 3, 1, 0)) * 100, 2) AS poor_query_percentage FROM Queries GROUP BY query_name; 1212. Team Scores in Football Tournament Description Table: Teams\nColumn Name Type team_id int team_name varchar team_id is the primary key of this table. Each row of this table represents a single football team.\nTable: Matches\nColumn Name Type match_id int host_team int guest_team int host_goals int guest_goals int match_id is the primary key of this table. Each row is a record of a finished match between two different teams. Teams host_team and guest_team are represented by their IDs in the Teams table (team_id) and they scored host_goals and guest_goals respectively.\nPoints are awarded as follows:\nA team receives 3 points if they win a match (score strictly more goals than the opponent). A team receives 1 point if they draw a match (same number of goals as the opponent). A team receives 0 points if they lose a match (score less than the opponent). Write an SQL query that selects the team_id, team_name, and num_points of each team in the tournament after all matches. The result table should be ordered by num_points (descending). In case of a tie, order by team_id (ascending).\nExample:\nTeams table:\nteam_id team_name 10 Leetcode FC 20 NewYork FC 30 Atlanta FC 40 Chicago FC 50 Toronto FC Matches table:\nmatch_id host_team guest_team host_goals guest_goals 1 10 20 3 0 2 30 10 2 2 3 10 50 5 1 4 20 30 1 0 5 50 30 1 0 Result table:\nteam_id team_name num_points 10 Leetcode FC 7 20 NewYork FC 3 50 Toronto FC 3 30 Atlanta FC 1 40 Chicago FC 0 Solution (MySQL) SELECT t.team_id, t.team_name, IFNULL(r.num_points, 0) AS num_points FROM Teams AS t LEFT JOIN ( SELECT team_id, SUM(num_points) AS num_points FROM ( -- Points as host SELECT host_team AS team_id, SUM(CASE WHEN host_goals \u003e guest_goals THEN 3 WHEN host_goals = guest_goals THEN 1 ELSE 0 END) AS num_points FROM Matches GROUP BY host_team UNION ALL -- Points as guest SELECT guest_team AS team_id, SUM(CASE WHEN host_goals \u003c guest_goals THEN 3 WHEN host_goals = guest_goals THEN 1 ELSE 0 END) AS num_points FROM Matches GROUP BY guest_team ) AS u GROUP BY team_id ) AS r ON t.team_id = r.team_id ORDER BY num_points DESC, team_id ASC; 1225. Report Contiguous Dates Description Table: Failed\nColumn Name Type fail_date date Primary key for this table is fail_date. Failed table contains the days of failed tasks.\nTable: Succeeded\nColumn Name Type success_date date Primary key for this table is success_date. Succeeded table contains the days of succeeded tasks.\nA system is running one task every day. Every task is independent of the previous tasks. The tasks can fail or succeed.\nWrite an SQL query to generate a report of period_state for each continuous interval of days in the period from 2019-01-01 to 2019-12-31.\nperiod_state is 'failed' if tasks in this interval failed or 'succeeded' if tasks in this interval succeeded. Interval of days are retrieved as start_date and end_date.\nOrder result by start_date.\nExample:\nFailed table:\nfail_date 2018-12-28 2018-12-29 2019-01-04 2019-01-05 Succeeded table:\nsuccess_date 2018-12-30 2018-12-31 2019-01-01 2019-01-02 2019-01-03 2019-01-06 Result table:\nperiod_state start_date end_date succeeded 2019-01-01 2019-01-03 failed 2019-01-04 2019-01-05 succeeded 2019-01-06 2019-01-06 Solution (MySQL) SELECT period_state, start_date, end_date FROM ( -- Failed intervals SELECT 'failed' AS period_state, f1.fail_date AS start_date, f2.fail_date AS end_date FROM ( SELECT fail_date FROM Failed WHERE fail_date BETWEEN '2019-01-01' AND '2019-12-31' AND DATE_SUB(fail_date, INTERVAL 1 DAY) NOT IN ( SELECT fail_date FROM Failed WHERE fail_date BETWEEN '2019-01-01' AND '2019-12-31' ) ) AS f1, ( SELECT fail_date FROM Failed WHERE fail_date BETWEEN '2019-01-01' AND '2019-12-31' AND DATE_ADD(fail_date, INTERVAL 1 DAY) NOT IN ( SELECT fail_date FROM Failed WHERE fail_date BETWEEN '2019-01-01' AND '2019-12-31' ) ) AS f2 WHERE f1.fail_date \u003c= f2.fail_date GROUP BY f1.fail_date UNION -- Succeeded intervals SELECT 'succeeded' AS period_state, s1.success_date AS start_date, s2.success_date AS end_date FROM ( SELECT success_date FROM Succeeded WHERE success_date BETWEEN '2019-01-01' AND '2019-12-31' AND DATE_SUB(success_date, INTERVAL 1 DAY) NOT IN ( SELECT success_date FROM Succeeded WHERE success_date BETWEEN '2019-01-01' AND '2019-12-31' ) ) AS s1, ( SELECT success_date FROM Succeeded WHERE success_date BETWEEN '2019-01-01' AND '2019-12-31' AND DATE_ADD(success_date, INTERVAL 1 DAY) NOT IN ( SELECT success_date FROM Succeeded WHERE success_date BETWEEN '2019-01-01' AND '2019-12-31' ) ) AS s2 WHERE s1.success_date \u003c= s2.success_date GROUP BY s1.success_date ) AS p ORDER BY start_date; 1241. Number of Comments per Post Description Table: Submissions\nColumn Name Type sub_id int parent_id int There is no primary key for this table; it may have duplicate rows. Each row can be a post or a comment on a post. parent_id is null for posts. parent_id for comments is the sub_id of another post in the table. Write an SQL query to find the number of comments per each post.\nResult table should contain post_id and its corresponding number_of_comments. Sort the result by post_id in ascending order. Count unique comments per post. Treat duplicate posts as one post. Ignore comments on posts that do not exist. Example:\nSubmissions table:\nsub_id parent_id 1 Null 2 Null 1 Null 12 Null 3 1 5 2 3 1 4 1 9 1 10 2 6 7 Result table:\npost_id number_of_comments 1 3 2 2 12 0 Solution (MySQL) SELECT t.post_id, COUNT(DISTINCT s.sub_id) AS number_of_comments FROM ( SELECT DISTINCT sub_id AS post_id FROM Submissions WHERE parent_id IS NULL ) AS t LEFT JOIN Submissions AS s ON t.post_id = s.parent_id GROUP BY t.post_id ORDER BY t.post_id; 1251. Average Selling Price Description Table: Prices\nColumn Name Type product_id int start_date date end_date date price int (product_id, start_date, end_date) is the primary key. Each row indicates the price of the product in the period from start_date to end_date. There are no overlapping periods for the same product. Table: UnitsSold\nColumn Name Type product_id int purchase_date date units int There is no primary key; duplicates may exist. Each row indicates the number of units sold for a product on a particular date. Write an SQL query to find the average selling price for each product.\naverage_price should be rounded to 2 decimal places. Example:\nPrices table:\nproduct_id start_date end_date price 1 2019-02-17 2019-02-28 5 1 2019-03-01 2019-03-22 20 2 2019-02-01 2019-02-20 15 2 2019-02-21 2019-03-31 30 UnitsSold table:\nproduct_id purchase_date units 1 2019-02-25 100 1 2019-03-01 15 2 2019-02-10 200 2 2019-03-22 30 Result table:\nproduct_id average_price 1 6.96 2 16.96 Explanation: Average selling price = Total price of product / Number of units sold Product 1: ((100 * 5) + (15 * 20)) / 115 = 6.96 Product 2: ((200 * 15) + (30 * 30)) / 230 = 16.96 Solution (MySQL) SELECT DISTINCT p.product_id, ROUND(SUM(price * units) / SUM(units), 2) AS average_price FROM Prices AS p JOIN UnitsSold AS u ON p.product_id = u.product_id AND u.purchase_date BETWEEN p.start_date AND p.end_date GROUP BY p.product_id ORDER BY p.product_id; 1264. Page Recommendations Description Table: Friendship\nColumn Name Type user1_id int user2_id int (user1_id, user2_id) is the primary key. Each row indicates a friendship relation between user1_id and user2_id. Table: Likes\nColumn Name Type user_id int page_id int (user_id, page_id) is the primary key. Each row indicates that user_id likes page_id. Write an SQL query to recommend pages to the user with user_id = 1 using the pages that their friends liked.\nDo not recommend pages the user already liked. Return the result table in any order without duplicates. Example:\nFriendship table:\nuser1_id user2_id 1 2 1 3 1 4 2 3 2 4 2 5 6 1 Likes table:\nuser_id page_id 1 88 2 23 3 24 4 56 5 11 6 33 2 77 3 77 6 88 Result table:\nrecommended_page 23 24 56 33 77 Explanation: User 1 is friends with users 2, 3, 4, and 6. Suggested pages: 23 from user 2, 24 from user 3, 56 from user 4, 33 from user 6, 77 from users 2 \u0026 3. Page 88 is not suggested because user 1 already likes it. Solution (MySQL) SELECT DISTINCT page_id AS recommended_page FROM Likes AS l LEFT JOIN Friendship AS f ON f.user2_id = l.user_id WHERE f.user1_id = 1 AND page_id NOT IN (SELECT page_id FROM Likes WHERE user_id = 1) UNION SELECT DISTINCT page_id AS recommended_page FROM Likes AS l LEFT JOIN Friendship AS f ON f.user1_id = l.user_id WHERE f.user2_id = 1 AND page_id NOT IN (SELECT page_id FROM Likes WHERE user_id = 1); 1270. All People Report to the Given Manager Description Table: Employees\nColumn Name Type employee_id int employee_name varchar manager_id int employee_id is the primary key. Each row indicates that the employee reports to their direct manager with manager_id. The head of the company is the employee with employee_id = 1. Write an SQL query to find employee_id of all employees that directly or indirectly report to the head of the company.\nThe indirect relation between managers will not exceed 3 levels. Return the result table in any order without duplicates. Example:\nEmployees table:\nemployee_id employee_name manager_id 1 Boss 1 3 Alice 3 2 Bob 1 4 Daniel 2 7 Luis 4 8 Jhon 3 9 Angela 8 77 Robert 1 Result table:\nemployee_id 2 77 4 7 Explanation: Employees 2 and 77 report directly to the head (1). Employee 4 reports indirectly: 4 → 2 → 1. Employee 7 reports indirectly: 7 → 4 → 2 → 1. Employees 3, 8, 9 do not report to the head directly or indirectly. Solution (MySQL) SELECT DISTINCT e1.employee_id FROM Employees AS e1 INNER JOIN Employees AS e2 INNER JOIN Employees AS e3 ON e1.manager_id = e2.employee_id AND e2.manager_id = e3.employee_id WHERE e1.employee_id \u003c\u003e 1 AND (e1.manager_id = 1 OR e2.manager_id = 1 OR e3.manager_id = 1); 1280. Students and Examinations Description Table: Students\nColumn Name Type student_id int student_name varchar student_id is the primary key. Each row contains the ID and name of a student. Table: Subjects\nColumn Name Type subject_name varchar subject_name is the primary key. Each row contains the name of a subject. Table: Examinations\nColumn Name Type student_id int subject_name varchar There is no primary key; duplicates may exist. Each student takes every subject. Each row indicates that a student attended the exam of a subject. Write an SQL query to find the number of times each student attended each exam.\nOrder the result table by student_id and subject_name. Example:\nStudents table:\nstudent_id student_name 1 Alice 2 Bob 13 John 6 Alex Subjects table:\nsubject_name Math Physics Programming Examinations table:\nstudent_id subject_name 1 Math 1 Physics 1 Programming 2 Programming 1 Physics 1 Math 13 Math 13 Programming 13 Physics 2 Math 1 Math Result table:\nstudent_id student_name subject_name attended_exams 1 Alice Math 3 1 Alice Physics 2 1 Alice Programming 1 2 Bob Math 1 2 Bob Physics 0 2 Bob Programming 1 6 Alex Math 0 6 Alex Physics 0 6 Alex Programming 0 13 John Math 1 13 John Physics 1 13 John Programming 1 Explanation: Alice attended Math 3 times, Physics 2 times, Programming 1 time. Bob attended Math and Programming once, Physics 0 times. Alex did not attend any exam. John attended all subjects once. Solution (MySQL) SELECT s.student_id, s.student_name, u.subject_name, COUNT(e.subject_name) AS attended_exams FROM Students AS s JOIN Subjects AS u LEFT JOIN Examinations AS e ON s.student_id = e.student_id AND u.subject_name = e.subject_name GROUP BY s.student_id, u.subject_name ORDER BY s.student_id, u.subject_name; 1285. Find the Start and End Number of Continuous Ranges Description Table: Logs\nColumn Name Type log_id int log_id is the primary key. Each row contains an ID in the log table. Some IDs may be missing. Write an SQL query to find the start and end number of continuous ranges in the table Logs.\nOrder the result table by start_id. Example:\nLogs table:\nlog_id 1 2 3 7 8 10 Result table:\nstart_id end_id 1 3 7 8 10 10 Explanation: Range 1–3 is continuous. 4–6 is missing. Range 7–8 is continuous. 9 is missing. 10 is a single-element range. Solution (MySQL) -- Solution using subqueries SELECT l1.log_id AS start_id, l2.log_id AS end_id FROM ( SELECT log_id FROM Logs WHERE log_id - 1 NOT IN (SELECT * FROM Logs) ) AS l1, ( SELECT log_id FROM Logs WHERE log_id + 1 NOT IN (SELECT * FROM Logs) ) AS l2 WHERE l1.log_id \u003c= l2.log_id GROUP BY l1.log_id; -- Solution using variables SELECT MIN(log_id) AS start_id, MAX(log_id) AS end_id FROM ( SELECT *, (@id := @id + 1) AS id FROM Logs, (SELECT @id := 0) AS init ) tmp GROUP BY log_id - id; 1294. Weather Type in Each Country Description Table: Countries\nColumn Name Type country_id int country_name varchar country_id is the primary key. Each row contains the ID and name of a country. Table: Weather\nColumn Name Type country_id int weather_state varchar day date (country_id, day) is the primary key. Each row indicates the weather state in a country for one day. Write an SQL query to find the type of weather in each country for November 2019.\nweather_type rules: “Cold” if average weather_state ≤ 15 “Hot” if average weather_state ≥ 25 “Warm” otherwise Return the result table in any order. Example:\nCountries table:\ncountry_id country_name 2 USA 3 Australia 7 Peru 5 China 8 Morocco 9 Spain Weather table:\ncountry_id weather_state day 2 15 2019-11-01 3 -2 2019-11-10 3 0 2019-11-11 3 3 2019-11-12 5 16 2019-11-07 5 18 2019-11-09 5 21 2019-11-23 7 25 2019-11-28 8 25 2019-11-05 8 27 2019-11-15 8 31 2019-11-25 Result table:\ncountry_name weather_type USA Cold Australia Cold Peru Hot China Warm Morocco Hot Explanation: USA average in November = 15 → Cold Australia average = (-2 + 0 + 3)/3 = 0.33 → Cold Peru average = 25 → Hot China average = (16 + 18 + 21)/3 ≈ 18.33 → Warm Morocco average = (25 + 27 + 31)/3 ≈ 27.67 → Hot Spain has no data for November → excluded Solution (MySQL) SELECT c.country_name, CASE WHEN AVG(weather_state) \u003c= 15 THEN 'Cold' WHEN AVG(weather_state) \u003e= 25 THEN 'Hot' ELSE 'Warm' END AS weather_type FROM Weather AS w LEFT JOIN Countries AS c ON c.country_id = w.country_id WHERE day BETWEEN '2019-11-01' AND '2019-11-30' GROUP BY w.country_id; 1303. Find the Team Size Description Table: Employee\nColumn Name Type employee_id int team_id int employee_id is the primary key. Each row contains the ID of an employee and their respective team. Write an SQL query to find the team size of each employee.\nReturn the result table in any order. Example:\nEmployee table:\nemployee_id team_id 1 8 2 8 3 8 4 7 5 9 6 9 Result table:\nemployee_id team_size 1 3 2 3 3 3 4 1 5 2 6 2 Explanation: Employees 1, 2, 3 are in team 8 → team size 3 Employee 4 is in team 7 → team size 1 Employees 5, 6 are in team 9 → team size 2 Solution (MySQL) SELECT e.employee_id, t.team_size FROM Employee AS e JOIN ( SELECT team_id, COUNT(*) AS team_size FROM Employee GROUP BY team_id ) AS t ON e.team_id = t.team_id; 1308. Running Total for Different Genders Description Table: Scores\nColumn Name Type player_name varchar gender varchar day date score_points int (gender, day) is the primary key. Each row indicates a player’s score on a certain day. Gender is 'F' for females team, 'M' for males team. Write an SQL query to find the total score for each gender at each day.\nOrder the result by gender and day. Example:\nScores table:\nplayer_name gender day score_points Aron F 2020-01-01 17 Alice F 2020-01-07 23 Bajrang M 2020-01-07 7 Khali M 2019-12-25 11 Slaman M 2019-12-30 13 Joe M 2019-12-31 3 Jose M 2019-12-18 2 Priya F 2019-12-31 23 Priyanka F 2019-12-30 17 Result table:\ngender day total F 2019-12-30 17 F 2019-12-31 40 F 2020-01-01 57 F 2020-01-07 80 M 2019-12-18 2 M 2019-12-25 13 M 2019-12-30 26 M 2019-12-31 29 M 2020-01-07 36 Explanation: Running total for females: 17 → 40 → 57 → 80 Running total for males: 2 → 13 → 26 → 29 → 36 Solution (MySQL) SELECT s1.gender, s1.day, SUM(s2.score_points) AS total FROM Scores AS s1 JOIN Scores AS s2 ON s1.gender = s2.gender AND s1.day \u003e= s2.day GROUP BY s1.gender, s1.day ORDER BY gender, day; 1321. Restaurant Growth Description Table: Customer\nColumn Name Type customer_id int name varchar visited_on date amount int (customer_id, visited_on) is the primary key. This table contains customer transactions in a restaurant. visited_on is the date of visit. amount is the total paid by a customer. Write an SQL query to compute moving average of customer payments in a 7-day window (current day + 6 previous days).\nReturn visited_on, total amount of that day, and average_amount (rounded to 2 decimals). Order by visited_on. Example:\nCustomer table:\ncustomer_id name visited_on amount 1 Jhon 2019-01-01 100 2 Daniel 2019-01-02 110 3 Jade 2019-01-03 120 4 Khaled 2019-01-04 130 5 Winston 2019-01-05 110 6 Elvis 2019-01-06 140 7 Anna 2019-01-07 150 8 Maria 2019-01-08 80 9 Jaze 2019-01-09 110 1 Jhon 2019-01-10 130 3 Jade 2019-01-10 150 Result table:\nvisited_on amount average_amount 2019-01-07 860 122.86 2019-01-08 840 120.00 2019-01-09 840 120.00 2019-01-10 1000 142.86 Explanation: First moving average (2019-01-01 to 2019-01-07): (100 + 110 + 120 + 130 + 110 + 140 + 150)/7 = 122.86 Second moving average (2019-01-02 to 2019-01-08): 120 Third moving average (2019-01-03 to 2019-01-09): 120 Fourth moving average (2019-01-04 to 2019-01-10): 142.86 Solution (MySQL, user defined variables) SELECT visited_on, amount, average_amount FROM ( SELECT visited_on, @cnt := @cnt + 1 AS cnt, @d7 := @d6, @d6 := @d5, @d5 := @d4, @d4 := @d3, @d3 := @d2, @d2 := @d1, @d1 := amount, @total := @d1 + @d2 + @d3 + @d4 + @d5 + @d6 + @d7 AS amount, ROUND(@total / 7, 2) AS average_amount FROM ( SELECT visited_on, SUM(amount) AS amount FROM Customer GROUP BY visited_on ) AS c, ( SELECT @cnt := 0, @total := 0, @d1 := 0, @d2 := 0, @d3 := 0, @d4 := 0, @d5 := 0, @d6 := 0, @d7 := 0 ) AS t ) AS s WHERE cnt \u003e= 7; 1322. Ads Performance Description Table: Ads\nColumn Name Type ad_id int user_id int action enum (ad_id, user_id) is the primary key. action can be one of: 'Clicked', 'Viewed', 'Ignored'. Each row represents a user’s action on an Ad. A company wants to calculate the performance of each Ad using Click-Through Rate (CTR):\nCTR formula: If total_clicks + total_views = 0, CTR = 0 Otherwise: CTR = total_clicks / (total_clicks + total_views) * 100 Write an SQL query to find CTR for each Ad.\nRound CTR to 2 decimals. Order by CTR descending, and by ad_id ascending in case of tie. Example:\nAds table:\nad_id user_id action 1 1 Clicked 2 2 Clicked 3 3 Viewed 5 5 Ignored 1 7 Ignored 2 7 Viewed 3 5 Clicked 1 4 Viewed 2 11 Viewed 1 2 Clicked Result table:\nad_id ctr 1 66.67 3 50.00 2 33.33 5 0.00 Explanation: ad_id=1: CTR = 2 / (2+1) * 100 = 66.67 ad_id=2: CTR = 1 / (1+2) * 100 = 33.33 ad_id=3: CTR = 1 / (1+1) * 100 = 50.00 ad_id=5: CTR = 0 (no clicks or views) Solution (MySQL) SELECT ad_id, ROUND(IF(clicks + views = 0, 0, clicks / (clicks + views) * 100), 2) AS ctr FROM ( SELECT ad_id, SUM(IF(action='Clicked', 1, 0)) AS clicks, SUM(IF(action='Viewed', 1, 0)) AS views FROM Ads GROUP BY ad_id ) AS a ORDER BY ctr DESC, ad_id ASC; 1327. List the Products Ordered in a Period Description Table: Products\nColumn Name Type product_id int product_name varchar product_category varchar product_id is the primary key. Table: Orders\nColumn Name Type product_id int order_date date unit int product_id is a foreign key to Products. unit is the number of products ordered on order_date. There may be duplicate rows. Write an SQL query to get the names of products with ≥ 100 units ordered in February 2020 and their total amount. Return the result in any order.\nExample:\nProducts table:\nproduct_id product_name product_category 1 Leetcode Solutions Book 2 Jewels of Stringology Book 3 HP Laptop 4 Lenovo Laptop 5 Leetcode Kit T-shirt Orders table:\nproduct_id order_date unit 1 2020-02-05 60 1 2020-02-10 70 2 2020-01-18 30 2 2020-02-11 80 3 2020-02-17 2 3 2020-02-24 3 4 2020-03-01 20 4 2020-03-04 30 4 2020-03-04 60 5 2020-02-25 50 5 2020-02-27 50 5 2020-03-01 50 Result table:\nproduct_name unit Leetcode Solutions 130 Leetcode Kit 100 Explanation: Product 1: 60 + 70 = 130 Product 5: 50 + 50 = 100 Products 2, 3, 4 do not reach 100 units in Feb 2020 Solution (MySQL) SELECT product_name, SUM(unit) AS unit FROM Orders AS o LEFT JOIN Products AS p ON o.product_id = p.product_id WHERE order_date BETWEEN '2020-02-01' AND '2020-02-29' GROUP BY o.product_id HAVING SUM(unit) \u003e= 100; 1336. Number of Transactions per Visit Description Table: Visits\nColumn Name Type user_id int visit_date date (user_id, visit_date) is the primary key. Each row indicates that a user visited the bank on that date. Table: Transactions\nColumn Name Type user_id int transaction_date date amount int There is no primary key; may contain duplicates. Each row indicates a transaction by a user on transaction_date. It is guaranteed that a transaction occurs on a day the user visited (Visits table contains the pair (user_id, transaction_date)). Write an SQL query to find the number of users who did 0, 1, 2, ... transactions per visit. The result should include all counts from 0 up to the maximum transactions by any user in a single visit.\nResult columns:\ntransactions_count: number of transactions in one visit visits_count: number of visits with that many transactions Order by transactions_count.\nExample Visits table:\nuser_id visit_date 1 2020-01-01 2 2020-01-02 12 2020-01-01 19 2020-01-03 1 2020-01-02 2 2020-01-03 1 2020-01-04 7 2020-01-11 9 2020-01-25 8 2020-01-28 Transactions table:\nuser_id transaction_date amount 1 2020-01-02 120 2 2020-01-03 22 7 2020-01-11 232 1 2020-01-04 7 9 2020-01-25 33 9 2020-01-25 66 8 2020-01-28 1 9 2020-01-25 99 Result table:\ntransactions_count visits_count 0 4 1 5 2 0 3 1 Explanation: 0 transactions: 4 visits 1 transaction: 5 visits 2 transactions: 0 visits 3 transactions: 1 visit Solution (MySQL) SELECT (SELECT 0) AS transactions_count, COUNT(*) AS visits_count FROM Visits WHERE (user_id, visit_date) NOT IN ( SELECT user_id, transaction_date FROM Transactions ) UNION SELECT s.transactions_count, IF(t.visits_count IS NULL, 0, t.visits_count) AS visits_count FROM ( SELECT tc AS transactions_count FROM ( SELECT t.user_id, @tc := @tc + 1 AS tc FROM Transactions AS t, (SELECT @tc := 0) AS u ) AS s WHERE tc \u003c= ( SELECT IFNULL(MAX(transactions_count), 0) FROM ( SELECT COUNT(*) AS transactions_count FROM Transactions GROUP BY user_id, transaction_date ) AS t ) ) AS s LEFT JOIN ( SELECT transactions_count, COUNT(*) AS visits_count FROM ( SELECT COUNT(*) AS transactions_count FROM Transactions GROUP BY user_id, transaction_date ) AS t GROUP BY transactions_count ) AS t ON s.transactions_count = t.transactions_count ORDER BY transactions_count;",
    "description": "175. Combine Two Tables Description Table: Person\nColumn Name Type PersonId int FirstName varchar LastName varchar PersonId is the primary key column for this table. Table: Address\nColumn Name Type AddressId int PersonId int City varchar State varchar AddressId is the primary key column for this table. Write a SQL query for a report that provides the following information for each person in the Person table, regardless if there is an address for them:",
    "tags": [],
    "title": "SQL LeetCode",
    "uri": "/practice/leetcode-sql/"
  },
  {
    "breadcrumb": "",
    "content": "1. Introduction Overview of Azure Data Factory Data Integration in Cloud ADF Architecture ADF Components (Pipelines, Activities, Datasets, Linked Services, Triggers) ADF vs SSIS vs Synapse Pipelines 2. ADF Basics Creating an ADF Instance ADF Studio Overview Linked Services Datasets Pipelines Activities Triggers (Schedule, Tumbling Window, Event-based, Manual) 3. Data Movement Copy Activity Integration Runtime (IR) Types Azure IR Self-hosted IR Azure-SSIS IR Data Movement Performance Parallelism \u0026 Batch Copy 4. Data Transformation Mapping Data Flows Wrangling Data Flows Data Flow Debugging Joins, Aggregations, Filters, Derived Columns Surrogate Keys \u0026 Window Functions Data Flow Performance Tuning 5. Orchestration Control Activities (Execute Pipeline, ForEach, If Condition, Until, Switch, Wait) Parameterization Variables in Pipelines Expressions \u0026 Functions Dynamic Content Error Handling \u0026 Retry Policies 6. Data Sources \u0026 Destinations Azure Blob Storage Azure Data Lake Storage (ADLS) Azure SQL Database Azure Synapse Analytics On-Premises SQL Server Cosmos DB REST API SAP, Oracle, Teradata, Snowflake Amazon S3 Google Cloud Storage 7. Integration with Other Azure Services Azure Key Vault Integration Azure Monitor \u0026 Log Analytics Power BI Integration Event Grid \u0026 Event Hub Logic Apps \u0026 Functions Azure Machine Learning 8. CI/CD \u0026 DevOps with ADF Source Control with Git (GitHub, Azure Repos) Branching \u0026 Collaboration Publishing \u0026 ARM Templates Continuous Integration \u0026 Deployment Automated Testing Environment Promotion (Dev → Test → Prod) 9. Security Managed Identity in ADF Service Principals Access Control (RBAC) Data Encryption Network Security (VNET Integration, Private Endpoints) Credential Management with Key Vault 10. Monitoring \u0026 Troubleshooting Pipeline Monitoring Activity Run Details Debugging Failures Alerts \u0026 Notifications Logging with Log Analytics Performance Optimization 11. Advanced Features Incremental Data Loading (Watermarking, Change Data Capture) Slowly Changing Dimensions (SCD) Implementation Parameterized Pipelines \u0026 Templates Global Parameters Data Lineage \u0026 Impact Analysis ADF REST API Managed VNET Data Integration 12. Best Practices Designing Efficient Pipelines Naming Conventions Error Handling Framework Cost Optimization in ADF Scalability \u0026 Performance Tuning Version Control Strategies 13. Real-World Use Cases Building an ETL Pipeline with ADF Data Lake to Synapse ETL Real-Time Event Processing Hybrid Data Integration (On-Prem + Cloud) Machine Learning Model Deployment Orchestration 14. Capstone Project End-to-End Data Warehouse Load using ADF Incremental Data Pipeline with Delta Processing Enterprise Data Integration with ADF and Synapse",
    "description": "1. Introduction Overview of Azure Data Factory Data Integration in Cloud ADF Architecture ADF Components (Pipelines, Activities, Datasets, Linked Services, Triggers) ADF vs SSIS vs Synapse Pipelines 2. ADF Basics Creating an ADF Instance ADF Studio Overview Linked Services Datasets Pipelines Activities Triggers (Schedule, Tumbling Window, Event-based, Manual) 3. Data Movement Copy Activity Integration Runtime (IR) Types Azure IR Self-hosted IR Azure-SSIS IR Data Movement Performance Parallelism \u0026 Batch Copy 4. Data Transformation Mapping Data Flows Wrangling Data Flows Data Flow Debugging Joins, Aggregations, Filters, Derived Columns Surrogate Keys \u0026 Window Functions Data Flow Performance Tuning 5. Orchestration Control Activities (Execute Pipeline, ForEach, If Condition, Until, Switch, Wait) Parameterization Variables in Pipelines Expressions \u0026 Functions Dynamic Content Error Handling \u0026 Retry Policies 6. Data Sources \u0026 Destinations Azure Blob Storage Azure Data Lake Storage (ADLS) Azure SQL Database Azure Synapse Analytics On-Premises SQL Server Cosmos DB REST API SAP, Oracle, Teradata, Snowflake Amazon S3 Google Cloud Storage 7. Integration with Other Azure Services Azure Key Vault Integration Azure Monitor \u0026 Log Analytics Power BI Integration Event Grid \u0026 Event Hub Logic Apps \u0026 Functions Azure Machine Learning 8. CI/CD \u0026 DevOps with ADF Source Control with Git (GitHub, Azure Repos) Branching \u0026 Collaboration Publishing \u0026 ARM Templates Continuous Integration \u0026 Deployment Automated Testing Environment Promotion (Dev → Test → Prod) 9. Security Managed Identity in ADF Service Principals Access Control (RBAC) Data Encryption Network Security (VNET Integration, Private Endpoints) Credential Management with Key Vault 10. Monitoring \u0026 Troubleshooting Pipeline Monitoring Activity Run Details Debugging Failures Alerts \u0026 Notifications Logging with Log Analytics Performance Optimization 11. Advanced Features Incremental Data Loading (Watermarking, Change Data Capture) Slowly Changing Dimensions (SCD) Implementation Parameterized Pipelines \u0026 Templates Global Parameters Data Lineage \u0026 Impact Analysis ADF REST API Managed VNET Data Integration 12. Best Practices Designing Efficient Pipelines Naming Conventions Error Handling Framework Cost Optimization in ADF Scalability \u0026 Performance Tuning Version Control Strategies 13. Real-World Use Cases Building an ETL Pipeline with ADF Data Lake to Synapse ETL Real-Time Event Processing Hybrid Data Integration (On-Prem + Cloud) Machine Learning Model Deployment Orchestration 14. Capstone Project End-to-End Data Warehouse Load using ADF Incremental Data Pipeline with Delta Processing Enterprise Data Integration with ADF and Synapse",
    "tags": [],
    "title": "Azure Data Factory",
    "uri": "/azure_data_factory/"
  },
  {
    "breadcrumb": "Interview Prep",
    "content": "Databricks Medallion Playbook (ADLS CSV → Delta) End‑to‑end guide for incremental loading, data validations/quality, and SCD Type 2 using Databricks + ADLS with Medallion layers: landing → raw (bronze) → rawint (silver‑staging) → curated (silver) → enriched (gold). Uses Delta Lake everywhere.\n0) Principles \u0026 Conventions General\nUse Delta for all managed tables; avoid plain CSV/Parquet beyond landing. Prefer Auto Loader for incremental files; fall back to COPY INTO for one‑off backfills. Treat raw as immutable; never update, only append/reprocess. Store ingestion metadata on every row: ingest_id, ingest_ts, source_file, source_mod_ts, batch_id. Naming\nWorkspaces \u0026 Catalogs: uc_catalog = \u003corg\u003e_prod|stg, uc_schema = \u003cdomain\u003e_\u003clayer\u003e (e.g., sales_raw, sales_curated). Tables: \u003centity\u003e_\u003cgranularity\u003e (snake_case). Examples: orders_header, orders_line. Columns: snake_case (e.g., order_id, valid_from). Notebook files: \u003clayer\u003e-\u003cdomain\u003e-\u003centity\u003e-\u003cpurpose\u003e.nb (kebab-case). Example: raw-sales-orders-ingest.nb. Jobs: \u003clayer\u003e.\u003cdomain\u003e.\u003centity\u003e.\u003cschedule\u003e (e.g., raw.sales.orders.hourly). Checkpoint \u0026 schema locations: /checkpoints/\u003cdomain\u003e/\u003centity\u003e/\u003clayer\u003e/ and /schemas/\u003cdomain\u003e/\u003centity\u003e/. Source control \u0026 CI/CD\nStore notebooks as Databricks Repos (files) with code‑review; use bundle/IaC to deploy. Parameterize via widgets or job parameters; keep secrets in Key Vault / Databricks secrets. Security \u0026 Governance\nUse Unity Catalog (UC) for data access, lineage, tags, and ownership. Assign owners per schema/table; define data classifications via UC tags (e.g., pii=true). 1) ADLS Folder Taxonomy (Sources as CSV) /adls/\u003ccontainer\u003e/landing/\r└── \u003cdomain\u003e/\r└── \u003csource_system\u003e/\r└── \u003centity\u003e/\r├── year=YYYY/ month=MM/ day=DD/ # preferred partitioned landing\r│ └── \u003centity\u003e_\u003cYYYYMMDD_HHMMSS\u003e_\u003cbatchid\u003e.csv\r└── quarantine/ # failed/invalid files Notes\nKeep one entity per folder; include date partitions or place date/batch in filename. Preserve file immutability; never delete sources, only move to quarantine/ when necessary. 2) Medallion Table Layout in Unity Catalog Catalog: \u003corg\u003e_prod (and \u003corg\u003e_stg for non‑prod).\nSchemas (per domain \u0026 layer):\nsales_raw (bronze), sales_rawint (silver‑staging), sales_curated (silver), sales_enriched (gold). Managed Storage: Delta tables stored in UC‑managed locations; external volumes for checkpoints/schemas.\n3) Incremental Ingestion (Landing → Raw/Bronze) Preferred: Auto Loader (cloudFiles)\nHandles new files only, scalable file discovery, schema inference \u0026 evolution. PySpark pattern\nfrom pyspark.sql.functions import input_file_name, current_timestamp source_path = \"/Volumes/\u003ccatalog\u003e/\u003cschema\u003e/\u003cvolume\u003e/landing/sales/erp/orders/\" # or abfss:// chkpt_path = \"/Volumes/\u003ccatalog\u003e/\u003cschema\u003e/\u003cvolume\u003e/checkpoints/sales/orders/raw/\" schema_loc = \"/Volumes/\u003ccatalog\u003e/\u003cschema\u003e/\u003cvolume\u003e/schemas/sales/orders/\" raw_tbl = \"\u003ccatalog\u003e.sales_raw.orders\" (df := (spark.readStream .format(\"cloudFiles\") .option(\"cloudFiles.format\", \"csv\") .option(\"header\", True) .option(\"inferSchema\", True) .option(\"cloudFiles.schemaLocation\", schema_loc) .load(source_path) .withColumn(\"source_file\", input_file_name()) .withColumn(\"ingest_ts\", current_timestamp()) )) \\ .writeStream \\ .format(\"delta\") \\ .option(\"checkpointLocation\", chkpt_path) \\ .trigger(availableNow=True) \\ .toTable(raw_tbl) Alternative: COPY INTO (batch/backfill)\nCOPY INTO \u003ccatalog\u003e.sales_raw.orders FROM 'abfss://\u003ccontainer\u003e@\u003caccount\u003e.dfs.core.windows.net/landing/sales/erp/orders/' FILEFORMAT = CSV FORMAT_OPTIONS ('header'='true') COPY_OPTIONS ('mergeSchema'='true'); Raw Table Schema (append‑only)\nOriginal columns as in source. ingest_ts TIMESTAMP, source_file STRING, batch_id STRING (optional), ingest_id STRING (uuid). 4) Data Quality \u0026 Validations (Raw → RawInt) Approach\nApply schema enforcement, type casting, conformance (trim, null handling, defaulting), and row‑level rules. Route failed rows to quarantine table with error reasons. Delta Expectations (simple built‑in checks)\nCREATE TABLE IF NOT EXISTS \u003ccatalog\u003e.sales_rawint.orders_expectations ( rule_name STRING, rule_expr STRING, violated_count BIGINT, batch_ts TIMESTAMP ); from pyspark.sql.functions import col, length, when, current_timestamp raw = spark.table(\"\u003ccatalog\u003e.sales_raw.orders\") validated = (raw .withColumn(\"order_id\", col(\"order_id\").cast(\"string\")) .withColumn(\"order_date\", col(\"order_date\").cast(\"date\")) .withColumn(\"amount\", col(\"amount\").cast(\"decimal(18,2)\")) ) # Rule examples rules = { \"not_null_order_id\": \"order_id IS NOT NULL\", \"valid_amount\": \"amount \u003e= 0\", \"date_present\": \"order_date IS NOT NULL\" } ok_df = validated.where(\" AND \".join(rules.values())) rejects = validated.exceptAll(ok_df) \\ .withColumn(\"error_reason\", when(col(\"order_id\").isNull(), \"order_id_null\") .when(col(\"amount\") \u003c 0, \"amount_negative\") .otherwise(\"generic_failure\")) \\ .withColumn(\"batch_ts\", current_timestamp()) ok_df.write.mode(\"append\").saveAsTable(\"\u003ccatalog\u003e.sales_rawint.orders\") rejects.write.mode(\"append\").saveAsTable(\"\u003ccatalog\u003e.sales_rawint.orders_rejects\") Best practices\nKeep idempotency: filter by ingest_ts/batch_id when reprocessing. Log metrics per batch to an observability table and dashboard in Lakeview/Grafana. 5) Deduplication \u0026 Keys (RawInt → Curated/Silver) Define business key (e.g., order_id + source_system). Deduplicate by key \u0026 latest source_mod_ts/ingest_ts. CREATE OR REPLACE TABLE \u003ccatalog\u003e.sales_curated.orders_dedup AS SELECT * EXCEPT (rn) FROM ( SELECT *, ROW_NUMBER() OVER (PARTITION BY order_id ORDER BY source_mod_ts DESC, ingest_ts DESC) rn FROM \u003ccatalog\u003e.sales_rawint.orders ) t WHERE rn = 1; 6) SCD Type 2 in Curated (Slowly Changing Dimensions) Target layout\nColumns: business keys + attributes + valid_from TIMESTAMP, valid_to TIMESTAMP, is_current BOOLEAN, optional record_hash. Upsert logic (MERGE)\n-- Staging changes (latest per key) CREATE OR REPLACE TEMP VIEW v_stg AS SELECT * FROM \u003ccatalog\u003e.sales_curated.customer_changes_latest; -- build from rawint MERGE INTO \u003ccatalog\u003e.sales_curated.dim_customer AS tgt USING v_stg AS src ON tgt.customer_id = src.customer_id AND tgt.is_current = true WHEN MATCHED AND tgt.hash \u003c\u003e src.hash THEN -- close old record UPDATE SET tgt.valid_to = current_timestamp(), tgt.is_current = false WHEN NOT MATCHED THEN -- insert new current version INSERT (customer_id, name, city, hash, valid_from, valid_to, is_current) VALUES (src.customer_id, src.name, src.city, src.hash, current_timestamp(), TIMESTAMP('9999-12-31'), true); Hash diff\nSELECT *, sha2(concat_ws('||', coalesce(name,''), coalesce(city,'')), 256) AS hash FROM \u003ccatalog\u003e.sales_rawint.customer_clean; Querying current/historical\n-- Current snapshot SELECT * FROM \u003ccatalog\u003e.sales_curated.dim_customer WHERE is_current = true; -- As-of time travel SELECT * FROM \u003ccatalog\u003e.sales_curated.dim_customer WHERE valid_from \u003c= ts AND valid_to \u003e ts; 7) Enriched/Gold Layer Build fact tables (e.g., fact_orders) and semantic marts (aggregates, BI‑ready models). Maintain surrogate keys via identity columns or mapping tables. Enforce Z‑ORDER on common predicates and OPTIMIZE periodically. OPTIMIZE \u003ccatalog\u003e.sales_curated.dim_customer ZORDER BY (customer_id); VACUUM \u003ccatalog\u003e.sales_curated.dim_customer RETAIN 168 HOURS; -- 7 days 8) Orchestration \u0026 Scheduling Databricks Jobs per domain/entity with dependency graph: raw → rawint → curated → enriched. Triggers: time‑based (cron) or event‑based (file arrival) using Auto Loader. Pass parameters: domain, entity, since_ts for incremental windows. Implement retry with backoff, timeouts, and alerts to email/Slack. 9) Notebooks You’ll Need (by layer) Landing/Raw\nraw-\u003cdomain\u003e-\u003centity\u003e-ingest.nb – Auto Loader stream job. raw-\u003cdomain\u003e-\u003centity\u003e-backfill.nb – COPY INTO for historical loads. RawInt (Validation/Conformance) 3. rawint-\u003cdomain\u003e-\u003centity\u003e-dq.nb – type casting, rules, rejects, metrics.\nCurated (Modeling/SCD2) 4. curated-\u003cdomain\u003e-\u003centity\u003e-dedup.nb – dedup/windowing. 5. curated-\u003cdomain\u003e-\u003centity\u003e-scd2.nb – hash diff + MERGE.\nEnriched 6. enriched-\u003cdomain\u003e-\u003centity\u003e-facts.nb – facts/aggregations. 7. enriched-\u003cdomain\u003e-\u003centity\u003e-optimize.nb – OPTIMIZE/VACUUM.\nOps/Utils 8. utils-params.nb – common params \u0026 widgets. 9. utils-common-fns.nb – reusable functions (ingest metadata, logging, hashing). 10. utils-observability.nb – metrics tables \u0026 dashboards.\n10) Parameters (shared fragment) # utils-params.nb import uuid, datetime as dt dbutils.widgets.text(\"domain\", \"sales\") dbutils.widgets.text(\"entity\", \"orders\") dbutils.widgets.text(\"source_path\", \"/Volumes/.../landing/${domain}/${entity}\") dbutils.widgets.text(\"catalog\", \"\u003corg\u003e_prod\") cfg = {k: dbutils.widgets.get(k) for k in [\"domain\",\"entity\",\"source_path\",\"catalog\"]} cfg[\"ingest_id\"] = str(uuid.uuid4()) 11) Copying from ADLS → Delta (summary) Auto Loader streaming job (preferred) from abfss:// paths with checkpoints \u0026 schema evolution. COPY INTO for bulk loads/backfills. Always write to Delta tables in UC; never keep CSV beyond landing. 12) Performance \u0026 Cost Best Practices Partition large tables by low‑cardinality/time columns (e.g., load_date); avoid over‑partitioning. Periodic OPTIMIZE + Z‑ORDER on query keys. Use availableNow triggers for micro‑batch bulk catch‑up. Use Auto Compaction and Optimize Write (Workspace settings / table properties). 13) Observability \u0026 Lineage Create metrics tables (rows in/out, rejects, rule violations, late files). Build Lakeview dashboards. Use Unity Catalog lineage for end‑to‑end traceability. 14) What You Might Have Missed Quarantine flows with reason codes. PII handling: column‑level masking (views) + UC privileges; store hashes/salts if needed. Idempotent reprocessing by batch_id or ingest_ts windows. Schema evolution policy: allow additive only in raw/rawint; controlled evolution in curated. Backfill strategy separate from incremental. Vacuum retention policy aligned to recovery/RPO. Dev/Stg/Prod isolation via separate catalogs \u0026 volumes. 15) Minimal End‑to‑End Flow (Checklist) Create UC catalog/schemas and ADLS volumes; set grants. Define ADLS landing taxonomy per domain/source/entity. Implement Auto Loader (landing→raw) with checkpoints \u0026 schema locations. Add validations (raw→rawint) + rejects/quarantine + metrics. Deduplicate \u0026 standardize (rawint→curated). Implement SCD2 dimensions and load facts (curated→enriched). Optimize tables and set maintenance (OPTIMIZE, VACUUM). Schedule jobs, retries, and alerts. Monitor with metrics dashboards \u0026 UC lineage. 16) Quick SCD2 Example (Complete) -- 1) Clean latest changes per key from rawint CREATE OR REPLACE TEMP VIEW v_latest AS SELECT * EXCEPT (rn) FROM ( SELECT *, ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY source_mod_ts DESC, ingest_ts DESC) rn FROM \u003ccatalog\u003e.sales_rawint.customer_clean ) t WHERE rn=1; -- 2) Stage with hash diffs CREATE OR REPLACE TEMP VIEW v_stage AS SELECT *, sha2(concat_ws('||', coalesce(name,''), coalesce(city,''), coalesce(status,'')), 256) AS hash FROM v_latest; -- 3) Merge into SCD2 dim MERGE INTO \u003ccatalog\u003e.sales_curated.dim_customer tgt USING v_stage src ON tgt.customer_id = src.customer_id AND tgt.is_current = true WHEN MATCHED AND tgt.hash \u003c\u003e src.hash THEN UPDATE SET tgt.valid_to = current_timestamp(), tgt.is_current = false WHEN NOT MATCHED THEN INSERT (customer_id, name, city, status, hash, valid_from, valid_to, is_current) VALUES (src.customer_id, src.name, src.city, src.status, src.hash, current_timestamp(), TIMESTAMP('9999-12-31'), true); 17) Housekeeping Snippets -- Optimize \u0026 clean OPTIMIZE \u003ccatalog\u003e.sales_curated.dim_customer ZORDER BY (customer_id); VACUUM \u003ccatalog\u003e.sales_curated.dim_customer RETAIN 168 HOURS; -- Table properties (enable auto tuning) ALTER TABLE \u003ccatalog\u003e.sales_curated.dim_customer SET TBLPROPERTIES ( 'delta.autoOptimize.optimizeWrite' = 'true', 'delta.autoOptimize.autoCompact' = 'true' ); Final Notes The correct term is Medallion (not medtallion). The rawint layer is optional; it’s useful to isolate validation from business modeling. Consider DLT for declarative pipelines (expectations + lineage) if you want fewer custom notebooks; patterns above map 1:1 to DLT.",
    "description": "Databricks Medallion Playbook (ADLS CSV → Delta) End‑to‑end guide for incremental loading, data validations/quality, and SCD Type 2 using Databricks + ADLS with Medallion layers: landing → raw (bronze) → rawint (silver‑staging) → curated (silver) → enriched (gold). Uses Delta Lake everywhere.\n0) Principles \u0026 Conventions General\nUse Delta for all managed tables; avoid plain CSV/Parquet beyond landing. Prefer Auto Loader for incremental files; fall back to COPY INTO for one‑off backfills. Treat raw as immutable; never update, only append/reprocess. Store ingestion metadata on every row: ingest_id, ingest_ts, source_file, source_mod_ts, batch_id. Naming",
    "tags": [],
    "title": "Data Pipeline",
    "uri": "/interviewprep/pipeline1/"
  },
  {
    "breadcrumb": "SQL",
    "content": "This guide covers the essential DDL (Data Definition Language) commands used for defining and managing database structures, including creating, modifying, and deleting tables.\n1. CREATE – Creating Tables -- Create a new table called persons -- with columns: id, person_name, birth_date, and phone CREATE TABLE persons ( id INT NOT NULL, person_name VARCHAR(50) NOT NULL, birth_date DATE, phone VARCHAR(15) NOT NULL, CONSTRAINT pk_persons PRIMARY KEY (id) ); 2. ALTER – Modifying Table Structure -- Add a new column called email to the persons table ALTER TABLE persons ADD email VARCHAR(50) NOT NULL; -- Remove the column phone from the persons table ALTER TABLE persons DROP COLUMN phone; 3. DROP – Removing Tables -- Delete the table persons from the database DROP TABLE persons;",
    "description": "This guide covers the essential DDL (Data Definition Language) commands used for defining and managing database structures, including creating, modifying, and deleting tables.\n1. CREATE – Creating Tables -- Create a new table called persons -- with columns: id, person_name, birth_date, and phone CREATE TABLE persons ( id INT NOT NULL, person_name VARCHAR(50) NOT NULL, birth_date DATE, phone VARCHAR(15) NOT NULL, CONSTRAINT pk_persons PRIMARY KEY (id) ); 2. ALTER – Modifying Table Structure -- Add a new column called email to the persons table ALTER TABLE persons ADD email VARCHAR(50) NOT NULL; -- Remove the column phone from the persons table ALTER TABLE persons DROP COLUMN phone; 3. DROP – Removing Tables -- Delete the table persons from the database DROP TABLE persons;",
    "tags": [],
    "title": "DDL",
    "uri": "/sql/ddl/"
  },
  {
    "breadcrumb": "ADB",
    "content": "Column Selection \u0026 Manipulation 1. Different Methods to Select Columns In PySpark, you can select specific columns in multiple ways:\n# Using col() function df.select(col(\"Name\")).show() # Using column() function df.select(column(\"Age\")).show() # Directly using string name df.select(\"Salary\").show() 2. Selecting Multiple Columns Together You can combine different methods to select multiple columns:\n# multiple column df2 = df.select(\"ID\", \"Name\", col(\"Salary\"), column(\"Department\"), df.Phone) df2.show() 3. Listing All Columns in a DataFrame To get a list of all the column names:\n# get all column name df.columns 4. Renaming Columns with alias() You can rename columns using the alias() method:\ndf.select( col(\"Name\").alias('EmployeeName'), # Rename \"Name\" to \"EmployeeName\" col(\"Salary\").alias('EmployeeSalary'), # Rename \"Salary\" to \"EmployeeSalary\" column(\"Department\"), # Select \"Department\" df.Joining_Date # Select \"Joining_Date\" ).show() 5. Using selectExpr() for Concise Column Selection selectExpr() allows you to use SQL expressions directly and rename columns concisely:\ndf.selectExpr(\"Name as EmployeeName\", \"Salary as EmployeeSalary\", \"Department\").show() Summary Use col(), column(), or string names to select columns. Use expr() and selectExpr() for SQL-like expressions and renaming. Use alias() to rename columns. Get the list of columns using df.columns. Adding, Renaming, and Dropping Columns 1. Adding New Columns with withColumn() In PySpark, the withColumn() function is widely used to add new columns to a DataFrame. You can either assign a constant value using lit() or perform transformations using existing columns.\nAdd a constant value column: newdf = df.withColumn(\"NewColumn\", lit(1)) Add a column based on an expression: newdf = df.withColumn(\"withinCountry\", expr(\"Country == 'India'\")) This function allows adding multiple columns, including calculated ones:\nExample:\nAssign a constant value with lit(). Perform calculations using existing columns like multiplying values. 2. Renaming Columns with withColumnRenamed() PySpark provides the withColumnRenamed() method to rename columns. This is especially useful when you want to change the names for clarity or to follow naming conventions:\nRenaming a column: python\nnew_df = df.withColumnRenamed(\"oldColumnName\", \"newColumnName\") Handling column names with special characters or spaces: If a column has special characters or spaces, you need to use backticks (`) to escape it:\npython\nnewdf.select(\"`New Column Name`\").show() 3. Dropping Columns with drop() To remove unwanted columns, you can use the drop() method:\nDrop a single column: df2 = df.drop(\"Country\") Drop multiple columns: df2 = df.drop(\"Country\", \"Region\") Dropping columns creates a new DataFrame, and the original DataFrame remains unchanged.\n4. Immutability of DataFrames In Spark, DataFrames are immutable by nature. This means that after creating a DataFrame, its contents cannot be changed. All transformations like adding, renaming, or dropping columns result in a new DataFrame, keeping the original one intact.\nFor instance, dropping columns creates a new DataFrame without altering the original: newdf = df.drop(\"ItemType\", \"SalesChannel\") This immutability ensures data consistency and supports Spark’s parallel processing, as transformations do not affect the source data.\nKey Points Use withColumn() for adding columns, with lit() for constant values and expressions for computed values. Use withColumnRenamed() to rename columns and backticks for special characters or spaces. Use drop() to remove one or more columns. DataFrames are immutable in Spark—transformations result in new DataFrames, leaving the original unchanged. Data Types, Filtering, and Unique Values Here’s a structured set of notes with code to cover changing data types, filtering data, and handling unique/distinct values in PySpark using the employee data:\n1. Changing Data Types (Schema Transformation) In PySpark, you can change the data type of a column using the cast() method. This is helpful when you need to convert data types for columns like Salary or Phone.\nfrom pyspark.sql.functions import col # Change the 'Salary' column from integer to double df = df.withColumn(\"Salary\", col(\"Salary\").cast(\"double\")) # Convert 'Phone' column to string df = df.withColumn(\"Phone\", col(\"Phone\").cast(\"string\")) df.printSchema() 2. Filtering Data You can filter rows based on specific conditions. For instance, to filter employees with a salary greater than 50,000:\n# Filter rows where Salary is greater than 50,000 filtered_df = df.filter(col(\"Salary\") \u003e 50000) filtered_df.show() # Filtering rows where Age is not null filtered_df = df.filter(df[\"Age\"].isNotNull()) filtered_df.show() 3. Multiple Filters (Chaining Conditions) You can also apply multiple conditions using \u0026 or | (AND/OR) to filter data. For example, finding employees over 30 years old and in the IT department:\n# Filter rows where Age \u003e 30 and Department is 'IT' filtered_df = df.filter((df[\"Age\"] \u003e 30) \u0026 (df[\"Department\"] == \"IT\")) filtered_df.show() 4. Filtering on Null or Non-Null Values Filtering based on whether a column has NULL values or not is crucial for data cleaning:\n# Filter rows where 'Address' is NULL filtered_df = df.filter(df[\"Address\"].isNull()) filtered_df.show() # Filter rows where 'Email' is NOT NULL filtered_df = df.filter(df[\"Email\"].isNotNull()) filtered_df.show() 5. Handling Unique or Distinct Data To get distinct rows or unique values from your dataset:\n# Get distinct rows from the entire DataFrame unique_df = df.distinct() unique_df.show() # Get distinct values from the 'Department' column unique_departments_df = df.select(\"Department\").distinct() unique_departments_df.show() To remove duplicates based on specific columns, such as Email or Phone, use dropDuplicates():\n# Remove duplicates based on 'Email' column unique_df = df.dropDuplicates([\"Email\"]) unique_df.show() # Remove duplicates based on both 'Phone' and 'Email' unique_df = df.dropDuplicates([\"Phone\", \"Email\"]) unique_df.show() 6. Counting Distinct Values You can count distinct values in a particular column, or combinations of columns:\n# Count distinct values in the 'Department' column distinct_count_department = df.select(\"Department\").distinct().count() print(\"Distinct Department Count:\", distinct_count_department) # Count distinct combinations of 'Department' and 'Performance_Rating' distinct_combinations_count = df.select(\"Department\", \"Performance_Rating\").distinct().count() print(\"Distinct Department and Performance Rating Combinations:\", distinct_combinations_count) This set of operations will help you efficiently manage and transform your data in PySpark, ensuring data integrity and accuracy for your analysis!\nMastering PySpark DataFrame Operations Changing Data Types: Easily modify column types using .cast(). E.g., change ‘Salary’ to double or ‘Phone’ to string for better data handling. Filtering Data: Use .filter() or .where() to extract specific rows. For example, filter employees with a salary over 50,000 or non-null Age. Multiple Conditions: Chain filters with \u0026 and | to apply complex conditions, such as finding employees over 30 in the IT department. Handling NULLs: Use .isNull() and .isNotNull() to filter rows with missing or available values, such as missing addresses or valid emails. Unique/Distinct Values: Use .distinct() to get unique rows or distinct values in a column. Remove duplicates based on specific fields like Email or Phone using .dropDuplicates(). Count Distinct Values: Count distinct values in one or multiple columns to analyze data diversity, such as counting unique departments or combinations of Department and Performance_Rating.",
    "description": "Column Selection \u0026 Manipulation 1. Different Methods to Select Columns In PySpark, you can select specific columns in multiple ways:\n# Using col() function df.select(col(\"Name\")).show() # Using column() function df.select(column(\"Age\")).show() # Directly using string name df.select(\"Salary\").show() 2. Selecting Multiple Columns Together You can combine different methods to select multiple columns:\n# multiple column df2 = df.select(\"ID\", \"Name\", col(\"Salary\"), column(\"Department\"), df.Phone) df2.show() 3. Listing All Columns in a DataFrame To get a list of all the column names:",
    "tags": [],
    "title": "DF Operations",
    "uri": "/azure_data_bricks/df-operations/"
  },
  {
    "breadcrumb": "Practice Sets",
    "content": "1. Write a PySpark query using below input to get below output? Input name Hobbies Alice Badminton, Tennis Bob Tennis, Cricket Julie Cricket, Carroms Output name Hobbies Alice Badminton Alice Tennis Bob Tennis Bob Cricket Julie Cricket Julie Carroms from pyspark.sql import SparkSession from pyspark.sql.functions import split, explode # Create Spark session spark = SparkSession.builder.appName(\"HobbiesSplit\").getOrCreate() # Sample data data = [ (\"Alice\", \"Badminton, Tennis\"), (\"Bob\", \"Tennis, Cricket\"), (\"Julie\", \"Cricket, Carroms\") ] columns = [\"name\", \"Hobbies\"] # Create DataFrame df = spark.createDataFrame(data, columns) Solution # Split and explode hobbies result = df.withColumn(\"Hobbies\", explode(split(df[\"Hobbies\"], \",\\s*\"))) result.show(truncate=False) 2. Write a PySpark query using the below input to get the required output. Input City1 City2 City3 Goa null Ap null AP null null null Bglr Expected Output Result Goa AP Bglr from pyspark.sql import functions as F # Sample Input Data data = [ (\"Goa\", None, \"Ap\"), (None, \"AP\", None), (None, None, \"Bglr\") ] df = spark.createDataFrame(data, [\"City1\", \"City2\", \"City3\"]) Solution # Use COALESCE to pick the first non-null city result_df = df.select( F.coalesce(\"City1\", \"City2\", \"City3\").alias(\"Result\") ) result_df.show() 3. Question – Student Result Classification You have a PySpark DataFrame df containing student marks in multiple subjects:\nId Name Subject Mark 1 Steve SQL 90 1 Steve PySpark 100 2 David SQL 70 2 David PySpark 60 3 John SQL 30 3 John PySpark 20 4 Shree SQL 50 Write a PySpark query to calculate the average percentage for each student across all subjects.\nBased on the percentage, assign a Result column using the following rules:\nDistinction → Percentage ≥ 70 First Class → 60 ≤ Percentage \u003c 70 Second Class → 50 ≤ Percentage \u003c 60 Third Class → 40 ≤ Percentage \u003c 50 Fail → Percentage \u003c 40 Show the final DataFrame with columns: Id, Name, Percentage, Result.\nfrom pyspark.sql import SparkSession from pyspark.sql.functions import col, avg, when spark = SparkSession.builder.appName(\"StudentMarks\").getOrCreate() # Sample DataFrame data = [ (1, 'Steve', 'SQL', 90), (1, 'Steve', 'PySpark', 100), (2, 'David', 'SQL', 70), (2, 'David', 'PySpark', 60), (3, 'John', 'SQL', 30), (3, 'John', 'PySpark', 20), (4, 'Shree', 'SQL', 50) ] columns = [\"Id\", \"Name\", \"Subject\", \"Mark\"] df = spark.createDataFrame(data, columns) Solution # Calculate average percentage per student df_per = df.groupBy(\"Id\", \"Name\").agg(avg(\"Mark\").alias(\"Percentage\")) # Assign Result based on percentage df_final = df_per.select( '*', when(col(\"Percentage\") \u003e= 70, \"Distinction\") .when((col(\"Percentage\") \u003c 70) \u0026 (col(\"Percentage\") \u003e= 60), \"First Class\") .when((col(\"Percentage\") \u003c 60) \u0026 (col(\"Percentage\") \u003e= 50), \"Second Class\") .when((col(\"Percentage\") \u003c 50) \u0026 (col(\"Percentage\") \u003e= 40), \"Third Class\") .otherwise(\"Fail\").alias(\"Result\") ) df_final.show() 4. Department-wise Employee Ranking Question You are given the following employee dataset:\nEmpId EmpName Salary DeptName 1 A 1000 IT 2 B 1500 IT 3 C 2500 IT 4 D 3000 HR 5 E 2000 HR 6 F 1000 HR 7 G 4000 Sales 8 H 4000 Sales 9 I 1000 Sales 10 J 2000 Sales You need to:\nAssign a dense rank to employees based on their salary within each department, sorted in descending order. Display only those employees who hold the 2nd highest salary in each department. from pyspark.sql import SparkSession from pyspark.sql.functions import * from pyspark.sql.window import * spark = SparkSession.builder.appName(\"EmployeeRanking\").getOrCreate() # Input Data data1 = [ (1,\"A\",1000,\"IT\"), (2,\"B\",1500,\"IT\"), (3,\"C\",2500,\"IT\"), (4,\"D\",3000,\"HR\"), (5,\"E\",2000,\"HR\"), (6,\"F\",1000,\"HR\"), (7,\"G\",4000,\"Sales\"), (8,\"H\",4000,\"Sales\"), (9,\"I\",1000,\"Sales\"), (10,\"J\",2000,\"Sales\") ] schema1 = [\"EmpId\",\"EmpName\",\"Salary\",\"DeptName\"] df = spark.createDataFrame(data1, schema1) Solution # Apply dense_rank() by department based on descending salary df_rank = df.select( '*', dense_rank().over(Window.partitionBy(df.DeptName).orderBy(df.Salary.desc())).alias('rank') ) # Filter employees with 2nd highest salary df_second_highest = df_rank.filter(df_rank.rank == 2) df_second_highest.show() 5. Employee Salary Report by Department and Manager Question You are given two DataFrames:\n1️⃣ Employee Salary Information EmpId EmpName Mgrid deptid salarydt salary 100 Raj null 1 01-04-23 50000 200 Joanne 100 1 01-04-23 4000 200 Joanne 100 1 13-04-23 4500 200 Joanne 100 1 14-04-23 4020 2️⃣ Department Information deptid deptname 1 IT 2 HR Task Convert the salarydt column to date type. Join both DataFrames to add department name. Perform a self-join to get each employee’s manager name (based on Mgrid = EmpId). Group the data to calculate the total salary paid for each employee, month, and year within their department, along with their manager’s name. from pyspark.sql import SparkSession from pyspark.sql.functions import col, to_date, year, date_format spark = SparkSession.builder.appName(\"EmployeeSalaryReport\").getOrCreate() # Employee Salary DataFrame data1 = [ (100, \"Raj\", None, 1, \"01-04-23\", 50000), (200, \"Joanne\", 100, 1, \"01-04-23\", 4000), (200, \"Joanne\", 100, 1, \"13-04-23\", 4500), (200, \"Joanne\", 100, 1, \"14-04-23\", 4020) ] schema1 = [\"EmpId\", \"EmpName\", \"Mgrid\", \"deptid\", \"salarydt\", \"salary\"] df_salary = spark.createDataFrame(data1, schema1) # Department DataFrame data2 = [(1, \"IT\"), (2, \"HR\")] schema2 = [\"deptid\", \"deptname\"] df_dept = spark.createDataFrame(data2, schema2) Solution # Step 1: Convert salarydt to Date Type df = df_salary.withColumn(\"Newsaldt\", to_date(\"salarydt\", \"dd-MM-yy\")) # Step 2: Join department data df1 = df.join(df_dept, [\"deptid\"]) # Step 3: Self join to get Manager Name df2 = ( df1.alias(\"a\") .join(df1.alias(\"b\"), col(\"a.Mgrid\") == col(\"b.EmpId\"), \"left\") .select( col(\"a.deptname\"), col(\"b.EmpName\").alias(\"ManagerName\"), col(\"a.EmpName\"), col(\"a.Newsaldt\"), col(\"a.salary\") ) ) # Step 4: Group by Dept, Manager, Employee, Year, and Month df3 = ( df2.groupBy( \"deptname\", \"ManagerName\", \"EmpName\", year(\"Newsaldt\").alias(\"Year\"), date_format(\"Newsaldt\", \"MMMM\").alias(\"Month\") ) .sum(\"salary\") ) df3.show() 6. Month-over-Month Sales Growth Calculation Question You are given the following sales order dataset:\nSOId SODate ItemId ItemQty ItemValue 1 2024-01-01 I1 10 1000 2 2024-01-15 I2 20 2000 3 2024-02-01 I3 10 1500 4 2024-02-15 I4 20 2500 5 2024-03-01 I5 30 3000 6 2024-03-10 I6 40 3500 7 2024-03-20 I7 20 2500 8 2024-03-30 I8 10 1000 Task Convert the SODate column into a DateType.\nExtract Month and Year from the sales order date.\nCalculate total sales per month (sum(ItemValue)).\nUse a window function (lag) to get the previous month’s total sales.\nCompute the month-over-month sales growth percentage using:\n[ Growth% = \\frac{(CurrentMonth - PrevMonth) * 100}{CurrentMonth} ]\nfrom pyspark.sql import SparkSession from pyspark.sql.types import * from pyspark.sql.functions import * from pyspark.sql.window import * spark = SparkSession.builder.appName(\"MonthlySalesGrowth\").getOrCreate() # Sample Data data = [ (1, '2024-01-01', \"I1\", 10, 1000), (2, \"2024-01-15\", \"I2\", 20, 2000), (3, \"2024-02-01\", \"I3\", 10, 1500), (4, \"2024-02-15\", \"I4\", 20, 2500), (5, \"2024-03-01\", \"I5\", 30, 3000), (6, \"2024-03-10\", \"I6\", 40, 3500), (7, \"2024-03-20\", \"I7\", 20, 2500), (8, \"2024-03-30\", \"I8\", 10, 1000) ] schema = [\"SOId\", \"SODate\", \"ItemId\", \"ItemQty\", \"ItemValue\"] df1 = spark.createDataFrame(data, schema) Solution # Step 1: Convert SODate to DateType df1 = df1.withColumn(\"SODate\", df1.SODate.cast(DateType())) # Step 2: Extract Month, Year df2 = df1.select( month(\"SODate\").alias(\"Month\"), year(\"SODate\").alias(\"Year\"), col(\"ItemValue\") ) # Step 3: Aggregate monthly total sales df3 = df2.groupBy(\"Month\", \"Year\").agg(sum(\"ItemValue\").alias(\"TotalSale\")) # Step 4: Use lag() to find previous month's sale df4 = df3.select( '*', lag(\"TotalSale\").over(Window.orderBy(\"Month\", \"Year\")).alias(\"PrevSale\") ) # Step 5: Calculate Month-over-Month growth % df_final = df4.select( \"*\", ((col(\"TotalSale\") - col(\"PrevSale\")) * 100 / col(\"TotalSale\")).alias(\"Growth_Percentage\") ) df_final.show() 7. Average Machine Processing Time Question You are given the following dataset representing machine process activity logs:\nMachine_id processid activityid timestamp 0 0 start 0.712 0 0 end 1.520 0 1 start 3.140 0 1 end 4.120 1 0 start 0.550 1 0 end 1.550 1 1 start 0.430 1 1 end 1.420 2 0 start 4.100 2 0 end 4.512 2 1 start 2.500 2 1 end 5.000 Task Identify start and end times for each processid and Machine_id. Calculate the processing duration (endtime - starttime) for each process. Find the average processing time per machine across all its processes. PySpark Solution from pyspark.sql import SparkSession from pyspark.sql.types import * from pyspark.sql.functions import * spark = SparkSession.builder.appName(\"MachineProcessingTime\").getOrCreate() # Step 1: Input Data data = [ (0, 0, 'start', 0.712), (0, 0, 'end', 1.520), (0, 1, 'start', 3.140), (0, 1, 'end', 4.120), (1, 0, 'start', 0.550), (1, 0, 'end', 1.550), (1, 1, 'start', 0.430), (1, 1, 'end', 1.420), (2, 0, 'start', 4.100), (2, 0, 'end', 4.512), (2, 1, 'start', 2.500), (2, 1, 'end', 5.000) ] schema = [\"Machine_id\", \"processid\", \"activityid\", \"timestamp\"] df1 = spark.createDataFrame(data, schema) Solution # Step 2: Split start and end timestamps df2 = df1.select( df1.Machine_id, df1.processid, when(df1.activityid == 'start', df1.timestamp).alias('starttime'), when(df1.activityid == 'end', df1.timestamp).alias('endtime') ) # Step 3: Aggregate start and end timestamps per process df3 = df2.groupBy(\"Machine_id\", \"processid\").agg( max(\"starttime\").alias(\"starttime\"), max(\"endtime\").alias(\"endtime\") ) # Step 4: Calculate processing duration df4 = df3.select( df3.Machine_id, (df3.endtime - df3.starttime).alias(\"diff\") ) # Step 5: Calculate average processing time per machine df_final = df4.groupBy(\"Machine_id\").agg( avg(\"diff\").alias(\"avg_processing_time\") ) df_final.show() 8. Combine Multiple Skills into a Single Column Question You are given the following employee skill dataset:\nEmpId EmpName Skill 1 John ADF 1 John ADB 1 John PowerBI 2 Joanne ADF 2 Joanne SQL 2 Joanne Crystal Report 3 Vikas ADF 3 Vikas SQL 3 Vikas SSIS 4 Monu SQL 4 Monu SSIS 4 Monu SSAS 4 Monu ADF Task Group the records by employee name. Collect all skills of each employee into a single list. Concatenate those skills into a single comma-separated string under a new column Skills. PySpark Solution from pyspark.sql import SparkSession from pyspark.sql.types import * from pyspark.sql.functions import collect_list, concat_ws spark = SparkSession.builder.appName(\"EmployeeSkills\").getOrCreate() # Step 1: Create DataFrame data = [ (1, 'John', 'ADF'), (1, 'John', 'ADB'), (1, 'John', 'PowerBI'), (2, 'Joanne', 'ADF'), (2, 'Joanne', 'SQL'), (2, 'Joanne', 'Crystal Report'), (3, 'Vikas', 'ADF'), (3, 'Vikas', 'SQL'), (3, 'Vikas', 'SSIS'), (4, 'Monu', 'SQL'), (4, 'Monu', 'SSIS'), (4, 'Monu', 'SSAS'), (4, 'Monu', 'ADF') ] schema = [\"EmpId\", \"EmpName\", \"Skill\"] df1 = spark.createDataFrame(data, schema) Solution # Step 2: Collect all skills per employee into a list df2 = df1.groupBy(\"EmpName\").agg(collect_list(\"Skill\").alias(\"Skill\")) # Step 3: Convert list to comma-separated string df3 = df2.select(\"EmpName\", concat_ws(\",\", \"Skill\").alias(\"Skills\")) df3.show(truncate=False) 9. Split Phone Numbers into STD Code and Phone Number Question You are given the following dataset containing employee names and their phone numbers:\nname phone Joanne 040-20215632 Tom 044-23651023 John 086-12456782 Each phone number is stored in the format:\n\u003cSTD Code\u003e-\u003cPhone Number\u003e\nTask Split the phone column into two new columns:\nstd_code → Contains the STD code before the hyphen. phone_num → Contains the actual phone number after the hyphen. Display the final DataFrame.\nfrom pyspark.sql import SparkSession from pyspark.sql.functions import split spark = SparkSession.builder.appName(\"PhoneNumberSplit\").getOrCreate() # Step 1: Create DataFrame data = [ ('Joanne', \"040-20215632\"), ('Tom', \"044-23651023\"), ('John', \"086-12456782\") ] schema = [\"name\", \"phone\"] df = spark.createDataFrame(data, schema) Solution # Step 2: Split phone number into STD code and phone number df = df.withColumn(\"std_code\", split(df.phone, \"-\").getItem(0)) df = df.withColumn(\"phone_num\", split(df.phone, \"-\").getItem(1)) df.show() 10.Find Actor-Director Pairs with More Than Two Collaborations Question You are given a dataset containing details of actors and directors, where each record represents a collaboration between them:\nActorId DirectorId timestamp 1 1 0 1 1 1 1 1 2 1 2 3 1 2 4 1 2 5 2 1 6 Task Find all Actor–Director pairs that have worked together on more than two occasions.\nPySpark Solution from pyspark.sql import SparkSession from pyspark.sql.types import * spark = SparkSession.builder.appName(\"ActorDirectorCollaboration\").getOrCreate() # Step 1: Define schema and data schema = StructType([ StructField(\"ActorId\", IntegerType(), True), StructField(\"DirectorId\", IntegerType(), True), StructField(\"timestamp\", IntegerType(), True) ]) data = [ (1, 1, 0), (1, 1, 1), (1, 1, 2), (1, 2, 3), (1, 2, 4), (1, 2, 5), (2, 1, 6) ] df = spark.createDataFrame(data, schema) Solution # Step 2: Group by ActorId and DirectorId and count occurrences result_df = df.groupBy(\"ActorId\", \"DirectorId\").count() # Step 3: Filter pairs with count \u003e 2 df1 = result_df.filter(result_df[\"count\"] \u003e 2) # Step 4: Display Actor-Director pairs with more than 2 collaborations df1.select(\"ActorId\", \"DirectorId\").show()",
    "description": "1. Write a PySpark query using below input to get below output? Input name Hobbies Alice Badminton, Tennis Bob Tennis, Cricket Julie Cricket, Carroms Output name Hobbies Alice Badminton Alice Tennis Bob Tennis Bob Cricket Julie Cricket Julie Carroms from pyspark.sql import SparkSession from pyspark.sql.functions import split, explode # Create Spark session spark = SparkSession.builder.appName(\"HobbiesSplit\").getOrCreate() # Sample data data = [ (\"Alice\", \"Badminton, Tennis\"), (\"Bob\", \"Tennis, Cricket\"), (\"Julie\", \"Cricket, Carroms\") ] columns = [\"name\", \"Hobbies\"] # Create DataFrame df = spark.createDataFrame(data, columns) Solution # Split and explode hobbies result = df.withColumn(\"Hobbies\", explode(split(df[\"Hobbies\"], \",\\s*\"))) result.show(truncate=False) 2. Write a PySpark query using the below input to get the required output. Input City1 City2 City3 Goa null Ap null AP null null null Bglr Expected Output Result Goa AP Bglr from pyspark.sql import functions as F # Sample Input Data data = [ (\"Goa\", None, \"Ap\"), (None, \"AP\", None), (None, None, \"Bglr\") ] df = spark.createDataFrame(data, [\"City1\", \"City2\", \"City3\"]) Solution # Use COALESCE to pick the first non-null city result_df = df.select( F.coalesce(\"City1\", \"City2\", \"City3\").alias(\"Result\") ) result_df.show() 3. Question – Student Result Classification You have a PySpark DataFrame df containing student marks in multiple subjects:",
    "tags": [],
    "title": "Pyspark 1",
    "uri": "/practice/pyspark1/"
  },
  {
    "breadcrumb": "",
    "content": "Spark DF Basics DF Operations Functions Date Functions Handling Nulls Aggregate functions Joins When|Cast|Union Window Functions Explode Pivot Comparisons 1. Introduction Big Data Overview Introduction to Apache Spark Introduction to Azure Databricks Databricks Architecture Databricks Workspace Overview 2. Spark Core Concepts RDDs (Resilient Distributed Datasets) Transformations and Actions Lazy Evaluation Spark Execution Model Caching and Persistence Partitioning 3. Spark SQL DataFrames and Datasets Spark SQL Engine Schema Inference Catalog and Metadata Temporary and Global Views SQL Queries in Spark Performance Optimization (Catalyst Optimizer, Tungsten) 4. Spark Data Sources Reading/Writing CSV, JSON, Parquet, ORC Working with Delta Lake JDBC Data Sources Streaming Data Sources Handling Nested Data (Struct, Array, Map) 5. Delta Lake Introduction to Delta Lake ACID Transactions Schema Enforcement \u0026 Evolution Time Travel Upserts \u0026 Deletes (MERGE) Delta Lake Optimization (Z-order, Optimize, Vacuum) 6. Spark Streaming \u0026 Structured Streaming Introduction to Streaming DStreams vs Structured Streaming Streaming Sources (Kafka, Event Hubs, Socket) Watermarking \u0026 Late Data State Management Streaming Sinks (Delta, Console, Kafka, Event Hubs) 7. Spark MLlib (Machine Learning) MLlib Basics Feature Engineering Pipelines \u0026 Transformers Classification Regression Clustering Model Persistence 8. Spark GraphX Introduction to GraphX Graph Processing Basics Graph Algorithms Use Cases 9. Performance Tuning Spark Configurations \u0026 Parameters Memory Management Shuffle Optimizations Caching Strategies Adaptive Query Execution (AQE) Skew Handling 10. Azure Databricks Essentials Databricks Clusters (Types, Autoscaling, Pools) Databricks Notebooks Databricks Repos \u0026 Git Integration Databricks Jobs \u0026 Scheduling Databricks Secrets Databricks Utilities (DBUtils) Workspace Security \u0026 Permissions 11. Integration with Azure Services Azure Data Lake Storage (ADLS) Integration Azure Blob Storage Integration Azure Synapse Analytics Azure Event Hubs Azure Data Factory (ADF) with Databricks Power BI with Databricks 12. Advanced Databricks Unity Catalog Databricks SQL Delta Live Tables (DLT) Databricks REST API Databricks CLI Monitoring \u0026 Logging Cost Optimization in Databricks 13. Real-World Use Cases ETL Pipeline with Databricks Streaming Data Pipeline Batch Data Processing Machine Learning Pipeline End-to-End Data Lakehouse Implementation 14. Best Practices Cluster Sizing \u0026 Configuration Code Versioning \u0026 Collaboration Data Governance Security \u0026 Compliance CI/CD with Databricks Productionizing Workloads 15. Capstone Project Build a Scalable Data Lakehouse with Databricks Real-Time Streaming Data Pipeline ML Model Training and Deployment",
    "description": "Spark DF Basics DF Operations Functions Date Functions Handling Nulls Aggregate functions Joins When|Cast|Union Window Functions Explode Pivot Comparisons 1. Introduction Big Data Overview Introduction to Apache Spark Introduction to Azure Databricks Databricks Architecture Databricks Workspace Overview 2. Spark Core Concepts RDDs (Resilient Distributed Datasets) Transformations and Actions Lazy Evaluation Spark Execution Model Caching and Persistence Partitioning 3. Spark SQL DataFrames and Datasets Spark SQL Engine Schema Inference Catalog and Metadata Temporary and Global Views SQL Queries in Spark Performance Optimization (Catalyst Optimizer, Tungsten) 4. Spark Data Sources Reading/Writing CSV, JSON, Parquet, ORC Working with Delta Lake JDBC Data Sources Streaming Data Sources Handling Nested Data (Struct, Array, Map) 5. Delta Lake Introduction to Delta Lake ACID Transactions Schema Enforcement \u0026 Evolution Time Travel Upserts \u0026 Deletes (MERGE) Delta Lake Optimization (Z-order, Optimize, Vacuum) 6. Spark Streaming \u0026 Structured Streaming Introduction to Streaming DStreams vs Structured Streaming Streaming Sources (Kafka, Event Hubs, Socket) Watermarking \u0026 Late Data State Management Streaming Sinks (Delta, Console, Kafka, Event Hubs) 7. Spark MLlib (Machine Learning) MLlib Basics Feature Engineering Pipelines \u0026 Transformers Classification Regression Clustering Model Persistence 8. Spark GraphX Introduction to GraphX Graph Processing Basics Graph Algorithms Use Cases 9. Performance Tuning Spark Configurations \u0026 Parameters Memory Management Shuffle Optimizations Caching Strategies Adaptive Query Execution (AQE) Skew Handling 10. Azure Databricks Essentials Databricks Clusters (Types, Autoscaling, Pools) Databricks Notebooks Databricks Repos \u0026 Git Integration Databricks Jobs \u0026 Scheduling Databricks Secrets Databricks Utilities (DBUtils) Workspace Security \u0026 Permissions 11. Integration with Azure Services Azure Data Lake Storage (ADLS) Integration Azure Blob Storage Integration Azure Synapse Analytics Azure Event Hubs Azure Data Factory (ADF) with Databricks Power BI with Databricks 12. Advanced Databricks Unity Catalog Databricks SQL Delta Live Tables (DLT) Databricks REST API Databricks CLI Monitoring \u0026 Logging Cost Optimization in Databricks 13. Real-World Use Cases ETL Pipeline with Databricks Streaming Data Pipeline Batch Data Processing Machine Learning Pipeline End-to-End Data Lakehouse Implementation 14. Best Practices Cluster Sizing \u0026 Configuration Code Versioning \u0026 Collaboration Data Governance Security \u0026 Compliance CI/CD with Databricks Productionizing Workloads 15. Capstone Project Build a Scalable Data Lakehouse with Databricks Real-Time Streaming Data Pipeline ML Model Training and Deployment",
    "tags": [],
    "title": "Azure Data Bricks",
    "uri": "/azure_data_bricks/"
  },
  {
    "breadcrumb": "SQL",
    "content": "This guide covers the essential DML (Data Manipulation Language) commands used for inserting, updating, and deleting data in database tables.\n1. INSERT – Adding Data to Tables Method 1: Manual INSERT using VALUES -- Insert new records into the customers table INSERT INTO customers (id, first_name, country, score) VALUES (6, 'Anna', 'USA', NULL), (7, 'Sam', NULL, 100); -- Incorrect column order INSERT INTO customers (id, first_name, country, score) VALUES (8, 'Max', 'USA', NULL); -- Incorrect data type in values INSERT INTO customers (id, first_name, country, score) VALUES ('Max', 9, 'Max', NULL); -- Insert a new record with full column values INSERT INTO customers (id, first_name, country, score) VALUES (8, 'Max', 'USA', 368); -- Insert without specifying column names (not recommended) INSERT INTO customers VALUES (9, 'Andreas', 'Germany', NULL); -- Insert a record with only id and first_name INSERT INTO customers (id, first_name) VALUES (10, 'Sahra'); Method 2: INSERT using SELECT (Copying Data) -- Copy data from customers table into persons INSERT INTO persons (id, person_name, birth_date, phone) SELECT id, first_name, NULL, 'Unknown' FROM customers; 2. UPDATE – Modifying Existing Data -- Change the score of customer with ID 6 to 0 UPDATE customers SET score = 0 WHERE id = 6; -- Change score and country for customer with ID 10 UPDATE customers SET score = 0, country = 'UK' WHERE id = 10; -- Update all NULL scores to 0 UPDATE customers SET score = 0 WHERE score IS NULL; -- Verify the update SELECT * FROM customers WHERE score IS NULL; 3. DELETE – Removing Data from Tables -- Select customers with ID greater than 5 SELECT * FROM customers WHERE id \u003e 5; -- Delete customers with ID greater than 5 DELETE FROM customers WHERE id \u003e 5; -- Delete all rows from persons DELETE FROM persons; -- Faster method to delete all rows TRUNCATE TABLE persons;",
    "description": "This guide covers the essential DML (Data Manipulation Language) commands used for inserting, updating, and deleting data in database tables.\n1. INSERT – Adding Data to Tables Method 1: Manual INSERT using VALUES -- Insert new records into the customers table INSERT INTO customers (id, first_name, country, score) VALUES (6, 'Anna', 'USA', NULL), (7, 'Sam', NULL, 100); -- Incorrect column order INSERT INTO customers (id, first_name, country, score) VALUES (8, 'Max', 'USA', NULL); -- Incorrect data type in values INSERT INTO customers (id, first_name, country, score) VALUES ('Max', 9, 'Max', NULL); -- Insert a new record with full column values INSERT INTO customers (id, first_name, country, score) VALUES (8, 'Max', 'USA', 368); -- Insert without specifying column names (not recommended) INSERT INTO customers VALUES (9, 'Andreas', 'Germany', NULL); -- Insert a record with only id and first_name INSERT INTO customers (id, first_name) VALUES (10, 'Sahra'); Method 2: INSERT using SELECT (Copying Data) -- Copy data from customers table into persons INSERT INTO persons (id, person_name, birth_date, phone) SELECT id, first_name, NULL, 'Unknown' FROM customers; 2. UPDATE – Modifying Existing Data -- Change the score of customer with ID 6 to 0 UPDATE customers SET score = 0 WHERE id = 6; -- Change score and country for customer with ID 10 UPDATE customers SET score = 0, country = 'UK' WHERE id = 10; -- Update all NULL scores to 0 UPDATE customers SET score = 0 WHERE score IS NULL; -- Verify the update SELECT * FROM customers WHERE score IS NULL; 3. DELETE – Removing Data from Tables -- Select customers with ID greater than 5 SELECT * FROM customers WHERE id \u003e 5; -- Delete customers with ID greater than 5 DELETE FROM customers WHERE id \u003e 5; -- Delete all rows from persons DELETE FROM persons; -- Faster method to delete all rows TRUNCATE TABLE persons;",
    "tags": [],
    "title": "DML",
    "uri": "/sql/dml/"
  },
  {
    "breadcrumb": "ADB",
    "content": "Sorting and String Functions from pyspark.sql import SparkSession from pyspark.sql.functions import col, desc, asc, concat, concat_ws, initcap, lower, upper, instr, length, lit # Create a Spark session spark = SparkSession.builder.appName(\"SortingAndStringFunctions\").getOrCreate() # Sample data data = [ (\"USA\", \"North America\", 100, 50.5), (\"India\", \"Asia\", 300, 20.0), (\"Germany\", \"Europe\", 200, 30.5), (\"Australia\", \"Oceania\", 150, 60.0), (\"Japan\", \"Asia\", 120, 45.0), (\"Brazil\", \"South America\", 180, 25.0) ] # Define the schema columns = [\"Country\", \"Region\", \"UnitsSold\", \"UnitPrice\"] # Create DataFrame df = spark.createDataFrame(data, columns) # Display the original DataFrame df.show() Sorting the DataFrame 1. Sort by a single column (ascending order) df.orderBy(\"Country\").show(5)\nBy default, sorting is ascending. This shows the first 5 countries alphabetically.\n2. Sort by multiple columns df.orderBy(\"Country\", \"UnitsSold\").show(5)\nFirst sorts by Country, then within each country sorts by UnitsSold.\n3. Sort by column in descending order and limit df.orderBy(desc(\"Country\")).limit(3).show(5)\nSorts by Country in descending order and returns the top 3 rows.\n4. Sorting with null values last df.orderBy(col(\"Country\").desc(), nulls_last=True).show(5)\nEnsures null values appear at the end when sorting.\nKey Functions:\nUse .orderBy() or .sort() to sort DataFrames.\nControl order with asc() or desc().\nString Functions 1. Capitalize first letter of each word df.select(initcap(col(\"Country\"))).show()\nConverts \"united states\" → \"United States\".\n2. Convert all text to lowercase df.select(lower(col(\"Country\"))).show()\n3. Convert all text to uppercase df.select(upper(col(\"Country\"))).show()\nKey Functions:\ninitcap() → Capitalize first letter of each word.\nlower() → Convert to lowercase.\nupper() → Convert to uppercase.\nConcatenation Functions 1. Concatenate two columns df.select(concat(col(\"Region\"), col(\"Country\"))).show()\nJoins Region and Country without separator.\n2. Concatenate with a separator df.select(concat_ws(\" | \", col(\"Region\"), col(\"Country\"))).show()\nJoins with \" | \" as separator.\n3. Create a new concatenated column df.withColumn(\"concatenated\", concat(df[\"Region\"], lit(\" \"), df[\"Country\"])).show()\nAdds a new column combining Region and Country.\nKey Functions:\nconcat() → Join columns directly.\nconcat_ws() → Join columns with a separator.\n📌 Summary Sorting: Use .orderBy() or .sort() with asc() / desc(). String Manipulation: Use initcap(), lower(), upper(). Concatenation: Use concat() or concat_ws() for flexible joins. Split Function in DataFrame Let’s create a PySpark DataFrame for employee data with columns such as EmployeeID, Name, Department, and Skills. We’ll explore split, explode, and other useful array functions.\nSample Data Creation for Employee Data from pyspark.sql import SparkSession from pyspark.sql.functions import split, explode, size, array_contains, col # Sample employee data data = [ (1, \"Alice\", \"HR\", \"Communication Management\"), (2, \"Bob\", \"IT\", \"Programming Networking\"), (3, \"Charlie\", \"Finance\", \"Accounting Analysis\"), (4, \"David\", \"HR\", \"Recruiting Communication\"), (5, \"Eve\", \"IT\", \"Cloud DevOps\") ] # Define the schema columns = [\"EmployeeID\", \"Name\", \"Department\", \"Skills\"] # Create DataFrame df = spark.createDataFrame(data, columns) # Display the original DataFrame df.show(truncate=False) Examples 1. Split the Skills column df.select(col(\"EmployeeID\"), col(\"Name\"), split(col(\"Skills\"), \" \").alias(\"Skills_Array\")).show(truncate=False)\nSplits the Skills column into an array of skills using space as a delimiter.\n2. Select the first skill from Skills_Array df2.select(col(\"EmployeeID\"), col(\"Name\"), col(\"Skills_Array\")[0].alias(\"First_Skill\")).show(truncate=False)\nUses index notation (Skills_Array[0]) to pick the first skill. Indexing starts from 0.\n3. Count the number of skills per employee df2.select(col(\"EmployeeID\"), col(\"Name\"), size(col(\"Skills_Array\")).alias(\"Number_of_Skills\")).show(truncate=False)\nThe size() function returns the number of elements in the array.\n4. Check if the employee has “Cloud” skill df.select(col(\"EmployeeID\"), col(\"Name\"), array_contains(split(col(\"Skills\"), \" \"), \"Cloud\").alias(\"Has_Cloud_Skill\")).show(truncate=False) `\narray_contains() returns True if \"Cloud\" exists in the skill set.\n5. Explode the Skills_Array into multiple rows df3 = df2.withColumn(\"Skill\", explode(col(\"Skills_Array\")))\ndf3.select(\"EmployeeID\", \"Name\", \"Skill\").show(truncate=False)\nexplode() flattens the array into rows, where each skill becomes a separate row for that employee.\n📌 Summary of Key Functions split() → Splits a string into an array. explode() → Converts an array into multiple rows. size() → Counts elements in an array. array_contains() → Checks if an array contains a value. selectExpr() → Lets you query arrays using SQL expressions like Skills_Array[0]. Trim Function in DataFrame Let’s create a sample dataset for employees and demonstrate string trimming and padding functions in PySpark:\nltrim() rtrim() trim() lpad() rpad() Sample Data Creation for Employees from pyspark.sql import SparkSession from pyspark.sql.functions import lit, ltrim, rtrim, rpad, lpad, trim, col # Sample employee data with leading and trailing spaces in the 'Name' column data = [ (1, \" Alice \", \"HR\"), (2, \" Bob\", \"IT\"), (3, \"Charlie \", \"Finance\"), (4, \" David \", \"HR\"), (5, \"Eve \", \"IT\") ] # Define the schema for the DataFrame columns = [\"EmployeeID\", \"Name\", \"Department\"] # Create DataFrame df = spark.createDataFrame(data, columns) # Show the original DataFrame df.show(truncate=False) Applying Trimming and Padding Functions 1. Trimming Functions ltrim() → Removes leading spaces.\nrtrim() → Removes trailing spaces.\ntrim() → Removes both leading and trailing spaces.\n2. Padding Functions lpad() → Pads the left side of a string with a character up to a given length.\nrpad() → Pads the right side of a string with a character up to a given length.\nExample # Apply trimming and padding functions result_df = df.select( col(\"EmployeeID\"), col(\"Department\"), ltrim(col(\"Name\")).alias(\"ltrim_Name\"), # Remove leading spaces rtrim(col(\"Name\")).alias(\"rtrim_Name\"), # Remove trailing spaces trim(col(\"Name\")).alias(\"trim_Name\"), # Remove both leading \u0026 trailing spaces lpad(col(\"Name\"), 10, \"X\").alias(\"lpad_Name\"), # Left pad with \"X\" to length 10 rpad(col(\"Name\"), 10, \"Y\").alias(\"rpad_Name\") # Right pad with \"Y\" to length 10 ) # Show the resulting DataFrame result_df.show(truncate=False) Output Explanation ltrim_Name → Leading spaces removed.\nrtrim_Name → Trailing spaces removed.\ntrim_Name → Both leading \u0026 trailing spaces removed.\nlpad_Name → Padded left with \"X\" until length = 10.\nrpad_Name → Padded right with \"Y\" until length = 10.\n📌 Summary Use trim functions (ltrim, rtrim, trim) to clean up unwanted spaces. Use padding functions (lpad, rpad) to format strings with fixed lengths.",
    "description": "Sorting and String Functions from pyspark.sql import SparkSession from pyspark.sql.functions import col, desc, asc, concat, concat_ws, initcap, lower, upper, instr, length, lit # Create a Spark session spark = SparkSession.builder.appName(\"SortingAndStringFunctions\").getOrCreate() # Sample data data = [ (\"USA\", \"North America\", 100, 50.5), (\"India\", \"Asia\", 300, 20.0), (\"Germany\", \"Europe\", 200, 30.5), (\"Australia\", \"Oceania\", 150, 60.0), (\"Japan\", \"Asia\", 120, 45.0), (\"Brazil\", \"South America\", 180, 25.0) ] # Define the schema columns = [\"Country\", \"Region\", \"UnitsSold\", \"UnitPrice\"] # Create DataFrame df = spark.createDataFrame(data, columns) # Display the original DataFrame df.show() Sorting the DataFrame 1. Sort by a single column (ascending order) df.orderBy(\"Country\").show(5)\nBy default, sorting is ascending. This shows the first 5 countries alphabetically.",
    "tags": [],
    "title": "Functions",
    "uri": "/azure_data_bricks/functions/"
  },
  {
    "breadcrumb": "Interview Prep",
    "content": "1. What is SQL and what is it used for? SQL (Structured Query Language) is a domain-specific, declarative programming language designed for managing relational databases. It is the primary language for tasks like data retrieval, data manipulation, and database administration.\nCore Components DDL (Data Definition Language): Used for defining and modifying the structure of the database. DML (Data Manipulation Language): Deals with adding, modifying, and removing data in the database. DCL (Data Control Language): Manages the permissions and access rights of the database. TCL (Transaction Control Language): Governs the transactional management of the database, such as commits or rollbacks. Common Database Management Tasks Data Retrieval and Reporting: Retrieve and analyze data, generate reports, and build dashboards.\nData Manipulation: Insert, update, or delete records from tables. Powerful features like Joins and Subqueries enable complex operations.\nData Integrity: Ensure data conform to predefined rules. Techniques like foreign keys, constraints, and triggers help maintain the integrity of the data.\nData Security: Manage user access permissions and roles.\nData Consistency: Enforce ACID properties (Atomicity, Consistency, Isolation, Durability) in database transactions.\nData Backups and Recovery: Perform database backups and ensure data is restorable in case of loss.\nData Normalization: Design databases for efficient storage and reduce data redundancy.\nIndices and Performance Tuning: Optimize queries for faster data retrieval.\nReplication and Sharding: Advanced techniques for distributed systems.\nBasic SQL Commands CREATE DATABASE: Used to create a new database. CREATE TABLE: Defines a new table. INSERT INTO: Adds a new record into a table. SELECT: Retrieves data from one or more tables. UPDATE: Modifies existing records. DELETE: Removes records from a table. ALTER TABLE: Modifies an existing table (e.g., adds a new column, renames an existing column, etc.). DROP TABLE: Deletes a table (along with its data) from the database. INDEX: Adds an index to a table for better performance. VIEW: Creates a virtual table that can be used for data retrieval. TRIGGER: Triggers a specified action when a database event occurs. PROCEDURE and FUNCTION: Store database logic for reuse and to simplify complex operations. Code Example: Basic SQL Queries Here is the SQL code:\n-- Create a database CREATE DATABASE Company; -- Use Company database USE Company; -- Create tables CREATE TABLE Department ( DeptID INT PRIMARY KEY AUTO_INCREMENT, DeptName VARCHAR(50) NOT NULL ); CREATE TABLE Employee ( EmpID INT PRIMARY KEY AUTO_INCREMENT, EmpName VARCHAR(100) NOT NULL, EmpDeptID INT, FOREIGN KEY (EmpDeptID) REFERENCES Department(DeptID) ); -- Insert data INSERT INTO Department (DeptName) VALUES ('Engineering'); INSERT INTO Department (DeptName) VALUES ('Sales'); INSERT INTO Employee (EmpName, EmpDeptID) VALUES ('John Doe', 1); INSERT INTO Employee (EmpName, EmpDeptID) VALUES ('Jane Smith', 2); -- Select data from database SELECT * FROM Department; SELECT * FROM Employee; -- Perform an inner join to combine data from two tables SELECT Employee.EmpID, Employee.EmpName, Department.DeptName FROM Employee JOIN Department ON Employee.EmpDeptID = Department.DeptID; 2. Describe the difference between SQL and NoSQL databases. SQL and NoSQL databases offer different paradigms, each designed to suit various types of data and data handling.\nTop-Level Differences SQL: Primarily designed for structured (structured, semi-structured) data — data conforming to a predefined schema.\nNoSQL: Suited for unstructured or semi-structured data that evolves gradually, thereby supporting flexible schemas.\nSQL: Employs SQL (Structured Query Language) for data modification and retrieval.\nNoSQL: Offers various APIs (like the document and key-value store interfaces) for data operations; the use of structured query languages can vary across different NoSQL implementations.\nSQL: Often provides ACID (Atomicity, Consistency, Isolation, Durability) compliance to ensure data integrity.\nNoSQL: Databases are oftentimes optimized for high performance and horizontal scalability, with potential trade-offs in consistency.\nCommon NoSQL Database Types Document Stores Example: MongoDB, Couchbase Key Features: Each record is a self-contained document, typically formatted as JSON. Relationship between documents is established through embedded documents or references. Example: Users and their blog posts could be encapsulated within a single document or linked via document references. Key-Value Stores Example: Redis, Amazon DynamoDB Key Features: Data is stored as a collection of unique keys and their corresponding values. No inherent structure or schema is enforced, providing flexibility in data content. Example: Shopping cart items keyed by a user’s ID. Wide-Column Stores (Column Families) Example: Apache Cassandra, HBase Key Features: Data is grouped into column families, akin to tables in traditional databases. Each column family can possess a distinct set of columns, granting a high degree of schema flexibility. Example: User profiles, where certain users might have additional or unique attributes. Graph Databases Example: Neo4j, JanusGraph Key Features: Tailored for data with complex relationships. Data entities are represented as nodes, and relationships between them are visualized as edges. Example: A social media platform could ensure efficient friend connections management. Data Modeling Differences SQL: Normalization is employed to minimize data redundancies and update anomalies. NoSQL: Data is often denormalized, packaging entities together to minimize the need for multiple queries. Auto-Incrementing IDs SQL: Often, each entry is assigned a unique auto-incrementing ID. NoSQL: The generation of unique IDs can be driven by external systems or even specific to individual documents within a collection. Handling Data Relationships SQL: Relationships between different tables are established using keys (e.g., primary, foreign). NoSQL: Relationships are handled either through embedded documents, referencing techniques, or as graph-like structures in dedicated graph databases. Transaction Support SQL: Transactions (a series of operations that execute as a single unit) are standard. NoSQL: The concept and features of transactions can be more varied based on the specific NoSQL implementation. Data Consistency Levels SQL: Traditionally ensures strong consistency across the database to maintain data integrity. NoSQL: Offers various consistency models, ranging from strong consistency to eventual consistency. This flexibility enables performance optimizations in distributed environments. Scalability SQL: Typically scales vertically, i.e., by upgrading hardware. NoSQL: Is often designed to scale horizontally, using commodity hardware across distributed systems. Data Flexibility SQL: Enforces a predefined, rigid schema, making it challenging to accommodate evolving data structures. NoSQL: Supports dynamic, ad-hoc schema updates for maximum flexibility. Data Integrity \u0026 Validation SQL: Often relies on constraints and strict data types to ensure data integrity and validity. NoSQL: Places greater emphasis on the application layer to manage data integrity and validation. 3. What are the different types of SQL commands? SQL commands fall into four primary categories: Data Query Language (DQL), Data Definition Language (DDL), Data Manipulation Language (DML), and Data Control Language (DCL).\nData Query Language (DQL) These commands focus on querying data within tables.\nKeywords and Examples: SELECT: Retrieve data. FROM: Identify the source table. WHERE: Apply filtering conditions. GROUP BY: Group results based on specified fields. HAVING: Establish qualifying conditions for grouped data. ORDER BY: Arrange data based on one or more fields. LIMIT: Specify result count (sometimes replaces SELECT TOP for certain databases). JOIN: Bring together related data from multiple tables. Data Definition Language (DDL) DDL commands are for managing the structure of the database, including tables and constraints.\nKeywords and Examples: CREATE TABLE: Generate new tables. ALTER TABLE: Modify existing tables. ADD, DROP: Incorporate or remove elements like columns, constraints, or properties. CREATE INDEX: Establish indexes to improve query performance. DROP INDEX: Remove existing indexes. TRUNCATE TABLE: Delete all rows from a table, but the table structure remains intact. DROP TABLE: Delete tables from the database. Data Manipulation Language (DML) These commands are useful for handling data within tables.\nKeywords and Examples: INSERT INTO: Add new rows of data. SELECT: Copy data from another table or tables. UPDATE: Modify existing data in a table. DELETE: Remove rows of data from a table. Data Control Language (DCL) DCL is all about managing the access and permissions to database objects.\nKeywords and Examples: GRANT: Assign permission to specified users or roles for specific database objects. REVOKE: Withdraw or remove these permissions previously granted. 4. Explain the purpose of the SELECT statement. The SELECT statement in SQL is fundamental to data retrieval and manipulation within relational databases. Its primary role is to precisely choose, transform, and organize data per specific business requirements.\nKey Components of the SELECT Statement The SELECT statement typically comprises the following elements:\nSELECT: Identifies the columns or expressions to be included in the result set. FROM: Specifies the table(s) from which the data should be retrieved. WHERE: Introduces conditional statements to filter rows based on specific criteria. GROUP BY: Aggregates data for summary or statistical reporting. HAVING: Functions like WHERE, but operates on aggregated data. ORDER BY: Defines the sort order for result sets. LIMIT or TOP: Limits the number of rows returned. Practical Applications of SELECT The robust design of the SELECT statement empowers data professionals across diverse functions, enabling:\nData Exploration: Gaining insights through filtered views or aggregated summaries. Data Transformation: Creating new fields via operations such as concatenation or mathematical calculations. Data Validation: Verifying data against defined criteria. Data Reporting: Generating formatted outputs for business reporting needs. Data Consolidation: Bringing together information from multiple tables or databases. Data Export: Facilitating the transfer of query results to other systems or for data backup. Beyond these functions, proper utilization of the other components ensures efficiency and consistency working with relational databases.\nSELECT Query Example Here is the SQL code:\nSELECT Orders.OrderID, Customers.CustomerName, Orders.OrderDate, OrderDetails.UnitPrice, OrderDetails.Quantity, Products.ProductName, Employees.LastName FROM ((Orders INNER JOIN Customers ON Orders.CustomerID = Customers.CustomerID) INNER JOIN Employees ON Orders.EmployeeID = Employees.EmployeeID) INNER JOIN OrderDetails ON Orders.OrderID = OrderDetails.OrderID 5. What is the difference between WHERE and HAVING clauses? WHERE and HAVING clauses are both used in SQL queries to filter data, but they operate in distinct ways.\nWHERE Clause The WHERE clause is primarily used to filter records before they are grouped or aggregated. It’s typically employed with non-aggregated fields or raw data.\nHAVING Clause Conversely, the HAVING clause filters data after the grouping step, often in conjunction with aggregate functions like SUM or COUNT. This makes it useful for setting group-level conditions.\n6. Define what a JOIN is in SQL and list its types. Join operations in SQL are responsible for combining rows from multiple tables, primarily based on related columns that are established using a foreign key relationship.\nThe three common types of joins in SQL are:\nInner Join Outer Join Left Outer Join Right Outer Join Full Outer Join Cross Join Self Join Inner Join Inner Join only returns rows where there is a match in both tables for the specified column(s).\nVisual Representation:\nTable1: Table2: Result (Inner Join):\rA B B C A B C\r- - - - - - -\r1 aa aa 20 1 aa 20\r2 bb bb 30 2 bb 30\r3 cc cc 40 SQL Query:\nSELECT Table1.A, Table1.B, Table2.C FROM Table1 INNER JOIN Table2 ON Table1.B = Table2.B; Outer Join Outer Joins—whether left, right or full—include all records from one table (the “left” or the “right” table\") and matched existing records from the other table. Unmatched records are filled with NULL values for missing columns from the other table.\nLeft Outer Join Left Outer Join (or simply Left Join) returns all records from the “left” table and the matched records from the “right” table.\nVisual Representation:\nTable1: Table2: Result (Left Outer Join):\rA B B C A B C\r- - - - - - -\r1 aa aa 20 1 aa 20\r2 bb bb 30 2 bb 30\r3 cc NULL NULL 3 cc NULL SQL Query:\nSELECT Table1.A, Table1.B, Table2.C FROM Table1 LEFT JOIN Table2 ON Table1.B = Table2.B; Right Outer Join Right Outer Join (or Right Join) returns all records from the “right” table and the matched records from the “left” table.\nVisual Representation:\nTable1: Table2: Result (Right Outer Join):\rA B B C A B C\r- - - - - - -\r1 aa aa 20 1 aa 20\r2 bb bb 30 2 bb 30\rNULL NULL cc 40 NULL NULL 40 SQL Query:\nSELECT Table1.A, Table1.B, Table2.C FROM Table1 RIGHT JOIN Table2 ON Table1.B = Table2.B; Full Outer Join Full Outer Join (or Full Join) returns all records when there is a match in either the left or the right table.\nVisual Representation:\nTable1: Table2: Result (Full Outer Join):\rA B B C A B C\r- - - - - - -\r1 aa aa 20 1 aa 20\r2 bb bb 30 2 bb 30\r3 cc NULL NULL 3 cc NULL NULL NULL cc 40 NULL NULL 40 SQL Query:\nSELECT COALESCE(Table1.A, Table2.A) AS A, Table1.B, Table2.C FROM Table1 FULL JOIN Table2 ON Table1.B = Table2.B; Cross Join A Cross Join, also known as a Cartesian Join, produces a result set that is the cartesian product of the two input sets. It will generate every possible combination of rows from both tables.\nVisual Representation:\nTable1: Table2: Result (Cross Join):\rA B C D A B C D\r- - - - - - - -\r1 aa 20 X 1 aa 20 X\r2 bb 30 Y 1 aa 30 Y\r3 cc 40 Z 1 aa 40 Z\r2 bb 20 X\r2 bb 30 Y\r2 bb 40 Z\r3 cc 20 X\r3 cc 30 Y\r3 cc 40 Z SQL Query:\nSELECT Table1.*, Table2.* FROM Table1 CROSS JOIN Table2; Self Join A Self Join is when a table is joined with itself. This is used when a table has a relationship with itself, typically when it has a parent-child relationship.\nVisual Representation:\nEmployee: Result (Self Join):\rEmpID Name ManagerID EmpID Name ManagerID\r- - - - - -\r1 John 3 1 John 3\r2 Amy 3 2 Amy 3\r3 Chris NULL 3 Chris NULL\r4 Lisa 2 4 Lisa 2\r5 Mike 2 5 Mike 2 SQL Query:\nSELECT E1.EmpID, E1.Name, E1.ManagerID FROM Employee AS E1 LEFT JOIN Employee AS E2 ON E1.ManagerID = E2.EmpID; 7. What is a primary key in a database? A primary key in a database is a unique identifier for each record in a table.\nKey Characteristics Uniqueness: Each value in the primary key column is unique, distinguishing every record.\nNon-Nullity: The primary key cannot be null, ensuring data integrity.\nStability: It generally does not change throughout the record’s lifetime, promoting consistency.\nData Integrity Benefits Entity Distinctness: Enforces that each record in the table represents a unique entity.\nAssociation Control: Helps manage relationships across tables and ensures referential integrity in foreign keys.\nPerformance Advantages Efficient Indexing: Primary keys are often auto-indexed, making data retrieval faster.\nOptimized Joins: When the primary key links to a foreign key, query performance improves for related tables.\nIndustry Best Practice Pick a Natural Key: Whenever possible, choose existing data values that are unique and stable.\nKeep It Simple: Single-column primary keys are easier to manage.\nAvoid Data in Column Attributes: Using data can lead to bloat, adds complexity, and can be restrictive.\nAvoid Data Sensitivity: Decrease potential risks associated with sensitive data by separating it from keys.\nEvaluate Multi-Column Keys Carefully: Identify and justify the need for such complexity.\nCode Example: Declaring a Primary Key Here is the SQL code:\nCREATE TABLE Students ( student_id INT PRIMARY KEY, grade_level INT, first_name VARCHAR(50), last_name VARCHAR(50) ); 8. Explain what a foreign key is and how it is used. A foreign key (FK) is a column or a set of columns in a table that uniquely identifies a row or a set of rows in another table. It establishes a relationship between two tables, often referred to as the parent table and the child table.\nKey Functions of a Foreign Key Data Integrity: Assures that each entry in the referencing table has a corresponding record in the referenced table, ensuring the data’s accuracy and reliability.\nRelationship Mapping: Defines logical connections between tables that can be used to retrieve related data.\nAction Propagation: Specify what action should be taken in the child table when a matching record in the parent table is created, updated, or deleted.\nCascade Control: Allows operations like deletion or updates to propagate to related tables, maintaining data consistency.\nForeign Key Constraints The database ensures the following with foreign key constraints:\nUniqueness: The referencing column or combination of columns in the child table is unique.\nConsistency: Each foreign key in the child table either matches a corresponding primary key or unique key in the parent table or contains a null value.\nUse Cases and Best Practices Data Integrity and Consistency: FKs ensure that references between tables are valid and up-to-date. For instance, a sales entry references a valid product ID and a customer ID.\nRelationship Representation: FKs depict relationships between tables, such as ‘One-to-Many’ (e.g., one department in a company can have multiple employees) or ‘Many-to-Many’ (like in associative entities).\nQuerying Simplification: They aid in performing joined operations to retrieve related data, abstracting away complex data relationships.\nCode Example: Creating a Foreign Key Relationship Here is the SQL code:\n-- Create the parent (referenced) table first CREATE TABLE departments ( id INT PRIMARY KEY, name VARCHAR(100) ); -- Add a foreign key reference to the child table CREATE TABLE employees ( id INT PRIMARY KEY, name VARCHAR(100), department_id INT, FOREIGN KEY (department_id) REFERENCES departments(id) ); 9. How can you prevent SQL injections? SQL injection occurs when untrusted data is mixed with SQL commands. To prevent these attacks, use parameterized queries and input validation.\nHere are specific methods to guard against SQL injection:\nParameterized Queries Description: Also known as a prepared statement, it separates SQL code from user input, rendering direct command injection impossible.\nCode Example:\nJava (JDBC): String query = \"SELECT * FROM users WHERE username = ? AND password = ?\"; PreparedStatement ps = con.prepareStatement(query); ps.setString(1, username); ps.setString(2, password); ResultSet rs = ps.executeQuery(); Python (MySQL): cursor.execute(\"SELECT * FROM users WHERE username = %s AND password = %s\", (username, password)) Benefits:\nImproved security. Reliability across different databases. No need for manual escaping. Stored Procedures Description: Allows the database to pre-compile and store your SQL code, providing a layer of abstraction between user input and database operations.\nCode Example:\nWith MySQL: Procedure definition: CREATE PROCEDURE login(IN p_username VARCHAR(50), IN p_password VARCHAR(50)) BEGIN SELECT * FROM users WHERE username = p_username AND password = p_password; END Calling the procedure: cursor.callproc('login', (username, password)) Advantages:\nReduction of code redundancy. Allows for granular permissions. Can improve performance through query plan caching. Input Validation Description: Examine user-supplied data to ensure it meets specific criteria before allowing it in a query.\nCode Example: Using regex:\nif not re.match(\"^[A-Za-z0-9_-]*$\", username): print(\"Invalid username format\") Drawbacks:\nNot a standalone method for preventing SQL injection. Might introduce false positives, limiting the user’s input freedom. Code Filtering Description: Sanitize incoming data based on its type, like strings or numbers. This approach works best in conjunction with other methods.\nCode Example: In Python:\nusername = re.sub(\"[^a-zA-Z0-9_-]\", \"\", username) Considerations:\nStill necessitates additional measures for robust security. Can restrict legitimate user input. 10. What is normalization? Explain with examples. Normalization is a database design method, refining table structures to reduce data redundancy and improve data integrity. It is a multi-step process, divided into five normal forms (1NF, 2NF, 3NF, BCNF, 4NF), each with specific rules.\nNormalization in Action Let’s consider a simplistic “Customer Invoices” scenario, starting from an unnormalized state:\nUnnormalized Table (0NF) ID Name Invoice No. Invoice Date Item No. Description Quantity Unit Price In this initial state, all data is stored in a single table without structural cohesion. Each record is a mix of customer and invoice information. This can lead to data redundancy and anomalies.\nFirst Normal Form (1NF) To reach 1NF, ensure all cells are atomic, meaning they hold single values. Make separate tables for related groups of data. In our example, let’s separate customer details from invoices and address multiple items on a single invoice.\nCustomer Details Table ID Name Invoices Table Invoice No. Customer_ID Invoice Date Items Table Invoice No. Item No. Description Quantity Unit Price Now, each table focuses on specific data, unique to 1NF.\n1NF is crucial for efficient database operations, especially for tasks like reporting and maintenance.\nSecond Normal Form (2NF) To achieve 2NF, consider the context of a complete data entry. Each non-key column should be dependent on the whole primary key.\nIn our example, the Items table already satisfies 2NF, as all non-key columns, like Description and Unit Price, depend on the entire primary key, formed by Invoice No. and Item No. together.\nThird Normal Form (3NF) For 3NF compliance, there should be no transitive dependencies. Non-key columns should rely only on the primary key.\nThe Invoices table requires further refinement:\nUpdated Invoices Table Invoice No. Customer_ID Invoice Date Here, Customer_ID is the sole attribute associated with the customer.\nPractical Implications Higher normal forms provide stronger data integrity but might be harder to maintain during regular data operations. Consider your specific application needs when determining the target normal form. Real-World Usage Many databases aim for 3NF. In scenarios requiring exhaustive data integrity, 4NF, and sometimes beyond, are appropriate. Code Example: Implementing 3NF Here is the SQL code:\n-- Create Customer and Invoices Table CREATE TABLE Customers ( ID INT PRIMARY KEY, Name VARCHAR(50) ); CREATE TABLE Invoices ( InvoiceNo INT PRIMARY KEY, Customer_ID INT, InvoiceDate DATE, FOREIGN KEY (Customer_ID) REFERENCES Customers(ID) ); -- Create Items Table CREATE TABLE Items ( InvoiceNo INT, ItemNo INT, Description VARCHAR(100), Quantity INT, UnitPrice DECIMAL(10,2), PRIMARY KEY (InvoiceNo, ItemNo), FOREIGN KEY (InvoiceNo) REFERENCES Invoices(InvoiceNo) ); This code demonstrates the specified 3NF structure with distinct tables for Customer, Invoices, and Items, ensuring data integrity during operations.\n11. Describe the concept of denormalization and when you would use it. Denormalization involves optimizing database performance by reducing redundancy at the cost of some data integrity.\nCommon Techniques for Denormalization Flattening Relationships:\nCombining related tables to minimize joins. Example: Order and Product tables are merged, eliminating the many-to-many relationship. Aggregating Data:\nPrecomputing derived values to minimize costly calculations. Example: a Sales_Total column in an Order table. Adding Additional Redundant Data:\nReplicating data from one table in another to reduce the need for joins. Example: The Customer and Sales tables can both have a Country column, even though the country is indirectly linked through the Customer table. Common Use Cases Reporting and Analytics:\nCompanies often need to run complex reports that span numerous tables. Denormalization can flatten these tables, making the reporting process more efficient. High-Volume Transaction Systems:\nIn systems where data consistency can be relaxed momentarily, denormalization can speed up operations. It’s commonly seen in e-commerce sites where a brief delay in updating the sales figures might be acceptable for faster checkouts and improved user experience. Read-Mostly Applications:\nSystems that are heavy on data reads and relatively light on writes can benefit from denormalization. Search- and Query-Intensive Applications:\nFor example, search engines often store data in a denormalized format to enhance retrieval speed. Partitioning Data:\nIn distributed systems like Hadoop or NoSQL databases, data is often stored redundantly across multiple nodes for enhanced performance. Considerations and Trade-offs Performance vs. Consistency:\nDenormalization can boost performance but at the expense of data consistency. Maintenance Challenges:\nRedundant data must be managed consistently, which can pose challenges. Operational Simplicity:\nSometimes, having a simple, denormalized structure can outweigh the benefits of granularity and normalization. Query Flexibility:\nA normalized structure can be more flexible for ad-hoc queries and schema changes. Denormalized structures might require more effort to adapt to such changes. 12. What are indexes and how can they improve query performance? Indexes are essential in SQL to accelerate queries by providing quick data lookups.\nHow Do Indexes Improve Performance? Faster Data Retrieval: Think of an index like a book’s table of contents, which leads you right to the desired section.\nSorted Data Access: With data logically ordered, lookups are more efficient.\nReduces Disk I/O: Queries may read fewer data pages when using an index.\nEnhances Joins: Indexes help optimize join conditions, particularly in larger tables.\nAggregates and Uniques: They can swiftly resolve aggregate functions and enforce data uniqueness.\nIndex Types B-Tree: Standard for most databases, arranges data in a balanced tree structure. Hash: Direct lookup based on a hash of the indexed column. Bitmap: Best used for columns with a low cardinality. R-Tree: Optimized for spatial data, such as maps. Different databases may offer additional specialized index types.\nWhen to Use Carefully Excessive or unnecessary indexing can:\nConsume Resources: Indexes require disk space and upkeep during data modifications. Slow Down Writes: Each write operation might trigger updates to associated indexes. Best Practices Appropriate Index Count: Identify crucial columns and refrain from over-indexing. Monitor and Refactor: Regularly assess index performance and refine or remove redundant ones. Consistency: Ensure all queries access data in a consistent manner to take full advantage of indexes. Data Type Consideration: Certain data types are better suited for indexing than others. Types of Keys Primary Key: Uniquely identifies each record in a table. Foreign Key: Establishes a link between tables, enforcing referential integrity. Compound Key: Combines two or more columns to form a unique identifier. 13. Explain the purpose of the GROUP BY clause. The GROUP BY clause in SQL serves to consolidate data and perform operations across groups of records.\nKey Functions Data Aggregation: Collapses rows into summary data. Filtering: Provides filtering criteria for groups. Calculated Fields: Allows computation on group-level data. Usage Examples Consider a Sales table with the following columns: Product, Region, and Amount.\nData Aggregation For data aggregation, we use aggregate functions such as SUM, AVG, COUNT, MIN, or MAX.\nThe query below calculates total sales by region:\nSELECT Region, SUM(Amount) AS TotalSales FROM Sales GROUP BY Region; Filtering The GROUP BY clause can include conditional statements. For example, to count only those sales that exceed $100 in amount:\nSELECT Region, COUNT(Amount) AS SalesAbove100 FROM Sales WHERE Amount \u003e 100 GROUP BY Region; Calculated Fields You can compute derived values for groups. For instance, to find what proportion each product contributes to the overall sales in a region, use this query:\nSELECT Region, Product, SUM(Amount) / (SELECT SUM(Amount) FROM Sales WHERE Region = s.Region) AS RelativeContribution FROM Sales s GROUP BY Region, Product; Performance Considerations Efficient database design aims to balance query performance with storage requirements. Aggregating data during retrieval can optimize performance, especially when dealing with huge datasets.\nIt’s essential to verify these calculations for accuracy, as improper data handling can lead to skewed results.\n14. What is a subquery, and when would you use one? Subqueries are embedded SQL select statements that provide inputs for an outer query. They can perform various tasks, such as filtering and aggregate computations. Subqueries can also be useful for complex join conditions, self-joins, and more.\nCommon Subquery Types Scalar Subquery A Scalar Subquery returns a single value. They’re frequently used for comparisons—like \u003e, =, or IN.\nExamples:\nGetting the maximum value:\nSELECT col1 FROM table1 WHERE col1 = (SELECT MAX(col1) FROM table1); Checking existence:\nSELECT col1, col2 FROM table1 WHERE col1 = (SELECT col1 FROM table2 WHERE condition); Using aggregates:\nSELECT col1 FROM table1 WHERE col1 = (SELECT SUM(col2) FROM table2); Table Subquery A Table Subquery is like a temporary table. It returns rows and columns and can be treated as a regular table for further processing.\nExamples:\nFiltering data:\nSELECT * FROM table1 WHERE col1 IN (SELECT col1 FROM table2 WHERE condition); Data deduplication:\nSELECT DISTINCT col1 FROM table1 WHERE condition1 AND col1 IN (SELECT col1 FROM table2 WHERE condition2); Advantages of Using Subqueries Simplicity: They offer cleaner syntax, especially for complex queries.\nStructured Data: Subqueries can ensure that intermediate data is properly processed, making them ideal for multi-step tasks.\nReduced Code Duplication: By encapsulating certain logic within a subquery, you can avoid repetitive code.\nDynamic Filtering: The data returned by a subquery can dynamically influence the scope of the outer query.\nMilestone Calculations: For long and complex queries, subqueries can provide clarity and help break down the logic into manageable parts.\nLimitations and Optimization Performance: Subqueries can sometimes be less efficient. Advanced databases like Oracle, SQL Server, and PostgreSQL offer optimizations, but it’s essential to monitor query performance.\nVersatility: While subqueries are powerful, they can be less flexible in some scenarios compared to other advanced features like Common Table Expressions (CTEs) and Window Functions.\nUnderstanding and Debugging: Nested logic might make a stored procedure or more advanced techniques like CTEs easier to follow and troubleshoot.\nCode Example: Using Subqueries Here is the SQL code:\n-- Assuming you have table1 and table2 -- Scalar Subquery Example SELECT col1 FROM table1 WHERE col1 = (SELECT MAX(col1) FROM table1); -- Table Subquery Example SELECT col1, col2 FROM table1 WHERE col1 = (SELECT col1 FROM table2 WHERE condition); 15. Describe the functions of the ORDER BY clause. The ORDER BY clause in SQL serves to sort the result set based on specified columns, in either ascending (ASC, default) or descending (DESC) order. It’s often used in conjunction with various SQL statements like SELECT or UNION to enhance result presentation.\nKey Features Column-Specific Sorting: You can designate one or more columns as the basis for sorting. For multiple columns, the order of precedence is from left to right. ASC and DESC Directives: These allow for both ascending and descending sorting. If neither is specified, it defaults to ascending. Use Cases Top-N Queries: Selecting a specific number of top or bottom records can be accomplished using ORDER BY along with LIMIT or OFFSET.\nTrends Identification: With ORDER BY, you can identify trends or patterns in your data, such as ranking by sales volume or time-based sequences.\nImproved Data Presentation: By sorting records in a logical order, you can enhance the visual appeal and comprehension of your data representations.\nCode Example: Order by Multiple Columns and Limit Results Let’s say you have a “sales” table with columns product_name, sale_date, and units_sold. You want to fetch the top 3 products that sold the most units on a specific date, sorted by units sold (in descending order) and product name (in ascending order).\nHere is the SQL query:\nSELECT product_name, sale_date, units_sold FROM sales WHERE sale_date = '2022-01-15' ORDER BY units_sold DESC, product_name ASC LIMIT 3; The expected result will show the top 3 products with the highest units sold on the given date. If two products have the same number of units sold, they will be sorted in alphabetical order by their names.\nSQL Server Specific: Order by Column Position In SQL Server, you can also use the column position in the ORDER BY clause. For example, instead of using column names, you can use 1 for the first column, 2 for the second, and so on. This syntax:\nSELECT product_name, sale_date, units_sold FROM sales WHERE sale_date = '2022-01-15' ORDER BY 3 DESC, 1 ASC LIMIT 3; performs the same operation as the previous example.\nMySQL Specific: Random Order In MySQL, you can reorder the results in a random sequence. This can be useful, for instance, in a quiz app to randomize the order of questions. The ORDER BY clause with the RAND() function looks like this:\nSELECT product_name FROM products ORDER BY RAND() LIMIT 1;",
    "description": "1. What is SQL and what is it used for? SQL (Structured Query Language) is a domain-specific, declarative programming language designed for managing relational databases. It is the primary language for tasks like data retrieval, data manipulation, and database administration.\nCore Components DDL (Data Definition Language): Used for defining and modifying the structure of the database. DML (Data Manipulation Language): Deals with adding, modifying, and removing data in the database. DCL (Data Control Language): Manages the permissions and access rights of the database. TCL (Transaction Control Language): Governs the transactional management of the database, such as commits or rollbacks. Common Database Management Tasks Data Retrieval and Reporting: Retrieve and analyze data, generate reports, and build dashboards.",
    "tags": [],
    "title": "SQL Theory",
    "uri": "/interviewprep/sql1/"
  },
  {
    "breadcrumb": "Interview Prep",
    "content": "This cheat sheet provides a quick reference to common SQL queries and concepts used in data analysis, from fundamental commands to advanced techniques.\nSection 1: Basic Queries (1–10) Use these to select and filter data from a table.\n-- 1. Select all columns from a table SELECT * FROM employees; -- 2. Select specific columns SELECT first_name, last_name, hire_date FROM employees; -- 3. Count all rows SELECT COUNT(*) FROM employees; -- 4. Count non-null values in a column SELECT COUNT(salary) FROM employees; -- 5. Find unique values in a column SELECT DISTINCT department FROM employees; -- 6. Filter data using WHERE SELECT * FROM employees WHERE department = 'Sales'; -- 7. Filter with multiple conditions using AND SELECT * FROM employees WHERE department = 'Sales' AND salary \u003e 60000; -- 8. Filter with multiple conditions using OR SELECT * FROM employees WHERE department = 'Sales' OR department = 'Marketing'; -- 9. Combine AND and OR with parentheses SELECT * FROM employees WHERE (department = 'Sales' OR department = 'Marketing') AND salary \u003e 70000; -- 10. Exclude a value using NOT SELECT * FROM employees WHERE NOT department = 'IT'; Section 2: Filtering \u0026 Pattern Matching (11–20) Advanced filtering using IN, BETWEEN, and LIKE.\n-- 11. Filter by a list of values using IN SELECT * FROM employees WHERE department IN ('Sales', 'Marketing', 'IT'); -- 12. Exclude a list of values using NOT IN SELECT * FROM employees WHERE department NOT IN ('Sales', 'Marketing'); -- 13. Filter values within a range using BETWEEN SELECT * FROM employees WHERE salary BETWEEN 50000 AND 75000; -- 14. Case-insensitive substring search SELECT * FROM employees WHERE first_name ILIKE '%jo%'; -- 15. Starts with a specific pattern SELECT * FROM employees WHERE last_name LIKE 'Smi%'; -- 16. Ends with a specific pattern SELECT * FROM employees WHERE email LIKE '%@gmail.com'; -- 17. Pattern at a specific position SELECT * FROM employees WHERE first_name LIKE '_a%'; -- 18. Find rows with NULL values SELECT * FROM employees WHERE manager_id IS NULL; -- 19. Find rows with non-NULL values SELECT * FROM employees WHERE manager_id IS NOT NULL; -- 20. Filter by date range SELECT * FROM employees WHERE hire_date BETWEEN '2023-01-01' AND '2023-12-31'; Section 3: Sorting \u0026 Limiting (21–30) Control the order of your results and limit the number of rows.\n-- 21. Sort results in ascending order SELECT * FROM employees ORDER BY last_name ASC; -- 22. Sort results in descending order SELECT * FROM employees ORDER BY salary DESC; -- 23. Sort by multiple columns SELECT * FROM employees ORDER BY department ASC, salary DESC; -- 24. Top 10 highest paid employees SELECT * FROM employees ORDER BY salary DESC LIMIT 10; -- 25. Latest 5 hired employees SELECT * FROM employees ORDER BY hire_date DESC LIMIT 5; -- 26. DISTINCT and ORDER BY SELECT DISTINCT department FROM employees ORDER BY department; -- 27. Fetch rows 11–20 SELECT * FROM employees ORDER BY employee_id OFFSET 10 LIMIT 10; -- 28. Employee with the highest salary SELECT * FROM employees ORDER BY salary DESC LIMIT 1; -- 29. Second-highest salary SELECT DISTINCT salary FROM employees ORDER BY salary DESC LIMIT 1 OFFSET 1; -- 30. Order by a calculated field SELECT first_name, last_name, (salary * 0.1) AS bonus FROM employees ORDER BY bonus DESC; Section 4: Aggregation \u0026 Grouping (31–40) Summarize and analyze data with aggregate functions.\n-- 31. Count rows for each group SELECT department, COUNT(*) FROM employees GROUP BY department; -- 32. Average salary per department SELECT department, AVG(salary) FROM employees GROUP BY department; -- 33. Total salary per department SELECT department, SUM(salary) FROM employees GROUP BY department; -- 34. Maximum salary in each department SELECT department, MAX(salary) FROM employees GROUP BY department; -- 35. Minimum salary in each department SELECT department, MIN(salary) FROM employees GROUP BY department; -- 36. Filter groups using HAVING SELECT department, AVG(salary) FROM employees GROUP BY department HAVING AVG(salary) \u003e 65000; -- 37. Count employees in departments with more than 5 employees SELECT department, COUNT(*) FROM employees GROUP BY department HAVING COUNT(*) \u003e 5; -- 38. Filter and then group SELECT department, AVG(salary) FROM employees WHERE hire_date \u003e '2022-01-01' GROUP BY department; -- 39. Group by multiple columns SELECT department, job_title, AVG(salary) FROM employees GROUP BY department, job_title; -- 40. Highest salary in each department (value) SELECT department, MAX(salary) FROM employees GROUP BY department; Section 5: Joins (41–60) Combine data from multiple tables.\n-- 41. INNER JOIN SELECT e.first_name, d.department_name FROM employees e INNER JOIN departments d ON e.department_id = d.department_id; -- 42. LEFT JOIN SELECT e.first_name, d.department_name FROM employees e LEFT JOIN departments d ON e.department_id = d.department_id; -- 43. LEFT JOIN to find rows in left table with no match in right SELECT e.first_name FROM employees e LEFT JOIN departments d ON e.department_id = d.department_id WHERE d.department_id IS NULL; -- 44. RIGHT JOIN SELECT e.first_name, d.department_name FROM employees e RIGHT JOIN departments d ON e.department_id = d.department_id; -- 45. FULL OUTER JOIN SELECT e.first_name, d.department_name FROM employees e FULL OUTER JOIN departments d ON e.department_id = d.department_id; -- 46. Join three tables SELECT e.first_name, p.project_name, d.department_name FROM employees e JOIN projects p ON e.employee_id = p.employee_id JOIN departments d ON e.department_id = d.department_id; -- 47. Self-Join (employee to manager) SELECT e.first_name AS employee, m.first_name AS manager FROM employees e JOIN employees m ON e.manager_id = m.employee_id; -- 48. Join on a non-key column SELECT o.order_id, c.customer_name FROM orders o JOIN customers c ON o.customer_zip_code = c.customer_zip_code; -- 49. CROSS JOIN SELECT * FROM employees CROSS JOIN departments; -- 50. LEFT JOIN with filtering (find products without sales) SELECT p.* FROM products p LEFT JOIN sales s ON p.product_id = s.product_id WHERE s.sale_id IS NULL; -- 51. Employees not assigned to any project SELECT e.first_name, e.last_name FROM employees e LEFT JOIN employee_projects ep ON e.employee_id = ep.employee_id WHERE ep.project_id IS NULL; -- 52. Projects without any employees assigned SELECT p.project_name FROM projects p LEFT JOIN employee_projects ep ON p.project_id = ep.project_id WHERE ep.employee_id IS NULL; -- 53. Total salary of each department, including departments with no employees SELECT d.department_name, SUM(e.salary) AS total_salary FROM departments d LEFT JOIN employees e ON d.department_id = e.department_id GROUP BY d.department_name; -- 54. Number of employees in each department (including zero) SELECT d.department_name, COUNT(e.employee_id) AS num_employees FROM departments d LEFT JOIN employees e ON d.department_id = e.department_id GROUP BY d.department_name; -- 55. Join based on multiple conditions SELECT o.*, c.customer_name FROM orders o JOIN customers c ON o.customer_id = c.customer_id AND o.order_date = c.last_purchase_date; -- 56. Employees, their managers, and managers' departments SELECT e.first_name, m.first_name AS manager_name, d.department_name FROM employees e JOIN employees m ON e.manager_id = m.employee_id JOIN departments d ON m.department_id = d.department_id; -- 57. Non-equi join (range join) SELECT e.employee_id, h.salary_grade FROM employees e JOIN salary_grades h ON e.salary BETWEEN h.min_salary AND h.max_salary; -- 58. Filter a joined table using WHERE SELECT e.first_name, d.department_name FROM employees e JOIN departments d ON e.department_id = d.department_id WHERE d.location_city = 'New York'; -- 59. Aggregation on a joined table SELECT d.department_name, AVG(e.salary) AS avg_salary FROM employees e JOIN departments d ON e.department_id = d.department_id GROUP BY d.department_name; -- 60. Join on common column with different names SELECT a.order_id, b.item_name FROM table_a a JOIN table_b b ON a.product_id = b.item_id; Section 6: Subqueries (61–75) Use a query within another query.\n-- 61. Subquery in WHERE (single value) SELECT * FROM employees WHERE salary \u003e (SELECT AVG(salary) FROM employees); -- 62. Subquery in WHERE (multiple values) SELECT * FROM employees WHERE department_id IN ( SELECT department_id FROM departments WHERE location_city = 'San Francisco' ); -- 63. Subquery in FROM (derived table) SELECT d.department_name, s.avg_salary FROM ( SELECT department_id, AVG(salary) AS avg_salary FROM employees GROUP BY department_id ) AS s JOIN departments d ON s.department_id = d.department_id; -- 64. Subquery in SELECT (scalar subquery) SELECT first_name, salary, (SELECT AVG(salary) FROM employees) AS company_avg_salary FROM employees; -- 65. Correlated subquery: higher than dept average SELECT * FROM employees e WHERE salary \u003e ( SELECT AVG(salary) FROM employees WHERE department_id = e.department_id ); -- 66. EXISTS to check existence SELECT d.department_name FROM departments d WHERE EXISTS ( SELECT 1 FROM employees e WHERE e.department_id = d.department_id AND e.salary \u003e 100000 ); -- 67. NOT EXISTS to find non-matching rows SELECT d.department_name FROM departments d WHERE NOT EXISTS ( SELECT 1 FROM employees e WHERE e.department_id = d.department_id ); -- 68. Departments with at least one employee SELECT * FROM departments WHERE department_id IN (SELECT DISTINCT department_id FROM employees); -- 69. Employees who have placed an order SELECT * FROM employees e WHERE EXISTS (SELECT 1 FROM orders o WHERE o.employee_id = e.employee_id); -- 70. Total salary for each department using a subquery SELECT d.department_name, (SELECT SUM(salary) FROM employees e WHERE e.department_id = d.department_id) AS total_department_salary FROM departments d; -- 71. Department with the highest average salary SELECT department_name FROM departments WHERE department_id = ( SELECT department_id FROM employees GROUP BY department_id ORDER BY AVG(salary) DESC LIMIT 1 ); -- 72. Customers who ordered a specific product SELECT customer_name FROM customers WHERE customer_id IN ( SELECT customer_id FROM orders WHERE product_id = 123 ); -- 73. Employees earning more than the average SELECT COUNT(*) FROM employees WHERE salary \u003e (SELECT AVG(salary) FROM employees); -- 74. Customers who have not placed an order SELECT customer_name FROM customers WHERE customer_id NOT IN ( SELECT DISTINCT customer_id FROM orders ); -- 75. Salary less than the average of their job title SELECT e.first_name, e.salary, e.job_title FROM employees e WHERE e.salary \u003c ( SELECT AVG(salary) FROM employees WHERE job_title = e.job_title ); Section 7: Window Functions (76–85) Perform calculations across a set of table rows related to the current row.\n-- 76. ROW_NUMBER() SELECT first_name, department, salary, ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) AS rn FROM employees; -- 77. RANK() SELECT first_name, department, salary, RANK() OVER (PARTITION BY department ORDER BY salary DESC) AS rank FROM employees; -- 78. DENSE_RANK() SELECT first_name, department, salary, DENSE_RANK() OVER (PARTITION BY department ORDER BY salary DESC) AS dense_rank FROM employees; -- 79. NTILE(n) SELECT first_name, salary, NTILE(4) OVER (ORDER BY salary DESC) AS quartile FROM employees; -- 80. LEAD() SELECT order_date, total_amount, LEAD(total_amount, 1) OVER (ORDER BY order_date) AS next_order_amount FROM orders; -- 81. LAG() SELECT order_date, total_amount, LAG(total_amount, 1, 0) OVER (ORDER BY order_date) AS previous_order_amount FROM orders; -- 82. Running total SELECT order_date, total_amount, SUM(total_amount) OVER (ORDER BY order_date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS running_total FROM orders; -- 83. Moving average (window = 3) SELECT order_date, total_amount, AVG(total_amount) OVER (ORDER BY order_date ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) AS moving_avg FROM orders; -- 84. Top 3 employees in each department SELECT * FROM ( SELECT first_name, department, salary, RANK() OVER (PARTITION BY department ORDER BY salary DESC) AS rn FROM employees ) AS ranked_employees WHERE rn \u003c= 3; -- 85. Percentage of total sales for each product SELECT product_id, SUM(sales) AS product_sales, SUM(SUM(sales)) OVER () AS total_sales, (SUM(sales) / SUM(SUM(sales)) OVER ()) * 100 AS percentage_of_total FROM sales GROUP BY product_id; Section 8: Common Table Expressions (CTEs) \u0026 Data Manipulation (86–100) Organize complex queries and modify data.\n-- 86. Use a CTE to simplify a query WITH department_avg AS ( SELECT department_id, AVG(salary) AS avg_dept_salary FROM employees GROUP BY department_id ) SELECT e.first_name, e.salary, da.avg_dept_salary FROM employees e JOIN department_avg da ON e.department_id = da.department_id WHERE e.salary \u003e da.avg_dept_salary; -- 87. INSERT INTO: Add a new row INSERT INTO employees (first_name, last_name, salary) VALUES ('John', 'Doe', 60000); -- 88. UPDATE: Modify existing data UPDATE employees SET salary = salary * 1.05 WHERE department = 'IT'; -- 89. DELETE: Remove rows DELETE FROM employees WHERE employee_id = 101; -- 90. TRUNCATE TABLE: Remove all rows quickly TRUNCATE TABLE old_data; -- 91. UNION: Combine distinct rows SELECT first_name FROM employees UNION SELECT first_name FROM customers; -- 92. UNION ALL: Combine including duplicates SELECT first_name FROM employees UNION ALL SELECT first_name FROM customers; -- 93. CASE statement for conditional logic SELECT first_name, salary, CASE WHEN salary \u003e 100000 THEN 'High Earner' WHEN salary BETWEEN 50000 AND 100000 THEN 'Mid-Range' ELSE 'Junior' END AS salary_level FROM employees; -- 94. Pivot data using CASE and GROUP BY SELECT department, COUNT(CASE WHEN salary \u003e 70000 THEN 1 END) AS high_salary_count, COUNT(CASE WHEN salary \u003c= 70000 THEN 1 END) AS low_salary_count FROM employees GROUP BY department; -- 95. CAST: Convert data type SELECT CAST(order_date AS DATE) FROM orders; -- 96. COALESCE: First non-null SELECT COALESCE(email, 'No Email Provided') FROM employees; -- 97. NULLIF: Returns NULL if equal SELECT NULLIF(salary, 0) FROM employees; -- 98. Recursive CTE WITH RECURSIVE subordinates AS ( SELECT employee_id, manager_id FROM employees WHERE employee_id = 1 UNION ALL SELECT e.employee_id, e.manager_id FROM employees e JOIN subordinates s ON e.manager_id = s.employee_id ) SELECT * FROM subordinates; -- 99. GROUPING SETS SELECT department, job_title, SUM(salary) FROM employees GROUP BY GROUPING SETS ((department), (job_title), ()); -- 100. ROLLUP SELECT department, job_title, SUM(salary) FROM employees GROUP BY ROLLUP(department, job_title);",
    "description": "This cheat sheet provides a quick reference to common SQL queries and concepts used in data analysis, from fundamental commands to advanced techniques.\nSection 1: Basic Queries (1–10) Use these to select and filter data from a table.\n-- 1. Select all columns from a table SELECT * FROM employees; -- 2. Select specific columns SELECT first_name, last_name, hire_date FROM employees; -- 3. Count all rows SELECT COUNT(*) FROM employees; -- 4. Count non-null values in a column SELECT COUNT(salary) FROM employees; -- 5. Find unique values in a column SELECT DISTINCT department FROM employees; -- 6. Filter data using WHERE SELECT * FROM employees WHERE department = 'Sales'; -- 7. Filter with multiple conditions using AND SELECT * FROM employees WHERE department = 'Sales' AND salary \u003e 60000; -- 8. Filter with multiple conditions using OR SELECT * FROM employees WHERE department = 'Sales' OR department = 'Marketing'; -- 9. Combine AND and OR with parentheses SELECT * FROM employees WHERE (department = 'Sales' OR department = 'Marketing') AND salary \u003e 70000; -- 10. Exclude a value using NOT SELECT * FROM employees WHERE NOT department = 'IT'; Section 2: Filtering \u0026 Pattern Matching (11–20) Advanced filtering using IN, BETWEEN, and LIKE.",
    "tags": [],
    "title": "100 SQL Queries",
    "uri": "/interviewprep/sql2/"
  },
  {
    "breadcrumb": "ADB",
    "content": "In PySpark, you can use various date functions to manipulate and analyze date and timestamp columns.\nWe’ll explore:\ncurrent_date current_timestamp date_add date_sub datediff months_between Sample Code from pyspark.sql import SparkSession from pyspark.sql.functions import current_date, current_timestamp, date_add, date_sub, col, datediff, months_between, to_date, lit # Generate a DataFrame with 10 rows, adding \"today\" and \"now\" columns dateDF = spark.range(10).withColumn(\"today\", current_date()).withColumn(\"now\", current_timestamp()) # Show the DataFrame with today and now columns dateDF.show(truncate=False) Key Functions current_date() → Returns current date. current_timestamp() → Returns current timestamp (date + time). date_sub(col(\"today\"), 5) → Subtracts 5 days. date_add(col(\"today\"), 5) → Adds 5 days. datediff(date1, date2) → Returns difference in days. months_between(date1, date2) → Returns difference in months. Working with dates and timestamps often requires converting formats and extracting components.\nWe’ll explore:\nto_date to_timestamp year, month, dayofmonth hour, minute, second 1. Add and Subtract Days dateDF.select( col(\"today\"), date_add(col(\"today\"), 5).alias(\"5_days_after\"), date_sub(col(\"today\"), 5).alias(\"5_days_before\") ).show(truncate=False) 2. Calculate Days Between Two Dates data = [(\"2024-10-01\", \"2024-10-09\"), (\"2024-01-01\", \"2024-01-31\")] columns = [\"start_date\", \"end_date\"] df = spark.createDataFrame(data, columns) df = df.select( to_date(col(\"start_date\")).alias(\"start_date\"), to_date(col(\"end_date\")).alias(\"end_date\"), datediff(col(\"end_date\"), col(\"start_date\")).alias(\"days_diff\") ) df.show(truncate=False) 1. to_date Converts a string to a date (default format: yyyy-MM-dd). If format doesn’t match, returns null. Example: to_date(lit(\"2017-12-11\"), \"yyyy-dd-MM\") 2. to_timestamp Converts a string with date \u0026 time into a timestamp. Allows extraction of time components. 3. Extracting Components year() → Extracts year. month() → Extracts month. dayofmonth() → Extracts day. hour() → Extracts hour. minute() → Extracts minute. second() → Extracts second. Example Output For input \"2017-12-11\" (format yyyy-dd-MM):\nYear: 2017 Month: 12 Day: 11 Hour: 0 Minute: 0 Second: 0 For invalid input (e.g., \"2017-20-12\"):\nResult: null",
    "description": "In PySpark, you can use various date functions to manipulate and analyze date and timestamp columns.\nWe’ll explore:\ncurrent_date current_timestamp date_add date_sub datediff months_between Sample Code from pyspark.sql import SparkSession from pyspark.sql.functions import current_date, current_timestamp, date_add, date_sub, col, datediff, months_between, to_date, lit # Generate a DataFrame with 10 rows, adding \"today\" and \"now\" columns dateDF = spark.range(10).withColumn(\"today\", current_date()).withColumn(\"now\", current_timestamp()) # Show the DataFrame with today and now columns dateDF.show(truncate=False) Key Functions current_date() → Returns current date. current_timestamp() → Returns current timestamp (date + time). date_sub(col(\"today\"), 5) → Subtracts 5 days. date_add(col(\"today\"), 5) → Adds 5 days. datediff(date1, date2) → Returns difference in days. months_between(date1, date2) → Returns difference in months. Working with dates and timestamps often requires converting formats and extracting components.\nWe’ll explore:",
    "tags": [],
    "title": "Date Functions",
    "uri": "/azure_data_bricks/date/"
  },
  {
    "breadcrumb": "SQL",
    "content": "This document provides an overview of SQL filtering techniques using WHERE and various operators for precise data retrieval.\n1. Comparison Operators (=, \u003c\u003e, \u003e, \u003e=, \u003c, \u003c=) -- Retrieve all customers from Germany SELECT * FROM customers WHERE country = 'Germany'; -- Retrieve all customers who are not from Germany SELECT * FROM customers WHERE country \u003c\u003e 'Germany'; -- Retrieve all customers with a score greater than 500 SELECT * FROM customers WHERE score \u003e 500; -- Retrieve all customers with a score of 500 or more SELECT * FROM customers WHERE score \u003e= 500; -- Retrieve all customers with a score less than 500 SELECT * FROM customers WHERE score \u003c 500; -- Retrieve all customers with a score of 500 or less SELECT * FROM customers WHERE score \u003c= 500; 2. Logical Operators (AND, OR, NOT) -- Customers from the USA and score \u003e 500 SELECT * FROM customers WHERE country = 'USA' AND score \u003e 500; -- Customers from the USA or score \u003e 500 SELECT * FROM customers WHERE country = 'USA' OR score \u003e 500; -- Customers with score not less than 500 SELECT * FROM customers WHERE NOT score \u003c 500; 3. Range Filtering – BETWEEN -- Customers with score between 100 and 500 SELECT * FROM customers WHERE score BETWEEN 100 AND 500; -- Equivalent to BETWEEN SELECT * FROM customers WHERE score \u003e= 100 AND score \u003c= 500; 4. Set Filtering – IN -- Customers from Germany or USA SELECT * FROM customers WHERE country IN ('Germany', 'USA'); 5. Pattern Matching – LIKE -- First name starts with 'M' SELECT * FROM customers WHERE first_name LIKE 'M%'; -- First name ends with 'n' SELECT * FROM customers WHERE first_name LIKE '%n'; -- First name contains 'r' SELECT * FROM customers WHERE first_name LIKE '%r%'; -- First name has 'r' in the third position SELECT * FROM customers WHERE first_name LIKE '__r%';",
    "description": "This document provides an overview of SQL filtering techniques using WHERE and various operators for precise data retrieval.\n1. Comparison Operators (=, \u003c\u003e, \u003e, \u003e=, \u003c, \u003c=) -- Retrieve all customers from Germany SELECT * FROM customers WHERE country = 'Germany'; -- Retrieve all customers who are not from Germany SELECT * FROM customers WHERE country \u003c\u003e 'Germany'; -- Retrieve all customers with a score greater than 500 SELECT * FROM customers WHERE score \u003e 500; -- Retrieve all customers with a score of 500 or more SELECT * FROM customers WHERE score \u003e= 500; -- Retrieve all customers with a score less than 500 SELECT * FROM customers WHERE score \u003c 500; -- Retrieve all customers with a score of 500 or less SELECT * FROM customers WHERE score \u003c= 500; 2. Logical Operators (AND, OR, NOT) -- Customers from the USA and score \u003e 500 SELECT * FROM customers WHERE country = 'USA' AND score \u003e 500; -- Customers from the USA or score \u003e 500 SELECT * FROM customers WHERE country = 'USA' OR score \u003e 500; -- Customers with score not less than 500 SELECT * FROM customers WHERE NOT score \u003c 500; 3. Range Filtering – BETWEEN -- Customers with score between 100 and 500 SELECT * FROM customers WHERE score BETWEEN 100 AND 500; -- Equivalent to BETWEEN SELECT * FROM customers WHERE score \u003e= 100 AND score \u003c= 500; 4. Set Filtering – IN -- Customers from Germany or USA SELECT * FROM customers WHERE country IN ('Germany', 'USA'); 5. Pattern Matching – LIKE -- First name starts with 'M' SELECT * FROM customers WHERE first_name LIKE 'M%'; -- First name ends with 'n' SELECT * FROM customers WHERE first_name LIKE '%n'; -- First name contains 'r' SELECT * FROM customers WHERE first_name LIKE '%r%'; -- First name has 'r' in the third position SELECT * FROM customers WHERE first_name LIKE '__r%';",
    "tags": [],
    "title": "Filtering Data",
    "uri": "/sql/filter/"
  },
  {
    "breadcrumb": "",
    "content": "SQL LeetCode Pyspark 1",
    "description": "SQL LeetCode Pyspark 1",
    "tags": [],
    "title": "Practice Sets",
    "uri": "/practice/"
  },
  {
    "breadcrumb": "Interview Prep",
    "content": "🧩 Data Manipulation Language (DML) Command Description Syntax Example SELECT Retrieves data from a database. SELECT column1, column2 FROM table_name; SELECT first_name, last_name FROM customers; INSERT Adds new records to a table. INSERT INTO table_name (column1, column2) VALUES (value1, value2); INSERT INTO customers (first_name, last_name) VALUES ('Mary', 'Doe'); UPDATE Modifies existing records in a table. UPDATE table_name SET column1 = value1, column2 = value2 WHERE condition; UPDATE employees SET employee_name = 'John Doe', department = 'Marketing'; DELETE Removes records from a table. DELETE FROM table_name WHERE condition; DELETE FROM employees WHERE employee_name = 'John Doe'; 🏗️ Data Definition Language (DDL) Command Description Syntax Example CREATE Creates a new database object (table, view, etc.). CREATE TABLE table_name (column1 datatype1, column2 datatype2, ...); CREATE TABLE employees (employee_id INT PRIMARY KEY, first_name VARCHAR(50), last_name VARCHAR(50), age INT); ALTER Adds, deletes, or modifies columns in an existing table. ALTER TABLE table_name ADD column_name datatype; ALTER TABLE customers ADD email VARCHAR(100); DROP Deletes an existing table or database object. DROP TABLE table_name; DROP TABLE customers; TRUNCATE Removes all rows from a table but keeps the structure. TRUNCATE TABLE table_name; TRUNCATE TABLE customers; 🔐 Data Control Language (DCL) Command Description Syntax Example GRANT Gives privileges to users or roles. GRANT SELECT, INSERT ON table_name TO user_name; GRANT SELECT, INSERT ON employees TO 'John Doe'; REVOKE Removes privileges previously granted. REVOKE SELECT, INSERT ON table_name FROM user_name; REVOKE SELECT, INSERT ON employees FROM 'John Doe'; 🔍 Querying Data Command Description Syntax Example SELECT Statement Retrieves data from one or more tables. SELECT column1, column2 FROM table_name; SELECT first_name, last_name FROM customers; WHERE Clause Filters rows based on specific conditions. SELECT * FROM table_name WHERE condition; SELECT * FROM customers WHERE age \u003e 30; ORDER BY Clause Sorts the result set by one or more columns. `SELECT * FROM table_name ORDER BY column_name ASC DESC;` GROUP BY Clause Groups rows sharing a property and is used with aggregates. SELECT column_name, COUNT(*) FROM table_name GROUP BY column_name; SELECT category, COUNT(*) FROM products GROUP BY category; HAVING Clause Filters grouped results (used with GROUP BY). SELECT column_name, COUNT(*) FROM table_name GROUP BY column_name HAVING condition; SELECT category, COUNT(*) FROM products GROUP BY category HAVING COUNT(*) \u003e 5;",
    "description": "🧩 Data Manipulation Language (DML) Command Description Syntax Example SELECT Retrieves data from a database. SELECT column1, column2 FROM table_name; SELECT first_name, last_name FROM customers; INSERT Adds new records to a table. INSERT INTO table_name (column1, column2) VALUES (value1, value2); INSERT INTO customers (first_name, last_name) VALUES ('Mary', 'Doe'); UPDATE Modifies existing records in a table. UPDATE table_name SET column1 = value1, column2 = value2 WHERE condition; UPDATE employees SET employee_name = 'John Doe', department = 'Marketing'; DELETE Removes records from a table. DELETE FROM table_name WHERE condition; DELETE FROM employees WHERE employee_name = 'John Doe'; 🏗️ Data Definition Language (DDL) Command Description Syntax Example CREATE Creates a new database object (table, view, etc.). CREATE TABLE table_name (column1 datatype1, column2 datatype2, ...); CREATE TABLE employees (employee_id INT PRIMARY KEY, first_name VARCHAR(50), last_name VARCHAR(50), age INT); ALTER Adds, deletes, or modifies columns in an existing table. ALTER TABLE table_name ADD column_name datatype; ALTER TABLE customers ADD email VARCHAR(100); DROP Deletes an existing table or database object. DROP TABLE table_name; DROP TABLE customers; TRUNCATE Removes all rows from a table but keeps the structure. TRUNCATE TABLE table_name; TRUNCATE TABLE customers; 🔐 Data Control Language (DCL) Command Description Syntax Example GRANT Gives privileges to users or roles. GRANT SELECT, INSERT ON table_name TO user_name; GRANT SELECT, INSERT ON employees TO 'John Doe'; REVOKE Removes privileges previously granted. REVOKE SELECT, INSERT ON table_name FROM user_name; REVOKE SELECT, INSERT ON employees FROM 'John Doe'; 🔍 Querying Data Command Description Syntax Example SELECT Statement Retrieves data from one or more tables. SELECT column1, column2 FROM table_name; SELECT first_name, last_name FROM customers; WHERE Clause Filters rows based on specific conditions. SELECT * FROM table_name WHERE condition; SELECT * FROM customers WHERE age \u003e 30; ORDER BY Clause Sorts the result set by one or more columns. `SELECT * FROM table_name ORDER BY column_name ASC DESC;` GROUP BY Clause Groups rows sharing a property and is used with aggregates. SELECT column_name, COUNT(*) FROM table_name GROUP BY column_name; SELECT category, COUNT(*) FROM products GROUP BY category; HAVING Clause Filters grouped results (used with GROUP BY). SELECT column_name, COUNT(*) FROM table_name GROUP BY column_name HAVING condition; SELECT category, COUNT(*) FROM products GROUP BY category HAVING COUNT(*) \u003e 5;",
    "tags": [],
    "title": "100 SQL Queries",
    "uri": "/interviewprep/sql3/"
  },
  {
    "breadcrumb": "ADB",
    "content": "Sample Sales Data with Null Values # Sample data: sales data with nulls data = [ (\"John\", \"North\", 100, None), (\"Doe\", \"East\", None, 50), (None, \"West\", 150, 30), (\"Alice\", None, 200, 40), (\"Bob\", \"South\", None, None), (None, None, None, None) ] columns = [\"Name\", \"Region\", \"UnitsSold\", \"Revenue\"] # Create DataFrame df = spark.createDataFrame(data, columns) df.show() 1. Detecting Null Values Use isNull() to identify rows where a column contains null values. The output is a boolean flag indicating whether the value is null. 2. Dropping Rows with Null Values dropna() removes rows with nulls in any column (default mode). Use how=\"all\" to remove rows only if all columns are null. Use subset=[\"col1\", \"col2\"] to target specific columns. 3. Filling Null Values fillna() replaces nulls with specified default values. Replace across all columns or selectively. Example: Replace Region nulls with \"Unknown\". Replace UnitsSold and Revenue nulls with 0. 4. Coalesce Function coalesce() returns the first non-null value among multiple columns. Useful when providing fallback values if some columns contain nulls. 5. Handling Nulls in Aggregations Nulls can distort aggregations like mean(). Use coalesce() to substitute nulls with defaults (e.g., 0.0). This prevents inaccurate results. 📌 Summary Detecting Nulls: Use isNull() to find null values. Dropping Nulls: Use dropna() to remove rows with nulls (all or specific columns). Filling Nulls: Use fillna() to replace nulls with defaults. Coalesce Function: Use coalesce() to return the first non-null value. Aggregations: Use coalesce() in aggregations to handle nulls safely.",
    "description": "Sample Sales Data with Null Values # Sample data: sales data with nulls data = [ (\"John\", \"North\", 100, None), (\"Doe\", \"East\", None, 50), (None, \"West\", 150, 30), (\"Alice\", None, 200, 40), (\"Bob\", \"South\", None, None), (None, None, None, None) ] columns = [\"Name\", \"Region\", \"UnitsSold\", \"Revenue\"] # Create DataFrame df = spark.createDataFrame(data, columns) df.show() 1. Detecting Null Values Use isNull() to identify rows where a column contains null values. The output is a boolean flag indicating whether the value is null. 2. Dropping Rows with Null Values dropna() removes rows with nulls in any column (default mode). Use how=\"all\" to remove rows only if all columns are null. Use subset=[\"col1\", \"col2\"] to target specific columns. 3. Filling Null Values fillna() replaces nulls with specified default values. Replace across all columns or selectively. Example: Replace Region nulls with \"Unknown\". Replace UnitsSold and Revenue nulls with 0. 4. Coalesce Function coalesce() returns the first non-null value among multiple columns. Useful when providing fallback values if some columns contain nulls. 5. Handling Nulls in Aggregations Nulls can distort aggregations like mean(). Use coalesce() to substitute nulls with defaults (e.g., 0.0). This prevents inaccurate results. 📌 Summary Detecting Nulls: Use isNull() to find null values. Dropping Nulls: Use dropna() to remove rows with nulls (all or specific columns). Filling Nulls: Use fillna() to replace nulls with defaults. Coalesce Function: Use coalesce() to return the first non-null value. Aggregations: Use coalesce() in aggregations to handle nulls safely.",
    "tags": [],
    "title": "Handling Nulls",
    "uri": "/azure_data_bricks/nulls/"
  },
  {
    "breadcrumb": "",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Interview Prep",
    "uri": "/interviewprep/"
  },
  {
    "breadcrumb": "SQL",
    "content": "This document provides an overview of SQL Joins, which allow combining data from multiple tables to retrieve meaningful insights.\n1. Basic Joins No Join -- Retrieve all data from customers and orders as separate results SELECT * FROM customers; SELECT * FROM orders; INNER JOIN -- Get all customers along with their orders (only those who placed an order) SELECT c.id, c.first_name, o.order_id, o.sales FROM customers AS c INNER JOIN orders AS o ON c.id = o.customer_id; LEFT JOIN -- Get all customers along with their orders (including customers without orders) SELECT c.id, c.first_name, o.order_id, o.sales FROM customers AS c LEFT JOIN orders AS o ON c.id = o.customer_id; RIGHT JOIN -- Get all customers along with their orders (including orders without customers) SELECT c.id, c.first_name, o.order_id, o.customer_id, o.sales FROM customers AS c RIGHT JOIN orders AS o ON c.id = o.customer_id; Alternative to RIGHT JOIN (using LEFT JOIN) SELECT c.id, c.first_name, o.order_id, o.sales FROM orders AS o LEFT JOIN customers AS c ON c.id = o.customer_id; FULL JOIN -- Get all customers and all orders, even if there’s no match SELECT c.id, c.first_name, o.order_id, o.customer_id, o.sales FROM customers AS c FULL JOIN orders AS o ON c.id = o.customer_id; 2. Advanced Joins LEFT ANTI JOIN -- Customers who haven't placed any order SELECT * FROM customers AS c LEFT JOIN orders AS o ON c.id = o.customer_id WHERE o.customer_id IS NULL; RIGHT ANTI JOIN -- Orders without matching customers SELECT * FROM customers AS c RIGHT JOIN orders AS o ON c.id = o.customer_id WHERE c.id IS NULL; Alternative to RIGHT ANTI JOIN (using LEFT JOIN) SELECT * FROM orders AS o LEFT JOIN customers AS c ON c.id = o.customer_id WHERE c.id IS NULL; Alternative to INNER JOIN (using LEFT JOIN) SELECT * FROM customers AS c LEFT JOIN orders AS o ON c.id = o.customer_id WHERE o.customer_id IS NOT NULL; FULL ANTI JOIN -- Find customers without orders and orders without customers SELECT c.id, c.first_name, o.order_id, o.customer_id, o.sales FROM customers AS c FULL JOIN orders AS o ON c.id = o.customer_id WHERE o.customer_id IS NULL OR c.id IS NULL; CROSS JOIN -- Generate all possible combinations of customers and orders SELECT * FROM customers CROSS JOIN orders; 3. Multiple Table Joins (4 Tables) Task: Using SalesDB, retrieve a list of all orders along with related customer, product, and employee details. For each order, display:\nOrder ID\nCustomer’s name\nProduct name\nSales amount\nProduct price\nSalesperson’s name\nUSE SalesDB; SELECT o.OrderID, o.Sales, c.FirstName AS CustomerFirstName, c.LastName AS CustomerLastName, p.Product AS ProductName, p.Price, e.FirstName AS EmployeeFirstName, e.LastName AS EmployeeLastName FROM Sales.Orders AS o LEFT JOIN Sales.Customers AS c ON o.CustomerID = c.CustomerID LEFT JOIN Sales.Products AS p ON o.ProductID = p.ProductID LEFT JOIN Sales.Employees AS e ON o.SalesPersonID = e.EmployeeID;",
    "description": "This document provides an overview of SQL Joins, which allow combining data from multiple tables to retrieve meaningful insights.\n1. Basic Joins No Join -- Retrieve all data from customers and orders as separate results SELECT * FROM customers; SELECT * FROM orders; INNER JOIN -- Get all customers along with their orders (only those who placed an order) SELECT c.id, c.first_name, o.order_id, o.sales FROM customers AS c INNER JOIN orders AS o ON c.id = o.customer_id; LEFT JOIN -- Get all customers along with their orders (including customers without orders) SELECT c.id, c.first_name, o.order_id, o.sales FROM customers AS c LEFT JOIN orders AS o ON c.id = o.customer_id; RIGHT JOIN -- Get all customers along with their orders (including orders without customers) SELECT c.id, c.first_name, o.order_id, o.customer_id, o.sales FROM customers AS c RIGHT JOIN orders AS o ON c.id = o.customer_id; Alternative to RIGHT JOIN (using LEFT JOIN) SELECT c.id, c.first_name, o.order_id, o.sales FROM orders AS o LEFT JOIN customers AS c ON c.id = o.customer_id; FULL JOIN -- Get all customers and all orders, even if there’s no match SELECT c.id, c.first_name, o.order_id, o.customer_id, o.sales FROM customers AS c FULL JOIN orders AS o ON c.id = o.customer_id; 2. Advanced Joins LEFT ANTI JOIN -- Customers who haven't placed any order SELECT * FROM customers AS c LEFT JOIN orders AS o ON c.id = o.customer_id WHERE o.customer_id IS NULL; RIGHT ANTI JOIN -- Orders without matching customers SELECT * FROM customers AS c RIGHT JOIN orders AS o ON c.id = o.customer_id WHERE c.id IS NULL; Alternative to RIGHT ANTI JOIN (using LEFT JOIN) SELECT * FROM orders AS o LEFT JOIN customers AS c ON c.id = o.customer_id WHERE c.id IS NULL; Alternative to INNER JOIN (using LEFT JOIN) SELECT * FROM customers AS c LEFT JOIN orders AS o ON c.id = o.customer_id WHERE o.customer_id IS NOT NULL; FULL ANTI JOIN -- Find customers without orders and orders without customers SELECT c.id, c.first_name, o.order_id, o.customer_id, o.sales FROM customers AS c FULL JOIN orders AS o ON c.id = o.customer_id WHERE o.customer_id IS NULL OR c.id IS NULL; CROSS JOIN -- Generate all possible combinations of customers and orders SELECT * FROM customers CROSS JOIN orders; 3. Multiple Table Joins (4 Tables) Task: Using SalesDB, retrieve a list of all orders along with related customer, product, and employee details. For each order, display:",
    "tags": [],
    "title": "Joins",
    "uri": "/sql/joins/"
  },
  {
    "breadcrumb": "ADB",
    "content": "Basic Aggregate Functions Sample Data from pyspark.sql import Row # Create sample data data = [ Row(id=1, value=10), Row(id=2, value=20), Row(id=3, value=30), Row(id=4, value=None), Row(id=5, value=40), Row(id=6, value=20) ] # Create DataFrame df = spark.createDataFrame(data) # Show the DataFrame df.show() Aggregate Functions in PySpark Summation (sum) – Adds up the values in a column. Average (avg) – Computes the average of values in a column. Count (count) – Counts the number of non-null values in a column. Maximum (max) / Minimum (min) – Finds the highest and lowest values. Distinct Count (countDistinct) – Counts unique values in a column. Notes Handling Nulls:\ncount() counts only non-null values. sum(), avg(), max(), and min() ignore null values. Performance:\nAggregate functions can be expensive on large datasets; partitioning improves performance.\nUse Cases:\nSummation: Total sales, total revenue. Average: Average sales per day. Count: Number of transactions. Max/Min: Highest and lowest values (e.g., max sales in a day). Distinct Count: Unique customers, unique products. Advanced Aggregation Functions Sample Data from pyspark.sql import SparkSession from pyspark.sql import functions as F from pyspark.sql.types import StructType, StructField, StringType, IntegerType # Create Spark session spark = SparkSession.builder.appName(\"AggregationExamples\").getOrCreate() # Sample data data = [ (\"HR\", 10000, 500, \"John\"), (\"Finance\", 20000, 1500, \"Doe\"), (\"HR\", 15000, 1000, \"Alice\"), (\"Finance\", 25000, 2000, \"Eve\"), (\"HR\", 20000, 1500, \"Mark\") ] # Define schema schema = StructType([ StructField(\"department\", StringType(), True), StructField(\"salary\", IntegerType(), True), StructField(\"bonus\", IntegerType(), True), StructField(\"employee_name\", StringType(), True) ]) # Create DataFrame df = spark.createDataFrame(data, schema) df.show() 1. Grouped Aggregation Perform aggregation within groups based on a column.\nsum() → Adds values within the group. avg() → Computes group average. max() → Finds maximum value. min() → Finds minimum value. 2. Multiple Aggregations Perform several aggregations in one step.\ncount() → Number of rows in each group. avg() → Average of values. max() → Maximum value in group. 3. Concatenate Strings concat_ws() → Concatenates string values within a column, separated by a delimiter (,). 4. First and Last first() → Retrieves the first value of a column in a group. last() → Retrieves the last value of a column in a group. 5. Standard Deviation and Variance stddev() → Standard deviation of values. variance() → Variance of values. 6. Aggregation with Alias .alias() → Rename the result columns after aggregation. 7. Sum of Distinct Values sumDistinct() → Sums only unique values in a column (avoids double-counting duplicates). 📌 Summary Use basic aggregations (sum, avg, count, max, min, countDistinct) for general metrics.\nApply advanced aggregations (grouped, concat_ws, first, last, stddev, variance, sumDistinct) for deeper analysis.\nAlways consider null handling and performance optimizations when using aggregate functions in PySpark.",
    "description": "Basic Aggregate Functions Sample Data from pyspark.sql import Row # Create sample data data = [ Row(id=1, value=10), Row(id=2, value=20), Row(id=3, value=30), Row(id=4, value=None), Row(id=5, value=40), Row(id=6, value=20) ] # Create DataFrame df = spark.createDataFrame(data) # Show the DataFrame df.show() Aggregate Functions in PySpark Summation (sum) – Adds up the values in a column. Average (avg) – Computes the average of values in a column. Count (count) – Counts the number of non-null values in a column. Maximum (max) / Minimum (min) – Finds the highest and lowest values. Distinct Count (countDistinct) – Counts unique values in a column. Notes Handling Nulls:",
    "tags": [],
    "title": "Aggregate functions",
    "uri": "/azure_data_bricks/aggregations/"
  },
  {
    "breadcrumb": "SQL",
    "content": "SQL set operations enable you to combine results from multiple queries into a single result set.\nThis guide demonstrates the rules and usage of UNION, UNION ALL, EXCEPT, and INTERSECT.\n1. SQL Operation Rules Rule: Data Types The data types of columns in each query should match.\nSELECT FirstName, LastName, Country FROM Sales.Customers UNION SELECT FirstName, LastName FROM Sales.Employees; Rule: Data Types (Example) SELECT CustomerID, LastName FROM Sales.Customers UNION SELECT FirstName, LastName FROM Sales.Employees; Rule: Column Order The order of the columns in each query must be the same.\nSELECT LastName, CustomerID FROM Sales.Customers UNION SELECT EmployeeID, LastName FROM Sales.Employees; Rule: Column Aliases The column names in the result set are determined by the column names specified in the first SELECT statement.\nSELECT CustomerID AS ID, LastName AS Last_Name FROM Sales.Customers UNION SELECT EmployeeID, LastName FROM Sales.Employees; Rule: Correct Columns Ensure that the correct columns are used to maintain data consistency.\nSELECT FirstName, LastName FROM Sales.Customers UNION SELECT LastName, FirstName FROM Sales.Employees; 2. UNION, UNION ALL, EXCEPT, INTERSECT Task 1: UNION Combine the data from Employees and Customers into one table.\nSELECT FirstName, LastName FROM Sales.Customers UNION SELECT FirstName, LastName FROM Sales.Employees; Task 2: UNION ALL Combine the data from Employees and Customers into one table, including duplicates.\nSELECT FirstName, LastName FROM Sales.Customers UNION ALL SELECT FirstName, LastName FROM Sales.Employees; Task 3: EXCEPT Find employees who are NOT customers.\nSELECT FirstName, LastName FROM Sales.Employees EXCEPT SELECT FirstName, LastName FROM Sales.Customers; Task 4: INTERSECT Find employees who are also customers.\nSELECT FirstName, LastName FROM Sales.Employees INTERSECT SELECT FirstName, LastName FROM Sales.Customers; Task 5: UNION with Orders Combine order data from Orders and OrdersArchive into one report without duplicates.\nSELECT 'Orders' AS SourceTable, OrderID, ProductID, CustomerID, SalesPersonID, OrderDate, ShipDate, OrderStatus, ShipAddress, BillAddress, Quantity, Sales, CreationTime FROM Sales.Orders UNION SELECT 'OrdersArchive' AS SourceTable, OrderID, ProductID, CustomerID, SalesPersonID, OrderDate, ShipDate, OrderStatus, ShipAddress, BillAddress, Quantity, Sales, CreationTime FROM Sales.OrdersArchive ORDER BY OrderID;",
    "description": "SQL set operations enable you to combine results from multiple queries into a single result set.\nThis guide demonstrates the rules and usage of UNION, UNION ALL, EXCEPT, and INTERSECT.\n1. SQL Operation Rules Rule: Data Types The data types of columns in each query should match.\nSELECT FirstName, LastName, Country FROM Sales.Customers UNION SELECT FirstName, LastName FROM Sales.Employees; Rule: Data Types (Example) SELECT CustomerID, LastName FROM Sales.Customers UNION SELECT FirstName, LastName FROM Sales.Employees; Rule: Column Order The order of the columns in each query must be the same.",
    "tags": [],
    "title": "Set Operations",
    "uri": "/sql/set-operations/"
  },
  {
    "breadcrumb": "ADB",
    "content": "Joins in PySpark In PySpark Joins are used to combine two DataFrames based on a common column or condition.\nTypes of Joins in PySpark Inner Join: Matches rows from both DataFrames. df1.join(df2, df1.common_column == df2.common_column, \"inner\") Left/Right Join: Keeps all rows from the left or right DataFrame and matches where possible. df1.join(df2, df1.common_column == df2.common_column, \"left\") df1.join(df2, df1.common_column == df2.common_column, \"right\") Full Join: Keeps all rows from both DataFrames. df1.join(df2, df1.common_column == df2.common_column, \"outer\") Left Semi: Filters df1 to rows that match df2 without including columns from df2. df1.join(df2, df1.common_column == df2.common_column, \"left_semi\") Left Anti: Filters df1 to rows that do not match df2. df1.join(df2, df1.common_column == df2.common_column, \"left_anti\") Cross Join: Returns the Cartesian product, combining all rows of both DataFrames. df1.crossJoin(df2) Explicit Condition Join: Allows complex join conditions, including columns with different names. df1.join(df2, df1.columnA == df2.columnB, \"inner\") df1.join(df2, df1.common_column == df2.common_column, \"inner\") df1.join(df2, df1.common_column == df2.common_column, \"left\") df1.join(df2, df1.common_column == df2.common_column, \"right\") df1.join(df2, df1.common_column == df2.common_column, \"outer\") df1.join(df2, df1.common_column == df2.common_column, \"left_semi\") df1.join(df2, df1.common_column == df2.common_column, \"left_anti\") df1.crossJoin(df2) df1.join(df2, df1.columnA == df2.columnB, \"inner\") Practice-1 from pyspark.sql import SparkSession from pyspark.sql import Row from pyspark.sql.functions import broadcast # Initialize Spark session spark = SparkSession.builder.appName(\"JoinsExample\").getOrCreate() # Sample DataFrames data1 = [Row(id=0), Row(id=1), Row(id=1), Row(id=None), Row(id=None)] data2 = [Row(id=1), Row(id=0), Row(id=None)] df1 = spark.createDataFrame(data1) df2 = spark.createDataFrame(data2) # Inner Join inner_join = df1.join(df2, on=\"id\", how=\"inner\") print(\"Inner Join:\") inner_join.show() # Right Join right_join = df1.join(df2, on=\"id\", how=\"right\") print(\"Right Join:\") right_join.show() # Full (Outer) Join full_join = df1.join(df2, on=\"id\", how=\"outer\") print(\"Full (Outer) Join:\") full_join.show() # Left Anti Join left_anti_join = df1.join(df2, on=\"id\", how=\"left_anti\") print(\"Left Anti Join:\") left_anti_join.show() # Right Anti Join (Equivalent to swapping DataFrames and performing Left Anti Join) right_anti_join = df2.join(df1, on=\"id\", how=\"left_anti\") print(\"Right Anti Join:\") right_anti_join.show() # Broadcast Join (Optimizing a join with a smaller DataFrame) broadcast_join = df1.join(broadcast(df2), on=\"id\", how=\"inner\") print(\"Broadcast Join:\") broadcast_join.show() Practice 2 PySpark Coding Questions Find employees whose location matches the location of their department\nDisplay: emp_id, emp_name, emp_location, dept_name, dept_location. Find departments that have no employees assigned to them\nDisplay: dept_id, dept_name, dept_head. Get the average salary of employees in each department\nDisplay: dept_name, average_salary. List the employees who earn more than the average salary of their department\nDisplay: emp_id, emp_name, emp_salary, dept_name, dept_location. # Sample DataFrames emp_data = [ Row(emp_id=1, emp_name=\"Alice\", emp_salary=50000, emp_dept_id=101, emp_location=\"New York\"), Row(emp_id=2, emp_name=\"Bob\", emp_salary=60000, emp_dept_id=102, emp_location=\"Los Angeles\"), Row(emp_id=3, emp_name=\"Charlie\", emp_salary=55000, emp_dept_id=101, emp_location=\"Chicago\"), Row(emp_id=4, emp_name=\"David\", emp_salary=70000, emp_dept_id=103, emp_location=\"San Francisco\"), Row(emp_id=5, emp_name=\"Eve\", emp_salary=48000, emp_dept_id=102, emp_location=\"Houston\") ] dept_data = [ Row(dept_id=101, dept_name=\"Engineering\", dept_head=\"John\", dept_location=\"New York\"), Row(dept_id=102, dept_name=\"Marketing\", dept_head=\"Mary\", dept_location=\"Los Angeles\"), Row(dept_id=103, dept_name=\"Finance\", dept_head=\"Frank\", dept_location=\"Chicago\") ] emp_columns = [\"emp_id\", \"emp_name\", \"emp_salary\", \"emp_dept_id\", \"emp_location\"] dept_columns = [\"dept_id\", \"dept_name\", \"dept_head\", \"dept_location\"] emp_df = spark.createDataFrame(emp_data, emp_columns) dept_df = spark.createDataFrame(dept_data, dept_columns) # Display emp data print(\"emp_data:\") emp_df.show() # Display dept data print(\"dept_data:\") dept_df.show() # Inner Join on emp_dept_id and dept_id inner_join = emp_df.join(dept_df, emp_df[\"emp_dept_id\"] == dept_df[\"dept_id\"], \"inner\") # Display the result print(\"Inner Join Result:\") inner_join.show() # Inner Join with Filtering Columns and WHERE Condition inner_join = emp_df.join(dept_df, emp_df[\"emp_dept_id\"] == dept_df[\"dept_id\"], \"inner\") \\ .select(\"emp_id\", \"emp_name\", \"emp_salary\", \"dept_name\", \"dept_location\") \\ .filter(\"emp_salary \u003e 55000\") # Add a WHERE condition # Display the result print(\"Inner Join with Filter and WHERE Condition:\") inner_join.show() # Left Join with Filtering Columns and WHERE Condition left_join_filtered = emp_df.join(dept_df, emp_df[\"emp_dept_id\"] == dept_df[\"dept_id\"], \"left\") \\ .select(\"emp_id\", \"emp_name\", \"dept_name\", \"dept_location\") \\ .filter(\"emp_salary \u003e 55000\") # Add a WHERE condition # Display the result print(\"Left Join with Filter and WHERE Condition:\") left_join_filtered.show() # Left Anti Join left_anti_join = emp_df.join( dept_df, emp_df[\"emp_dept_id\"] == dept_df[\"dept_id\"], \"left_anti\" ) # Display the result print(\"Left Anti Join Result:\") left_anti_join.show() Practice 3 PySpark Coding Questions List each employee along with their manager’s name\nDisplay: employee, manager. Find employees who do not have a manager (CEO-level employees)\nDisplay: employee, manager. Find all employees who directly report to “Manager A”\nDisplay: empid, ename, mrgid. Determine the hierarchy level of each employee\nCEO → Level 1, direct reports to CEO → Level 2, and so on. Display: empid, ename, mrgid, level. from pyspark.sql import SparkSession from pyspark.sql.functions import col, expr # Create a Spark session spark = SparkSession.builder.appName(\"EmployeeHierarchy\").getOrCreate() # Sample data data = [ (1, None, \"CEO\"), (2, 1, \"Manager A\"), (3, 1, \"Manager B\"), (4, 2, \"Employee X\"), (5, 3, \"Employee Y\"), ] columns = [\"empid\", \"mrgid\", \"ename\"] employee_df = spark.createDataFrame(data, columns) # Display the result print(\"emp_data:\") employee_df.show() # Self-join to find the manager and CEO manager_df = employee_df.alias(\"e\") \\ .join(employee_df.alias(\"m\"), col(\"e.mrgid\") == col(\"m.empid\"), \"left\") \\ .select( col(\"e.ename\").alias(\"employee\"), col(\"m.ename\").alias(\"manager\") ) # Display the result print(\"mgr:\") manager_df.show() # Filter for employees without a manager (CEO) manager_df2 = employee_df.alias(\"e1\") \\ .join(employee_df.alias(\"m1\"), col(\"e1.mrgid\") == col(\"m1.empid\"), \"left\") \\ .select( col(\"e1.ename\").alias(\"employee\"), col(\"m1.ename\").alias(\"manager\") ) \\ .filter(col(\"manager\").isNull()) # Display the result manager_df2.show()",
    "description": "Joins in PySpark In PySpark Joins are used to combine two DataFrames based on a common column or condition.\nTypes of Joins in PySpark Inner Join: Matches rows from both DataFrames. df1.join(df2, df1.common_column == df2.common_column, \"inner\") Left/Right Join: Keeps all rows from the left or right DataFrame and matches where possible. df1.join(df2, df1.common_column == df2.common_column, \"left\") df1.join(df2, df1.common_column == df2.common_column, \"right\") Full Join: Keeps all rows from both DataFrames. df1.join(df2, df1.common_column == df2.common_column, \"outer\") Left Semi: Filters df1 to rows that match df2 without including columns from df2. df1.join(df2, df1.common_column == df2.common_column, \"left_semi\") Left Anti: Filters df1 to rows that do not match df2. df1.join(df2, df1.common_column == df2.common_column, \"left_anti\") Cross Join: Returns the Cartesian product, combining all rows of both DataFrames. df1.crossJoin(df2) Explicit Condition Join: Allows complex join conditions, including columns with different names. df1.join(df2, df1.columnA == df2.columnB, \"inner\") df1.join(df2, df1.common_column == df2.common_column, \"inner\") df1.join(df2, df1.common_column == df2.common_column, \"left\") df1.join(df2, df1.common_column == df2.common_column, \"right\") df1.join(df2, df1.common_column == df2.common_column, \"outer\") df1.join(df2, df1.common_column == df2.common_column, \"left_semi\") df1.join(df2, df1.common_column == df2.common_column, \"left_anti\") df1.crossJoin(df2) df1.join(df2, df1.columnA == df2.columnB, \"inner\") Practice-1 from pyspark.sql import SparkSession from pyspark.sql import Row from pyspark.sql.functions import broadcast # Initialize Spark session spark = SparkSession.builder.appName(\"JoinsExample\").getOrCreate() # Sample DataFrames data1 = [Row(id=0), Row(id=1), Row(id=1), Row(id=None), Row(id=None)] data2 = [Row(id=1), Row(id=0), Row(id=None)] df1 = spark.createDataFrame(data1) df2 = spark.createDataFrame(data2) # Inner Join inner_join = df1.join(df2, on=\"id\", how=\"inner\") print(\"Inner Join:\") inner_join.show() # Right Join right_join = df1.join(df2, on=\"id\", how=\"right\") print(\"Right Join:\") right_join.show() # Full (Outer) Join full_join = df1.join(df2, on=\"id\", how=\"outer\") print(\"Full (Outer) Join:\") full_join.show() # Left Anti Join left_anti_join = df1.join(df2, on=\"id\", how=\"left_anti\") print(\"Left Anti Join:\") left_anti_join.show() # Right Anti Join (Equivalent to swapping DataFrames and performing Left Anti Join) right_anti_join = df2.join(df1, on=\"id\", how=\"left_anti\") print(\"Right Anti Join:\") right_anti_join.show() # Broadcast Join (Optimizing a join with a smaller DataFrame) broadcast_join = df1.join(broadcast(df2), on=\"id\", how=\"inner\") print(\"Broadcast Join:\") broadcast_join.show() Practice 2 PySpark Coding Questions Find employees whose location matches the location of their department",
    "tags": [],
    "title": "Joins",
    "uri": "/azure_data_bricks/joins/"
  },
  {
    "breadcrumb": "SQL",
    "content": "This document provides an overview of SQL string functions, which allow manipulation, transformation, and extraction of text data efficiently.\n1. Manipulations CONCAT() – String Concatenation -- Concatenate first name and country into one column SELECT CONCAT(first_name, '-', country) AS full_info FROM customers; LOWER() \u0026 UPPER() – Case Transformation -- Convert the first name to lowercase SELECT LOWER(first_name) AS lower_case_name FROM customers; -- Convert the first name to uppercase SELECT UPPER(first_name) AS upper_case_name FROM customers; TRIM() – Remove White Spaces -- Find customers whose first name contains leading or trailing spaces SELECT first_name, LEN(first_name) AS len_name, LEN(TRIM(first_name)) AS len_trim_name, LEN(first_name) - LEN(TRIM(first_name)) AS flag FROM customers WHERE LEN(first_name) != LEN(TRIM(first_name)); -- Alternative: -- WHERE first_name != TRIM(first_name) REPLACE() – Replace or Remove Values -- Remove dashes (-) from a phone number SELECT '123-456-7890' AS phone, REPLACE('123-456-7890', '-', '/') AS clean_phone; -- Replace file extension from .txt to .csv SELECT 'report.txt' AS old_filename, REPLACE('report.txt', '.txt', '.csv') AS new_filename; 2. Calculation LEN() – String Length -- Calculate the length of each customer's first name SELECT first_name, LEN(first_name) AS name_length FROM customers; 3. Substring Extraction LEFT() \u0026 RIGHT() -- Retrieve the first two characters of each first name SELECT first_name, LEFT(TRIM(first_name), 2) AS first_2_chars FROM customers; -- Retrieve the last two characters of each first name SELECT first_name, RIGHT(first_name, 2) AS last_2_chars FROM customers; SUBSTRING() -- Retrieve a list of customers' first names after removing the first character SELECT first_name, SUBSTRING(TRIM(first_name), 2, LEN(first_name)) AS trimmed_name FROM customers; 4. Nesting Functions – Nesting example SELECT first_name, UPPER(LOWER(first_name)) AS nesting FROM customers;",
    "description": "This document provides an overview of SQL string functions, which allow manipulation, transformation, and extraction of text data efficiently.\n1. Manipulations CONCAT() – String Concatenation -- Concatenate first name and country into one column SELECT CONCAT(first_name, '-', country) AS full_info FROM customers; LOWER() \u0026 UPPER() – Case Transformation -- Convert the first name to lowercase SELECT LOWER(first_name) AS lower_case_name FROM customers; -- Convert the first name to uppercase SELECT UPPER(first_name) AS upper_case_name FROM customers; TRIM() – Remove White Spaces -- Find customers whose first name contains leading or trailing spaces SELECT first_name, LEN(first_name) AS len_name, LEN(TRIM(first_name)) AS len_trim_name, LEN(first_name) - LEN(TRIM(first_name)) AS flag FROM customers WHERE LEN(first_name) != LEN(TRIM(first_name)); -- Alternative: -- WHERE first_name != TRIM(first_name) REPLACE() – Replace or Remove Values -- Remove dashes (-) from a phone number SELECT '123-456-7890' AS phone, REPLACE('123-456-7890', '-', '/') AS clean_phone; -- Replace file extension from .txt to .csv SELECT 'report.txt' AS old_filename, REPLACE('report.txt', '.txt', '.csv') AS new_filename; 2. Calculation LEN() – String Length -- Calculate the length of each customer's first name SELECT first_name, LEN(first_name) AS name_length FROM customers; 3. Substring Extraction LEFT() \u0026 RIGHT() -- Retrieve the first two characters of each first name SELECT first_name, LEFT(TRIM(first_name), 2) AS first_2_chars FROM customers; -- Retrieve the last two characters of each first name SELECT first_name, RIGHT(first_name, 2) AS last_2_chars FROM customers; SUBSTRING() -- Retrieve a list of customers' first names after removing the first character SELECT first_name, SUBSTRING(TRIM(first_name), 2, LEN(first_name)) AS trimmed_name FROM customers; 4. Nesting Functions – Nesting example SELECT first_name, UPPER(LOWER(first_name)) AS nesting FROM customers;",
    "tags": [],
    "title": "String Functions",
    "uri": "/sql/string-functions/"
  },
  {
    "breadcrumb": "SQL",
    "content": "This document provides an overview of SQL number functions, which allow performing mathematical operations and formatting numerical values.\n1. Rounding Functions ROUND() – Rounding Numbers -- Demonstrate rounding a number to different decimal places SELECT 3.516 AS original_number, ROUND(3.516, 2) AS round_2, ROUND(3.516, 1) AS round_1, ROUND(3.516, 0) AS round_0; 2. Absolute Value Function ABS() – Absolute Value -- Demonstrate absolute value function SELECT -10 AS original_number, ABS(-10) AS absolute_value_negative, ABS(10) AS absolute_value_positive;",
    "description": "This document provides an overview of SQL number functions, which allow performing mathematical operations and formatting numerical values.\n1. Rounding Functions ROUND() – Rounding Numbers -- Demonstrate rounding a number to different decimal places SELECT 3.516 AS original_number, ROUND(3.516, 2) AS round_2, ROUND(3.516, 1) AS round_1, ROUND(3.516, 0) AS round_0; 2. Absolute Value Function ABS() – Absolute Value -- Demonstrate absolute value function SELECT -10 AS original_number, ABS(-10) AS absolute_value_negative, ABS(10) AS absolute_value_positive;",
    "tags": [],
    "title": "Number Functions",
    "uri": "/sql/number-functions/"
  },
  {
    "breadcrumb": "ADB",
    "content": "when and otherwise The when and otherwise functions in PySpark provide a way to create conditional expressions within a DataFrame, allowing you to specify different values for new or existing columns based on specific conditions.\nwhen:\nThe when function in PySpark is used to define a condition.\nIf the condition is met, it returns the specified value.\nYou can chain multiple when conditions to handle various cases.\notherwise:\nThe otherwise function specifies a default value to return if none of the conditions in the when statements are met.\nfrom pyspark.sql import SparkSession from pyspark.sql.functions import when from pyspark.sql.types import StructType, StructField, IntegerType, StringType # Initialize Spark session spark = SparkSession.builder.appName(\"WhenOtherwiseExample\").getOrCreate() # Define the schema for the dataset schema = StructType([ StructField(\"name\", StringType(), True), StructField(\"age\", IntegerType(), True), StructField(\"salary\", IntegerType(), True) ]) # Create a sample dataset data = [ (\"Alice\", 25, 3000), (\"Bob\", 35, 4000), (\"Charlie\", 40, 5000), (\"David\", 28, 4500), (\"Eve\", 32, 3500) ] # Create DataFrame df = spark.createDataFrame(data, schema) df.show() # Apply 'when' and 'otherwise' to add new columns based on conditions df = ( df.withColumn(\"status\", when(df.age \u003c 30, \"Young\").otherwise(\"Adult\")) .withColumn(\"income_bracket\", when(df.salary \u003c 4000, \"Low\") .when((df.salary \u003e= 4000) \u0026 (df.salary \u003c= 4500), \"Medium\") .otherwise(\"High\")) ) # Show the result df.show() Explanation status column\nAssigns \"Young\" if age \u003c 30. Otherwise assigns \"Adult\". income_bracket column\nAssigns \"Low\" if salary \u003c 4000. Assigns \"Medium\" if 4000 \u003c= salary \u003c= 4500. Assigns \"High\" for any other salary values. This approach allows for flexible handling of multiple conditions in PySpark DataFrames using when and otherwise.\ncast() and printSchema() In PySpark, the cast() function is used to change the data type of a column within a DataFrame.\nThis is helpful when you need to standardize column data types for data processing, schema consistency, or compatibility with other operations.\nPurpose:\nThe cast() function allows you to change the data type of a column, useful in situations like standardizing formats (e.g., converting strings to dates or integers).\nSyntax:\nThe cast() function is applied on individual columns and requires specifying the target data type in quotes.\nMultiple Columns:\nYou can cast multiple columns at once by using a list of cast expressions and passing them to select().\nSupported Data Types:\nPySpark supports various data types for casting, including:\nStringType IntegerType (or \"int\") DoubleType (or \"double\") DateType TimestampType BooleanType Others, based on the data types available in PySpark. from pyspark.sql.functions import col # Single column cast df = df.withColumn(\"column_name\", col(\"column_name\").cast(\"target_data_type\")) # Multiple columns cast with select cast_expr = [ col(\"column1_name\").cast(\"target_data_type1\"), col(\"column2_name\").cast(\"target_data_type2\"), # More columns and data types as needed ] df = df.select(*cast_expr) Example Let’s create a dataset and apply cast() to change the data types of multiple columns:\nfrom pyspark.sql import SparkSession from pyspark.sql.functions import col from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType # Initialize Spark session spark = SparkSession.builder.appName(\"CastExample\").getOrCreate() # Define the schema for the dataset schema = StructType([ StructField(\"name\", StringType(), True), StructField(\"age\", StringType(), True), # Stored as StringType initially StructField(\"height\", StringType(), True) # Stored as StringType initially ]) # Create a sample dataset data = [ (\"Alice\", \"25\", \"5.5\"), (\"Bob\", \"35\", \"6.1\"), (\"Charlie\", \"40\", \"5.8\"), ] # Create DataFrame df = spark.createDataFrame(data, schema) # Show schema and data before casting df.printSchema() df.show() # Define cast expressions for multiple columns cast_expr = [ col(\"name\").cast(\"string\"), col(\"age\").cast(\"int\"), # Casting age to IntegerType col(\"height\").cast(\"double\") # Casting height to DoubleType ] # Apply the cast expressions to the DataFrame df = df.select(*cast_expr) # Show the result df.printSchema() df.show() Explanation age column: Initially stored as StringType, it’s cast to IntegerType (or \"int\"). height column: Initially stored as StringType, it’s cast to DoubleType (or \"double\"). Advantages of Using cast() Schema Alignment: Ensures data types in different tables or DataFrames are compatible for joining or union operations. Data Consistency: Ensures all columns conform to expected data types for downstream data processing. Error Reduction: Minimizes issues arising from mismatched data types in computations or transformations. This approach using cast() provides a flexible and powerful way to manage data types in PySpark.\nprintSchema() Method in PySpark Purpose:\nTo display the schema of a DataFrame, which includes the column names, data types, and nullability of each column. Output Structure:\nThe schema is presented in a tree-like structure showing:\nColumn Name: The name of the column. Data Type: The data type of the column (e.g., string, integer, double, boolean, etc.). Nullability: Indicates whether the column can contain null values (e.g., nullable = true). Usage:\nCall df.printSchema() on a DataFrame df to see its structure. Useful for verifying the structure of the DataFrame after operations like select(), withColumn(), or cast(). union and unionAll Overview Purpose: Both union and unionAll are used to combine two DataFrames into a single DataFrame. DataFrame Compatibility: The two DataFrames must have the same schema (i.e., the same column names and data types) to perform the union operation. union() Functionality:\nCombines two DataFrames and retains all rows, including duplicates. Behavior:\nThe union() method does not remove duplicate rows, resulting in a DataFrame that may contain duplicates. unionAll() Functionality:\nCombines two DataFrames and retains all rows, including duplicates. Behavior:\nThe unionAll() method performs the union operation but does not eliminate duplicate rows (similar to union). Syntax # Using union to retain all rows including duplicates unioned_df = df1.union(df2) # Using unionAll to retain all rows including duplicates unionAll_df = df1.unionAll(df2) Example: Using union and unionAll in PySpark from pyspark.sql import SparkSession # Initialize Spark session spark = SparkSession.builder.appName(\"UnionExample\").getOrCreate() # Sample DataFrames data1 = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)] data2 = [(\"David\", 40), (\"Eve\", 45), (\"Alice\", 25)] columns = [\"name\", \"age\"] df1 = spark.createDataFrame(data1, columns) df2 = spark.createDataFrame(data2, columns) # Using union to retain all rows including duplicates unioned_df = df1.union(df2) # Using unionAll to retain all rows unionAll_df = df1.unionAll(df2) # Show the results print(\"unioned_df (No duplicates removed):\") unioned_df.show() print(\"unionAll_df (duplicates retained):\") unionAll_df.show() ## Removing Duplicate Rows in PySpark # Remove duplicate rows and create a new DataFrame unique_df = unioned_df.dropDuplicates() # or unique_df = unioned_df.distinct() print(\"unique_df (after removing duplicates):\") unique_df.show() Union and UnionByName In PySpark, both union and unionByName are operations that allow you to combine two or more DataFrames. However, they do this in slightly different ways, particularly regarding how they handle column names.\n1. union Definition:\nThe union() function is used to combine two DataFrames with the same schema (i.e., the same number of columns with the same data types).\nIt appends the rows of one DataFrame to the other.\nKey Characteristics:\nThe DataFrames must have the same number of columns. The columns must have compatible data types. It does not automatically handle column names that differ between DataFrames. Syntax DataFrame.union(otherDataFrame) from pyspark.sql import SparkSession # Create a Spark session spark = SparkSession.builder.appName(\"Union Example\").getOrCreate() # Create two DataFrames with the same schema data1 = [(\"Alice\", 1), (\"Bob\", 2)] data2 = [(\"Cathy\", 3), (\"David\", 4)] columns = [\"Name\", \"Id\"] df1 = spark.createDataFrame(data1, columns) df2 = spark.createDataFrame(data2, columns) # Perform union result_union = df1.union(df2) # Show the result result_union.show() 2. unionByName Definition:\nThe unionByName() function allows you to combine two DataFrames by matching column names.\nIf the DataFrames do not have the same schema, it will fill in missing columns with null.\nKey Characteristics:\nMatches DataFrames by column names rather than position. If the DataFrames have different columns, it will include all columns and fill in null for missing values in any DataFrame. You can specify allowMissingColumns=True to ignore missing columns. Syntax DataFrame.unionByName(otherDataFrame, allowMissingColumns=False) # Create two DataFrames with different schemas data3 = [(\"Eve\", 5), (\"Frank\", 6)] data4 = [(\"Grace\", \"New York\"), (\"Hannah\", \"Los Angeles\")] columns1 = [\"Name\", \"Id\"] columns2 = [\"Name\", \"City\"] df3 = spark.createDataFrame(data3, columns1) df4 = spark.createDataFrame(data4, columns2) # Perform unionByName (with allowMissingColumns=True to handle schema differences) result_union_by_name = df3.unionByName(df4, allowMissingColumns=True) # Show the result result_union_by_name.show() result_union_by_name.show() Summary of Differences union(): Requires DataFrames to have the same schema (same number of columns and compatible data types). It combines rows without checking column names. unionByName(): Matches DataFrames by column names. It can handle different schemas and fill missing columns with null (when allowMissingColumns=True). Feature Union UnionByName Column Matching Positional By Name Missing Columns Handling Does not allow Allows with null for missing Schema Requirement Must be identical Can differ Conclusion In PySpark:\nUse union() when you have DataFrames with the same schema and need a straightforward concatenation. Use unionByName() when your DataFrames have different schemas and you want to combine them by matching column names while handling missing columns.",
    "description": "when and otherwise The when and otherwise functions in PySpark provide a way to create conditional expressions within a DataFrame, allowing you to specify different values for new or existing columns based on specific conditions.\nwhen:\nThe when function in PySpark is used to define a condition.\nIf the condition is met, it returns the specified value.\nYou can chain multiple when conditions to handle various cases.\notherwise:\nThe otherwise function specifies a default value to return if none of the conditions in the when statements are met.",
    "tags": [],
    "title": "When|Cast|Union",
    "uri": "/azure_data_bricks/when/"
  },
  {
    "breadcrumb": "SQL",
    "content": "This guide demonstrates various date and time functions in SQL.\nIt covers GETDATE, DATETRUNC, DATENAME, DATEPART, YEAR, MONTH, DAY, EOMONTH, FORMAT, CONVERT, CAST, DATEADD, DATEDIFF, and ISDATE.\n1. GETDATE() | Date Values -- Display OrderID, CreationTime, a hard-coded date, and the current system date SELECT OrderID, CreationTime, '2025-08-20' AS HardCoded, GETDATE() AS Today FROM Sales.Orders; 2. Date Part Extractions (DATETRUNC, DATENAME, DATEPART, YEAR, MONTH, DAY) -- Extract various parts of CreationTime SELECT OrderID, CreationTime, DATETRUNC(year, CreationTime) AS Year_dt, DATETRUNC(day, CreationTime) AS Day_dt, DATETRUNC(minute, CreationTime) AS Minute_dt, DATENAME(month, CreationTime) AS Month_dn, DATENAME(weekday, CreationTime) AS Weekday_dn, DATENAME(day, CreationTime) AS Day_dn, DATENAME(year, CreationTime) AS Year_dn, DATEPART(year, CreationTime) AS Year_dp, DATEPART(month, CreationTime) AS Month_dp, DATEPART(day, CreationTime) AS Day_dp, DATEPART(hour, CreationTime) AS Hour_dp, DATEPART(quarter, CreationTime) AS Quarter_dp, DATEPART(week, CreationTime) AS Week_dp, YEAR(CreationTime) AS Year, MONTH(CreationTime) AS Month, DAY(CreationTime) AS Day FROM Sales.Orders; 3. DATETRUNC() Data Aggregation -- Aggregate orders by year SELECT DATETRUNC(year, CreationTime) AS Creation, COUNT(*) AS OrderCount FROM Sales.Orders GROUP BY DATETRUNC(year, CreationTime); 4. EOMONTH() -- Display end-of-month date for each order SELECT OrderID, CreationTime, EOMONTH(CreationTime) AS EndOfMonth FROM Sales.Orders; 5. Date Parts | Use Cases -- Orders per year SELECT YEAR(OrderDate) AS OrderYear, COUNT(*) AS TotalOrders FROM Sales.Orders GROUP BY YEAR(OrderDate); -- Orders per month SELECT MONTH(OrderDate) AS OrderMonth, COUNT(*) AS TotalOrders FROM Sales.Orders GROUP BY MONTH(OrderDate); -- Orders per month (friendly names) SELECT DATENAME(month, OrderDate) AS OrderMonth, COUNT(*) AS TotalOrders FROM Sales.Orders GROUP BY DATENAME(month, OrderDate); -- Orders placed in February SELECT * FROM Sales.Orders WHERE MONTH(OrderDate) = 2; 6. FORMAT() -- Format CreationTime into different formats SELECT OrderID, CreationTime, FORMAT(CreationTime, 'MM-dd-yyyy') AS USA_Format, FORMAT(CreationTime, 'dd-MM-yyyy') AS EURO_Format, FORMAT(CreationTime, 'dd') AS dd, FORMAT(CreationTime, 'ddd') AS ddd, FORMAT(CreationTime, 'dddd') AS dddd, FORMAT(CreationTime, 'MM') AS MM, FORMAT(CreationTime, 'MMM') AS MMM, FORMAT(CreationTime, 'MMMM') AS MMMM FROM Sales.Orders; -- Custom format example SELECT OrderID, CreationTime, 'Day ' + FORMAT(CreationTime, 'ddd MMM') + ' Q' + DATENAME(quarter, CreationTime) + ' ' + FORMAT(CreationTime, 'yyyy hh:mm:ss tt') AS CustomFormat FROM Sales.Orders; -- Orders per month (formatted \"MMM yy\") SELECT FORMAT(CreationTime, 'MMM yy') AS OrderDate, COUNT(*) AS TotalOrders FROM Sales.Orders GROUP BY FORMAT(CreationTime, 'MMM yy'); 7. CONVERT() -- Conversion using CONVERT SELECT CONVERT(INT, '123') AS [String to Int CONVERT], CONVERT(DATE, '2025-08-20') AS [String to Date CONVERT], CreationTime, CONVERT(DATE, CreationTime) AS [Datetime to Date CONVERT], CONVERT(VARCHAR, CreationTime, 32) AS [USA Std. Style:32], CONVERT(VARCHAR, CreationTime, 34) AS [EURO Std. Style:34] FROM Sales.Orders; 8. CAST() -- Conversion using CAST SELECT CAST('123' AS INT) AS [String to Int], CAST(123 AS VARCHAR) AS [Int to String], CAST('2025-08-20' AS DATE) AS [String to Date], CAST('2025-08-20' AS DATETIME2) AS [String to Datetime], CreationTime, CAST(CreationTime AS DATE) AS [Datetime to Date] FROM Sales.Orders; 9. DATEADD / DATEDIFF -- Date arithmetic SELECT OrderID, OrderDate, DATEADD(day, -10, OrderDate) AS TenDaysBefore, DATEADD(month, 3, OrderDate) AS ThreeMonthsLater, DATEADD(year, 2, OrderDate) AS TwoYearsLater FROM Sales.Orders; -- Employee ages SELECT EmployeeID, BirthDate, DATEDIFF(year, BirthDate, GETDATE()) AS Age FROM Sales.Employees; -- Avg shipping duration SELECT MONTH(OrderDate) AS OrderMonth, AVG(DATEDIFF(day, OrderDate, ShipDate)) AS AvgShip FROM Sales.Orders GROUP BY MONTH(OrderDate); -- Time gap analysis SELECT OrderID, OrderDate AS CurrentOrderDate, LAG(OrderDate) OVER (ORDER BY OrderDate) AS PreviousOrderDate, DATEDIFF(day, LAG(OrderDate) OVER (ORDER BY OrderDate), OrderDate) AS NrOfDays FROM Sales.Orders; 10. ISDATE() -- Validate OrderDate using ISDATE SELECT OrderDate, ISDATE(OrderDate) AS IsValidDate, CASE WHEN ISDATE(OrderDate) = 1 THEN CAST(OrderDate AS DATE) ELSE '9999-01-01' END AS NewOrderDate FROM ( SELECT '2025-08-20' AS OrderDate UNION SELECT '2025-08-21' UNION SELECT '2025-08-23' UNION SELECT '2025-08' ) AS t; -- WHERE ISDATE(OrderDate) = 0",
    "description": "This guide demonstrates various date and time functions in SQL.\nIt covers GETDATE, DATETRUNC, DATENAME, DATEPART, YEAR, MONTH, DAY, EOMONTH, FORMAT, CONVERT, CAST, DATEADD, DATEDIFF, and ISDATE.\n1. GETDATE() | Date Values -- Display OrderID, CreationTime, a hard-coded date, and the current system date SELECT OrderID, CreationTime, '2025-08-20' AS HardCoded, GETDATE() AS Today FROM Sales.Orders; 2. Date Part Extractions (DATETRUNC, DATENAME, DATEPART, YEAR, MONTH, DAY) -- Extract various parts of CreationTime SELECT OrderID, CreationTime, DATETRUNC(year, CreationTime) AS Year_dt, DATETRUNC(day, CreationTime) AS Day_dt, DATETRUNC(minute, CreationTime) AS Minute_dt, DATENAME(month, CreationTime) AS Month_dn, DATENAME(weekday, CreationTime) AS Weekday_dn, DATENAME(day, CreationTime) AS Day_dn, DATENAME(year, CreationTime) AS Year_dn, DATEPART(year, CreationTime) AS Year_dp, DATEPART(month, CreationTime) AS Month_dp, DATEPART(day, CreationTime) AS Day_dp, DATEPART(hour, CreationTime) AS Hour_dp, DATEPART(quarter, CreationTime) AS Quarter_dp, DATEPART(week, CreationTime) AS Week_dp, YEAR(CreationTime) AS Year, MONTH(CreationTime) AS Month, DAY(CreationTime) AS Day FROM Sales.Orders; 3. DATETRUNC() Data Aggregation -- Aggregate orders by year SELECT DATETRUNC(year, CreationTime) AS Creation, COUNT(*) AS OrderCount FROM Sales.Orders GROUP BY DATETRUNC(year, CreationTime); 4. EOMONTH() -- Display end-of-month date for each order SELECT OrderID, CreationTime, EOMONTH(CreationTime) AS EndOfMonth FROM Sales.Orders; 5. Date Parts | Use Cases -- Orders per year SELECT YEAR(OrderDate) AS OrderYear, COUNT(*) AS TotalOrders FROM Sales.Orders GROUP BY YEAR(OrderDate); -- Orders per month SELECT MONTH(OrderDate) AS OrderMonth, COUNT(*) AS TotalOrders FROM Sales.Orders GROUP BY MONTH(OrderDate); -- Orders per month (friendly names) SELECT DATENAME(month, OrderDate) AS OrderMonth, COUNT(*) AS TotalOrders FROM Sales.Orders GROUP BY DATENAME(month, OrderDate); -- Orders placed in February SELECT * FROM Sales.Orders WHERE MONTH(OrderDate) = 2; 6. FORMAT() -- Format CreationTime into different formats SELECT OrderID, CreationTime, FORMAT(CreationTime, 'MM-dd-yyyy') AS USA_Format, FORMAT(CreationTime, 'dd-MM-yyyy') AS EURO_Format, FORMAT(CreationTime, 'dd') AS dd, FORMAT(CreationTime, 'ddd') AS ddd, FORMAT(CreationTime, 'dddd') AS dddd, FORMAT(CreationTime, 'MM') AS MM, FORMAT(CreationTime, 'MMM') AS MMM, FORMAT(CreationTime, 'MMMM') AS MMMM FROM Sales.Orders; -- Custom format example SELECT OrderID, CreationTime, 'Day ' + FORMAT(CreationTime, 'ddd MMM') + ' Q' + DATENAME(quarter, CreationTime) + ' ' + FORMAT(CreationTime, 'yyyy hh:mm:ss tt') AS CustomFormat FROM Sales.Orders; -- Orders per month (formatted \"MMM yy\") SELECT FORMAT(CreationTime, 'MMM yy') AS OrderDate, COUNT(*) AS TotalOrders FROM Sales.Orders GROUP BY FORMAT(CreationTime, 'MMM yy'); 7. CONVERT() -- Conversion using CONVERT SELECT CONVERT(INT, '123') AS [String to Int CONVERT], CONVERT(DATE, '2025-08-20') AS [String to Date CONVERT], CreationTime, CONVERT(DATE, CreationTime) AS [Datetime to Date CONVERT], CONVERT(VARCHAR, CreationTime, 32) AS [USA Std. Style:32], CONVERT(VARCHAR, CreationTime, 34) AS [EURO Std. Style:34] FROM Sales.Orders; 8. CAST() -- Conversion using CAST SELECT CAST('123' AS INT) AS [String to Int], CAST(123 AS VARCHAR) AS [Int to String], CAST('2025-08-20' AS DATE) AS [String to Date], CAST('2025-08-20' AS DATETIME2) AS [String to Datetime], CreationTime, CAST(CreationTime AS DATE) AS [Datetime to Date] FROM Sales.Orders; 9. DATEADD / DATEDIFF -- Date arithmetic SELECT OrderID, OrderDate, DATEADD(day, -10, OrderDate) AS TenDaysBefore, DATEADD(month, 3, OrderDate) AS ThreeMonthsLater, DATEADD(year, 2, OrderDate) AS TwoYearsLater FROM Sales.Orders; -- Employee ages SELECT EmployeeID, BirthDate, DATEDIFF(year, BirthDate, GETDATE()) AS Age FROM Sales.Employees; -- Avg shipping duration SELECT MONTH(OrderDate) AS OrderMonth, AVG(DATEDIFF(day, OrderDate, ShipDate)) AS AvgShip FROM Sales.Orders GROUP BY MONTH(OrderDate); -- Time gap analysis SELECT OrderID, OrderDate AS CurrentOrderDate, LAG(OrderDate) OVER (ORDER BY OrderDate) AS PreviousOrderDate, DATEDIFF(day, LAG(OrderDate) OVER (ORDER BY OrderDate), OrderDate) AS NrOfDays FROM Sales.Orders; 10. ISDATE() -- Validate OrderDate using ISDATE SELECT OrderDate, ISDATE(OrderDate) AS IsValidDate, CASE WHEN ISDATE(OrderDate) = 1 THEN CAST(OrderDate AS DATE) ELSE '9999-01-01' END AS NewOrderDate FROM ( SELECT '2025-08-20' AS OrderDate UNION SELECT '2025-08-21' UNION SELECT '2025-08-23' UNION SELECT '2025-08' ) AS t; -- WHERE ISDATE(OrderDate) = 0",
    "tags": [],
    "title": "Date \u0026 Time Functions",
    "uri": "/sql/date-time1/"
  },
  {
    "breadcrumb": "ADB",
    "content": "Windows Function in PySpark 1. Introduction to Window Functions Window functions allow you to perform calculations across a set of rows related to the current row within a specified partition.\nUnlike groupBy functions, window functions do not reduce the number of rows in the result; instead, they calculate a value for each row based on the specified window.\n2. Importing Required Libraries To use window functions, import the necessary modules from PySpark:\nfrom pyspark.sql import SparkSession from pyspark.sql import functions as F from pyspark.sql.window import Window 3. Creating a Window Specification A window specification defines how rows will be grouped (partitioned) and ordered within each group.\nExample – Basic Window Specification:\nwindow_spec = Window.partitionBy(\"category\").orderBy(\"timestamp\")\nExample – Advanced Window Specification:\nwindow_spec = Window.partitionBy(\"category\", \"sub_category\").orderBy(F.col(\"timestamp\"), F.col(\"score\"))\n4. Common Window Functions a. Row Number Function: row_number()\nDescription: Assigns a unique integer to each row within the partition (starting from 1).\ndf = df.withColumn(\"row_number\", F.row_number().over(window_spec))\nb. Rank Function: rank()\nDescription: Assigns the same rank to rows with the same values in the order criteria. The next rank has a gap.\ndf = df.withColumn(\"rank\", F.rank().over(window_spec))\nc. Dense Rank Function: dense_rank()\nDescription: Similar to rank, but does not leave gaps in ranking.\ndf = df.withColumn(\"dense_rank\", F.dense_rank().over(window_spec))\nd. Lead and Lag Functions: lead(), lag()\nDescription:\nlead() → value of the next row within the window.\nlag() → value of the previous row within the window.\ndf = df.withColumn(\"next_value\", F.lead(\"value\").over(window_spec)) df = df.withColumn(\"previous_value\", F.lag(\"value\").over(window_spec))\ne. Aggregation Functions Window functions can also compute aggregated values across the specified window.\ndf = df.withColumn(\"avg_value\", F.avg(\"value\").over(window_spec))\nOther common aggregations:\nSum: F.sum(\"column_name\").over(window_spec)\nMin: F.min(\"column_name\").over(window_spec)\nMax: F.max(\"column_name\").over(window_spec)\n5. Putting It All Together – Example from pyspark.sql import SparkSession from pyspark.sql import functions as F from pyspark.sql.window import Window # Initialize Spark session spark = SparkSession.builder.appName(\"WindowFunctionsExample\").getOrCreate() # Sample DataFrame data = [ (\"A\", \"X\", 1, \"2023-01-01\"), (\"A\", \"X\", 2, \"2023-01-02\"), (\"A\", \"Y\", 3, \"2023-01-01\"), (\"A\", \"Y\", 3, \"2023-01-02\"), (\"B\", \"X\", 5, \"2023-01-01\"), (\"B\", \"X\", 4, \"2023-01-02\"), ] columns = [\"category\", \"sub_category\", \"value\", \"timestamp\"] df = spark.createDataFrame(data, columns) # Define window specification window_spec = Window.partitionBy(\"category\", \"sub_category\") \\ .orderBy(F.col(\"timestamp\"), F.col(\"value\")) # Apply window functions df = df.withColumn(\"row_number\", F.row_number().over(window_spec)) df = df.withColumn(\"rank\", F.rank().over(window_spec)) df = df.withColumn(\"dense_rank\", F.dense_rank().over(window_spec)) df = df.withColumn(\"next_value\", F.lead(\"value\").over(window_spec)) df = df.withColumn(\"previous_value\", F.lag(\"value\").over(window_spec)) df = df.withColumn(\"avg_value\", F.avg(\"value\").over(window_spec)) df.show() 6. Conclusion Window functions in PySpark are powerful tools for analyzing data within groups while retaining row-level details.\nBy defining window specifications and applying functions like rank, dense_rank, lead, lag, and aggregations, you can perform complex analytics efficiently.\nWindows Function in PySpark – Part 2 from pyspark.sql import SparkSession from pyspark.sql.window import Window import pyspark.sql.functions as F # Sample data data = [ (\"Alice\", 100), (\"Bob\", 200), (\"Charlie\", 200), (\"David\", 300), (\"Eve\", 400), (\"Frank\", 500), (\"Grace\", 500), (\"Hank\", 600), (\"Ivy\", 700), (\"Jack\", 800) ] columns = [\"Name\", \"Score\"] df = spark.createDataFrame(data, columns) # Define window window_spec = Window.orderBy(\"Score\") # Ranking functions df1 = df.withColumn(\"Rank\", F.rank().over(window_spec)) df2 = df.withColumn(\"DenseRank\", F.dense_rank().over(window_spec)) df3 = df.withColumn(\"RowNumber\", F.row_number().over(window_spec)) # Lead \u0026 Lag df4 = df.withColumn(\"ScoreDifferenceWithNext\", F.lead(\"Score\").over(window_spec) - df[\"Score\"]) df5 = df.withColumn(\"ScoreDifferenceWithPrevious\", df[\"Score\"] - F.lag(\"Score\").over(window_spec)) Windows Function in PySpark – Part 3 (Student Marks Analysis) from pyspark.sql import SparkSession from pyspark.sql.window import Window import pyspark.sql.functions as F # Updated sample data data = [ (\"Alice\", \"Math\", 90, 1), (\"Alice\", \"Science\", 85, 1), (\"Alice\", \"History\", 78, 1), (\"Bob\", \"Math\", 80, 1), (\"Bob\", \"Science\", 81, 1), (\"Bob\", \"History\", 77, 1), (\"Charlie\", \"Math\", 75, 1), (\"Charlie\", \"Science\", 82, 1), (\"Charlie\", \"History\", 79, 1), (\"Alice\", \"Physics\", 86, 2), (\"Alice\", \"Chemistry\", 92, 2), (\"Alice\", \"Biology\", 80, 2), (\"Bob\", \"Physics\", 94, 2), (\"Bob\", \"Chemistry\", 91, 2), (\"Bob\", \"Biology\", 96, 2), (\"Charlie\", \"Physics\", 89, 2), (\"Charlie\", \"Chemistry\", 88, 2), (\"Charlie\", \"Biology\", 85, 2), (\"Alice\", \"Computer Science\", 95, 3), (\"Alice\", \"Electronics\", 91, 3), (\"Alice\", \"Geography\", 97, 3), (\"Bob\", \"Computer Science\", 88, 3), (\"Bob\", \"Electronics\", 66, 3), (\"Bob\", \"Geography\", 92, 3), (\"Charlie\", \"Computer Science\", 92, 3), (\"Charlie\", \"Electronics\", 97, 3), (\"Charlie\", \"Geography\", 99, 3) ] columns = [\"First Name\", \"Subject\", \"Marks\", \"Semester\"] df = spark.createDataFrame(data, columns) # 1. Max marks in each semester window_spec_max_marks = Window.partitionBy(\"Semester\").orderBy(F.desc(\"Marks\")) max_marks_df = df.withColumn(\"Rank\", F.rank().over(window_spec_max_marks)) top_scorer = max_marks_df.filter(max_marks_df[\"Rank\"] == 1) # 2. Percentage of each student window_spec_total_marks = Window.partitionBy(\"First Name\", \"Semester\") df = df.withColumn(\"TotalMarks\", F.sum(\"Marks\").over(window_spec_total_marks)) df = df.withColumn(\"Percentage\", (F.col(\"TotalMarks\") / (3*100)).cast(\"decimal(5,2)\")*100) df2 = df.groupBy(\"First Name\",\"Semester\").agg(F.max(\"TotalMarks\").alias(\"TotalMarks\"), F.max(\"Percentage\").alias(\"Percentage\")) # 3. Top rank holder in each semester window_spec_rank = Window.partitionBy(\"Semester\").orderBy(F.desc(\"Percentage\")) rank_df = df.withColumn(\"Rank\", F.rank().over(window_spec_rank)) top_rank_holder = rank_df.filter(rank_df[\"Rank\"] == 1).select(\"First Name\",\"Semester\",\"Rank\",\"Percentage\").distinct() # 4. Max marks in each subject in each semester window_spec_max_subject_marks = Window.partitionBy(\"Semester\",\"Subject\").orderBy(F.desc(\"Marks\")) max_subject_marks_df = df.withColumn(\"Rank\", F.rank().over(window_spec_max_subject_marks)) max_subject_scorer = max_subject_marks_df.filter(max_subject_marks_df[\"Rank\"] == 1) Windows Function in PySpark – Part 4 (Highest Salary per Department) from pyspark.sql import functions as F from pyspark.sql.window import Window # Employee Data emp_data = [ (1, \"Alice\", 1, 6300), (2, \"Bob\", 1, 6200), (3, \"Charlie\", 2, 7000), (4, \"David\", 2, 7200), (5, \"Eve\", 1, 6300), (6, \"Frank\", 2, 7100) ] # Department Data dept_data = [ (1, \"HR\"), (2, \"Finance\") ] # Create DataFrames emp_df = spark.createDataFrame(emp_data, [\"EmpId\",\"EmpName\",\"DeptId\",\"Salary\"]) dept_df = spark.createDataFrame(dept_data, [\"DeptId\",\"DeptName\"]) # Window for salary ranking window_spec = Window.partitionBy(\"DeptId\").orderBy(F.desc(\"Salary\")) # Add rank \u0026 filter top salary ranked_salary_df = emp_df.withColumn(\"Rank\", F.rank().over(window_spec)) result_df = ranked_salary_df.filter(F.col(\"Rank\") == 1) # Join department names result_df = result_df.join(dept_df, [\"DeptId\"], \"left\") # Final Output result_df.select(\"EmpName\",\"DeptName\",\"Salary\").show()",
    "description": "Windows Function in PySpark 1. Introduction to Window Functions Window functions allow you to perform calculations across a set of rows related to the current row within a specified partition.\nUnlike groupBy functions, window functions do not reduce the number of rows in the result; instead, they calculate a value for each row based on the specified window.\n2. Importing Required Libraries To use window functions, import the necessary modules from PySpark:",
    "tags": [],
    "title": "Window Functions",
    "uri": "/azure_data_bricks/window-functions/"
  },
  {
    "breadcrumb": "SQL",
    "content": "This guide demonstrates all possible date parts, number formats, and culture-specific styles available in SQL Server.\n1. Numeric Format Specifiers SELECT 'N' AS FormatType, FORMAT(1234.56, 'N') AS FormattedValue UNION ALL SELECT 'P', FORMAT(1234.56, 'P') UNION ALL SELECT 'C', FORMAT(1234.56, 'C') UNION ALL SELECT 'E', FORMAT(1234.56, 'E') UNION ALL SELECT 'F', FORMAT(1234.56, 'F') UNION ALL SELECT 'N0', FORMAT(1234.56, 'N0') UNION ALL SELECT 'N1', FORMAT(1234.56, 'N1') UNION ALL SELECT 'N2', FORMAT(1234.56, 'N2') UNION ALL SELECT 'N_de-DE', FORMAT(1234.56, 'N', 'de-DE') UNION ALL SELECT 'N_en-US', FORMAT(1234.56, 'N', 'en-US'); 2. Date Format Specifiers SELECT 'D' AS FormatType, FORMAT(GETDATE(), 'D') AS FormattedValue, 'Full date pattern' AS Description UNION ALL SELECT 'd', FORMAT(GETDATE(), 'd'), 'Short date pattern' UNION ALL SELECT 'dd', FORMAT(GETDATE(), 'dd'), 'Day of month with leading zero' UNION ALL SELECT 'ddd', FORMAT(GETDATE(), 'ddd'), 'Abbreviated name of day' UNION ALL SELECT 'dddd', FORMAT(GETDATE(), 'dddd'), 'Full name of day' UNION ALL SELECT 'M', FORMAT(GETDATE(), 'M'), 'Month without leading zero' UNION ALL SELECT 'MM', FORMAT(GETDATE(), 'MM'), 'Month with leading zero' UNION ALL SELECT 'MMM', FORMAT(GETDATE(), 'MMM'), 'Abbreviated name of month' UNION ALL SELECT 'MMMM', FORMAT(GETDATE(), 'MMMM'), 'Full name of month' UNION ALL SELECT 'yy', FORMAT(GETDATE(), 'yy'), 'Two-digit year' UNION ALL SELECT 'yyyy', FORMAT(GETDATE(), 'yyyy'), 'Four-digit year' UNION ALL SELECT 'hh', FORMAT(GETDATE(), 'hh'), '12-hour clock with leading zero' UNION ALL SELECT 'HH', FORMAT(GETDATE(), 'HH'), '24-hour clock with leading zero' UNION ALL SELECT 'm', FORMAT(GETDATE(), 'm'), 'Minutes without leading zero' UNION ALL SELECT 'mm', FORMAT(GETDATE(), 'mm'), 'Minutes with leading zero' UNION ALL SELECT 's', FORMAT(GETDATE(), 's'), 'Seconds without leading zero' UNION ALL SELECT 'ss', FORMAT(GETDATE(), 'ss'), 'Seconds with leading zero' UNION ALL SELECT 'f', FORMAT(GETDATE(), 'f'), 'Tenths of a second' UNION ALL SELECT 'ff', FORMAT(GETDATE(), 'ff'), 'Hundredths of a second' UNION ALL SELECT 'fff', FORMAT(GETDATE(), 'fff'), 'Milliseconds' UNION ALL SELECT 'T', FORMAT(GETDATE(), 'T'), 'Full AM/PM designator' UNION ALL SELECT 't', FORMAT(GETDATE(), 't'), 'Single char AM/PM designator' UNION ALL SELECT 'tt', FORMAT(GETDATE(), 'tt'), 'Two char AM/PM designator'; 3. DatePart / DateName / DateTrunc Comparisons SELECT 'Year' AS DatePart, DATEPART(year, GETDATE()), DATENAME(year, GETDATE()), DATETRUNC(year, GETDATE()) UNION ALL SELECT 'Quarter', DATEPART(quarter, GETDATE()), DATENAME(quarter, GETDATE()), DATETRUNC(quarter, GETDATE()) UNION ALL SELECT 'Month', DATEPART(month, GETDATE()), DATENAME(month, GETDATE()), DATETRUNC(month, GETDATE()) UNION ALL SELECT 'DayOfYear', DATEPART(dayofyear, GETDATE()), DATENAME(dayofyear, GETDATE()), DATETRUNC(dayofyear, GETDATE()) UNION ALL SELECT 'Day', DATEPART(day, GETDATE()), DATENAME(day, GETDATE()), DATETRUNC(day, GETDATE()) UNION ALL SELECT 'Week', DATEPART(week, GETDATE()), DATENAME(week, GETDATE()), DATETRUNC(week, GETDATE()) UNION ALL SELECT 'Weekday', DATEPART(weekday, GETDATE()), DATENAME(weekday, GETDATE()), NULL UNION ALL SELECT 'Hour', DATEPART(hour, GETDATE()), DATENAME(hour, GETDATE()), DATETRUNC(hour, GETDATE()) UNION ALL SELECT 'Minute', DATEPART(minute, GETDATE()), DATENAME(minute, GETDATE()), DATETRUNC(minute, GETDATE()) UNION ALL SELECT 'Second', DATEPART(second, GETDATE()), DATENAME(second, GETDATE()), DATETRUNC(second, GETDATE()) UNION ALL SELECT 'Millisecond', DATEPART(millisecond, GETDATE()), DATENAME(millisecond, GETDATE()), DATETRUNC(millisecond, GETDATE()) UNION ALL SELECT 'Microsecond', DATEPART(microsecond, GETDATE()), DATENAME(microsecond, GETDATE()), NULL UNION ALL SELECT 'Nanosecond', DATEPART(nanosecond, GETDATE()), DATENAME(nanosecond, GETDATE()), NULL UNION ALL SELECT 'ISOWeek', DATEPART(iso_week, GETDATE()), DATENAME(iso_week, GETDATE()), DATETRUNC(iso_week, GETDATE()); 4. Culture Code Formatting SELECT 'en-US' AS CultureCode, FORMAT(1234567.89, 'N', 'en-US'), FORMAT(GETDATE(), 'D', 'en-US') UNION ALL SELECT 'en-GB', FORMAT(1234567.89, 'N', 'en-GB'), FORMAT(GETDATE(), 'D', 'en-GB') UNION ALL SELECT 'fr-FR', FORMAT(1234567.89, 'N', 'fr-FR'), FORMAT(GETDATE(), 'D', 'fr-FR') UNION ALL SELECT 'de-DE', FORMAT(1234567.89, 'N', 'de-DE'), FORMAT(GETDATE(), 'D', 'de-DE') UNION ALL SELECT 'es-ES', FORMAT(1234567.89, 'N', 'es-ES'), FORMAT(GETDATE(), 'D', 'es-ES') UNION ALL SELECT 'zh-CN', FORMAT(1234567.89, 'N', 'zh-CN'), FORMAT(GETDATE(), 'D', 'zh-CN') UNION ALL SELECT 'ja-JP', FORMAT(1234567.89, 'N', 'ja-JP'), FORMAT(GETDATE(), 'D', 'ja-JP') UNION ALL SELECT 'ko-KR', FORMAT(1234567.89, 'N', 'ko-KR'), FORMAT(GETDATE(), 'D', 'ko-KR') UNION ALL SELECT 'pt-BR', FORMAT(1234567.89, 'N', 'pt-BR'), FORMAT(GETDATE(), 'D', 'pt-BR') UNION ALL SELECT 'it-IT', FORMAT(1234567.89, 'N', 'it-IT'), FORMAT(GETDATE(), 'D', 'it-IT') UNION ALL SELECT 'nl-NL', FORMAT(1234567.89, 'N', 'nl-NL'), FORMAT(GETDATE(), 'D', 'nl-NL') UNION ALL SELECT 'ru-RU', FORMAT(1234567.89, 'N', 'ru-RU'), FORMAT(GETDATE(), 'D', 'ru-RU') UNION ALL SELECT 'ar-SA', FORMAT(1234567.89, 'N', 'ar-SA'), FORMAT(GETDATE(), 'D', 'ar-SA') UNION ALL SELECT 'el-GR', FORMAT(1234567.89, 'N', 'el-GR'), FORMAT(GETDATE(), 'D', 'el-GR') UNION ALL SELECT 'tr-TR', FORMAT(1234567.89, 'N', 'tr-TR'), FORMAT(GETDATE(), 'D', 'tr-TR') UNION ALL SELECT 'he-IL', FORMAT(1234567.89, 'N', 'he-IL'), FORMAT(GETDATE(), 'D', 'he-IL') UNION ALL SELECT 'hi-IN', FORMAT(1234567.89, 'N', 'hi-IN'), FORMAT(GETDATE(), 'D', 'hi-IN');",
    "description": "This guide demonstrates all possible date parts, number formats, and culture-specific styles available in SQL Server.\n1. Numeric Format Specifiers SELECT 'N' AS FormatType, FORMAT(1234.56, 'N') AS FormattedValue UNION ALL SELECT 'P', FORMAT(1234.56, 'P') UNION ALL SELECT 'C', FORMAT(1234.56, 'C') UNION ALL SELECT 'E', FORMAT(1234.56, 'E') UNION ALL SELECT 'F', FORMAT(1234.56, 'F') UNION ALL SELECT 'N0', FORMAT(1234.56, 'N0') UNION ALL SELECT 'N1', FORMAT(1234.56, 'N1') UNION ALL SELECT 'N2', FORMAT(1234.56, 'N2') UNION ALL SELECT 'N_de-DE', FORMAT(1234.56, 'N', 'de-DE') UNION ALL SELECT 'N_en-US', FORMAT(1234.56, 'N', 'en-US'); 2. Date Format Specifiers SELECT 'D' AS FormatType, FORMAT(GETDATE(), 'D') AS FormattedValue, 'Full date pattern' AS Description UNION ALL SELECT 'd', FORMAT(GETDATE(), 'd'), 'Short date pattern' UNION ALL SELECT 'dd', FORMAT(GETDATE(), 'dd'), 'Day of month with leading zero' UNION ALL SELECT 'ddd', FORMAT(GETDATE(), 'ddd'), 'Abbreviated name of day' UNION ALL SELECT 'dddd', FORMAT(GETDATE(), 'dddd'), 'Full name of day' UNION ALL SELECT 'M', FORMAT(GETDATE(), 'M'), 'Month without leading zero' UNION ALL SELECT 'MM', FORMAT(GETDATE(), 'MM'), 'Month with leading zero' UNION ALL SELECT 'MMM', FORMAT(GETDATE(), 'MMM'), 'Abbreviated name of month' UNION ALL SELECT 'MMMM', FORMAT(GETDATE(), 'MMMM'), 'Full name of month' UNION ALL SELECT 'yy', FORMAT(GETDATE(), 'yy'), 'Two-digit year' UNION ALL SELECT 'yyyy', FORMAT(GETDATE(), 'yyyy'), 'Four-digit year' UNION ALL SELECT 'hh', FORMAT(GETDATE(), 'hh'), '12-hour clock with leading zero' UNION ALL SELECT 'HH', FORMAT(GETDATE(), 'HH'), '24-hour clock with leading zero' UNION ALL SELECT 'm', FORMAT(GETDATE(), 'm'), 'Minutes without leading zero' UNION ALL SELECT 'mm', FORMAT(GETDATE(), 'mm'), 'Minutes with leading zero' UNION ALL SELECT 's', FORMAT(GETDATE(), 's'), 'Seconds without leading zero' UNION ALL SELECT 'ss', FORMAT(GETDATE(), 'ss'), 'Seconds with leading zero' UNION ALL SELECT 'f', FORMAT(GETDATE(), 'f'), 'Tenths of a second' UNION ALL SELECT 'ff', FORMAT(GETDATE(), 'ff'), 'Hundredths of a second' UNION ALL SELECT 'fff', FORMAT(GETDATE(), 'fff'), 'Milliseconds' UNION ALL SELECT 'T', FORMAT(GETDATE(), 'T'), 'Full AM/PM designator' UNION ALL SELECT 't', FORMAT(GETDATE(), 't'), 'Single char AM/PM designator' UNION ALL SELECT 'tt', FORMAT(GETDATE(), 'tt'), 'Two char AM/PM designator'; 3. DatePart / DateName / DateTrunc Comparisons SELECT 'Year' AS DatePart, DATEPART(year, GETDATE()), DATENAME(year, GETDATE()), DATETRUNC(year, GETDATE()) UNION ALL SELECT 'Quarter', DATEPART(quarter, GETDATE()), DATENAME(quarter, GETDATE()), DATETRUNC(quarter, GETDATE()) UNION ALL SELECT 'Month', DATEPART(month, GETDATE()), DATENAME(month, GETDATE()), DATETRUNC(month, GETDATE()) UNION ALL SELECT 'DayOfYear', DATEPART(dayofyear, GETDATE()), DATENAME(dayofyear, GETDATE()), DATETRUNC(dayofyear, GETDATE()) UNION ALL SELECT 'Day', DATEPART(day, GETDATE()), DATENAME(day, GETDATE()), DATETRUNC(day, GETDATE()) UNION ALL SELECT 'Week', DATEPART(week, GETDATE()), DATENAME(week, GETDATE()), DATETRUNC(week, GETDATE()) UNION ALL SELECT 'Weekday', DATEPART(weekday, GETDATE()), DATENAME(weekday, GETDATE()), NULL UNION ALL SELECT 'Hour', DATEPART(hour, GETDATE()), DATENAME(hour, GETDATE()), DATETRUNC(hour, GETDATE()) UNION ALL SELECT 'Minute', DATEPART(minute, GETDATE()), DATENAME(minute, GETDATE()), DATETRUNC(minute, GETDATE()) UNION ALL SELECT 'Second', DATEPART(second, GETDATE()), DATENAME(second, GETDATE()), DATETRUNC(second, GETDATE()) UNION ALL SELECT 'Millisecond', DATEPART(millisecond, GETDATE()), DATENAME(millisecond, GETDATE()), DATETRUNC(millisecond, GETDATE()) UNION ALL SELECT 'Microsecond', DATEPART(microsecond, GETDATE()), DATENAME(microsecond, GETDATE()), NULL UNION ALL SELECT 'Nanosecond', DATEPART(nanosecond, GETDATE()), DATENAME(nanosecond, GETDATE()), NULL UNION ALL SELECT 'ISOWeek', DATEPART(iso_week, GETDATE()), DATENAME(iso_week, GETDATE()), DATETRUNC(iso_week, GETDATE()); 4. Culture Code Formatting SELECT 'en-US' AS CultureCode, FORMAT(1234567.89, 'N', 'en-US'), FORMAT(GETDATE(), 'D', 'en-US') UNION ALL SELECT 'en-GB', FORMAT(1234567.89, 'N', 'en-GB'), FORMAT(GETDATE(), 'D', 'en-GB') UNION ALL SELECT 'fr-FR', FORMAT(1234567.89, 'N', 'fr-FR'), FORMAT(GETDATE(), 'D', 'fr-FR') UNION ALL SELECT 'de-DE', FORMAT(1234567.89, 'N', 'de-DE'), FORMAT(GETDATE(), 'D', 'de-DE') UNION ALL SELECT 'es-ES', FORMAT(1234567.89, 'N', 'es-ES'), FORMAT(GETDATE(), 'D', 'es-ES') UNION ALL SELECT 'zh-CN', FORMAT(1234567.89, 'N', 'zh-CN'), FORMAT(GETDATE(), 'D', 'zh-CN') UNION ALL SELECT 'ja-JP', FORMAT(1234567.89, 'N', 'ja-JP'), FORMAT(GETDATE(), 'D', 'ja-JP') UNION ALL SELECT 'ko-KR', FORMAT(1234567.89, 'N', 'ko-KR'), FORMAT(GETDATE(), 'D', 'ko-KR') UNION ALL SELECT 'pt-BR', FORMAT(1234567.89, 'N', 'pt-BR'), FORMAT(GETDATE(), 'D', 'pt-BR') UNION ALL SELECT 'it-IT', FORMAT(1234567.89, 'N', 'it-IT'), FORMAT(GETDATE(), 'D', 'it-IT') UNION ALL SELECT 'nl-NL', FORMAT(1234567.89, 'N', 'nl-NL'), FORMAT(GETDATE(), 'D', 'nl-NL') UNION ALL SELECT 'ru-RU', FORMAT(1234567.89, 'N', 'ru-RU'), FORMAT(GETDATE(), 'D', 'ru-RU') UNION ALL SELECT 'ar-SA', FORMAT(1234567.89, 'N', 'ar-SA'), FORMAT(GETDATE(), 'D', 'ar-SA') UNION ALL SELECT 'el-GR', FORMAT(1234567.89, 'N', 'el-GR'), FORMAT(GETDATE(), 'D', 'el-GR') UNION ALL SELECT 'tr-TR', FORMAT(1234567.89, 'N', 'tr-TR'), FORMAT(GETDATE(), 'D', 'tr-TR') UNION ALL SELECT 'he-IL', FORMAT(1234567.89, 'N', 'he-IL'), FORMAT(GETDATE(), 'D', 'he-IL') UNION ALL SELECT 'hi-IN', FORMAT(1234567.89, 'N', 'hi-IN'), FORMAT(GETDATE(), 'D', 'hi-IN');",
    "tags": [],
    "title": "Date \u0026 Time Formats",
    "uri": "/sql/date-time2/"
  },
  {
    "breadcrumb": "ADB",
    "content": "Explode vs Explode_outer In PySpark, explode and explode_outer are functions used to work with nested data structures, like arrays or maps, by “exploding” (flattening) each element of an array or key-value pair in a map into separate rows.\nThe key difference between explode and explode_outer is in handling null or empty arrays, which makes them useful in different scenarios.\n1. explode() The explode() function takes a column with array or map data and creates a new row for each element in the array (or each key-value pair in the map).\nIf the array is empty or null, explode() will drop the row entirely.\nKey Characteristics Converts each element in an array or each entry in a map into its own row. Drops rows with null or empty arrays. Example: Using explode() in PySpark from pyspark.sql import SparkSession from pyspark.sql.functions import explode # Initialize Spark session spark = SparkSession.builder.appName(\"ExplodeExample\").getOrCreate() # Sample DataFrame with arrays data = [ (\"Alice\", [\"Math\", \"Science\"]), (\"Bob\", [\"History\"]), (\"Cathy\", []), # Empty array (\"David\", None) # Null array ] df = spark.createDataFrame(data, [\"Name\", \"Subjects\"]) df.show() # Use explode to flatten the array exploded_df = df.select(\"Name\", explode(\"Subjects\").alias(\"Subject\")) # Show the result exploded_df.show() Explanation: explode() expands the Subjects array into individual rows. Rows with empty ([]) or null arrays (None) are removed, which is why Cathy and David do not appear in the output. 2. explode_outer() The explode_outer() function works similarly to explode(), but it keeps rows with null or empty arrays.\nWhen explode_outer() encounters a null or empty array, it still generates a row for that entry, with null as the value in the resulting column.\nKey Characteristics Converts each element in an array or each entry in a map into its own row. Retains rows with null or empty arrays, using null values in the exploded column. Example: Using explode_outer() in PySpark # Use explode_outer to flatten the array while keeping null or empty rows exploded_outer_df = df.select( \"Name\", F.explode_outer(\"Subjects\").alias(\"Subject\") ) # Show the result exploded_outer_df.show() Explanation: explode_outer() explode_outer() expands the Subjects array into individual rows. Unlike explode(), rows with empty ([]) or null arrays (None) are kept in the result, with null values in the Subject column for these cases. Summary Table of Differences Function Description Null/Empty Arrays Behavior explode() Expands each element of an array or map into rows Drops rows with null or empty arrays explode_outer() Similar to explode(), but retains null/empty Keeps rows with null/empty arrays, fills with null These functions are very useful when working with complex, nested data structures, especially when dealing with JSON or other hierarchical data.",
    "description": "Explode vs Explode_outer In PySpark, explode and explode_outer are functions used to work with nested data structures, like arrays or maps, by “exploding” (flattening) each element of an array or key-value pair in a map into separate rows.\nThe key difference between explode and explode_outer is in handling null or empty arrays, which makes them useful in different scenarios.\n1. explode() The explode() function takes a column with array or map data and creates a new row for each element in the array (or each key-value pair in the map).\nIf the array is empty or null, explode() will drop the row entirely.",
    "tags": [],
    "title": "Explode",
    "uri": "/azure_data_bricks/explode/"
  },
  {
    "breadcrumb": "SQL",
    "content": "This guide covers essential SQL functions for handling NULL values in different scenarios such as aggregation, mathematical operations, sorting, joins, and comparisons.\n1. Handle NULL - Data Aggregation Replace NULL values using COALESCE to ensure accurate averages.\nSELECT CustomerID, Score, COALESCE(Score, 0) AS Score2, AVG(Score) OVER () AS AvgScores, AVG(COALESCE(Score, 0)) OVER () AS AvgScores2 FROM Sales.Customers; 2. Handle NULL - Mathematical Operators Concatenate first and last names safely and add bonus points with COALESCE.\nSELECT CustomerID, FirstName, LastName, FirstName + ' ' + COALESCE(LastName, '') AS FullName, Score, COALESCE(Score, 0) + 10 AS ScoreWithBonus FROM Sales.Customers; 3. Handle NULL - Sorting Data Order results so that NULL values appear last.\nSELECT CustomerID, Score FROM Sales.Customers ORDER BY CASE WHEN Score IS NULL THEN 1 ELSE 0 END, Score; 4. NULLIF - Division by Zero Prevent division errors by using NULLIF.\nSELECT OrderID, Sales, Quantity, Sales / NULLIF(Quantity, 0) AS Price FROM Sales.Orders; 5. IS NULL / IS NOT NULL Identify rows with or without NULL values.\n-- Customers with no scores SELECT * FROM Sales.Customers WHERE Score IS NULL; -- Customers with scores SELECT * FROM Sales.Customers WHERE Score IS NOT NULL; 6. LEFT ANTI JOIN Retrieve customers who have not placed any orders.\nSELECT c.*, o.OrderID FROM Sales.Customers AS c LEFT JOIN Sales.Orders AS o ON c.CustomerID = o.CustomerID WHERE o.CustomerID IS NULL; 7. NULLs vs Empty String vs Blank Spaces Differentiate between NULL, empty strings (’’), and blank spaces (’ ‘).\nWITH Orders AS ( SELECT 1 AS Id, 'A' AS Category UNION SELECT 2, NULL UNION SELECT 3, '' UNION SELECT 4, ' ' ) SELECT *, DATALENGTH(Category) AS LenCategory, TRIM(Category) AS Policy1, NULLIF(TRIM(Category), '') AS Policy2, COALESCE(NULLIF(TRIM(Category), ''), 'unknown') AS Policy3 FROM Orders;",
    "description": "This guide covers essential SQL functions for handling NULL values in different scenarios such as aggregation, mathematical operations, sorting, joins, and comparisons.\n1. Handle NULL - Data Aggregation Replace NULL values using COALESCE to ensure accurate averages.\nSELECT CustomerID, Score, COALESCE(Score, 0) AS Score2, AVG(Score) OVER () AS AvgScores, AVG(COALESCE(Score, 0)) OVER () AS AvgScores2 FROM Sales.Customers; 2. Handle NULL - Mathematical Operators Concatenate first and last names safely and add bonus points with COALESCE.",
    "tags": [],
    "title": "NULL Functions",
    "uri": "/sql/null-functions/"
  },
  {
    "breadcrumb": "ADB",
    "content": "Pivot in PySpark The pivot operation in PySpark is used to transpose rows into columns based on a specified column’s unique values.\nIt’s particularly useful for creating wide-format data, where values in one column become new column headers, and corresponding values from another column fill those headers.\nKey Concepts groupBy and pivot\nThe pivot method is typically used in combination with groupBy. You group by certain columns and pivot one column to create new columns. Aggregation Function\nYou need to specify an aggregation function (like sum, avg, count, etc.) to fill the values in the pivoted columns. Performance Consideration\nPivoting can be computationally expensive, especially with a high number of unique values in the pivot column. For better performance, explicitly specify the values to pivot if possible. Syntax\ndataframe.groupBy(\"group_column\").pivot(\"pivot_column\").agg(aggregation_function) Example Code: Pivot in PySpark Sample Data Imagine we have a DataFrame of sales data with the following schema:\nProduct Region Sales A North 100 B North 150 A South 200 B South 300 We want to pivot the data so that regions (North, South) become columns and the sales values are aggregated.\nfrom pyspark.sql import SparkSession from pyspark.sql.functions import sum # Create a Spark session spark = SparkSession.builder.appName(\"PivotExample\").getOrCreate() # Create a sample DataFrame data = [ (\"A\", \"North\", 100), (\"B\", \"North\", 150), (\"A\", \"South\", 200), (\"B\", \"South\", 300) ] columns = [\"Product\", \"Region\", \"Sales\"] df = spark.createDataFrame(data, columns) # Pivot the DataFrame pivoted_df = df.groupBy(\"Product\").pivot(\"Region\").agg(sum(\"Sales\")) # Show the results pivoted_df.show() Output Product North South A 100 200 B 150 300 Explanation of Code groupBy(“Product”)\nGroups the data by the Product column. pivot(“Region”)\nTransforms unique values in the Region column (North, South) into new columns. agg(sum(“Sales”))\nComputes the sum of Sales for each combination of Product and the new columns created by the pivot. Notes Explicit Pivot Values: To improve performance, you can specify the pivot values explicitly. df.groupBy(\"Product\").pivot(\"Region\", [\"North\", \"South\"]).agg(sum(\"Sales\"))\nHandling Null Values: If some combinations of groupBy and pivot values have no corresponding rows, the resulting cells will contain null. Alternative Aggregations: You can use other aggregation functions like avg, max, min, etc. This approach is commonly used in creating summary reports or preparing data for machine learning models where wide-format data is required.\nUnpivot in PySpark The unpivot operation (also called melting) is used to transform a wide-format table into a long-format table. This means columns are turned into rows, effectively reversing the pivot operation. PySpark doesn’t have a direct unpivot function like Pandas’ melt, but you can achieve it using the selectExpr method or a combination of stack and other DataFrame transformations.\nKey Concepts 1. Purpose of Unpivot:\nSimplifies data analysis by converting column headers into a single column (e.g., categorical variables).\nIdeal for scenarios where you need to aggregate data further or visualize it in a long format.\n2. Syntax Overview:\nUse the stack function inside a selectExpr to unpivot.\nStack reshapes the DataFrame by creating multiple rows for specified columns.\n3. Performance:\nUnpivoting can generate many rows, especially if the original DataFrame is wide with numerous columns. Ensure your environment can handle the resulting data volume. Example: Unpivot in PySpark Sample Data Suppose we have the following DataFrame:\nProduct North South East West A 100 200 150 130 B 150 300 200 180 from pyspark.sql import SparkSession # Create a Spark session spark = SparkSession.builder.appName(\"UnpivotExample\").getOrCreate() # Sample data data = [ (\"A\", 100, 200, 150, 130), (\"B\", 150, 300, 200, 180) ] columns = [\"Product\", \"North\", \"South\", \"East\", \"West\"] # Create the DataFrame df = spark.createDataFrame(data, columns) # Unpivot the DataFrame using stack unpivoted_df = df.selectExpr( \"Product\", \"stack(4, 'North', North, 'South', South, 'East', East, 'West', West) as (Region, Sales)\" ) # Show the results unpivoted_df.show() Explanation of Code Input DataFrame:\nEach column (North, South, East, West) represents a region’s sales for each product. selectExpr with stack:\nThe stack function takes two arguments: The number of columns being unpivoted (4 in this case). A sequence of column-value pairs: ‘ColumnName1’, ColumnValue1, ‘ColumnName2’, ColumnValue2, …. The result is two new columns: the first contains the column names (now rows, Region), and the second contains the corresponding values (Sales). Aliasing Columns:\nThe stack result is aliased as (Region, Sales) to give meaningful names to the new columns. Alternative Methods Using withColumn and union:\nIf stack isn’t flexible enough, you can manually combine rows for each column:\nfrom pyspark.sql import functions as F # Create a DataFrame with union operations for unpivoting north = df.select(\"Product\", F.lit(\"North\").alias(\"Region\"), F.col(\"North\").alias(\"Sales\")) south = df.select(\"Product\", F.lit(\"South\").alias(\"Region\"), F.col(\"South\").alias(\"Sales\")) east = df.select(\"Product\", F.lit(\"East\").alias(\"Region\"), F.col(\"East\").alias(\"Sales\")) west = df.select(\"Product\", F.lit(\"West\").alias(\"Region\"), F.col(\"West\").alias(\"Sales\")) # Combine all rows using union unpivoted_df = north.union(south).union(east).union(west) # Show results unpivoted_df.show() Notes Performance Considerations\nstack is efficient for unpivoting a large number of columns. The union method may become unwieldy for many columns, but it offers more control over the transformation process. Dynamic Column Unpivoting\nIf the column names are not fixed (dynamic), you can: Collect the column names dynamically using df.columns. Construct the selectExpr or union queries programmatically. Resulting Format\nAfter unpivoting, the data will have more rows but fewer columns. Ensure downstream processes are optimized to handle the increased row count. Unpivoting is a powerful operation for restructuring data and is frequently used in data preprocessing, reporting, and machine learning pipelines.",
    "description": "Pivot in PySpark The pivot operation in PySpark is used to transpose rows into columns based on a specified column’s unique values.\nIt’s particularly useful for creating wide-format data, where values in one column become new column headers, and corresponding values from another column fill those headers.\nKey Concepts groupBy and pivot\nThe pivot method is typically used in combination with groupBy. You group by certain columns and pivot one column to create new columns. Aggregation Function",
    "tags": [],
    "title": "Pivot",
    "uri": "/azure_data_bricks/pivot/"
  },
  {
    "breadcrumb": "SQL",
    "content": "1. Categorize Data Create sales categories based on value ranges.\nSELECT Category, SUM(Sales) AS TotalSales FROM ( SELECT OrderID, Sales, CASE WHEN Sales \u003e 50 THEN 'High' WHEN Sales \u003e 20 THEN 'Medium' ELSE 'Low' END AS Category FROM Sales.Orders ) AS t GROUP BY Category ORDER BY TotalSales DESC; 2. Mapping Map country names to abbreviations.\nSELECT CustomerID, FirstName, LastName, Country, CASE WHEN Country = 'Germany' THEN 'DE' WHEN Country = 'USA' THEN 'US' ELSE 'n/a' END AS CountryAbbr FROM Sales.Customers; 3. Quick Form of CASE Statement Use the shorthand CASE syntax for direct equality comparisons.\nSELECT CustomerID, FirstName, LastName, Country, CASE WHEN Country = 'Germany' THEN 'DE' WHEN Country = 'USA' THEN 'US' ELSE 'n/a' END AS CountryAbbr, CASE Country WHEN 'Germany' THEN 'DE' WHEN 'USA' THEN 'US' ELSE 'n/a' END AS CountryAbbr2 FROM Sales.Customers; 4. Handling Nulls Replace NULL values with defaults and compute averages.\nSELECT CustomerID, LastName, Score, CASE WHEN Score IS NULL THEN 0 ELSE Score END AS ScoreClean, AVG( CASE WHEN Score IS NULL THEN 0 ELSE Score END ) OVER () AS AvgCustomerClean, AVG(Score) OVER () AS AvgCustomer FROM Sales.Customers; 5. Conditional Aggregation Count orders with sales above a threshold.\nSELECT CustomerID, SUM( CASE WHEN Sales \u003e 30 THEN 1 ELSE 0 END ) AS TotalOrdersHighSales, COUNT(*) AS TotalOrders FROM Sales.Orders GROUP BY CustomerID;",
    "description": "1. Categorize Data Create sales categories based on value ranges.\nSELECT Category, SUM(Sales) AS TotalSales FROM ( SELECT OrderID, Sales, CASE WHEN Sales \u003e 50 THEN 'High' WHEN Sales \u003e 20 THEN 'Medium' ELSE 'Low' END AS Category FROM Sales.Orders ) AS t GROUP BY Category ORDER BY TotalSales DESC; 2. Mapping Map country names to abbreviations.\nSELECT CustomerID, FirstName, LastName, Country, CASE WHEN Country = 'Germany' THEN 'DE' WHEN Country = 'USA' THEN 'US' ELSE 'n/a' END AS CountryAbbr FROM Sales.Customers; 3. Quick Form of CASE Statement Use the shorthand CASE syntax for direct equality comparisons.",
    "tags": [],
    "title": "CASE Statement",
    "uri": "/sql/case-statement/"
  },
  {
    "breadcrumb": "ADB",
    "content": "Hadoop vs. Spark Architecture Aspect Hadoop Spark Storage Uses HDFS for storage Uses in-memory processing for speed Processing MapReduce is disk-based In-memory processing improves performance Integration Runs independently or with Hadoop ecosystem Can run on top of Hadoop; more flexible Complexity More complex setup and deployment Simpler to deploy and configure Performance Slower for iterative tasks due to disk I/O Better performance for iterative tasks RDD vs. DataFrame vs. Dataset Aspect RDD DataFrame Dataset API Level Low-level, more control High-level, optimized with Catalyst High-level, type-safe Schema No schema, unstructured Uses schema for structured data Strongly typed, compile-time type safety Optimization No built-in optimization Optimized using Catalyst Optimized using Catalyst, with type safety Type Safety No type safety No compile-time type safety Provides compile-time type safety Performance Less optimized for performance Better performance due to optimizations Combines type safety with optimization Action vs. Transformation in Spark Aspect Action Transformation Execution Triggers execution of the Spark job Builds up a logical plan of data operations Return Type Returns results or output Returns a new RDD/DataFrame Evaluation Eager evaluation; executes immediately Lazy evaluation; executed when an action is triggered Computation Involves actual computation (e.g., collect()) Defines data transformations (e.g., map()) Performance Can cause data processing; affects performance Does not affect performance until an action is called Map vs. FlatMap Aspect Map FlatMap Output Returns one output element per input element Can return zero or more output elements per input Flattening Does not flatten output Flattens the output into a single level Use Case Suitable for one-to-one transformations Suitable for one-to-many transformations Complexity Simpler, straightforward More complex due to variable number of outputs Examples map(x =\u003e x * 2) flatMap(x =\u003e x.split(\" \")) GroupByKey vs. ReduceByKey Aspect GroupByKey ReduceByKey Operation Groups all values by key Aggregates values with the same key Efficiency Can lead to high shuffling More efficient due to partial aggregation Data Movement Requires shuffling of all values Minimizes data movement through local aggregation Use Case Useful for simple grouping Preferred for aggregations and reductions Performance Less efficient with large datasets Better performance for large datasets Repartition vs. Coalesce Aspect Repartition Coalesce Partitioning Can increase or decrease the number of partitions Only decreases the number of partitions Shuffling Involves full shuffle Avoids full shuffle, more efficient Efficiency More expensive due to shuffling More efficient for reducing partitions Use Case Used for increasing partitions or balancing load Used for reducing partitions, typically after filtering Performance Can be costly for large datasets More cost-effective for reducing partitions Cache vs. Persist Aspect Cache Persist Storage Level Defaults to MEMORY_ONLY Can use various storage levels (e.g., MEMORY_AND_DISK) Flexibility Simplified, with default storage level Offers more options for storage levels Use Case Suitable for simple caching scenarios Suitable for complex caching scenarios requiring different storage levels Implementation Easier to use, shorthand for MEMORY_ONLY More flexible, allows custom storage options Performance Suitable when memory suffices More efficient when dealing with larger datasets and limited memory Narrow vs. Wide Transformation Aspect Narrow Transformation Wide Transformation Partitioning Each parent partition is used by one child partition Requires data from multiple partitions Shuffling No shuffling required Involves shuffling of data Performance More efficient and less costly Less efficient due to data movement Examples map(), filter() groupByKey(), join() Complexity Simpler and faster More complex and slower due to data movement Collect vs. Take Aspect Collect Take Output Retrieves all data from the RDD/DataFrame Retrieves a specified number of elements Memory Usage Can be expensive and use a lot of memory More memory-efficient Use Case Used when you need the entire dataset Useful for sampling or debugging Performance Can cause performance issues with large data Faster and more controlled Action Type Triggers full data retrieval Triggers partial data retrieval Broadcast Variable vs. Accumulator Aspect Broadcast Variable Accumulator Purpose Efficiently shares read-only data across tasks Tracks metrics and aggregates values Data Type Data that is shared and read-only Counters and sums, often numerical Use Case Useful for large lookup tables or configurations Useful for aggregating metrics like counts Efficiency Reduces data transfer by broadcasting data once Efficient for aggregating values across tasks Mutability Immutable, read-only Mutable, can be updated during computation Spark SQL vs. DataFrame API Aspect Spark SQL DataFrame API Interface Executes SQL queries Provides a programmatic interface Syntax Uses SQL-like syntax Uses function-based syntax Optimization Optimized with Catalyst Optimized with Catalyst Spark Streaming vs. Structured Streaming Aspect Spark Streaming Structured Streaming Processing Micro-batch processing Micro-batch and continuous processing API RDD-based API SQL-based API with DataFrame/Dataset support Complexity More complex and lower-level Simplified with high-level APIs Consistency Can be less consistent due to micro-batches Provides stronger consistency guarantees Performance Can be slower for complex queries Better performance with optimizations Shuffle vs. MapReduce Aspect Shuffle MapReduce Operation Data reorganization across partitions Data processing model for distributed computing Efficiency Can be costly due to data movement Designed for batch processing with high I/O Performance Affects performance based on the amount of data movement Optimized for large-scale data processing but less efficient for iterative tasks Use Case Used in Spark for data redistribution Used in Hadoop for data processing tasks Implementation Integrated into Spark operations Core component of the Hadoop ecosystem Union vs. Join Aspect Union Join Operation Combines two DataFrames/RDDs into one Combines rows from two DataFrames/RDDs based on a key Data Requirements Requires same schema for both DataFrames/RDDs Requires a common key for joining Performance Generally faster as it does not require key matching Can be slower due to key matching and shuffling Output Stacks data vertically Merges data horizontally based on keys Use Case Appending data or combining datasets Merging related data based on keys Executor vs. Driver Aspect Executor Driver Role Executes tasks and processes data Coordinates and manages the Spark application Memory Memory allocated per executor for data processing Memory used for managing application execution Lifecycle Exists throughout the application execution Starts and stops the Spark application Tasks Runs the tasks assigned by the driver Schedules and coordinates tasks and jobs Parallelism Multiple executors run in parallel Single driver coordinates multiple executors Checkpointing vs. Caching Aspect Checkpointing Caching Purpose Provides fault tolerance and reliability Improves performance by storing intermediate data Storage Writes data to stable storage (e.g., HDFS) Stores data in memory or on disk (depends on storage level) Use Case Used for recovery in case of failures Used for optimizing repeated operations Impact Can be more costly and slow Generally faster but not suitable for fault tolerance Data Data is written to external storage Data is kept in memory or disk storage for quick access ReduceByKey vs. AggregateByKey Aspect ReduceByKey AggregateByKey Operation Combines values with the same key using a function Performs custom aggregation and combinatory operations Efficiency More efficient for simple aggregations Flexible for complex aggregation scenarios Shuffling Involves shuffling but can be optimized Can be more complex due to custom aggregation Use Case Suitable for straightforward aggregations Ideal for advanced and custom aggregations Performance Generally faster for simple operations Performance varies with complexity SQLContext vs. HiveContext vs. SparkSession Aspect SQLContext HiveContext SparkSession Purpose Provides SQL query capabilities Provides integration with Hive for SQL queries Unified entry point for Spark functionality Integration Basic SQL capabilities Integrates with Hive Metastore Combines SQL, DataFrame, and Streaming APIs Usage Legacy, less functionality Supports HiveQL and Hive UDFs Supports all Spark functionalities including Hive Configuration Less flexible and older Requires Hive setup and configuration Modern and flexible, manages configurations Capabilities Limited to SQL queries Extends SQL capabilities with Hive integration Comprehensive access to all Spark features Broadcast Join vs. Shuffle Join Aspect Broadcast Join Shuffle Join Operation Broadcasts a small dataset to all nodes Shuffles data across nodes for joining Data Size Suitable for small datasets Suitable for larger datasets Efficiency More efficient for small tables More suited for large datasets Performance Faster due to reduced shuffling Can be slower due to extensive shuffling Use Case Use when one dataset is small relative to others Use when both datasets are large SparkContext vs. SparkSession Aspect SparkContext SparkSession Purpose Entry point for Spark functionality Unified entry point for Spark functionalities Lifecycle Created before Spark jobs start Manages the Spark application lifecycle Functionality Provides access to RDD and basic Spark functionality Provides access to RDD, DataFrame, SQL, and Streaming APIs Configuration Configuration is less flexible More flexible and easier to configure Usage Older, used for legacy applications Modern and recommended for new applications Structured Streaming vs. Spark Streaming Aspect Structured Streaming Spark Streaming Processing Micro-batch and continuous processing Micro-batch processing API SQL-based API with DataFrame/Dataset support RDD-based API Complexity Simplified and high-level More complex and low-level Consistency Provides stronger consistency guarantees Can be less consistent due to micro-batches Performance Better performance with built-in optimizations Can be slower for complex queries Partitioning vs. Bucketing Aspect Partitioning Bucketing Purpose Divides data into multiple partitions based on a key Divides data into buckets based on a hash function Usage Used to optimize queries by reducing data scanned Used to improve join performance and maintain sorted data Shuffling Reduces shuffling by placing related data together Reduces shuffle during joins and aggregations Data Layout Data is physically separated based on partition key Data is organized into fixed-size buckets Performance Improves performance for queries involving partition keys Enhances performance for join operations",
    "description": "Hadoop vs. Spark Architecture Aspect Hadoop Spark Storage Uses HDFS for storage Uses in-memory processing for speed Processing MapReduce is disk-based In-memory processing improves performance Integration Runs independently or with Hadoop ecosystem Can run on top of Hadoop; more flexible Complexity More complex setup and deployment Simpler to deploy and configure Performance Slower for iterative tasks due to disk I/O Better performance for iterative tasks RDD vs. DataFrame vs. Dataset Aspect RDD DataFrame Dataset API Level Low-level, more control High-level, optimized with Catalyst High-level, type-safe Schema No schema, unstructured Uses schema for structured data Strongly typed, compile-time type safety Optimization No built-in optimization Optimized using Catalyst Optimized using Catalyst, with type safety Type Safety No type safety No compile-time type safety Provides compile-time type safety Performance Less optimized for performance Better performance due to optimizations Combines type safety with optimization Action vs. Transformation in Spark Aspect Action Transformation Execution Triggers execution of the Spark job Builds up a logical plan of data operations Return Type Returns results or output Returns a new RDD/DataFrame Evaluation Eager evaluation; executes immediately Lazy evaluation; executed when an action is triggered Computation Involves actual computation (e.g., collect()) Defines data transformations (e.g., map()) Performance Can cause data processing; affects performance Does not affect performance until an action is called Map vs. FlatMap Aspect Map FlatMap Output Returns one output element per input element Can return zero or more output elements per input Flattening Does not flatten output Flattens the output into a single level Use Case Suitable for one-to-one transformations Suitable for one-to-many transformations Complexity Simpler, straightforward More complex due to variable number of outputs Examples map(x =\u003e x * 2) flatMap(x =\u003e x.split(\" \")) GroupByKey vs. ReduceByKey Aspect GroupByKey ReduceByKey Operation Groups all values by key Aggregates values with the same key Efficiency Can lead to high shuffling More efficient due to partial aggregation Data Movement Requires shuffling of all values Minimizes data movement through local aggregation Use Case Useful for simple grouping Preferred for aggregations and reductions Performance Less efficient with large datasets Better performance for large datasets Repartition vs. Coalesce Aspect Repartition Coalesce Partitioning Can increase or decrease the number of partitions Only decreases the number of partitions Shuffling Involves full shuffle Avoids full shuffle, more efficient Efficiency More expensive due to shuffling More efficient for reducing partitions Use Case Used for increasing partitions or balancing load Used for reducing partitions, typically after filtering Performance Can be costly for large datasets More cost-effective for reducing partitions Cache vs. Persist Aspect Cache Persist Storage Level Defaults to MEMORY_ONLY Can use various storage levels (e.g., MEMORY_AND_DISK) Flexibility Simplified, with default storage level Offers more options for storage levels Use Case Suitable for simple caching scenarios Suitable for complex caching scenarios requiring different storage levels Implementation Easier to use, shorthand for MEMORY_ONLY More flexible, allows custom storage options Performance Suitable when memory suffices More efficient when dealing with larger datasets and limited memory Narrow vs. Wide Transformation Aspect Narrow Transformation Wide Transformation Partitioning Each parent partition is used by one child partition Requires data from multiple partitions Shuffling No shuffling required Involves shuffling of data Performance More efficient and less costly Less efficient due to data movement Examples map(), filter() groupByKey(), join() Complexity Simpler and faster More complex and slower due to data movement Collect vs. Take Aspect Collect Take Output Retrieves all data from the RDD/DataFrame Retrieves a specified number of elements Memory Usage Can be expensive and use a lot of memory More memory-efficient Use Case Used when you need the entire dataset Useful for sampling or debugging Performance Can cause performance issues with large data Faster and more controlled Action Type Triggers full data retrieval Triggers partial data retrieval Broadcast Variable vs. Accumulator Aspect Broadcast Variable Accumulator Purpose Efficiently shares read-only data across tasks Tracks metrics and aggregates values Data Type Data that is shared and read-only Counters and sums, often numerical Use Case Useful for large lookup tables or configurations Useful for aggregating metrics like counts Efficiency Reduces data transfer by broadcasting data once Efficient for aggregating values across tasks Mutability Immutable, read-only Mutable, can be updated during computation Spark SQL vs. DataFrame API Aspect Spark SQL DataFrame API Interface Executes SQL queries Provides a programmatic interface Syntax Uses SQL-like syntax Uses function-based syntax Optimization Optimized with Catalyst Optimized with Catalyst Spark Streaming vs. Structured Streaming Aspect Spark Streaming Structured Streaming Processing Micro-batch processing Micro-batch and continuous processing API RDD-based API SQL-based API with DataFrame/Dataset support Complexity More complex and lower-level Simplified with high-level APIs Consistency Can be less consistent due to micro-batches Provides stronger consistency guarantees Performance Can be slower for complex queries Better performance with optimizations Shuffle vs. MapReduce Aspect Shuffle MapReduce Operation Data reorganization across partitions Data processing model for distributed computing Efficiency Can be costly due to data movement Designed for batch processing with high I/O Performance Affects performance based on the amount of data movement Optimized for large-scale data processing but less efficient for iterative tasks Use Case Used in Spark for data redistribution Used in Hadoop for data processing tasks Implementation Integrated into Spark operations Core component of the Hadoop ecosystem Union vs. Join Aspect Union Join Operation Combines two DataFrames/RDDs into one Combines rows from two DataFrames/RDDs based on a key Data Requirements Requires same schema for both DataFrames/RDDs Requires a common key for joining Performance Generally faster as it does not require key matching Can be slower due to key matching and shuffling Output Stacks data vertically Merges data horizontally based on keys Use Case Appending data or combining datasets Merging related data based on keys Executor vs. Driver Aspect Executor Driver Role Executes tasks and processes data Coordinates and manages the Spark application Memory Memory allocated per executor for data processing Memory used for managing application execution Lifecycle Exists throughout the application execution Starts and stops the Spark application Tasks Runs the tasks assigned by the driver Schedules and coordinates tasks and jobs Parallelism Multiple executors run in parallel Single driver coordinates multiple executors Checkpointing vs. Caching Aspect Checkpointing Caching Purpose Provides fault tolerance and reliability Improves performance by storing intermediate data Storage Writes data to stable storage (e.g., HDFS) Stores data in memory or on disk (depends on storage level) Use Case Used for recovery in case of failures Used for optimizing repeated operations Impact Can be more costly and slow Generally faster but not suitable for fault tolerance Data Data is written to external storage Data is kept in memory or disk storage for quick access ReduceByKey vs. AggregateByKey Aspect ReduceByKey AggregateByKey Operation Combines values with the same key using a function Performs custom aggregation and combinatory operations Efficiency More efficient for simple aggregations Flexible for complex aggregation scenarios Shuffling Involves shuffling but can be optimized Can be more complex due to custom aggregation Use Case Suitable for straightforward aggregations Ideal for advanced and custom aggregations Performance Generally faster for simple operations Performance varies with complexity SQLContext vs. HiveContext vs. SparkSession Aspect SQLContext HiveContext SparkSession Purpose Provides SQL query capabilities Provides integration with Hive for SQL queries Unified entry point for Spark functionality Integration Basic SQL capabilities Integrates with Hive Metastore Combines SQL, DataFrame, and Streaming APIs Usage Legacy, less functionality Supports HiveQL and Hive UDFs Supports all Spark functionalities including Hive Configuration Less flexible and older Requires Hive setup and configuration Modern and flexible, manages configurations Capabilities Limited to SQL queries Extends SQL capabilities with Hive integration Comprehensive access to all Spark features Broadcast Join vs. Shuffle Join Aspect Broadcast Join Shuffle Join Operation Broadcasts a small dataset to all nodes Shuffles data across nodes for joining Data Size Suitable for small datasets Suitable for larger datasets Efficiency More efficient for small tables More suited for large datasets Performance Faster due to reduced shuffling Can be slower due to extensive shuffling Use Case Use when one dataset is small relative to others Use when both datasets are large SparkContext vs. SparkSession Aspect SparkContext SparkSession Purpose Entry point for Spark functionality Unified entry point for Spark functionalities Lifecycle Created before Spark jobs start Manages the Spark application lifecycle Functionality Provides access to RDD and basic Spark functionality Provides access to RDD, DataFrame, SQL, and Streaming APIs Configuration Configuration is less flexible More flexible and easier to configure Usage Older, used for legacy applications Modern and recommended for new applications Structured Streaming vs. Spark Streaming Aspect Structured Streaming Spark Streaming Processing Micro-batch and continuous processing Micro-batch processing API SQL-based API with DataFrame/Dataset support RDD-based API Complexity Simplified and high-level More complex and low-level Consistency Provides stronger consistency guarantees Can be less consistent due to micro-batches Performance Better performance with built-in optimizations Can be slower for complex queries Partitioning vs. Bucketing Aspect Partitioning Bucketing Purpose Divides data into multiple partitions based on a key Divides data into buckets based on a hash function Usage Used to optimize queries by reducing data scanned Used to improve join performance and maintain sorted data Shuffling Reduces shuffling by placing related data together Reduces shuffle during joins and aggregations Data Layout Data is physically separated based on partition key Data is organized into fixed-size buckets Performance Improves performance for queries involving partition keys Enhances performance for join operations",
    "tags": [],
    "title": "Comparisons",
    "uri": "/azure_data_bricks/comparisons/"
  },
  {
    "breadcrumb": "SQL",
    "content": "SQL aggregate functions perform calculations on multiple rows of data and return summary results.\n1. Basic Aggregate Functions COUNT – Count rows SELECT COUNT(*) AS total_customers FROM customers; SUM – Total of values SELECT SUM(sales) AS total_sales FROM orders; AVG – Average of values SELECT AVG(sales) AS avg_sales FROM orders; MAX – Maximum value SELECT MAX(score) AS max_score FROM customers; MIN – Minimum value SELECT MIN(score) AS min_score FROM customers; 2. Grouped Aggregations – GROUP BY Aggregate results per group.\nSELECT customer_id, COUNT(*) AS total_orders, SUM(sales) AS total_sales, AVG(sales) AS avg_sales, MAX(sales) AS highest_sales, MIN(sales) AS lowest_sales FROM orders GROUP BY customer_id;",
    "description": "SQL aggregate functions perform calculations on multiple rows of data and return summary results.\n1. Basic Aggregate Functions COUNT – Count rows SELECT COUNT(*) AS total_customers FROM customers; SUM – Total of values SELECT SUM(sales) AS total_sales FROM orders; AVG – Average of values SELECT AVG(sales) AS avg_sales FROM orders; MAX – Maximum value SELECT MAX(score) AS max_score FROM customers; MIN – Minimum value SELECT MIN(score) AS min_score FROM customers; 2. Grouped Aggregations – GROUP BY Aggregate results per group.",
    "tags": [],
    "title": "Aggregate Functions",
    "uri": "/sql/aggregate-functions/"
  },
  {
    "breadcrumb": "SQL",
    "content": "SQL window functions enable advanced calculations across sets of rows related to the current row without needing complex subqueries or joins. They support clauses like OVER, PARTITION, ORDER, FRAME, along with important rules and group-based use cases.\nTable of Contents SQL Window Basics SQL Window OVER Clause SQL Window PARTITION Clause SQL Window ORDER Clause SQL Window FRAME Clause SQL Window Rules SQL Window with GROUP BY 1. SQL Window Basics TASK 1 – Calculate the Total Sales Across All Orders SELECT SUM(Sales) AS Total_Sales FROM Sales.Orders; TASK 2 – Calculate the Total Sales for Each Product SELECT ProductID, SUM(Sales) AS Total_Sales FROM Sales.Orders GROUP BY ProductID; 2. SQL Window OVER Clause TASK 3 – Total sales across all orders with order details SELECT OrderID, OrderDate, ProductID, Sales, SUM(Sales) OVER () AS Total_Sales FROM Sales.Orders; 3. SQL Window PARTITION Clause TASK 4 – Total sales overall and per product SELECT OrderID, OrderDate, ProductID, Sales, SUM(Sales) OVER () AS Total_Sales, SUM(Sales) OVER (PARTITION BY ProductID) AS Sales_By_Product FROM Sales.Orders; TASK 5 – Total sales overall, per product, and per product-status combination SELECT OrderID, OrderDate, ProductID, OrderStatus, Sales, SUM(Sales) OVER () AS Total_Sales, SUM(Sales) OVER (PARTITION BY ProductID) AS Sales_By_Product, SUM(Sales) OVER (PARTITION BY ProductID, OrderStatus) AS Sales_By_Product_Status FROM Sales.Orders; 4. SQL Window ORDER Clause TASK 6 – Rank each order by sales (highest to lowest) SELECT OrderID, OrderDate, Sales, RANK() OVER (ORDER BY Sales DESC) AS Rank_Sales FROM Sales.Orders; 5. SQL Window FRAME Clause TASK 7 – Total sales by order status (current + next two orders) SELECT OrderID, OrderDate, ProductID, OrderStatus, Sales, SUM(Sales) OVER ( PARTITION BY OrderStatus ORDER BY OrderDate ROWS BETWEEN CURRENT ROW AND 2 FOLLOWING ) AS Total_Sales FROM Sales.Orders; TASK 8 – Total sales by order status (current + previous two orders) SELECT OrderID, OrderDate, ProductID, OrderStatus, Sales, SUM(Sales) OVER ( PARTITION BY OrderStatus ORDER BY OrderDate ROWS BETWEEN 2 PRECEDING AND CURRENT ROW ) AS Total_Sales FROM Sales.Orders; TASK 9 – Total sales by order status (previous two orders only) SELECT OrderID, OrderDate, ProductID, OrderStatus, Sales, SUM(Sales) OVER ( PARTITION BY OrderStatus ORDER BY OrderDate ROWS 2 PRECEDING ) AS Total_Sales FROM Sales.Orders; TASK 10 – Cumulative sales up to the current order SELECT OrderID, OrderDate, ProductID, OrderStatus, Sales, SUM(Sales) OVER ( PARTITION BY OrderStatus ORDER BY OrderDate ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW ) AS Total_Sales FROM Sales.Orders; TASK 11 – Cumulative sales from start to current row SELECT OrderID, OrderDate, ProductID, OrderStatus, Sales, SUM(Sales) OVER ( PARTITION BY OrderStatus ORDER BY OrderDate ROWS UNBOUNDED PRECEDING ) AS Total_Sales FROM Sales.Orders; 6. SQL Window Rules RULE 1 – Window functions can only be used in SELECT or ORDER BY SELECT OrderID, OrderDate, ProductID, OrderStatus, Sales, SUM(Sales) OVER (PARTITION BY OrderStatus) AS Total_Sales FROM Sales.Orders WHERE SUM(Sales) OVER (PARTITION BY OrderStatus) \u003e 100; -- ❌ Invalid RULE 2 – Window functions cannot be nested SELECT OrderID, OrderDate, ProductID, OrderStatus, Sales, SUM(SUM(Sales) OVER (PARTITION BY OrderStatus)) OVER (PARTITION BY OrderStatus) AS Total_Sales -- ❌ Invalid nesting FROM Sales.Orders; 7. SQL Window with GROUP BY TASK 12 – Rank customers by total sales SELECT CustomerID, SUM(Sales) AS Total_Sales, RANK() OVER (ORDER BY SUM(Sales) DESC) AS Rank_Customers FROM Sales.Orders GROUP BY CustomerID;",
    "description": "SQL window functions enable advanced calculations across sets of rows related to the current row without needing complex subqueries or joins. They support clauses like OVER, PARTITION, ORDER, FRAME, along with important rules and group-based use cases.\nTable of Contents SQL Window Basics SQL Window OVER Clause SQL Window PARTITION Clause SQL Window ORDER Clause SQL Window FRAME Clause SQL Window Rules SQL Window with GROUP BY 1. SQL Window Basics TASK 1 – Calculate the Total Sales Across All Orders SELECT SUM(Sales) AS Total_Sales FROM Sales.Orders; TASK 2 – Calculate the Total Sales for Each Product SELECT ProductID, SUM(Sales) AS Total_Sales FROM Sales.Orders GROUP BY ProductID; 2. SQL Window OVER Clause TASK 3 – Total sales across all orders with order details SELECT OrderID, OrderDate, ProductID, Sales, SUM(Sales) OVER () AS Total_Sales FROM Sales.Orders; 3. SQL Window PARTITION Clause TASK 4 – Total sales overall and per product SELECT OrderID, OrderDate, ProductID, Sales, SUM(Sales) OVER () AS Total_Sales, SUM(Sales) OVER (PARTITION BY ProductID) AS Sales_By_Product FROM Sales.Orders; TASK 5 – Total sales overall, per product, and per product-status combination SELECT OrderID, OrderDate, ProductID, OrderStatus, Sales, SUM(Sales) OVER () AS Total_Sales, SUM(Sales) OVER (PARTITION BY ProductID) AS Sales_By_Product, SUM(Sales) OVER (PARTITION BY ProductID, OrderStatus) AS Sales_By_Product_Status FROM Sales.Orders; 4. SQL Window ORDER Clause TASK 6 – Rank each order by sales (highest to lowest) SELECT OrderID, OrderDate, Sales, RANK() OVER (ORDER BY Sales DESC) AS Rank_Sales FROM Sales.Orders; 5. SQL Window FRAME Clause TASK 7 – Total sales by order status (current + next two orders) SELECT OrderID, OrderDate, ProductID, OrderStatus, Sales, SUM(Sales) OVER ( PARTITION BY OrderStatus ORDER BY OrderDate ROWS BETWEEN CURRENT ROW AND 2 FOLLOWING ) AS Total_Sales FROM Sales.Orders; TASK 8 – Total sales by order status (current + previous two orders) SELECT OrderID, OrderDate, ProductID, OrderStatus, Sales, SUM(Sales) OVER ( PARTITION BY OrderStatus ORDER BY OrderDate ROWS BETWEEN 2 PRECEDING AND CURRENT ROW ) AS Total_Sales FROM Sales.Orders; TASK 9 – Total sales by order status (previous two orders only) SELECT OrderID, OrderDate, ProductID, OrderStatus, Sales, SUM(Sales) OVER ( PARTITION BY OrderStatus ORDER BY OrderDate ROWS 2 PRECEDING ) AS Total_Sales FROM Sales.Orders; TASK 10 – Cumulative sales up to the current order SELECT OrderID, OrderDate, ProductID, OrderStatus, Sales, SUM(Sales) OVER ( PARTITION BY OrderStatus ORDER BY OrderDate ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW ) AS Total_Sales FROM Sales.Orders; TASK 11 – Cumulative sales from start to current row SELECT OrderID, OrderDate, ProductID, OrderStatus, Sales, SUM(Sales) OVER ( PARTITION BY OrderStatus ORDER BY OrderDate ROWS UNBOUNDED PRECEDING ) AS Total_Sales FROM Sales.Orders; 6. SQL Window Rules RULE 1 – Window functions can only be used in SELECT or ORDER BY SELECT OrderID, OrderDate, ProductID, OrderStatus, Sales, SUM(Sales) OVER (PARTITION BY OrderStatus) AS Total_Sales FROM Sales.Orders WHERE SUM(Sales) OVER (PARTITION BY OrderStatus) \u003e 100; -- ❌ Invalid RULE 2 – Window functions cannot be nested SELECT OrderID, OrderDate, ProductID, OrderStatus, Sales, SUM(SUM(Sales) OVER (PARTITION BY OrderStatus)) OVER (PARTITION BY OrderStatus) AS Total_Sales -- ❌ Invalid nesting FROM Sales.Orders; 7. SQL Window with GROUP BY TASK 12 – Rank customers by total sales SELECT CustomerID, SUM(Sales) AS Total_Sales, RANK() OVER (ORDER BY SUM(Sales) DESC) AS Rank_Customers FROM Sales.Orders GROUP BY CustomerID;",
    "tags": [],
    "title": "Window Functions",
    "uri": "/sql/window-functions/"
  },
  {
    "breadcrumb": "SQL",
    "content": "These functions allow you to perform aggregate calculations over a set of rows without the need for complex subqueries. They enable you to compute counts, sums, averages, minimums, and maximums while still retaining access to individual row details.\nTable of Contents COUNT SUM AVG MAX / MIN ROLLING SUM \u0026 AVERAGE Use Case COUNT Task 1: Find the Total Number of Orders and the Total Number of Orders for Each Customer SELECT OrderID, OrderDate, CustomerID, COUNT(*) OVER() AS TotalOrders, COUNT(*) OVER(PARTITION BY CustomerID) AS OrdersByCustomers FROM Sales.Orders Task 2: Find the Total Number of Customers, Scores, and Countries SELECT *, COUNT(*) OVER () AS TotalCustomersStar, COUNT(1) OVER () AS TotalCustomersOne, COUNT(Score) OVER() AS TotalScores, COUNT(Country) OVER() AS TotalCountries FROM Sales.Customers Task 3: Check whether the table ‘OrdersArchive’ contains any duplicate rows SELECT * FROM ( SELECT *, COUNT(*) OVER(PARTITION BY OrderID) AS CheckDuplicates FROM Sales.OrdersArchive ) t WHERE CheckDuplicates \u003e 1 SUM Task 4: Find the Total Sales Across All Orders and per Product SELECT OrderID, OrderDate, Sales, ProductID, SUM(Sales) OVER () AS TotalSales, SUM(Sales) OVER (PARTITION BY ProductID) AS SalesByProduct FROM Sales.Orders Task 5: Find the Percentage Contribution of Each Product’s Sales to the Total Sales SELECT OrderID, ProductID, Sales, SUM(Sales) OVER () AS TotalSales, ROUND(CAST(Sales AS FLOAT) / SUM(Sales) OVER () * 100, 2) AS PercentageOfTotal FROM Sales.Orders AVG Task 6: Find the Average Sales Across All Orders and per Product SELECT OrderID, OrderDate, Sales, ProductID, AVG(Sales) OVER () AS AvgSales, AVG(Sales) OVER (PARTITION BY ProductID) AS AvgSalesByProduct FROM Sales.Orders Task 7: Find the Average Scores of Customers SELECT CustomerID, LastName, Score, COALESCE(Score, 0) AS CustomerScore, AVG(Score) OVER () AS AvgScore, AVG(COALESCE(Score, 0)) OVER () AS AvgScoreWithoutNull FROM Sales.Customers Task 8: Find all orders where Sales exceed the average Sales across all orders SELECT * FROM ( SELECT OrderID, ProductID, Sales, AVG(Sales) OVER () AS Avg_Sales FROM Sales.Orders ) t WHERE Sales \u003e Avg_Sales MAX / MIN Task 9: Find the Highest and Lowest Sales across all orders SELECT MIN(Sales) AS MinSales, MAX(Sales) AS MaxSales FROM Sales.Orders Task 10: Find the Lowest Sales across all orders and by Product SELECT OrderID, ProductID, OrderDate, Sales, MIN(Sales) OVER () AS LowestSales, MIN(Sales) OVER (PARTITION BY ProductID) AS LowestSalesByProduct FROM Sales.Orders Task 11: Show the employees who have the highest salaries SELECT * FROM ( SELECT *, MAX(Salary) OVER() AS HighestSalary FROM Sales.Employees ) t WHERE Salary = HighestSalary Task 12: Find the deviation of each Sale from the minimum and maximum Sales SELECT OrderID, OrderDate, ProductID, Sales, MAX(Sales) OVER () AS HighestSales, MIN(Sales) OVER () AS LowestSales, Sales - MIN(Sales) OVER () AS DeviationFromMin, MAX(Sales) OVER () - Sales AS DeviationFromMax FROM Sales.Orders ROLLING SUM \u0026 AVERAGE Use Case Task 13: Calculate the moving average of Sales for each Product over time SELECT OrderID, ProductID, OrderDate, Sales, AVG(Sales) OVER (PARTITION BY ProductID) AS AvgByProduct, AVG(Sales) OVER (PARTITION BY ProductID ORDER BY OrderDate) AS MovingAvg FROM Sales.Orders Task 14: Calculate the moving average of Sales for each Product over time, including only the next order SELECT OrderID, ProductID, OrderDate, Sales, AVG(Sales) OVER (PARTITION BY ProductID ORDER BY OrderDate ROWS BETWEEN CURRENT ROW AND 1 FOLLOWING) AS RollingAvg FROM Sales.Orders",
    "description": "These functions allow you to perform aggregate calculations over a set of rows without the need for complex subqueries. They enable you to compute counts, sums, averages, minimums, and maximums while still retaining access to individual row details.\nTable of Contents COUNT SUM AVG MAX / MIN ROLLING SUM \u0026 AVERAGE Use Case COUNT Task 1: Find the Total Number of Orders and the Total Number of Orders for Each Customer SELECT OrderID, OrderDate, CustomerID, COUNT(*) OVER() AS TotalOrders, COUNT(*) OVER(PARTITION BY CustomerID) AS OrdersByCustomers FROM Sales.Orders Task 2: Find the Total Number of Customers, Scores, and Countries SELECT *, COUNT(*) OVER () AS TotalCustomersStar, COUNT(1) OVER () AS TotalCustomersOne, COUNT(Score) OVER() AS TotalScores, COUNT(Country) OVER() AS TotalCountries FROM Sales.Customers Task 3: Check whether the table ‘OrdersArchive’ contains any duplicate rows SELECT * FROM ( SELECT *, COUNT(*) OVER(PARTITION BY OrderID) AS CheckDuplicates FROM Sales.OrdersArchive ) t WHERE CheckDuplicates \u003e 1 SUM Task 4: Find the Total Sales Across All Orders and per Product SELECT OrderID, OrderDate, Sales, ProductID, SUM(Sales) OVER () AS TotalSales, SUM(Sales) OVER (PARTITION BY ProductID) AS SalesByProduct FROM Sales.Orders Task 5: Find the Percentage Contribution of Each Product’s Sales to the Total Sales SELECT OrderID, ProductID, Sales, SUM(Sales) OVER () AS TotalSales, ROUND(CAST(Sales AS FLOAT) / SUM(Sales) OVER () * 100, 2) AS PercentageOfTotal FROM Sales.Orders AVG Task 6: Find the Average Sales Across All Orders and per Product SELECT OrderID, OrderDate, Sales, ProductID, AVG(Sales) OVER () AS AvgSales, AVG(Sales) OVER (PARTITION BY ProductID) AS AvgSalesByProduct FROM Sales.Orders Task 7: Find the Average Scores of Customers SELECT CustomerID, LastName, Score, COALESCE(Score, 0) AS CustomerScore, AVG(Score) OVER () AS AvgScore, AVG(COALESCE(Score, 0)) OVER () AS AvgScoreWithoutNull FROM Sales.Customers Task 8: Find all orders where Sales exceed the average Sales across all orders SELECT * FROM ( SELECT OrderID, ProductID, Sales, AVG(Sales) OVER () AS Avg_Sales FROM Sales.Orders ) t WHERE Sales \u003e Avg_Sales MAX / MIN Task 9: Find the Highest and Lowest Sales across all orders SELECT MIN(Sales) AS MinSales, MAX(Sales) AS MaxSales FROM Sales.Orders Task 10: Find the Lowest Sales across all orders and by Product SELECT OrderID, ProductID, OrderDate, Sales, MIN(Sales) OVER () AS LowestSales, MIN(Sales) OVER (PARTITION BY ProductID) AS LowestSalesByProduct FROM Sales.Orders Task 11: Show the employees who have the highest salaries SELECT * FROM ( SELECT *, MAX(Salary) OVER() AS HighestSalary FROM Sales.Employees ) t WHERE Salary = HighestSalary Task 12: Find the deviation of each Sale from the minimum and maximum Sales SELECT OrderID, OrderDate, ProductID, Sales, MAX(Sales) OVER () AS HighestSales, MIN(Sales) OVER () AS LowestSales, Sales - MIN(Sales) OVER () AS DeviationFromMin, MAX(Sales) OVER () - Sales AS DeviationFromMax FROM Sales.Orders ROLLING SUM \u0026 AVERAGE Use Case Task 13: Calculate the moving average of Sales for each Product over time SELECT OrderID, ProductID, OrderDate, Sales, AVG(Sales) OVER (PARTITION BY ProductID) AS AvgByProduct, AVG(Sales) OVER (PARTITION BY ProductID ORDER BY OrderDate) AS MovingAvg FROM Sales.Orders Task 14: Calculate the moving average of Sales for each Product over time, including only the next order SELECT OrderID, ProductID, OrderDate, Sales, AVG(Sales) OVER (PARTITION BY ProductID ORDER BY OrderDate ROWS BETWEEN CURRENT ROW AND 1 FOLLOWING) AS RollingAvg FROM Sales.Orders",
    "tags": [],
    "title": "Window Aggregate Functions",
    "uri": "/sql/window_aggregations/"
  },
  {
    "breadcrumb": "SQL",
    "content": "These functions allow you to rank and order rows within a result set without the need for complex joins or subqueries. They enable you to assign unique or non-unique rankings, group rows into buckets, and analyze data distributions on ordered data.\nSQL WINDOW RANKING | ROW_NUMBER, RANK, DENSE_RANK Task 1: Rank Orders Based on Sales from Highest to Lowest SELECT OrderID, ProductID, Sales, ROW_NUMBER() OVER (ORDER BY Sales DESC) AS SalesRank_Row, RANK() OVER (ORDER BY Sales DESC) AS SalesRank_Rank, DENSE_RANK() OVER (ORDER BY Sales DESC) AS SalesRank_Dense FROM Sales.Orders; Task 2: Top-N Analysis → Find the Highest Sale for Each Product SELECT * FROM ( SELECT OrderID, ProductID, Sales, ROW_NUMBER() OVER (PARTITION BY ProductID ORDER BY Sales DESC) AS RankByProduct FROM Sales.Orders ) AS TopProductSales WHERE RankByProduct = 1; Task 3: Bottom-N Analysis → Find the Lowest 2 Customers Based on Total Sales SELECT * FROM ( SELECT CustomerID, SUM(Sales) AS TotalSales, ROW_NUMBER() OVER (ORDER BY SUM(Sales)) AS RankCustomers FROM Sales.Orders GROUP BY CustomerID ) AS BottomCustomerSales WHERE RankCustomers \u003c= 2; Task 4: Assign Unique IDs to Rows of the ‘Order Archive’ SELECT ROW_NUMBER() OVER (ORDER BY OrderID, OrderDate) AS UniqueID, * FROM Sales.OrdersArchive; Task 5: Identify Duplicates in ‘Order Archive’ SELECT * FROM ( SELECT ROW_NUMBER() OVER (PARTITION BY OrderID ORDER BY CreationTime DESC) AS rn, * FROM Sales.OrdersArchive ) AS UniqueOrdersArchive WHERE rn = 1; SQL WINDOW RANKING | NTILE Task 6: Divide Orders into Groups Based on Sales SELECT OrderID, Sales, NTILE(1) OVER (ORDER BY Sales) AS OneBucket, NTILE(2) OVER (ORDER BY Sales) AS TwoBuckets, NTILE(3) OVER (ORDER BY Sales) AS ThreeBuckets, NTILE(4) OVER (ORDER BY Sales) AS FourBuckets, NTILE(2) OVER (PARTITION BY ProductID ORDER BY Sales) AS TwoBucketByProducts FROM Sales.Orders; Task 7: Segment Orders into High, Medium, and Low Sales SELECT OrderID, Sales, Buckets, CASE WHEN Buckets = 1 THEN 'High' WHEN Buckets = 2 THEN 'Medium' WHEN Buckets = 3 THEN 'Low' END AS SalesSegmentations FROM ( SELECT OrderID, Sales, NTILE(3) OVER (ORDER BY Sales DESC) AS Buckets FROM Sales.Orders ) AS SalesBuckets; Task 8: Divide Orders into Groups for Processing SELECT NTILE(5) OVER (ORDER BY OrderID) AS Buckets, * FROM Sales.Orders; SQL WINDOW RANKING | CUME_DIST Task 9: Find Products Within the Highest 40% of Prices SELECT Product, Price, DistRank, CONCAT(DistRank * 100, '%') AS DistRankPerc FROM ( SELECT Product, Price, CUME_DIST() OVER (ORDER BY Price DESC) AS DistRank FROM Sales.Products ) AS PriceDistribution WHERE DistRank \u003c= 0.4;",
    "description": "These functions allow you to rank and order rows within a result set without the need for complex joins or subqueries. They enable you to assign unique or non-unique rankings, group rows into buckets, and analyze data distributions on ordered data.\nSQL WINDOW RANKING | ROW_NUMBER, RANK, DENSE_RANK Task 1: Rank Orders Based on Sales from Highest to Lowest SELECT OrderID, ProductID, Sales, ROW_NUMBER() OVER (ORDER BY Sales DESC) AS SalesRank_Row, RANK() OVER (ORDER BY Sales DESC) AS SalesRank_Rank, DENSE_RANK() OVER (ORDER BY Sales DESC) AS SalesRank_Dense FROM Sales.Orders; Task 2: Top-N Analysis → Find the Highest Sale for Each Product SELECT * FROM ( SELECT OrderID, ProductID, Sales, ROW_NUMBER() OVER (PARTITION BY ProductID ORDER BY Sales DESC) AS RankByProduct FROM Sales.Orders ) AS TopProductSales WHERE RankByProduct = 1; Task 3: Bottom-N Analysis → Find the Lowest 2 Customers Based on Total Sales SELECT * FROM ( SELECT CustomerID, SUM(Sales) AS TotalSales, ROW_NUMBER() OVER (ORDER BY SUM(Sales)) AS RankCustomers FROM Sales.Orders GROUP BY CustomerID ) AS BottomCustomerSales WHERE RankCustomers \u003c= 2; Task 4: Assign Unique IDs to Rows of the ‘Order Archive’ SELECT ROW_NUMBER() OVER (ORDER BY OrderID, OrderDate) AS UniqueID, * FROM Sales.OrdersArchive; Task 5: Identify Duplicates in ‘Order Archive’ SELECT * FROM ( SELECT ROW_NUMBER() OVER (PARTITION BY OrderID ORDER BY CreationTime DESC) AS rn, * FROM Sales.OrdersArchive ) AS UniqueOrdersArchive WHERE rn = 1; SQL WINDOW RANKING | NTILE Task 6: Divide Orders into Groups Based on Sales SELECT OrderID, Sales, NTILE(1) OVER (ORDER BY Sales) AS OneBucket, NTILE(2) OVER (ORDER BY Sales) AS TwoBuckets, NTILE(3) OVER (ORDER BY Sales) AS ThreeBuckets, NTILE(4) OVER (ORDER BY Sales) AS FourBuckets, NTILE(2) OVER (PARTITION BY ProductID ORDER BY Sales) AS TwoBucketByProducts FROM Sales.Orders; Task 7: Segment Orders into High, Medium, and Low Sales SELECT OrderID, Sales, Buckets, CASE WHEN Buckets = 1 THEN 'High' WHEN Buckets = 2 THEN 'Medium' WHEN Buckets = 3 THEN 'Low' END AS SalesSegmentations FROM ( SELECT OrderID, Sales, NTILE(3) OVER (ORDER BY Sales DESC) AS Buckets FROM Sales.Orders ) AS SalesBuckets; Task 8: Divide Orders into Groups for Processing SELECT NTILE(5) OVER (ORDER BY OrderID) AS Buckets, * FROM Sales.Orders; SQL WINDOW RANKING | CUME_DIST Task 9: Find Products Within the Highest 40% of Prices SELECT Product, Price, DistRank, CONCAT(DistRank * 100, '%') AS DistRankPerc FROM ( SELECT Product, Price, CUME_DIST() OVER (ORDER BY Price DESC) AS DistRank FROM Sales.Products ) AS PriceDistribution WHERE DistRank \u003c= 0.4;",
    "tags": [],
    "title": "Window Ranking Functions",
    "uri": "/sql/window-ranking/"
  },
  {
    "breadcrumb": "SQL",
    "content": "These functions let you reference and compare values from other rows in a result set without complex joins or subqueries, enabling advanced analysis on ordered data.\nSQL WINDOW VALUE | LEAD, LAG Task 1: Analyze Month-over-Month Performance Find the percentage change in sales between the current and previous months.\nSELECT *, CurrentMonthSales - PreviousMonthSales AS MoM_Change, ROUND( CAST((CurrentMonthSales - PreviousMonthSales) AS FLOAT) / PreviousMonthSales * 100, 1 ) AS MoM_Perc FROM ( SELECT MONTH(OrderDate) AS OrderMonth, SUM(Sales) AS CurrentMonthSales, LAG(SUM(Sales)) OVER (ORDER BY MONTH(OrderDate)) AS PreviousMonthSales FROM Sales.Orders GROUP BY MONTH(OrderDate) ) AS MonthlySales; Task 2: Customer Loyalty Analysis Rank customers based on the average days between their orders.\nSELECT CustomerID, AVG(DaysUntilNextOrder) AS AvgDays, RANK() OVER (ORDER BY COALESCE(AVG(DaysUntilNextOrder), 999999)) AS RankAvg FROM ( SELECT OrderID, CustomerID, OrderDate AS CurrentOrder, LEAD(OrderDate) OVER (PARTITION BY CustomerID ORDER BY OrderDate) AS NextOrder, DATEDIFF( day, OrderDate, LEAD(OrderDate) OVER (PARTITION BY CustomerID ORDER BY OrderDate) ) AS DaysUntilNextOrder FROM Sales.Orders ) AS CustomerOrdersWithNext GROUP BY CustomerID; SQL WINDOW VALUE | FIRST \u0026 LAST VALUE Task 3: Lowest and Highest Sales Per Product Find the lowest and highest sales for each product and compute the difference between the current sale and the lowest sale.\nSELECT OrderID, ProductID, Sales, FIRST_VALUE(Sales) OVER (PARTITION BY ProductID ORDER BY Sales) AS LowestSales, LAST_VALUE(Sales) OVER ( PARTITION BY ProductID ORDER BY Sales ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING ) AS HighestSales, Sales - FIRST_VALUE(Sales) OVER (PARTITION BY ProductID ORDER BY Sales) AS SalesDifference FROM Sales.Orders;",
    "description": "These functions let you reference and compare values from other rows in a result set without complex joins or subqueries, enabling advanced analysis on ordered data.\nSQL WINDOW VALUE | LEAD, LAG Task 1: Analyze Month-over-Month Performance Find the percentage change in sales between the current and previous months.\nSELECT *, CurrentMonthSales - PreviousMonthSales AS MoM_Change, ROUND( CAST((CurrentMonthSales - PreviousMonthSales) AS FLOAT) / PreviousMonthSales * 100, 1 ) AS MoM_Perc FROM ( SELECT MONTH(OrderDate) AS OrderMonth, SUM(Sales) AS CurrentMonthSales, LAG(SUM(Sales)) OVER (ORDER BY MONTH(OrderDate)) AS PreviousMonthSales FROM Sales.Orders GROUP BY MONTH(OrderDate) ) AS MonthlySales; Task 2: Customer Loyalty Analysis Rank customers based on the average days between their orders.",
    "tags": [],
    "title": "Window Value Functions",
    "uri": "/sql/window-value-functions/"
  },
  {
    "breadcrumb": "SQL",
    "content": "This script demonstrates various subquery techniques in SQL.\nIt covers result types, subqueries in the FROM clause, in SELECT, in JOIN clauses, with comparison operators, IN, ANY, correlated subqueries, and EXISTS.\nTable of Contents SUBQUERY - RESULT TYPES SUBQUERY - FROM CLAUSE SUBQUERY - SELECT SUBQUERY - JOIN CLAUSE SUBQUERY - COMPARISON OPERATORS SUBQUERY - IN OPERATOR SUBQUERY - ANY OPERATOR SUBQUERY - CORRELATED SUBQUERY - EXISTS OPERATOR SUBQUERY | RESULT TYPES Scalar Query SELECT AVG(Sales) FROM Sales.Orders; Row Query SELECT CustomerID FROM Sales.Orders; Table Query SELECT OrderID, OrderDate FROM Sales.Orders; SUBQUERY | FROM CLAUSE Task 1: Products with Price Higher than Average Price SELECT * FROM ( SELECT ProductID, Price, AVG(Price) OVER () AS AvgPrice FROM Sales.Products ) AS t WHERE Price \u003e AvgPrice; Task 2: Rank Customers by Total Sales SELECT *, RANK() OVER (ORDER BY TotalSales DESC) AS CustomerRank FROM ( SELECT CustomerID, SUM(Sales) AS TotalSales FROM Sales.Orders GROUP BY CustomerID ) AS t; SUBQUERY | SELECT Task 3: Product Details with Total Number of Orders SELECT ProductID, Product, Price, (SELECT COUNT(*) FROM Sales.Orders) AS TotalOrders FROM Sales.Products; SUBQUERY | JOIN CLAUSE Task 4: Customer Details with Total Sales SELECT c.*, t.TotalSales FROM Sales.Customers AS c LEFT JOIN ( SELECT CustomerID, SUM(Sales) AS TotalSales FROM Sales.Orders GROUP BY CustomerID ) AS t ON c.CustomerID = t.CustomerID; Task 5: Customer Details with Total Orders SELECT c.*, o.TotalOrders FROM Sales.Customers AS c LEFT JOIN ( SELECT CustomerID, COUNT(*) AS TotalOrders FROM Sales.Orders GROUP BY CustomerID ) AS o ON c.CustomerID = o.CustomerID; SUBQUERY | COMPARISON OPERATORS Task 6: Products with Price Higher than Average Price SELECT ProductID, Price, (SELECT AVG(Price) FROM Sales.Products) AS AvgPrice FROM Sales.Products WHERE Price \u003e (SELECT AVG(Price) FROM Sales.Products); SUBQUERY | IN OPERATOR Task 7: Orders Made by Customers in Germany SELECT * FROM Sales.Orders WHERE CustomerID IN ( SELECT CustomerID FROM Sales.Customers WHERE Country = 'Germany' ); Task 8: Orders Made by Customers Not in Germany SELECT * FROM Sales.Orders WHERE CustomerID NOT IN ( SELECT CustomerID FROM Sales.Customers WHERE Country = 'Germany' ); SUBQUERY | ANY OPERATOR Task 9: Female Employees with Salaries Greater than Any Male Employee SELECT EmployeeID, FirstName, Salary FROM Sales.Employees WHERE Gender = 'F' AND Salary \u003e ANY ( SELECT Salary FROM Sales.Employees WHERE Gender = 'M' ); CORRELATED SUBQUERY Task 10: Customer Details with Total Orders (Correlated) SELECT *, (SELECT COUNT(*) FROM Sales.Orders o WHERE o.CustomerID = c.CustomerID) AS TotalSales FROM Sales.Customers AS c; SUBQUERY | EXISTS OPERATOR Task 11: Orders Made by Customers in Germany SELECT * FROM Sales.Orders AS o WHERE EXISTS ( SELECT 1 FROM Sales.Customers AS c WHERE Country = 'Germany' AND o.CustomerID = c.CustomerID ); Task 12: Orders Made by Customers Not in Germany SELECT * FROM Sales.Orders AS o WHERE NOT EXISTS ( SELECT 1 FROM Sales.Customers AS c WHERE Country = 'Germany' AND o.CustomerID = c.CustomerID );",
    "description": "This script demonstrates various subquery techniques in SQL.\nIt covers result types, subqueries in the FROM clause, in SELECT, in JOIN clauses, with comparison operators, IN, ANY, correlated subqueries, and EXISTS.\nTable of Contents SUBQUERY - RESULT TYPES SUBQUERY - FROM CLAUSE SUBQUERY - SELECT SUBQUERY - JOIN CLAUSE SUBQUERY - COMPARISON OPERATORS SUBQUERY - IN OPERATOR SUBQUERY - ANY OPERATOR SUBQUERY - CORRELATED SUBQUERY - EXISTS OPERATOR SUBQUERY | RESULT TYPES Scalar Query SELECT AVG(Sales) FROM Sales.Orders; Row Query SELECT CustomerID FROM Sales.Orders; Table Query SELECT OrderID, OrderDate FROM Sales.Orders; SUBQUERY | FROM CLAUSE Task 1: Products with Price Higher than Average Price SELECT * FROM ( SELECT ProductID, Price, AVG(Price) OVER () AS AvgPrice FROM Sales.Products ) AS t WHERE Price \u003e AvgPrice; Task 2: Rank Customers by Total Sales SELECT *, RANK() OVER (ORDER BY TotalSales DESC) AS CustomerRank FROM ( SELECT CustomerID, SUM(Sales) AS TotalSales FROM Sales.Orders GROUP BY CustomerID ) AS t; SUBQUERY | SELECT Task 3: Product Details with Total Number of Orders SELECT ProductID, Product, Price, (SELECT COUNT(*) FROM Sales.Orders) AS TotalOrders FROM Sales.Products; SUBQUERY | JOIN CLAUSE Task 4: Customer Details with Total Sales SELECT c.*, t.TotalSales FROM Sales.Customers AS c LEFT JOIN ( SELECT CustomerID, SUM(Sales) AS TotalSales FROM Sales.Orders GROUP BY CustomerID ) AS t ON c.CustomerID = t.CustomerID; Task 5: Customer Details with Total Orders SELECT c.*, o.TotalOrders FROM Sales.Customers AS c LEFT JOIN ( SELECT CustomerID, COUNT(*) AS TotalOrders FROM Sales.Orders GROUP BY CustomerID ) AS o ON c.CustomerID = o.CustomerID; SUBQUERY | COMPARISON OPERATORS Task 6: Products with Price Higher than Average Price SELECT ProductID, Price, (SELECT AVG(Price) FROM Sales.Products) AS AvgPrice FROM Sales.Products WHERE Price \u003e (SELECT AVG(Price) FROM Sales.Products); SUBQUERY | IN OPERATOR Task 7: Orders Made by Customers in Germany SELECT * FROM Sales.Orders WHERE CustomerID IN ( SELECT CustomerID FROM Sales.Customers WHERE Country = 'Germany' ); Task 8: Orders Made by Customers Not in Germany SELECT * FROM Sales.Orders WHERE CustomerID NOT IN ( SELECT CustomerID FROM Sales.Customers WHERE Country = 'Germany' ); SUBQUERY | ANY OPERATOR Task 9: Female Employees with Salaries Greater than Any Male Employee SELECT EmployeeID, FirstName, Salary FROM Sales.Employees WHERE Gender = 'F' AND Salary \u003e ANY ( SELECT Salary FROM Sales.Employees WHERE Gender = 'M' ); CORRELATED SUBQUERY Task 10: Customer Details with Total Orders (Correlated) SELECT *, (SELECT COUNT(*) FROM Sales.Orders o WHERE o.CustomerID = c.CustomerID) AS TotalSales FROM Sales.Customers AS c; SUBQUERY | EXISTS OPERATOR Task 11: Orders Made by Customers in Germany SELECT * FROM Sales.Orders AS o WHERE EXISTS ( SELECT 1 FROM Sales.Customers AS c WHERE Country = 'Germany' AND o.CustomerID = c.CustomerID ); Task 12: Orders Made by Customers Not in Germany SELECT * FROM Sales.Orders AS o WHERE NOT EXISTS ( SELECT 1 FROM Sales.Customers AS c WHERE Country = 'Germany' AND o.CustomerID = c.CustomerID );",
    "tags": [],
    "title": "Subquery Functions",
    "uri": "/sql/subqueries/"
  },
  {
    "breadcrumb": "SQL",
    "content": "This script demonstrates the use of Common Table Expressions (CTEs) in SQL Server.\nIt includes examples of non-recursive CTEs for data aggregation and segmentation, as well as recursive CTEs for generating sequences and building hierarchical data.\nNON-RECURSIVE CTE -- Step 1 → Total Sales Per Customer WITH CTE_Total_Sales AS ( SELECT CustomerID, SUM(Sales) AS TotalSales FROM Sales.Orders GROUP BY CustomerID ) -- Step 2 → Last Order Date for Each Customer , CTE_Last_Order AS ( SELECT CustomerID, MAX(OrderDate) AS Last_Order FROM Sales.Orders GROUP BY CustomerID ) -- Step 3 → Rank Customers by Total Sales , CTE_Customer_Rank AS ( SELECT CustomerID, TotalSales, RANK() OVER (ORDER BY TotalSales DESC) AS CustomerRank FROM CTE_Total_Sales ) -- Step 4 → Segment Customers by Sales , CTE_Customer_Segments AS ( SELECT CustomerID, TotalSales, CASE WHEN TotalSales \u003e 100 THEN 'High' WHEN TotalSales \u003e 80 THEN 'Medium' ELSE 'Low' END AS CustomerSegments FROM CTE_Total_Sales ) -- Final Query Combining All CTEs SELECT c.CustomerID, c.FirstName, c.LastName, cts.TotalSales, clo.Last_Order, ccr.CustomerRank, ccs.CustomerSegments FROM Sales.Customers AS c LEFT JOIN CTE_Total_Sales AS cts ON cts.CustomerID = c.CustomerID LEFT JOIN CTE_Last_Order AS clo ON clo.CustomerID = c.CustomerID LEFT JOIN CTE_Customer_Rank AS ccr ON ccr.CustomerID = c.CustomerID LEFT JOIN CTE_Customer_Segments AS ccs ON ccs.CustomerID = c.CustomerID; RECURSIVE CTE | GENERATE SEQUENCE Task 2: Generate Numbers 1 to 20 WITH Series AS ( SELECT 1 AS MyNumber UNION ALL SELECT MyNumber + 1 FROM Series WHERE MyNumber \u003c 20 ) SELECT * FROM Series; Task 3: Generate Numbers 1 to 1000 WITH Series AS ( SELECT 1 AS MyNumber UNION ALL SELECT MyNumber + 1 FROM Series WHERE MyNumber \u003c 1000 ) SELECT * FROM Series OPTION (MAXRECURSION 5000); RECURSIVE CTE | BUILD HIERARCHY Task 4: Build Employee Hierarchy Display each employee’s level within the organization.\nAnchor Query: Select employees with no manager.\nRecursive Query: Select subordinates and increment the level.\nWITH CTE_Emp_Hierarchy AS ( -- Anchor Query: Top-level employees (no manager) SELECT EmployeeID, FirstName, ManagerID, 1 AS Level FROM Sales.Employees WHERE ManagerID IS NULL UNION ALL -- Recursive Query: Get subordinate employees and increment level SELECT e.EmployeeID, e.FirstName, e.ManagerID, Level + 1 FROM Sales.Employees AS e INNER JOIN CTE_Emp_Hierarchy AS ceh ON e.ManagerID = ceh.EmployeeID ) SELECT * FROM CTE_Emp_Hierarchy;",
    "description": "This script demonstrates the use of Common Table Expressions (CTEs) in SQL Server.\nIt includes examples of non-recursive CTEs for data aggregation and segmentation, as well as recursive CTEs for generating sequences and building hierarchical data.\nNON-RECURSIVE CTE -- Step 1 → Total Sales Per Customer WITH CTE_Total_Sales AS ( SELECT CustomerID, SUM(Sales) AS TotalSales FROM Sales.Orders GROUP BY CustomerID ) -- Step 2 → Last Order Date for Each Customer , CTE_Last_Order AS ( SELECT CustomerID, MAX(OrderDate) AS Last_Order FROM Sales.Orders GROUP BY CustomerID ) -- Step 3 → Rank Customers by Total Sales , CTE_Customer_Rank AS ( SELECT CustomerID, TotalSales, RANK() OVER (ORDER BY TotalSales DESC) AS CustomerRank FROM CTE_Total_Sales ) -- Step 4 → Segment Customers by Sales , CTE_Customer_Segments AS ( SELECT CustomerID, TotalSales, CASE WHEN TotalSales \u003e 100 THEN 'High' WHEN TotalSales \u003e 80 THEN 'Medium' ELSE 'Low' END AS CustomerSegments FROM CTE_Total_Sales ) -- Final Query Combining All CTEs SELECT c.CustomerID, c.FirstName, c.LastName, cts.TotalSales, clo.Last_Order, ccr.CustomerRank, ccs.CustomerSegments FROM Sales.Customers AS c LEFT JOIN CTE_Total_Sales AS cts ON cts.CustomerID = c.CustomerID LEFT JOIN CTE_Last_Order AS clo ON clo.CustomerID = c.CustomerID LEFT JOIN CTE_Customer_Rank AS ccr ON ccr.CustomerID = c.CustomerID LEFT JOIN CTE_Customer_Segments AS ccs ON ccs.CustomerID = c.CustomerID; RECURSIVE CTE | GENERATE SEQUENCE Task 2: Generate Numbers 1 to 20 WITH Series AS ( SELECT 1 AS MyNumber UNION ALL SELECT MyNumber + 1 FROM Series WHERE MyNumber \u003c 20 ) SELECT * FROM Series; Task 3: Generate Numbers 1 to 1000 WITH Series AS ( SELECT 1 AS MyNumber UNION ALL SELECT MyNumber + 1 FROM Series WHERE MyNumber \u003c 1000 ) SELECT * FROM Series OPTION (MAXRECURSION 5000); RECURSIVE CTE | BUILD HIERARCHY Task 4: Build Employee Hierarchy Display each employee’s level within the organization.",
    "tags": [],
    "title": "Common Table Expressions (CTEs)",
    "uri": "/sql/cte/"
  },
  {
    "breadcrumb": "SQL",
    "content": "This script demonstrates various view use cases in SQL Server.\nIt includes examples for creating, dropping, and modifying views, hiding query complexity, and implementing data security by controlling data access.\nCREATE, DROP, MODIFY VIEW Task: Create a Monthly Sales Summary View Aggregate by OrderMonth with total sales, total orders, and total quantities.\n-- Create View CREATE VIEW Sales.V_Monthly_Summary AS ( SELECT DATETRUNC(month, OrderDate) AS OrderMonth, SUM(Sales) AS TotalSales, COUNT(OrderID) AS TotalOrders, SUM(Quantity) AS TotalQuantities FROM Sales.Orders GROUP BY DATETRUNC(month, OrderDate) ); GO -- Query the View SELECT * FROM Sales.V_Monthly_Summary; -- Drop View if it exists IF OBJECT_ID('Sales.V_Monthly_Summary', 'V') IS NOT NULL DROP VIEW Sales.V_Monthly_Summary; GO -- Re-create the View with modified logic CREATE VIEW Sales.V_Monthly_Summary AS SELECT DATETRUNC(month, OrderDate) AS OrderMonth, SUM(Sales) AS TotalSales, COUNT(OrderID) AS TotalOrders FROM Sales.Orders GROUP BY DATETRUNC(month, OrderDate); GO VIEW USE CASE | HIDE COMPLEXITY Task: Abstract Multi-Table Joins with a View Join Orders, Products, Customers, and Employees into a single view.\nCREATE VIEW Sales.V_Order_Details AS ( SELECT o.OrderID, o.OrderDate, p.Product, p.Category, COALESCE(c.FirstName, '') + ' ' + COALESCE(c.LastName, '') AS CustomerName, c.Country AS CustomerCountry, COALESCE(e.FirstName, '') + ' ' + COALESCE(e.LastName, '') AS SalesName, e.Department, o.Sales, o.Quantity FROM Sales.Orders AS o LEFT JOIN Sales.Products AS p ON p.ProductID = o.ProductID LEFT JOIN Sales.Customers AS c ON c.CustomerID = o.CustomerID LEFT JOIN Sales.Employees AS e ON e.EmployeeID = o.SalesPersonID ); GO VIEW USE CASE | DATA SECURITY Task: Restrict Access for EU Sales Team Exclude USA data from order details.\nCREATE VIEW Sales.V_Order_Details_EU AS ( SELECT o.OrderID, o.OrderDate, p.Product, p.Category, COALESCE(c.FirstName, '') + ' ' + COALESCE(c.LastName, '') AS CustomerName, c.Country AS CustomerCountry, COALESCE(e.FirstName, '') + ' ' + COALESCE(e.LastName, '') AS SalesName, e.Department, o.Sales, o.Quantity FROM Sales.Orders AS o LEFT JOIN Sales.Products AS p ON p.ProductID = o.ProductID LEFT JOIN Sales.Customers AS c ON c.CustomerID = o.CustomerID LEFT JOIN Sales.Employees AS e ON e.EmployeeID = o.SalesPersonID WHERE c.Country != 'USA' ); GO",
    "description": "This script demonstrates various view use cases in SQL Server.\nIt includes examples for creating, dropping, and modifying views, hiding query complexity, and implementing data security by controlling data access.\nCREATE, DROP, MODIFY VIEW Task: Create a Monthly Sales Summary View Aggregate by OrderMonth with total sales, total orders, and total quantities.\n-- Create View CREATE VIEW Sales.V_Monthly_Summary AS ( SELECT DATETRUNC(month, OrderDate) AS OrderMonth, SUM(Sales) AS TotalSales, COUNT(OrderID) AS TotalOrders, SUM(Quantity) AS TotalQuantities FROM Sales.Orders GROUP BY DATETRUNC(month, OrderDate) ); GO -- Query the View SELECT * FROM Sales.V_Monthly_Summary; -- Drop View if it exists IF OBJECT_ID('Sales.V_Monthly_Summary', 'V') IS NOT NULL DROP VIEW Sales.V_Monthly_Summary; GO -- Re-create the View with modified logic CREATE VIEW Sales.V_Monthly_Summary AS SELECT DATETRUNC(month, OrderDate) AS OrderMonth, SUM(Sales) AS TotalSales, COUNT(OrderID) AS TotalOrders FROM Sales.Orders GROUP BY DATETRUNC(month, OrderDate); GO VIEW USE CASE | HIDE COMPLEXITY Task: Abstract Multi-Table Joins with a View Join Orders, Products, Customers, and Employees into a single view.",
    "tags": [],
    "title": "Views",
    "uri": "/sql/views/"
  },
  {
    "breadcrumb": "SQL",
    "content": "This script provides a generic example of data migration using a temporary table.\nStep 1: Create Temporary Table (#Orders) SELECT * INTO #Orders FROM Sales.Orders; Step 2: Clean Data in Temporary Table DELETE FROM #Orders WHERE OrderStatus = 'Delivered'; Step 3: Load Cleaned Data into Permanent Table SELECT * INTO Sales.OrdersTest FROM #Orders;",
    "description": "This script provides a generic example of data migration using a temporary table.\nStep 1: Create Temporary Table (#Orders) SELECT * INTO #Orders FROM Sales.Orders; Step 2: Clean Data in Temporary Table DELETE FROM #Orders WHERE OrderStatus = 'Delivered'; Step 3: Load Cleaned Data into Permanent Table SELECT * INTO Sales.OrdersTest FROM #Orders;",
    "tags": [],
    "title": "Temporary Tables",
    "uri": "/sql/temporary-tables/"
  },
  {
    "breadcrumb": "SQL",
    "content": "This script shows how to work with stored procedures in SQL Server, starting from basic implementations and advancing to more sophisticated techniques.\n1. Basic Stored Procedure -- Define the Stored Procedure CREATE PROCEDURE GetCustomerSummary AS BEGIN SELECT COUNT(*) AS TotalCustomers, AVG(Score) AS AvgScore FROM Sales.Customers WHERE Country = 'USA'; END GO -- Execute Stored Procedure EXEC GetCustomerSummary; 2. Parameters in Stored Procedure -- Edit the Stored Procedure ALTER PROCEDURE GetCustomerSummary @Country NVARCHAR(50) = 'USA' AS BEGIN -- Reports: Summary from Customers and Orders SELECT COUNT(*) AS TotalCustomers, AVG(Score) AS AvgScore FROM Sales.Customers WHERE Country = @Country; END GO -- Execute Stored Procedure EXEC GetCustomerSummary @Country = 'Germany'; EXEC GetCustomerSummary @Country = 'USA'; EXEC GetCustomerSummary; 3. Multiple Queries in Stored Procedure -- Edit the Stored Procedure ALTER PROCEDURE GetCustomerSummary @Country NVARCHAR(50) = 'USA' AS BEGIN -- Query 1: Find the Total Nr. of Customers and the Average Score SELECT COUNT(*) AS TotalCustomers, AVG(Score) AS AvgScore FROM Sales.Customers WHERE Country = @Country; -- Query 2: Find the Total Nr. of Orders and Total Sales SELECT COUNT(OrderID) AS TotalOrders, SUM(Sales) AS TotalSales FROM Sales.Orders AS o JOIN Sales.Customers AS c ON c.CustomerID = o.CustomerID WHERE c.Country = @Country; END GO -- Execute Stored Procedure EXEC GetCustomerSummary @Country = 'Germany'; EXEC GetCustomerSummary @Country = 'USA'; EXEC GetCustomerSummary; 4. Variables in Stored Procedure -- Edit the Stored Procedure ALTER PROCEDURE GetCustomerSummary @Country NVARCHAR(50) = 'USA' AS BEGIN -- Declare Variables DECLARE @TotalCustomers INT, @AvgScore FLOAT; -- Query 1: Find the Total Nr. of Customers and the Average Score SELECT @TotalCustomers = COUNT(*), @AvgScore = AVG(Score) FROM Sales.Customers WHERE Country = @Country; PRINT('Total Customers from ' + @Country + ':' + CAST(@TotalCustomers AS NVARCHAR)); PRINT('Average Score from ' + @Country + ':' + CAST(@AvgScore AS NVARCHAR)); -- Query 2: Find the Total Nr. of Orders and Total Sales SELECT COUNT(OrderID) AS TotalOrders, SUM(Sales) AS TotalSales FROM Sales.Orders AS o JOIN Sales.Customers AS c ON c.CustomerID = o.CustomerID WHERE c.Country = @Country; END GO -- Execute Stored Procedure EXEC GetCustomerSummary @Country = 'Germany'; EXEC GetCustomerSummary @Country = 'USA'; EXEC GetCustomerSummary; 5. Control Flow with IF/ELSE ALTER PROCEDURE GetCustomerSummary @Country NVARCHAR(50) = 'USA' AS BEGIN -- Declare Variables DECLARE @TotalCustomers INT, @AvgScore FLOAT; /* -------------------------------------------------------------------------- Prepare \u0026 Cleanup Data -------------------------------------------------------------------------- */ IF EXISTS (SELECT 1 FROM Sales.Customers WHERE Score IS NULL AND Country = @Country) BEGIN PRINT('Updating NULL Scores to 0'); UPDATE Sales.Customers SET Score = 0 WHERE Score IS NULL AND Country = @Country; END ELSE BEGIN PRINT('No NULL Scores found'); END; /* -------------------------------------------------------------------------- Generating Reports -------------------------------------------------------------------------- */ SELECT @TotalCustomers = COUNT(*), @AvgScore = AVG(Score) FROM Sales.Customers WHERE Country = @Country; PRINT('Total Customers from ' + @Country + ':' + CAST(@TotalCustomers AS NVARCHAR)); PRINT('Average Score from ' + @Country + ':' + CAST(@AvgScore AS NVARCHAR)); SELECT COUNT(OrderID) AS TotalOrders, SUM(Sales) AS TotalSales, 1/0 AS FaultyCalculation -- Intentional error for demonstration FROM Sales.Orders AS o JOIN Sales.Customers AS c ON c.CustomerID = o.CustomerID WHERE c.Country = @Country; END GO -- Execute Stored Procedure EXEC GetCustomerSummary @Country = 'Germany'; EXEC GetCustomerSummary @Country = 'USA'; EXEC GetCustomerSummary; 6. Error Handling with TRY/CATCH ALTER PROCEDURE GetCustomerSummary @Country NVARCHAR(50) = 'USA' AS BEGIN BEGIN TRY -- Declare Variables DECLARE @TotalCustomers INT, @AvgScore FLOAT; /* -------------------------------------------------------------------------- Prepare \u0026 Cleanup Data -------------------------------------------------------------------------- */ IF EXISTS (SELECT 1 FROM Sales.Customers WHERE Score IS NULL AND Country = @Country) BEGIN PRINT('Updating NULL Scores to 0'); UPDATE Sales.Customers SET Score = 0 WHERE Score IS NULL AND Country = @Country; END ELSE BEGIN PRINT('No NULL Scores found'); END; /* -------------------------------------------------------------------------- Generating Reports -------------------------------------------------------------------------- */ SELECT @TotalCustomers = COUNT(*), @AvgScore = AVG(Score) FROM Sales.Customers WHERE Country = @Country; PRINT('Total Customers from ' + @Country + ':' + CAST(@TotalCustomers AS NVARCHAR)); PRINT('Average Score from ' + @Country + ':' + CAST(@AvgScore AS NVARCHAR)); SELECT COUNT(OrderID) AS TotalOrders, SUM(Sales) AS TotalSales, 1/0 AS FaultyCalculation -- Intentional error for demonstration FROM Sales.Orders AS o JOIN Sales.Customers AS c ON c.CustomerID = o.CustomerID WHERE c.Country = @Country; END TRY BEGIN CATCH /* -------------------------------------------------------------------------- Error Handling -------------------------------------------------------------------------- */ PRINT('An error occurred.'); PRINT('Error Message: ' + ERROR_MESSAGE()); PRINT('Error Number: ' + CAST(ERROR_NUMBER() AS NVARCHAR)); PRINT('Error Severity: ' + CAST(ERROR_SEVERITY() AS NVARCHAR)); PRINT('Error State: ' + CAST(ERROR_STATE() AS NVARCHAR)); PRINT('Error Line: ' + CAST(ERROR_LINE() AS NVARCHAR)); PRINT('Error Procedure: ' + ISNULL(ERROR_PROCEDURE(), 'N/A')); END CATCH; END GO -- Execute Stored Procedure EXEC GetCustomerSummary @Country = 'Germany'; EXEC GetCustomerSummary @Country = 'USA'; EXEC GetCustomerSummary;",
    "description": "This script shows how to work with stored procedures in SQL Server, starting from basic implementations and advancing to more sophisticated techniques.\n1. Basic Stored Procedure -- Define the Stored Procedure CREATE PROCEDURE GetCustomerSummary AS BEGIN SELECT COUNT(*) AS TotalCustomers, AVG(Score) AS AvgScore FROM Sales.Customers WHERE Country = 'USA'; END GO -- Execute Stored Procedure EXEC GetCustomerSummary; 2. Parameters in Stored Procedure -- Edit the Stored Procedure ALTER PROCEDURE GetCustomerSummary @Country NVARCHAR(50) = 'USA' AS BEGIN -- Reports: Summary from Customers and Orders SELECT COUNT(*) AS TotalCustomers, AVG(Score) AS AvgScore FROM Sales.Customers WHERE Country = @Country; END GO -- Execute Stored Procedure EXEC GetCustomerSummary @Country = 'Germany'; EXEC GetCustomerSummary @Country = 'USA'; EXEC GetCustomerSummary; 3. Multiple Queries in Stored Procedure -- Edit the Stored Procedure ALTER PROCEDURE GetCustomerSummary @Country NVARCHAR(50) = 'USA' AS BEGIN -- Query 1: Find the Total Nr. of Customers and the Average Score SELECT COUNT(*) AS TotalCustomers, AVG(Score) AS AvgScore FROM Sales.Customers WHERE Country = @Country; -- Query 2: Find the Total Nr. of Orders and Total Sales SELECT COUNT(OrderID) AS TotalOrders, SUM(Sales) AS TotalSales FROM Sales.Orders AS o JOIN Sales.Customers AS c ON c.CustomerID = o.CustomerID WHERE c.Country = @Country; END GO -- Execute Stored Procedure EXEC GetCustomerSummary @Country = 'Germany'; EXEC GetCustomerSummary @Country = 'USA'; EXEC GetCustomerSummary; 4. Variables in Stored Procedure -- Edit the Stored Procedure ALTER PROCEDURE GetCustomerSummary @Country NVARCHAR(50) = 'USA' AS BEGIN -- Declare Variables DECLARE @TotalCustomers INT, @AvgScore FLOAT; -- Query 1: Find the Total Nr. of Customers and the Average Score SELECT @TotalCustomers = COUNT(*), @AvgScore = AVG(Score) FROM Sales.Customers WHERE Country = @Country; PRINT('Total Customers from ' + @Country + ':' + CAST(@TotalCustomers AS NVARCHAR)); PRINT('Average Score from ' + @Country + ':' + CAST(@AvgScore AS NVARCHAR)); -- Query 2: Find the Total Nr. of Orders and Total Sales SELECT COUNT(OrderID) AS TotalOrders, SUM(Sales) AS TotalSales FROM Sales.Orders AS o JOIN Sales.Customers AS c ON c.CustomerID = o.CustomerID WHERE c.Country = @Country; END GO -- Execute Stored Procedure EXEC GetCustomerSummary @Country = 'Germany'; EXEC GetCustomerSummary @Country = 'USA'; EXEC GetCustomerSummary; 5. Control Flow with IF/ELSE ALTER PROCEDURE GetCustomerSummary @Country NVARCHAR(50) = 'USA' AS BEGIN -- Declare Variables DECLARE @TotalCustomers INT, @AvgScore FLOAT; /* -------------------------------------------------------------------------- Prepare \u0026 Cleanup Data -------------------------------------------------------------------------- */ IF EXISTS (SELECT 1 FROM Sales.Customers WHERE Score IS NULL AND Country = @Country) BEGIN PRINT('Updating NULL Scores to 0'); UPDATE Sales.Customers SET Score = 0 WHERE Score IS NULL AND Country = @Country; END ELSE BEGIN PRINT('No NULL Scores found'); END; /* -------------------------------------------------------------------------- Generating Reports -------------------------------------------------------------------------- */ SELECT @TotalCustomers = COUNT(*), @AvgScore = AVG(Score) FROM Sales.Customers WHERE Country = @Country; PRINT('Total Customers from ' + @Country + ':' + CAST(@TotalCustomers AS NVARCHAR)); PRINT('Average Score from ' + @Country + ':' + CAST(@AvgScore AS NVARCHAR)); SELECT COUNT(OrderID) AS TotalOrders, SUM(Sales) AS TotalSales, 1/0 AS FaultyCalculation -- Intentional error for demonstration FROM Sales.Orders AS o JOIN Sales.Customers AS c ON c.CustomerID = o.CustomerID WHERE c.Country = @Country; END GO -- Execute Stored Procedure EXEC GetCustomerSummary @Country = 'Germany'; EXEC GetCustomerSummary @Country = 'USA'; EXEC GetCustomerSummary; 6. Error Handling with TRY/CATCH ALTER PROCEDURE GetCustomerSummary @Country NVARCHAR(50) = 'USA' AS BEGIN BEGIN TRY -- Declare Variables DECLARE @TotalCustomers INT, @AvgScore FLOAT; /* -------------------------------------------------------------------------- Prepare \u0026 Cleanup Data -------------------------------------------------------------------------- */ IF EXISTS (SELECT 1 FROM Sales.Customers WHERE Score IS NULL AND Country = @Country) BEGIN PRINT('Updating NULL Scores to 0'); UPDATE Sales.Customers SET Score = 0 WHERE Score IS NULL AND Country = @Country; END ELSE BEGIN PRINT('No NULL Scores found'); END; /* -------------------------------------------------------------------------- Generating Reports -------------------------------------------------------------------------- */ SELECT @TotalCustomers = COUNT(*), @AvgScore = AVG(Score) FROM Sales.Customers WHERE Country = @Country; PRINT('Total Customers from ' + @Country + ':' + CAST(@TotalCustomers AS NVARCHAR)); PRINT('Average Score from ' + @Country + ':' + CAST(@AvgScore AS NVARCHAR)); SELECT COUNT(OrderID) AS TotalOrders, SUM(Sales) AS TotalSales, 1/0 AS FaultyCalculation -- Intentional error for demonstration FROM Sales.Orders AS o JOIN Sales.Customers AS c ON c.CustomerID = o.CustomerID WHERE c.Country = @Country; END TRY BEGIN CATCH /* -------------------------------------------------------------------------- Error Handling -------------------------------------------------------------------------- */ PRINT('An error occurred.'); PRINT('Error Message: ' + ERROR_MESSAGE()); PRINT('Error Number: ' + CAST(ERROR_NUMBER() AS NVARCHAR)); PRINT('Error Severity: ' + CAST(ERROR_SEVERITY() AS NVARCHAR)); PRINT('Error State: ' + CAST(ERROR_STATE() AS NVARCHAR)); PRINT('Error Line: ' + CAST(ERROR_LINE() AS NVARCHAR)); PRINT('Error Procedure: ' + ISNULL(ERROR_PROCEDURE(), 'N/A')); END CATCH; END GO -- Execute Stored Procedure EXEC GetCustomerSummary @Country = 'Germany'; EXEC GetCustomerSummary @Country = 'USA'; EXEC GetCustomerSummary;",
    "tags": [],
    "title": "Stored Procedures",
    "uri": "/sql/stored-procedures/"
  },
  {
    "breadcrumb": "SQL",
    "content": "This script demonstrates:\nThe creation of a logging table. A trigger on the Sales.Employees table. An insert operation that fires the trigger. The trigger logs details of newly added employees into the Sales.EmployeeLogs table.\nStep 1: Create Log Table CREATE TABLE Sales.EmployeeLogs ( LogID INT IDENTITY(1,1) PRIMARY KEY, EmployeeID INT, LogMessage VARCHAR(255), LogDate DATE ); GO Step 2: Create Trigger on Employees Table CREATE TRIGGER trg_AfterInsertEmployee ON Sales.Employees AFTER INSERT AS BEGIN INSERT INTO Sales.EmployeeLogs (EmployeeID, LogMessage, LogDate) SELECT EmployeeID, 'New Employee Added = ' + CAST(EmployeeID AS VARCHAR), GETDATE() FROM INSERTED; END; GO Step 3: Insert New Data Into Employees INSERT INTO Sales.Employees VALUES (6, 'Maria', 'Doe', 'HR', '1988-01-12', 'F', 80000, 3); GO Step 4: Check the Logs SELECT * FROM Sales.EmployeeLogs; GO",
    "description": "This script demonstrates:\nThe creation of a logging table. A trigger on the Sales.Employees table. An insert operation that fires the trigger. The trigger logs details of newly added employees into the Sales.EmployeeLogs table.\nStep 1: Create Log Table CREATE TABLE Sales.EmployeeLogs ( LogID INT IDENTITY(1,1) PRIMARY KEY, EmployeeID INT, LogMessage VARCHAR(255), LogDate DATE ); GO Step 2: Create Trigger on Employees Table CREATE TRIGGER trg_AfterInsertEmployee ON Sales.Employees AFTER INSERT AS BEGIN INSERT INTO Sales.EmployeeLogs (EmployeeID, LogMessage, LogDate) SELECT EmployeeID, 'New Employee Added = ' + CAST(EmployeeID AS VARCHAR), GETDATE() FROM INSERTED; END; GO Step 3: Insert New Data Into Employees INSERT INTO Sales.Employees VALUES (6, 'Maria', 'Doe', 'HR', '1988-01-12', 'F', 80000, 3); GO Step 4: Check the Logs SELECT * FROM Sales.EmployeeLogs; GO",
    "tags": [],
    "title": "Triggers",
    "uri": "/sql/triggers/"
  },
  {
    "breadcrumb": "SQL",
    "content": "This script demonstrates various index types in SQL Server including clustered, non-clustered, columnstore, unique, and filtered indexes.\nIt also covers index monitoring techniques like usage stats, missing/duplicate indexes, updating statistics, and fragmentation.\n📑 Table of Contents Index Types Clustered and Non-Clustered Indexes Leftmost Prefix Rule Explanation Columnstore Indexes Unique Indexes Filtered Indexes Index Monitoring Monitor Index Usage Monitor Missing Indexes Monitor Duplicate Indexes Update Statistics Fragmentations Index Types Clustered and Non-Clustered Indexes -- Create a Heap Table as a copy of Sales.Customers SELECT * INTO Sales.DBCustomers FROM Sales.Customers; -- Test Query SELECT * FROM Sales.DBCustomers WHERE CustomerID = 1; -- Create a Clustered Index CREATE CLUSTERED INDEX idx_DBCustomers_CustomerID ON Sales.DBCustomers (CustomerID); -- Attempt to create a second Clustered Index (will fail) CREATE CLUSTERED INDEX idx_DBCustomers_CustomerID ON Sales.DBCustomers (CustomerID); -- Drop the Clustered Index DROP INDEX idx_DBCustomers_CustomerID ON Sales.DBCustomers; -- Query using LastName filter SELECT * FROM Sales.DBCustomers WHERE LastName = 'Brown'; -- Create a Non-Clustered Index on LastName CREATE NONCLUSTERED INDEX idx_DBCustomers_LastName ON Sales.DBCustomers (LastName); -- Additional Non-Clustered Index on FirstName CREATE INDEX idx_DBCustomers_FirstName ON Sales.DBCustomers (FirstName); -- Composite Index on Country and Score CREATE INDEX idx_DBCustomers_CountryScore ON Sales.DBCustomers (Country, Score); -- Query that uses Composite Index SELECT * FROM Sales.DBCustomers WHERE Country = 'USA' AND Score \u003e 500; -- Query that may not use Composite Index due to column order SELECT * FROM Sales.DBCustomers WHERE Score \u003e 500 AND Country = 'USA'; Leftmost Prefix Rule Explanation For a composite index (A, B, C, D) the index is useful when filtering on:\nA only\nA, B\nA, B, C\nIt is not effective when filtering on:\nB only\nA, C\nA, B, D\nColumnstore Indexes -- Create Clustered Columnstore Index CREATE CLUSTERED COLUMNSTORE INDEX idx_DBCustomers_CS ON Sales.DBCustomers; GO -- Non-Clustered Columnstore Index on FirstName CREATE NONCLUSTERED COLUMNSTORE INDEX idx_DBCustomers_CS_FirstName ON Sales.DBCustomers (FirstName); GO -- Switch context to AdventureWorksDW2022 USE AdventureWorksDW2022; -- Create Heap Table SELECT * INTO FactInternetSales_HP FROM FactInternetSales; -- Create RowStore Table SELECT * INTO FactInternetSales_RS FROM FactInternetSales; -- Clustered Index on RowStore CREATE CLUSTERED INDEX idx_FactInternetSales_RS_PK ON FactInternetSales_RS (SalesOrderNumber, SalesOrderLineNumber); -- Create Columnstore Table SELECT * INTO FactInternetSales_CS FROM FactInternetSales; -- Clustered Columnstore Index CREATE CLUSTERED COLUMNSTORE INDEX idx_FactInternetSales_CS_PK ON FactInternetSales_CS; Unique Indexes -- Attempt Unique Index on Category (fails if duplicates exist) CREATE UNIQUE INDEX idx_Products_Category ON Sales.Products (Category); -- Unique Index on Product CREATE UNIQUE INDEX idx_Products_Product ON Sales.Products (Product); -- Test Insert (should fail if duplicate constraint holds) INSERT INTO Sales.Products (ProductID, Product) VALUES (106, 'Caps'); Filtered Indexes -- Test Query SELECT * FROM Sales.Customers WHERE Country = 'USA'; -- Filtered Index CREATE NONCLUSTERED INDEX idx_Customers_Country ON Sales.Customers (Country) WHERE Country = 'USA'; Index Monitoring Monitor Index Usage -- List all indexes on a table sp_helpindex 'Sales.DBCustomers'; -- Monitor Index Usage SELECT tbl.name AS TableName, idx.name AS IndexName, idx.type_desc AS IndexType, idx.is_primary_key AS IsPrimaryKey, idx.is_unique AS IsUnique, idx.is_disabled AS IsDisabled, s.user_seeks AS UserSeeks, s.user_scans AS UserScans, s.user_lookups AS UserLookups, s.user_updates AS UserUpdates, COALESCE(s.last_user_seek, s.last_user_scan) AS LastUpdate FROM sys.indexes idx JOIN sys.tables tbl ON idx.object_id = tbl.object_id LEFT JOIN sys.dm_db_index_usage_stats s ON s.object_id = idx.object_id AND s.index_id = idx.index_id ORDER BY tbl.name, idx.name; Monitor Missing Indexes SELECT * FROM sys.dm_db_missing_index_details; Monitor Duplicate Indexes SELECT tbl.name AS TableName, col.name AS IndexColumn, idx.name AS IndexName, idx.type_desc AS IndexType, COUNT(*) OVER (PARTITION BY tbl.name, col.name) ColumnCount FROM sys.indexes idx JOIN sys.tables tbl ON idx.object_id = tbl.object_id JOIN sys.index_columns ic ON idx.object_id = ic.object_id AND idx.index_id = ic.index_id JOIN sys.columns col ON ic.object_id = col.object_id AND ic.column_id = col.column_id ORDER BY ColumnCount DESC; Update Statistics SELECT SCHEMA_NAME(t.schema_id) AS SchemaName, t.name AS TableName, s.name AS StatisticName, sp.last_updated AS LastUpdate, DATEDIFF(day, sp.last_updated, GETDATE()) AS LastUpdateDay, sp.rows AS 'Rows', sp.modification_counter AS ModificationsSinceLastUpdate FROM sys.stats AS s JOIN sys.tables AS t ON s.object_id = t.object_id CROSS APPLY sys.dm_db_stats_properties(s.object_id, s.stats_id) AS sp ORDER BY sp.modification_counter DESC; -- Update specific system statistic UPDATE STATISTICS Sales.DBCustomers _WA_Sys_00000001_6EF57B66; GO -- Update all statistics for a table UPDATE STATISTICS Sales.DBCustomers; GO -- Update all statistics in the database EXEC sp_updatestats; GO Fragmentations -- Retrieve fragmentation stats SELECT tbl.name AS TableName, idx.name AS IndexName, s.avg_fragmentation_in_percent, s.page_count FROM sys.dm_db_index_physical_stats(DB_ID(), NULL, NULL, NULL, 'LIMITED') AS s INNER JOIN sys.tables tbl ON s.object_id = tbl.object_id INNER JOIN sys.indexes AS idx ON idx.object_id = s.object_id AND idx.index_id = s.index_id ORDER BY s.avg_fragmentation_in_percent DESC; -- Reorganize index (lightweight) ALTER INDEX idx_Customers_CS_Country ON Sales.Customers REORGANIZE; GO -- Rebuild index (full) ALTER INDEX idx_Customers_Country ON Sales.Customers REBUILD; GO",
    "description": "This script demonstrates various index types in SQL Server including clustered, non-clustered, columnstore, unique, and filtered indexes.\nIt also covers index monitoring techniques like usage stats, missing/duplicate indexes, updating statistics, and fragmentation.\n📑 Table of Contents Index Types Clustered and Non-Clustered Indexes Leftmost Prefix Rule Explanation Columnstore Indexes Unique Indexes Filtered Indexes Index Monitoring Monitor Index Usage Monitor Missing Indexes Monitor Duplicate Indexes Update Statistics Fragmentations Index Types Clustered and Non-Clustered Indexes -- Create a Heap Table as a copy of Sales.Customers SELECT * INTO Sales.DBCustomers FROM Sales.Customers; -- Test Query SELECT * FROM Sales.DBCustomers WHERE CustomerID = 1; -- Create a Clustered Index CREATE CLUSTERED INDEX idx_DBCustomers_CustomerID ON Sales.DBCustomers (CustomerID); -- Attempt to create a second Clustered Index (will fail) CREATE CLUSTERED INDEX idx_DBCustomers_CustomerID ON Sales.DBCustomers (CustomerID); -- Drop the Clustered Index DROP INDEX idx_DBCustomers_CustomerID ON Sales.DBCustomers; -- Query using LastName filter SELECT * FROM Sales.DBCustomers WHERE LastName = 'Brown'; -- Create a Non-Clustered Index on LastName CREATE NONCLUSTERED INDEX idx_DBCustomers_LastName ON Sales.DBCustomers (LastName); -- Additional Non-Clustered Index on FirstName CREATE INDEX idx_DBCustomers_FirstName ON Sales.DBCustomers (FirstName); -- Composite Index on Country and Score CREATE INDEX idx_DBCustomers_CountryScore ON Sales.DBCustomers (Country, Score); -- Query that uses Composite Index SELECT * FROM Sales.DBCustomers WHERE Country = 'USA' AND Score \u003e 500; -- Query that may not use Composite Index due to column order SELECT * FROM Sales.DBCustomers WHERE Score \u003e 500 AND Country = 'USA'; Leftmost Prefix Rule Explanation For a composite index (A, B, C, D) the index is useful when filtering on:",
    "tags": [],
    "title": "Indexes",
    "uri": "/sql/indexes/"
  },
  {
    "breadcrumb": "SQL",
    "content": "This script demonstrates SQL Server partitioning features including:\nPartition functions, filegroups, data files, partition schemes Partitioned tables and data insertion Verification queries and execution plan comparisons Step 1: Create a Partition Function -- Create Left Range Partition Functions based on Years CREATE PARTITION FUNCTION PartitionByYear (DATE) AS RANGE LEFT FOR VALUES ('2023-12-31', '2024-12-31', '2025-12-31'); -- List all existing Partition Functions SELECT name, function_id, type, type_desc, boundary_value_on_right FROM sys.partition_functions; Step 2: Create Filegroups -- Create Filegroups in SalesDB ALTER DATABASE SalesDB ADD FILEGROUP FG_2023; ALTER DATABASE SalesDB ADD FILEGROUP FG_2024; ALTER DATABASE SalesDB ADD FILEGROUP FG_2025; ALTER DATABASE SalesDB ADD FILEGROUP FG_2026; -- Optional: Remove a Filegroup ALTER DATABASE SalesDB REMOVE FILEGROUP FG_2023; -- List existing Filegroups SELECT * FROM sys.filegroups WHERE type = 'FG'; Step 3: Create Data Files -- Create Data Files and map them to Filegroups ALTER DATABASE SalesDB ADD FILE ( NAME = P_2023, FILENAME = 'C:\\Program Files\\Microsoft SQL Server\\MSSQL16.SQLEXPRESS\\MSSQL\\DATA\\P_2023.ndf' ) TO FILEGROUP FG_2023; ALTER DATABASE SalesDB ADD FILE ( NAME = P_2024, FILENAME = 'C:\\Program Files\\Microsoft SQL Server\\MSSQL16.SQLEXPRESS\\MSSQL\\DATA\\P_2024.ndf' ) TO FILEGROUP FG_2024; ALTER DATABASE SalesDB ADD FILE ( NAME = P_2025, FILENAME = 'C:\\Program Files\\Microsoft SQL Server\\MSSQL16.SQLEXPRESS\\MSSQL\\DATA\\P_2025.ndf' ) TO FILEGROUP FG_2025; ALTER DATABASE SalesDB ADD FILE ( NAME = P_2026, FILENAME = 'C:\\Program Files\\Microsoft SQL Server\\MSSQL16.SQLEXPRESS\\MSSQL\\DATA\\P_2026.ndf' ) TO FILEGROUP FG_2026; -- List all files in SalesDB SELECT fg.name AS FilegroupName, mf.name AS LogicalFileName, mf.physical_name AS PhysicalFilePath, mf.size / 128 AS SizeInMB FROM sys.filegroups fg JOIN sys.master_files mf ON fg.data_space_id = mf.data_space_id WHERE mf.database_id = DB_ID('SalesDB'); Step 4: Create Partition Scheme -- Partition Scheme using the Partition Function CREATE PARTITION SCHEME SchemePartitionByYear AS PARTITION PartitionByYear TO (FG_2023, FG_2024, FG_2025, FG_2026); -- List Partition Schemes SELECT ps.name AS PartitionSchemeName, pf.name AS PartitionFunctionName, ds.destination_id AS PartitionNumber, fg.name AS FilegroupName FROM sys.partition_schemes ps JOIN sys.partition_functions pf ON ps.function_id = pf.function_id JOIN sys.destination_data_spaces ds ON ps.data_space_id = ds.partition_scheme_id JOIN sys.filegroups fg ON ds.data_space_id = fg.data_space_id; Step 5: Create the Partitioned Table CREATE TABLE Sales.Orders_Partitioned ( OrderID INT, OrderDate DATE, Sales INT ) ON SchemePartitionByYear (OrderDate); Step 6: Insert Data Into the Partitioned Table INSERT INTO Sales.Orders_Partitioned VALUES (1, '2023-05-15', 100); INSERT INTO Sales.Orders_Partitioned VALUES (2, '2024-07-20', 50); INSERT INTO Sales.Orders_Partitioned VALUES (3, '2025-12-31', 20); INSERT INTO Sales.Orders_Partitioned VALUES (4, '2026-01-01', 100); Step 7: Verify Partitioning and Compare Execution Plans -- Verify that data is correctly partitioned SELECT p.partition_number AS PartitionNumber, f.name AS PartitionFilegroup, p.rows AS NumberOfRows FROM sys.partitions p JOIN sys.destination_data_spaces dds ON p.partition_number = dds.destination_id JOIN sys.filegroups f ON dds.data_space_id = f.data_space_id WHERE OBJECT_NAME(p.object_id) = 'Orders_Partitioned'; -- Non-partitioned copy for comparison SELECT * INTO Sales.Orders_NoPartition FROM Sales.Orders_Partitioned; -- Query on Partitioned Table SELECT * FROM Sales.Orders_Partitioned WHERE OrderDate IN ('2026-01-01', '2025-12-31'); -- Query on Non-Partitioned Table SELECT * FROM Sales.Orders_NoPartition WHERE OrderDate IN ('2026-01-01', '2025-12-31');",
    "description": "This script demonstrates SQL Server partitioning features including:\nPartition functions, filegroups, data files, partition schemes Partitioned tables and data insertion Verification queries and execution plan comparisons Step 1: Create a Partition Function -- Create Left Range Partition Functions based on Years CREATE PARTITION FUNCTION PartitionByYear (DATE) AS RANGE LEFT FOR VALUES ('2023-12-31', '2024-12-31', '2025-12-31'); -- List all existing Partition Functions SELECT name, function_id, type, type_desc, boundary_value_on_right FROM sys.partition_functions; Step 2: Create Filegroups -- Create Filegroups in SalesDB ALTER DATABASE SalesDB ADD FILEGROUP FG_2023; ALTER DATABASE SalesDB ADD FILEGROUP FG_2024; ALTER DATABASE SalesDB ADD FILEGROUP FG_2025; ALTER DATABASE SalesDB ADD FILEGROUP FG_2026; -- Optional: Remove a Filegroup ALTER DATABASE SalesDB REMOVE FILEGROUP FG_2023; -- List existing Filegroups SELECT * FROM sys.filegroups WHERE type = 'FG'; Step 3: Create Data Files -- Create Data Files and map them to Filegroups ALTER DATABASE SalesDB ADD FILE ( NAME = P_2023, FILENAME = 'C:\\Program Files\\Microsoft SQL Server\\MSSQL16.SQLEXPRESS\\MSSQL\\DATA\\P_2023.ndf' ) TO FILEGROUP FG_2023; ALTER DATABASE SalesDB ADD FILE ( NAME = P_2024, FILENAME = 'C:\\Program Files\\Microsoft SQL Server\\MSSQL16.SQLEXPRESS\\MSSQL\\DATA\\P_2024.ndf' ) TO FILEGROUP FG_2024; ALTER DATABASE SalesDB ADD FILE ( NAME = P_2025, FILENAME = 'C:\\Program Files\\Microsoft SQL Server\\MSSQL16.SQLEXPRESS\\MSSQL\\DATA\\P_2025.ndf' ) TO FILEGROUP FG_2025; ALTER DATABASE SalesDB ADD FILE ( NAME = P_2026, FILENAME = 'C:\\Program Files\\Microsoft SQL Server\\MSSQL16.SQLEXPRESS\\MSSQL\\DATA\\P_2026.ndf' ) TO FILEGROUP FG_2026; -- List all files in SalesDB SELECT fg.name AS FilegroupName, mf.name AS LogicalFileName, mf.physical_name AS PhysicalFilePath, mf.size / 128 AS SizeInMB FROM sys.filegroups fg JOIN sys.master_files mf ON fg.data_space_id = mf.data_space_id WHERE mf.database_id = DB_ID('SalesDB'); Step 4: Create Partition Scheme -- Partition Scheme using the Partition Function CREATE PARTITION SCHEME SchemePartitionByYear AS PARTITION PartitionByYear TO (FG_2023, FG_2024, FG_2025, FG_2026); -- List Partition Schemes SELECT ps.name AS PartitionSchemeName, pf.name AS PartitionFunctionName, ds.destination_id AS PartitionNumber, fg.name AS FilegroupName FROM sys.partition_schemes ps JOIN sys.partition_functions pf ON ps.function_id = pf.function_id JOIN sys.destination_data_spaces ds ON ps.data_space_id = ds.partition_scheme_id JOIN sys.filegroups fg ON ds.data_space_id = fg.data_space_id; Step 5: Create the Partitioned Table CREATE TABLE Sales.Orders_Partitioned ( OrderID INT, OrderDate DATE, Sales INT ) ON SchemePartitionByYear (OrderDate); Step 6: Insert Data Into the Partitioned Table INSERT INTO Sales.Orders_Partitioned VALUES (1, '2023-05-15', 100); INSERT INTO Sales.Orders_Partitioned VALUES (2, '2024-07-20', 50); INSERT INTO Sales.Orders_Partitioned VALUES (3, '2025-12-31', 20); INSERT INTO Sales.Orders_Partitioned VALUES (4, '2026-01-01', 100); Step 7: Verify Partitioning and Compare Execution Plans -- Verify that data is correctly partitioned SELECT p.partition_number AS PartitionNumber, f.name AS PartitionFilegroup, p.rows AS NumberOfRows FROM sys.partitions p JOIN sys.destination_data_spaces dds ON p.partition_number = dds.destination_id JOIN sys.filegroups f ON dds.data_space_id = f.data_space_id WHERE OBJECT_NAME(p.object_id) = 'Orders_Partitioned'; -- Non-partitioned copy for comparison SELECT * INTO Sales.Orders_NoPartition FROM Sales.Orders_Partitioned; -- Query on Partitioned Table SELECT * FROM Sales.Orders_Partitioned WHERE OrderDate IN ('2026-01-01', '2025-12-31'); -- Query on Non-Partitioned Table SELECT * FROM Sales.Orders_NoPartition WHERE OrderDate IN ('2026-01-01', '2025-12-31');",
    "tags": [],
    "title": "Partitioning",
    "uri": "/sql/partitions/"
  },
  {
    "breadcrumb": "SQL",
    "content": "This guide demonstrates best practices for:\nFetching data Filtering Joins UNION Aggregations Subqueries/CTE DDL Indexing It covers techniques such as selecting only necessary columns, proper filtering methods, explicit joins, avoiding redundant logic, and efficient indexing strategies.\nFetching Data Tip 1: Select Only What You Need -- Bad Practice SELECT * FROM Sales.Customers; -- Good Practice SELECT CustomerID, FirstName, LastName FROM Sales.Customers; Tip 2: Avoid unnecessary DISTINCT \u0026 ORDER BY -- Bad Practice SELECT DISTINCT FirstName FROM Sales.Customers ORDER BY FirstName; -- Good Practice SELECT FirstName FROM Sales.Customers; Tip 3: For Exploration Purpose, Limit Rows! -- Bad Practice SELECT OrderID, Sales FROM Sales.Orders; -- Good Practice SELECT TOP 10 OrderID, Sales FROM Sales.Orders; Filtering Tip 4: Create nonclustered index on frequently used columns SELECT * FROM Sales.Orders WHERE OrderStatus = 'Delivered'; CREATE NONCLUSTERED INDEX Idx_Orders_OrderStatus ON Sales.Orders(OrderStatus); Tip 5: Avoid applying functions in WHERE -- Bad SELECT * FROM Sales.Orders WHERE LOWER(OrderStatus) = 'delivered'; -- Good SELECT * FROM Sales.Orders WHERE OrderStatus = 'Delivered'; -- Bad SELECT * FROM Sales.Customers WHERE SUBSTRING(FirstName, 1, 1) = 'A'; -- Good SELECT * FROM Sales.Customers WHERE FirstName LIKE 'A%'; -- Bad SELECT * FROM Sales.Orders WHERE YEAR(OrderDate) = 2025; -- Good SELECT * FROM Sales.Orders WHERE OrderDate BETWEEN '2025-01-01' AND '2025-12-31'; Tip 6: Avoid leading wildcards -- Bad SELECT * FROM Sales.Customers WHERE LastName LIKE '%Gold%'; -- Good SELECT * FROM Sales.Customers WHERE LastName LIKE 'Gold%'; Tip 7: Use IN instead of multiple OR -- Bad SELECT * FROM Sales.Orders WHERE CustomerID = 1 OR CustomerID = 2 OR CustomerID = 3; -- Good SELECT * FROM Sales.Orders WHERE CustomerID IN (1, 2, 3); Joins Tip 8: Use INNER JOIN when possible -- Best SELECT c.FirstName, o.OrderID FROM Sales.Customers c INNER JOIN Sales.Orders o ON c.CustomerID = o.CustomerID; -- Slower SELECT c.FirstName, o.OrderID FROM Sales.Customers c LEFT JOIN Sales.Orders o ON c.CustomerID = o.CustomerID; -- Worst SELECT c.FirstName, o.OrderID FROM Sales.Customers c OUTER JOIN Sales.Orders o ON c.CustomerID = o.CustomerID; Tip 9: Use explicit (ANSI) joins -- Bad SELECT o.OrderID, c.FirstName FROM Sales.Customers c, Sales.Orders o WHERE c.CustomerID = o.CustomerID; -- Good SELECT o.OrderID, c.FirstName FROM Sales.Customers c INNER JOIN Sales.Orders o ON c.CustomerID = o.CustomerID; Tip 10: Index columns used in ON clause SELECT c.FirstName, o.OrderID FROM Sales.Orders o INNER JOIN Sales.Customers c ON c.CustomerID = o.CustomerID; CREATE NONCLUSTERED INDEX IX_Orders_CustomerID ON Sales.Orders(CustomerID); Tip 11: Filter before joining big tables -- Best for big tables SELECT c.FirstName, o.OrderID FROM Sales.Customers c INNER JOIN ( SELECT OrderID, CustomerID FROM Sales.Orders WHERE OrderStatus = 'Delivered' ) o ON c.CustomerID = o.CustomerID; Tip 12: Aggregate before joining -- Best for big tables SELECT c.CustomerID, c.FirstName, o.OrderCount FROM Sales.Customers c INNER JOIN ( SELECT CustomerID, COUNT(OrderID) AS OrderCount FROM Sales.Orders GROUP BY CustomerID ) o ON c.CustomerID = o.CustomerID; Tip 13: Use UNION instead of OR in joins -- Bad SELECT o.OrderID, c.FirstName FROM Sales.Customers c INNER JOIN Sales.Orders o ON c.CustomerID = o.CustomerID OR c.CustomerID = o.SalesPersonID; -- Good SELECT o.OrderID, c.FirstName FROM Sales.Customers c INNER JOIN Sales.Orders o ON c.CustomerID = o.CustomerID UNION SELECT o.OrderID, c.FirstName FROM Sales.Customers c INNER JOIN Sales.Orders o ON c.CustomerID = o.SalesPersonID; Tip 14: Use SQL hints for optimization SELECT o.OrderID, c.FirstName FROM Sales.Customers c INNER JOIN Sales.Orders o ON c.CustomerID = o.CustomerID OPTION (HASH JOIN); Union Tip 15: Use UNION ALL when duplicates are acceptable -- Bad SELECT CustomerID FROM Sales.Orders UNION SELECT CustomerID FROM Sales.OrdersArchive; -- Good SELECT CustomerID FROM Sales.Orders UNION ALL SELECT CustomerID FROM Sales.OrdersArchive; Tip 16: Use UNION ALL + DISTINCT when duplicates not allowed SELECT DISTINCT CustomerID FROM ( SELECT CustomerID FROM Sales.Orders UNION ALL SELECT CustomerID FROM Sales.OrdersArchive ) CombinedData; Aggregations Tip 17: Use columnstore indexes SELECT CustomerID, COUNT(OrderID) AS OrderCount FROM Sales.Orders GROUP BY CustomerID; CREATE CLUSTERED COLUMNSTORE INDEX Idx_Orders_Columnstore ON Sales.Orders; Tip 18: Pre-aggregate data SELECT MONTH(OrderDate) AS OrderYear, SUM(Sales) AS TotalSales INTO Sales.SalesSummary FROM Sales.Orders GROUP BY MONTH(OrderDate); SELECT OrderYear, TotalSales FROM Sales.SalesSummary; Subqueries \u0026 CTE Tip 19: Prefer JOIN or EXISTS over IN -- Good SELECT o.OrderID, o.Sales FROM Sales.Orders o WHERE EXISTS ( SELECT 1 FROM Sales.Customers c WHERE c.CustomerID = o.CustomerID AND c.Country = 'USA' ); -- Bad SELECT o.OrderID, o.Sales FROM Sales.Orders o WHERE o.CustomerID IN ( SELECT CustomerID FROM Sales.Customers WHERE Country = 'USA' ); Tip 20: Avoid redundant logic -- Good SELECT EmployeeID, FirstName, CASE WHEN Salary \u003e AVG(Salary) OVER () THEN 'Above Average' WHEN Salary \u003c AVG(Salary) OVER () THEN 'Below Average' ELSE 'Average' END AS Status FROM Sales.Employees; DDL Tip 21: Avoid VARCHAR(MAX) unless necessary. Tip 22: Avoid overly large lengths. Tip 23: Use NOT NULL when possible. Tip 24: All tables should have a clustered primary key. Tip 25: Create nonclustered indexes on foreign keys when frequently used. -- Good Practice CREATE TABLE CustomersInfo ( CustomerID INT PRIMARY KEY CLUSTERED, FirstName VARCHAR(50) NOT NULL, LastName VARCHAR(50) NOT NULL, Country VARCHAR(50) NOT NULL, TotalPurchases FLOAT, Score INT, BirthDate DATE, EmployeeID INT, CONSTRAINT FK_CustomersInfo_EmployeeID FOREIGN KEY (EmployeeID) REFERENCES Sales.Employees(EmployeeID) ); CREATE NONCLUSTERED INDEX IX_CustomersInfo_EmployeeID ON CustomersInfo(EmployeeID); Indexing Tip 26: Avoid over-indexing (slows down writes). Tip 27: Drop unused indexes regularly. Tip 28: Update table statistics weekly. Tip 29: Reorganize/rebuild fragmented indexes weekly. Tip 30: For very large tables, partition + columnstore index.",
    "description": "This guide demonstrates best practices for:\nFetching data Filtering Joins UNION Aggregations Subqueries/CTE DDL Indexing It covers techniques such as selecting only necessary columns, proper filtering methods, explicit joins, avoiding redundant logic, and efficient indexing strategies.\nFetching Data Tip 1: Select Only What You Need -- Bad Practice SELECT * FROM Sales.Customers; -- Good Practice SELECT CustomerID, FirstName, LastName FROM Sales.Customers; Tip 2: Avoid unnecessary DISTINCT \u0026 ORDER BY -- Bad Practice SELECT DISTINCT FirstName FROM Sales.Customers ORDER BY FirstName; -- Good Practice SELECT FirstName FROM Sales.Customers; Tip 3: For Exploration Purpose, Limit Rows! -- Bad Practice SELECT OrderID, Sales FROM Sales.Orders; -- Good Practice SELECT TOP 10 OrderID, Sales FROM Sales.Orders; Filtering Tip 4: Create nonclustered index on frequently used columns SELECT * FROM Sales.Orders WHERE OrderStatus = 'Delivered'; CREATE NONCLUSTERED INDEX Idx_Orders_OrderStatus ON Sales.Orders(OrderStatus); Tip 5: Avoid applying functions in WHERE -- Bad SELECT * FROM Sales.Orders WHERE LOWER(OrderStatus) = 'delivered'; -- Good SELECT * FROM Sales.Orders WHERE OrderStatus = 'Delivered'; -- Bad SELECT * FROM Sales.Customers WHERE SUBSTRING(FirstName, 1, 1) = 'A'; -- Good SELECT * FROM Sales.Customers WHERE FirstName LIKE 'A%'; -- Bad SELECT * FROM Sales.Orders WHERE YEAR(OrderDate) = 2025; -- Good SELECT * FROM Sales.Orders WHERE OrderDate BETWEEN '2025-01-01' AND '2025-12-31'; Tip 6: Avoid leading wildcards -- Bad SELECT * FROM Sales.Customers WHERE LastName LIKE '%Gold%'; -- Good SELECT * FROM Sales.Customers WHERE LastName LIKE 'Gold%'; Tip 7: Use IN instead of multiple OR -- Bad SELECT * FROM Sales.Orders WHERE CustomerID = 1 OR CustomerID = 2 OR CustomerID = 3; -- Good SELECT * FROM Sales.Orders WHERE CustomerID IN (1, 2, 3); Joins Tip 8: Use INNER JOIN when possible -- Best SELECT c.FirstName, o.OrderID FROM Sales.Customers c INNER JOIN Sales.Orders o ON c.CustomerID = o.CustomerID; -- Slower SELECT c.FirstName, o.OrderID FROM Sales.Customers c LEFT JOIN Sales.Orders o ON c.CustomerID = o.CustomerID; -- Worst SELECT c.FirstName, o.OrderID FROM Sales.Customers c OUTER JOIN Sales.Orders o ON c.CustomerID = o.CustomerID; Tip 9: Use explicit (ANSI) joins -- Bad SELECT o.OrderID, c.FirstName FROM Sales.Customers c, Sales.Orders o WHERE c.CustomerID = o.CustomerID; -- Good SELECT o.OrderID, c.FirstName FROM Sales.Customers c INNER JOIN Sales.Orders o ON c.CustomerID = o.CustomerID; Tip 10: Index columns used in ON clause SELECT c.FirstName, o.OrderID FROM Sales.Orders o INNER JOIN Sales.Customers c ON c.CustomerID = o.CustomerID; CREATE NONCLUSTERED INDEX IX_Orders_CustomerID ON Sales.Orders(CustomerID); Tip 11: Filter before joining big tables -- Best for big tables SELECT c.FirstName, o.OrderID FROM Sales.Customers c INNER JOIN ( SELECT OrderID, CustomerID FROM Sales.Orders WHERE OrderStatus = 'Delivered' ) o ON c.CustomerID = o.CustomerID; Tip 12: Aggregate before joining -- Best for big tables SELECT c.CustomerID, c.FirstName, o.OrderCount FROM Sales.Customers c INNER JOIN ( SELECT CustomerID, COUNT(OrderID) AS OrderCount FROM Sales.Orders GROUP BY CustomerID ) o ON c.CustomerID = o.CustomerID; Tip 13: Use UNION instead of OR in joins -- Bad SELECT o.OrderID, c.FirstName FROM Sales.Customers c INNER JOIN Sales.Orders o ON c.CustomerID = o.CustomerID OR c.CustomerID = o.SalesPersonID; -- Good SELECT o.OrderID, c.FirstName FROM Sales.Customers c INNER JOIN Sales.Orders o ON c.CustomerID = o.CustomerID UNION SELECT o.OrderID, c.FirstName FROM Sales.Customers c INNER JOIN Sales.Orders o ON c.CustomerID = o.SalesPersonID; Tip 14: Use SQL hints for optimization SELECT o.OrderID, c.FirstName FROM Sales.Customers c INNER JOIN Sales.Orders o ON c.CustomerID = o.CustomerID OPTION (HASH JOIN); Union Tip 15: Use UNION ALL when duplicates are acceptable -- Bad SELECT CustomerID FROM Sales.Orders UNION SELECT CustomerID FROM Sales.OrdersArchive; -- Good SELECT CustomerID FROM Sales.Orders UNION ALL SELECT CustomerID FROM Sales.OrdersArchive; Tip 16: Use UNION ALL + DISTINCT when duplicates not allowed SELECT DISTINCT CustomerID FROM ( SELECT CustomerID FROM Sales.Orders UNION ALL SELECT CustomerID FROM Sales.OrdersArchive ) CombinedData; Aggregations Tip 17: Use columnstore indexes SELECT CustomerID, COUNT(OrderID) AS OrderCount FROM Sales.Orders GROUP BY CustomerID; CREATE CLUSTERED COLUMNSTORE INDEX Idx_Orders_Columnstore ON Sales.Orders; Tip 18: Pre-aggregate data SELECT MONTH(OrderDate) AS OrderYear, SUM(Sales) AS TotalSales INTO Sales.SalesSummary FROM Sales.Orders GROUP BY MONTH(OrderDate); SELECT OrderYear, TotalSales FROM Sales.SalesSummary; Subqueries \u0026 CTE Tip 19: Prefer JOIN or EXISTS over IN -- Good SELECT o.OrderID, o.Sales FROM Sales.Orders o WHERE EXISTS ( SELECT 1 FROM Sales.Customers c WHERE c.CustomerID = o.CustomerID AND c.Country = 'USA' ); -- Bad SELECT o.OrderID, o.Sales FROM Sales.Orders o WHERE o.CustomerID IN ( SELECT CustomerID FROM Sales.Customers WHERE Country = 'USA' ); Tip 20: Avoid redundant logic -- Good SELECT EmployeeID, FirstName, CASE WHEN Salary \u003e AVG(Salary) OVER () THEN 'Above Average' WHEN Salary \u003c AVG(Salary) OVER () THEN 'Below Average' ELSE 'Average' END AS Status FROM Sales.Employees; DDL Tip 21: Avoid VARCHAR(MAX) unless necessary. Tip 22: Avoid overly large lengths. Tip 23: Use NOT NULL when possible. Tip 24: All tables should have a clustered primary key. Tip 25: Create nonclustered indexes on foreign keys when frequently used. -- Good Practice CREATE TABLE CustomersInfo ( CustomerID INT PRIMARY KEY CLUSTERED, FirstName VARCHAR(50) NOT NULL, LastName VARCHAR(50) NOT NULL, Country VARCHAR(50) NOT NULL, TotalPurchases FLOAT, Score INT, BirthDate DATE, EmployeeID INT, CONSTRAINT FK_CustomersInfo_EmployeeID FOREIGN KEY (EmployeeID) REFERENCES Sales.Employees(EmployeeID) ); CREATE NONCLUSTERED INDEX IX_CustomersInfo_EmployeeID ON CustomersInfo(EmployeeID); Indexing Tip 26: Avoid over-indexing (slows down writes). Tip 27: Drop unused indexes regularly. Tip 28: Update table statistics weekly. Tip 29: Reorganize/rebuild fragmented indexes weekly. Tip 30: For very large tables, partition + columnstore index.",
    "tags": [],
    "title": "Performance Tips",
    "uri": "/sql/performance-optimization/"
  },
  {
    "breadcrumb": "SQL",
    "content": "This document contains a series of AI-powered prompts designed to help SQL developers and learners improve skills in writing, optimizing, and understanding SQL queries.\nThe prompts cover tasks, readability, performance, debugging, interview/exam prep, and more.\nEach section provides clear instructions and sample code to support self-learning and real-world application.\nTable of Contents Solve an SQL Task Improve the Readability Optimize the Performance Query Optimize Execution Plan Debugging Explain the Result Styling \u0026 Formatting Documentations \u0026 Comments Improve Database DDL Generate Test Dataset Create SQL Course Understand SQL Concept Comparing SQL Concepts SQL Questions with Options Prepare for a SQL Interview Prepare for a SQL Exam 1. Solve an SQL Task Prompt:\nIn my SQL Server database, we have two tables:\norders(order_id, sales, customer_id, product_id) customers(customer_id, first_name, last_name, country) Tasks:\nWrite a query to rank customers based on their sales. Result must include: customer_id, full name, country, total sales, rank. Write 3 different versions of the query. Include comments (avoid obvious ones). Evaluate which version is best in terms of readability and performance. 2. Improve the Readability The following SQL Server query is long and hard to understand.\nTask:\nImprove its readability. Remove any redundancy and consolidate it. Include comments (avoid obvious ones). Explain each improvement. Original Query:\nWITH CTE_Total_Sales_By_Customer AS ( SELECT c.CustomerID, c.FirstName + ' ' + c.LastName AS FullName, SUM(o.Sales) AS TotalSales FROM Sales.Customers c INNER JOIN Sales.Orders o ON c.CustomerID = o.CustomerID GROUP BY c.CustomerID, c.FirstName, c.LastName ),CTE_Highest_Order_Product AS ( SELECT o.CustomerID, p.Product, ROW_NUMBER() OVER (PARTITION BY o.CustomerID ORDER BY o.Sales DESC) AS rn FROM Sales.Orders o INNER JOIN Sales.Products p ON o.ProductID = p.ProductID ), CTE_Highest_Category AS ( SELECT o.CustomerID, p.Category, ROW_NUMBER() OVER (PARTITION BY o.CustomerID ORDER BY SUM(o.Sales) DESC) AS rn FROM Sales.Orders o INNER JOIN Sales.Products p ON o.ProductID = p.ProductID GROUP BY o.CustomerID, p.Category ), CTE_Last_Order_Date AS ( SELECT CustomerID, MAX(OrderDate) AS LastOrderDate FROM Sales.Orders GROUP BY CustomerID ), CTE_Total_Discounts_By_Customer AS ( SELECT o.CustomerID, SUM(o.Quantity * p.Price * 0.1) AS TotalDiscounts FROM Sales.Orders o INNER JOIN Sales.Products p ON o.ProductID = p.ProductID GROUP BY o.CustomerID ) SELECT ts.CustomerID, ts.FullName, ts.TotalSales,hop.Product AS HighestOrderProduct,hc.Category AS HighestCategory, lod.LastOrderDate, td.TotalDiscounts FROM CTE_Total_Sales_By_Customer ts LEFT JOIN (SELECT CustomerID, Product FROM CTE_Highest_Order_Product WHERE rn = 1) hop ON ts.CustomerID = hop.CustomerID LEFT JOIN (SELECT CustomerID, Category FROM CTE_Highest_Category WHERE rn = 1) hc ON ts.CustomerID = hc.CustomerID LEFT JOIN CTE_Last_Order_Date lod ON ts.CustomerID = lod.CustomerID LEFT JOIN CTE_Total_Discounts_By_Customer td ON ts.CustomerID = td.CustomerID WHERE ts.TotalSales \u003e 0 ORDER BY ts.TotalSales DESC 3. Optimize the Performance Query The following query is slow.\nTask:\nPropose optimizations.\nProvide an improved query.\nExplain each improvement.\nOriginal Query:\nSELECT o.OrderID, o.CustomerID, c.FirstName AS CustomerFirstName, (SELECT COUNT(o2.OrderID) FROM Sales.Orders o2 WHERE o2.CustomerID = c.CustomerID) AS OrderCount FROM Sales.Orders o LEFT JOIN Sales.Customers c ON o.CustomerID = c.CustomerID WHERE LOWER(o.OrderStatus) = 'delivered' OR YEAR(o.OrderDate) = 2025 OR o.CustomerID =1 OR o.CustomerID =2 OR o.CustomerID =3 OR o.CustomerID IN ( SELECT CustomerID FROM Sales.Customers WHERE Country LIKE '%USA%' ) 4. Optimize Execution Plan Task:\nDescribe the execution plan step by step.\nIdentify performance bottlenecks.\nSuggest optimizations.\n5. Debugging The following query causes an error: Msg 8120, Level 16, State 1, Line 5\nTask:\nExplain the error.\nFind the root cause.\nSuggest a fix.\nSELECT C.CustomerID, C.Country, SUM(O.Sales) AS TotalSales, RANK() OVER (PARTITION BY C.Country ORDER BY O.Sales DESC) AS RankInCountry FROM Sales.Customers C LEFT JOIN Sales.Orders O ON C.CustomerID = O.CustomerID GROUP BY C.CustomerID, C.Country 6. Explain the Result Query:\nWITH Series AS ( -- Anchor Query SELECT 1 AS MyNumber UNION ALL -- Recursive Query SELECT MyNumber + 1 FROM Series WHERE MyNumber \u003c 20 ) SELECT * FROM Series Task:\nBreak down how SQL processes this query step by step.\nExplain how the result is formed.\n7. Styling \u0026 Formatting The following query is hard to read.\nTask:\nRestyle for readability.\nAlign column aliases.\nKeep compact.\nOriginal Query:\nwith CTE_Total_Sales as (Select CustomerID, sum(Sales) as TotalSales from Sales.Orders group by CustomerID), cte_customer_segments as (SELECT CustomerID, case when TotalSales \u003e 100 then 'High Value' when TotalSales between 50 and 100 then 'Medium Value' else 'Low Value' end as CustomerSegment from CTE_Total_Sales) select c.CustomerID, c.FirstName, c.LastName, cts.TotalSales, ccs.CustomerSegment FROM sales.customers c left join CTE_Total_Sales cts ON cts.CustomerID = c.CustomerID left JOIN cte_customer_segments ccs ON ccs.CustomerID = c.CustomerID 8. Documentations \u0026 Comments The following query lacks comments.\nTask:\nAdd a leading comment.\nInsert clarifying comments where needed.\nCreate two documents:\nBusiness rules implemented.\nHow the query works.\nWITH CTE_Total_Sales AS ( SELECT CustomerID, SUM(Sales) AS TotalSales FROM Sales.Orders GROUP BY CustomerID ), CTE_Customer_Segements AS ( SELECT CustomerID, CASE WHEN TotalSales \u003e 100 THEN 'High Value' WHEN TotalSales BETWEEN 50 AND 100 THEN 'Medium Value' ELSE 'Low Value' END CustomerSegment FROM CTE_Total_Sales ) SELECT c.CustomerID, c.FirstName, c.LastName, cts.TotalSales, ccs.CustomerSegment FROM Sales.Customers c LEFT JOIN CTE_Total_Sales cts ON cts.CustomerID = c.CustomerID LEFT JOIN CTE_Customer_Segements ccs ON ccs.CustomerID = c.CustomerID 9. Improve Database DDL Task:\nCheck naming consistency.\nOptimize data types.\nVerify PK/FK integrity.\nReview indexes.\nEnsure normalization.\n10. Generate Test Dataset Task:\nGenerate realistic test dataset with INSERT statements.\nKeep it small.\nEnsure valid PK/FK relationships.\nAvoid NULL.\n11. Create SQL Course Task:\nBuild a beginner-to-advanced SQL roadmap.\nInclude analytics-focused topics.\nUse real-world scenarios.\n12. Understand SQL Concept Task:\nExplain SQL Window Functions.\nProvide an analogy.\nDescribe when/why to use them.\nShow syntax \u0026 examples.\nList top 3 use cases.\n13. Comparing SQL Concepts Task:\nCompare Window Functions vs GROUP BY.\nExplain key differences.\nShow examples.\nProvide pros/cons.\nSummarize in a side-by-side table.\n14. SQL Questions with Options Task:\nAct as SQL trainer.\nProvide sample dataset.\nCreate progressive tasks.\nSimulate SQL Server results.\nReview solutions \u0026 suggest improvements.\n15. Prepare for a SQL Interview Task:\nAct as interviewer.\nAsk common SQL questions.\nProgress to advanced topics.\nEvaluate answers \u0026 give feedback.\n16. Prepare for a SQL Exam Task:\nAsk SQL exam-style questions.\nProgress gradually.\nEvaluate answers \u0026 give feedback.",
    "description": "This document contains a series of AI-powered prompts designed to help SQL developers and learners improve skills in writing, optimizing, and understanding SQL queries.\nThe prompts cover tasks, readability, performance, debugging, interview/exam prep, and more.\nEach section provides clear instructions and sample code to support self-learning and real-world application.\nTable of Contents Solve an SQL Task Improve the Readability Optimize the Performance Query Optimize Execution Plan Debugging Explain the Result Styling \u0026 Formatting Documentations \u0026 Comments Improve Database DDL Generate Test Dataset Create SQL Course Understand SQL Concept Comparing SQL Concepts SQL Questions with Options Prepare for a SQL Interview Prepare for a SQL Exam 1. Solve an SQL Task Prompt:\nIn my SQL Server database, we have two tables:",
    "tags": [],
    "title": "AI and SQL",
    "uri": "/sql/ai-and-sql/"
  },
  {
    "breadcrumb": "",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/categories/"
  },
  {
    "breadcrumb": "",
    "content": "Contents SQL SELECT Query DDL DML Filtering Data Joins Set Operations String Functions Number Functions Date \u0026 Time Functions Date \u0026 Time Formats NULL Functions CASE Statement Aggregate Functions Window Functions Window Aggregate Functions Window Ranking Functions Window Value Functions Subquery Functions Common Table Expressions (CTEs) Views Temporary Tables Stored Procedures Triggers Indexes Partitioning Performance Tips AI and SQL ADF ADB Spark DF Basics DF Operations Functions Date Functions Handling Nulls Aggregate functions Joins When|Cast|Union Window Functions Explode Pivot Comparisons Practice Sets SQL LeetCode Pyspark 1 Interview Prep ADF 1 Data Pipeline SQL Theory 100 SQL Queries 100 SQL Queries Azure DataBricks SQL Azure Data Factory",
    "description": "Contents SQL SELECT Query DDL DML Filtering Data Joins Set Operations String Functions Number Functions Date \u0026 Time Functions Date \u0026 Time Formats NULL Functions CASE Statement Aggregate Functions Window Functions Window Aggregate Functions Window Ranking Functions Window Value Functions Subquery Functions Common Table Expressions (CTEs) Views Temporary Tables Stored Procedures Triggers Indexes Partitioning Performance Tips AI and SQL ADF ADB Spark DF Basics DF Operations Functions Date Functions Handling Nulls Aggregate functions Joins When|Cast|Union Window Functions Explode Pivot Comparisons Practice Sets SQL LeetCode Pyspark 1 Interview Prep ADF 1 Data Pipeline SQL Theory 100 SQL Queries 100 SQL Queries Azure DataBricks SQL Azure Data Factory",
    "tags": [],
    "title": "Data Engineering Notes",
    "uri": "/"
  },
  {
    "breadcrumb": "",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/tags/"
  }
]
